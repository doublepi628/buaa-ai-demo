{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 北航软件学院人工智能作业一演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入本次演示必要的库\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "def plot_data_internal(X, y):\n",
    "    x_min, x_max = X[ :, 0 ].min() - .5, X[ :, 0 ].max() + .5\n",
    "    y_min, y_max = X[ :, 1 ].min() - .5, X[ :, 1 ].max() + .5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    plt.figure()\n",
    "    plt.xlim(xx.min(None), xx.max(None))\n",
    "    plt.ylim(yy.min(None), yy.max(None))\n",
    "    ax = plt.gca()\n",
    "    ax.plot(X[y == 0, 0], X[y == 0, 1], 'ro', label = 'Class 1')\n",
    "    ax.plot(X[y == 1, 0], X[y == 1, 1], 'bo', label = 'Class 2')\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.title('Plot data')\n",
    "    plt.legend(loc = 'upper left', scatterpoints = 1, numpoints = 1)\n",
    "    return xx, yy\n",
    "\n",
    "# 预览数据\n",
    "def plot_data(X, y):\n",
    "    plot_data_internal(X, y)\n",
    "    plt.show()\n",
    "\n",
    "# 预览损失\n",
    "def plot_loss(ll, title=None):\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    plt.xlim(0, len(ll) + 2)\n",
    "    plt.ylim(min(ll) - 0.1, max(ll) + 0.1)\n",
    "    ax.plot(np.arange(1, len(ll) + 1), ll, 'r-')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average Cross Entropy Loss')\n",
    "    plt.title('Plot Cross Entropy Loss Curve' if title is None else title)\n",
    "    plt.show()\n",
    "\n",
    "# 混淆矩阵\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    return np.array([[tn, fp],\n",
    "                     [fn, tp]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76069, 24128, 24490)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "txt_path = './data/txt_data.txt' # txt格式: 存储文本，可以直接预览\n",
    "npy_path = './data/npy_data.npy' # npy格式: 用于存储单个numpy数组的二进制格式\n",
    "npz_path = './data/npz_data.npz' # npz格式: 用于存储多个numpy数组的压缩格式\n",
    "\n",
    "read_txt = np.loadtxt(txt_path)\n",
    "read_np = np.load(npy_path)\n",
    "read_npz = np.load(npz_path)\n",
    "\n",
    "os.path.getsize(txt_path), os.path.getsize(npy_path), os.path.getsize(npz_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 预测任务：$f(x_1, x_2) → [p_0, p_1]$，其中$x_i$为点坐标，$p_j$为属于第j类的概率\n",
    "- 线性回归：$f(x_1, x_2) = \\sigma([w_1, w_2] \\cdot [x_1, x_2] ^ T +b)$，等价于$\\sigma([w_1, w_2, w_3] \\cdot [x_1, x_2, 1] ^ T)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.62294615, -0.57879556,  0.        ],\n",
       "       [ 1.09961316,  1.55006583,  0.        ],\n",
       "       [ 0.75478522, -1.97615881,  1.        ],\n",
       "       [-1.13634801, -0.0386258 ,  1.        ],\n",
       "       [-0.12266627, -1.85082517,  1.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取、检查数据\n",
    "\n",
    "data = read_txt\n",
    "\n",
    "# data = read_np # 这段代码的作用和上一段一样，替换下试试\n",
    "\n",
    "# data = np.concatenate([read_npz['x'], np.expand_dims(read_npz['y'], axis=1)], axis=1) # 这段代码的作用和上一段一样，替换下试试\n",
    "\n",
    "data[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.60493168, -1.40361878,  0.        ],\n",
       "       [ 0.25356742, -0.32514766,  0.        ],\n",
       "       [ 0.09827929,  0.14445879,  0.        ],\n",
       "       [ 0.32126251,  0.01279   ,  0.        ],\n",
       "       [-0.50094323,  0.09299223,  0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成随机排列\n",
    "permutation = np.random.permutation(data.shape[0])\n",
    "data = data[permutation]\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data(data[ :, :2], data[ :, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 2), (200, 2), (800,), (200,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 划分训练集、测试集\n",
    "n_train = 800\n",
    "X = data[ :, :2] # 1000 x 2\n",
    "y = data[ :, 2 ] # 1000\n",
    "X_train = X[ 0 : n_train, : ] # 800 x 2\n",
    "X_test = X[ n_train :, : ] # 200 x 2\n",
    "y_train = y[ 0 : n_train ] # 800\n",
    "y_test = y[ n_train : ] # 200\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手工实现logistic回归\n",
    "\n",
    "- logistic 函数：$f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- 预测函数（简单的线性变换）：$X\\in\\mathbb{R}^{N_{train} \\times 2}, w \\in\\mathbb{R}^{2}, Xw \\in\\mathbb{R}^{N_{train}}$\n",
    "- 对数似然：$\\text{LogLikelihood} = y \\text{log}(\\hat{y}) + (1-y) \\text{log}(1-\\hat{y})$\n",
    "    - 当$y = 0$，有$\\text{LogLikelihood} = \\text{log}(1 - \\hat{y})$\n",
    "    - 当$y = 1$，有$\\text{LogLikelihood} = \\text{log}(\\hat{y})$\n",
    "- 交叉熵损失：$\\text{Loss} = - [y \\text{log}(\\hat{y}) + (1-y) \\text{log}(1-\\hat{y})]$\n",
    "    - 当$y = 0$，有$\\text{Loss} = - \\text{log}(1 - \\hat{y})$\n",
    "    - 当$y = 1$，有$\\text{Loss} = - \\text{log}(\\hat{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression 的手工实现\n",
    "\n",
    "# logistic 函数\n",
    "def logistic(x): return 1.0 / (1.0 + np.exp(-x)) # N -> N\n",
    "\n",
    "# 预测函数\n",
    "def predict(X_tilde, w): return logistic(np.dot(X_tilde, w)) # N x (D + 1) -> N\n",
    "\n",
    "# 计算平均对数似然\n",
    "def compute_average_ll(X_tilde, y, w):\n",
    "    output_prob = predict(X_tilde, w)\n",
    "    return np.mean(y * np.log(output_prob) + (1 - y) * np.log(1.0 - output_prob)) \n",
    "\n",
    "# 添加偏置项\n",
    "def get_x_tilde(X): return np.concatenate((np.ones((X.shape[ 0 ], 1 )), X), 1)\n",
    "\n",
    "# 梯度下降函数\n",
    "# TODO: 试试把一层的logistic regression改成多层的logistic regression，观察效果是否会提升\n",
    "def fit_w(X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha):\n",
    "    w = np.random.randn(X_tilde_train.shape[ 1 ]) \n",
    "    ll_train = np.zeros(n_steps)\n",
    "    ll_test = np.zeros(n_steps)\n",
    "    for i in range(n_steps):\n",
    "        sigmoid_value = predict(X_tilde_train, w)\n",
    "\n",
    "        w = w + alpha * np.dot(X_tilde_train.T, (y_train - sigmoid_value)) # TODO: 试试加入L2正则化，观察效果是否会提升\n",
    "\n",
    "        ll_train[ i ] = compute_average_ll(X_tilde_train, y_train, w)\n",
    "        ll_test[ i ] = compute_average_ll(X_tilde_test, y_test, w)\n",
    "        print(f'epoch {i+1}: train loss: {-ll_train[ i ]}, test loss: {-ll_test[ i ]}')\n",
    "\n",
    "    return w, ll_train, ll_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss: 0.7267062830459861, test loss: 0.7352054837655365\n",
      "epoch 2: train loss: 0.7052801528729217, test loss: 0.7133225250984313\n",
      "epoch 3: train loss: 0.6879839239003749, test loss: 0.6957532493303478\n",
      "epoch 4: train loss: 0.6741416524833452, test loss: 0.6817855763866958\n",
      "epoch 5: train loss: 0.6631348641043364, test loss: 0.6707692564612456\n",
      "epoch 6: train loss: 0.6544208124713283, test loss: 0.6621338457848593\n",
      "epoch 7: train loss: 0.647539261376929, test loss: 0.6553954188190927\n",
      "epoch 8: train loss: 0.6421102976151439, test loss: 0.650154329225443\n",
      "epoch 9: train loss: 0.6378265774035491, test loss: 0.6460872788870821\n",
      "epoch 10: train loss: 0.6344430323267098, test loss: 0.642936689097664\n",
      "epoch 11: train loss: 0.6317661573078756, test loss: 0.6404995401279046\n",
      "epoch 12: train loss: 0.6296441011030033, test loss: 0.6386169668497108\n",
      "epoch 13: train loss: 0.6279581058121851, test loss: 0.6371652145289382\n",
      "epoch 14: train loss: 0.6266154225134024, test loss: 0.6360481184945966\n",
      "epoch 15: train loss: 0.6255436086120668, test loss: 0.6351910301524052\n",
      "epoch 16: train loss: 0.6246860187951577, test loss: 0.6345360042083665\n",
      "epoch 17: train loss: 0.6239982798182452, test loss: 0.634038032311994\n",
      "epoch 18: train loss: 0.6234455530452074, test loss: 0.6336621184015874\n",
      "epoch 19: train loss: 0.6230004163546442, test loss: 0.6333810178323048\n",
      "epoch 20: train loss: 0.6226412275501589, test loss: 0.6331734934512319\n",
      "epoch 21: train loss: 0.6223508596081195, test loss: 0.6330229711765822\n",
      "epoch 22: train loss: 0.6221157220681477, test loss: 0.632916502972613\n",
      "epoch 23: train loss: 0.6219250023325913, test loss: 0.6328439658639347\n",
      "epoch 24: train loss: 0.6217700760102002, test loss: 0.632797442121014\n",
      "epoch 25: train loss: 0.6216440473721486, test loss: 0.6327707386033046\n",
      "epoch 26: train loss: 0.6215413901579343, test loss: 0.6327590131508226\n",
      "epoch 27: train loss: 0.6214576659719576, test loss: 0.632758483492013\n",
      "epoch 28: train loss: 0.6213893028437856, test loss: 0.6327661999093782\n",
      "epoch 29: train loss: 0.621333420580548, test loss: 0.6327798672960084\n",
      "epoch 30: train loss: 0.6212876926253584, test loss: 0.6327977055759177\n",
      "epoch 31: train loss: 0.6212502364863168, test loss: 0.6328183400031667\n",
      "epoch 32: train loss: 0.6212195265951888, test loss: 0.6328407147928629\n",
      "epoch 33: train loss: 0.6211943248283854, test loss: 0.632864025018071\n",
      "epoch 34: train loss: 0.6211736249771969, test loss: 0.6328876628411847\n",
      "epoch 35: train loss: 0.6211566082660549, test loss: 0.6329111750199182\n",
      "epoch 36: train loss: 0.6211426076447545, test loss: 0.6329342292997061\n",
      "epoch 37: train loss: 0.6211310790666174, test loss: 0.6329565878233815\n",
      "epoch 38: train loss: 0.6211215783425601, test loss: 0.6329780860914656\n",
      "epoch 39: train loss: 0.6211137424558996, test loss: 0.6329986163193381\n",
      "epoch 40: train loss: 0.6211072744535229, test loss: 0.6330181142816454\n",
      "epoch 41: train loss: 0.6211019312102307, test loss: 0.6330365489252121\n",
      "epoch 42: train loss: 0.6210975135057555, test loss: 0.6330539141814586\n",
      "epoch 43: train loss: 0.6210938579666262, test loss: 0.6330702225270717\n",
      "epoch 44: train loss: 0.6210908305142994, test loss: 0.633085499934497\n",
      "epoch 45: train loss: 0.621088321031825, test loss: 0.6330997819271634\n",
      "epoch 46: train loss: 0.6210862390177236, test loss: 0.6331131105124207\n",
      "epoch 47: train loss: 0.6210845100407569, test loss: 0.6331255318112449\n",
      "epoch 48: train loss: 0.6210830728452646, test loss: 0.6331370942403902\n",
      "epoch 49: train loss: 0.6210818769855836, test loss: 0.6331478471318146\n",
      "epoch 50: train loss: 0.621080880891233, test loss: 0.6331578396974537\n",
      "Confusion Matrix:\n",
      "[[77 26]\n",
      " [32 65]]\n",
      "Accuracy: 0.71\n",
      "Classification Report (scikit-learn):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.75      0.73       103\n",
      "         1.0       0.71      0.67      0.69        97\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.71      0.71      0.71       200\n",
      "weighted avg       0.71      0.71      0.71       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 预览决策边界\n",
    "def plot_predictive_distribution(X, y, w, map_inputs = lambda x : x):\n",
    "    xx, yy = plot_data_internal(X, y)\n",
    "    ax = plt.gca()\n",
    "    X_tilde = get_x_tilde(map_inputs(np.concatenate((xx.ravel().reshape((-1, 1)), yy.ravel().reshape((-1, 1))), 1)))\n",
    "    Z = predict(X_tilde, w)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs2 = ax.contour(xx, yy, Z, cmap = 'RdBu', linewidths = 2)\n",
    "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize = 14)\n",
    "    plt.show()\n",
    "\n",
    "alpha = 0.0005 # 学习率，试着自己调整一下\n",
    "n_steps = 50 # epoch迭代次数，试着自己调整一下\n",
    "\n",
    "X_tilde_train = get_x_tilde(X_train)\n",
    "X_tilde_test = get_x_tilde(X_test)\n",
    "w, ll_train, ll_test = fit_w(X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha)\n",
    "\n",
    "X_tilde_test = get_x_tilde(X_test)\n",
    "y_pred_prob = predict(X_tilde_test, w)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy:\", np.mean(y_test == y_pred))\n",
    "# 阅读文档，理解报告里每个指标的含义\n",
    "# https://blog.csdn.net/weixin_62528784/article/details/145379239\n",
    "print(\"Classification Report (scikit-learn):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "plot_loss(-ll_train, title='Train Loss')\n",
    "plot_loss(-ll_test, title='Test Loss')\n",
    "plot_predictive_distribution(X, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://s2.loli.net/2025/03/03/QOCGngtfVIM9D6k.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  1.52842665, -1.9777212 ],\n",
       "        [ 1.        ,  0.4123317 , -0.90846135],\n",
       "        [ 1.        ,  0.17472024,  0.97339573],\n",
       "        [ 1.        ,  0.32151701,  0.03979067],\n",
       "        [ 1.        ,  0.5095014 ,  2.95804768]]),\n",
       " array([[ 1.        ,  2.15771815, -1.48806717],\n",
       "        [ 1.        ,  1.04025967,  0.78192786],\n",
       "        [ 1.        ,  1.71014529, -1.47024784],\n",
       "        [ 1.        ,  2.1669051 , -2.36688354],\n",
       "        [ 1.        ,  1.1313808 , -2.32418073]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 核函数：用于扩充数据维度，常常用于SVM\n",
    "# polar: 极坐标核，用来把直角坐标点转换为极坐标点\n",
    "def polar(coords):\n",
    "    x = coords[ :, 0 ]\n",
    "    y = coords[ :, 1 ]\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    theta = np.arctan2(y, x)\n",
    "    return np.column_stack((r, theta))\n",
    "\n",
    "# 极坐标绘图函数\n",
    "def polar_draw(a, b, c):\n",
    "    theta = np.linspace(0, 10 * np.pi, 1000)\n",
    "    # 计算 r 值\n",
    "    r = - (a / b) * theta - (c / b)\n",
    "    # 过滤掉负半径的部分（因为 r 必须是非负数）\n",
    "    valid_idx = r >= 0\n",
    "    r = r[valid_idx]\n",
    "    theta = theta[valid_idx]\n",
    "    # 转换为笛卡尔坐标\n",
    "    x = r * np.cos(theta)\n",
    "    y = r * np.sin(theta)\n",
    "    # 画图\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.legend()\n",
    "    plt.axis(\"equal\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "X_tilde_train = get_x_tilde(polar(X_train))\n",
    "X_tilde_test = get_x_tilde(polar(X_test))\n",
    "X_tilde_train[:5], X_tilde_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss: 2.4965097592506695, test loss: 2.689507358622945\n",
      "epoch 2: train loss: 2.0921758759873077, test loss: 2.2520907912832544\n",
      "epoch 3: train loss: 1.7061625377770315, test loss: 1.8326377841209023\n",
      "epoch 4: train loss: 1.3538626398193856, test loss: 1.4473245961085002\n",
      "epoch 5: train loss: 1.0615153878913595, test loss: 1.1240266066393254\n",
      "epoch 6: train loss: 0.8560992142678763, test loss: 0.8927532274402235\n",
      "epoch 7: train loss: 0.739008028034482, test loss: 0.7576593046265501\n",
      "epoch 8: train loss: 0.6821361480948983, test loss: 0.6901807778571702\n",
      "epoch 9: train loss: 0.6558845736702945, test loss: 0.6581617149418045\n",
      "epoch 10: train loss: 0.6433951145309684, test loss: 0.6425881244666711\n",
      "epoch 11: train loss: 0.6370532326397388, test loss: 0.6345990198375632\n",
      "epoch 12: train loss: 0.6335795591729907, test loss: 0.6302558193979949\n",
      "epoch 13: train loss: 0.6315278438603078, test loss: 0.6277616558546089\n",
      "epoch 14: train loss: 0.630229429544255, test loss: 0.6262561677518996\n",
      "epoch 15: train loss: 0.6293572190189338, test loss: 0.6253047398517471\n",
      "epoch 16: train loss: 0.6287410161848628, test loss: 0.6246758916454569\n",
      "epoch 17: train loss: 0.6282864498181846, test loss: 0.6242401570187651\n",
      "epoch 18: train loss: 0.6279379338900148, test loss: 0.6239220272241219\n",
      "epoch 19: train loss: 0.62766097886581, test loss: 0.6236759976663148\n",
      "epoch 20: train loss: 0.627433298707055, test loss: 0.6234740192823204\n",
      "epoch 21: train loss: 0.627240069821751, test loss: 0.6232985856453065\n",
      "epoch 22: train loss: 0.6270712414446183, test loss: 0.6231387393857887\n",
      "epoch 23: train loss: 0.626919917054657, test loss: 0.6229876679218722\n",
      "epoch 24: train loss: 0.626781330731777, test loss: 0.6228412080073416\n",
      "epoch 25: train loss: 0.6266521751767758, test loss: 0.6226968936019652\n",
      "epoch 26: train loss: 0.6265301492771751, test loss: 0.6225533406591053\n",
      "epoch 27: train loss: 0.6264136486714801, test loss: 0.6224098466316581\n",
      "epoch 28: train loss: 0.6263015522291392, test loss: 0.6222661293794018\n",
      "epoch 29: train loss: 0.6261930740612881, test loss: 0.6221221575891851\n",
      "epoch 30: train loss: 0.6260876607736989, test loss: 0.6219780415835736\n",
      "Confusion Matrix:\n",
      "[[85 18]\n",
      " [36 61]]\n",
      "Accuracy: 0.73\n",
      "Classification Report (scikit-learn):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.83      0.76       103\n",
      "         1.0       0.77      0.63      0.69        97\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.74      0.73      0.73       200\n",
      "weighted avg       0.74      0.73      0.73       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAHFCAYAAAD/kYOsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOydd3wU1d6Hn9nd9EoKELKhg3TFhg2BK/aCBkRRsXdFEMVyQQHLxdcKesXCVbgWlBbEq14LCJoLFlRAQFBKKAmhhPS+5bx/TLa32c2mwXk+nyFk5syZMyVzvnPOryhCCIFEIpFIJBLJMYSupRsgkUgkEolEEm6kwJFIJBKJRHLMIQWORCKRSCSSYw4pcCQSiUQikRxzSIEjkUgkEonkmEMKHIlEIpFIJMccUuBIJBKJRCI55pACRyKRSCQSyTGHFDgSiUQikUiOOaTAkUgkIbNgwQIURbEvBoMBo9HILbfcQkFBgb3cmjVrUBSFNWvWBH2MdevWMWPGDEpLSxvV1ptvvpmuXbuGtO/cuXNZsGBBo44vkUiaFylwJBJJo5k/fz4//PAD33zzDXfccQcfffQRQ4cOpaqqqtF1r1u3jpkzZzZa4DQGKXAkkraHoaUbIJFI2j4DBgzg1FNPBWDEiBFYLBaefvppPvnkE66//voWbp1EIjkekSM4Eokk7JxxxhkA7N2712+5Tz/9lDPPPJPY2FgSEhI4//zz+eGHH+zbZ8yYwZQpUwDo1q2bfSos0FTXggULOOGEE4iKiqJv37689957XsvNnDmTIUOGkJKSQmJiIieffDLvvPMOzjmIu3btytatW/nuu+/sx7dNddXW1vLQQw9x0kknkZSUREpKCmeeeSYrVqwIdIkkEkkTI0dwJBJJ2Nm5cycA6enpPsssXLiQ66+/ngsuuICPPvqIuro6nn/+eYYPH86qVas455xzuP322ykuLua1114jJyeHjIwMAPr16+ez3gULFnDLLbcwatQoXnrpJcrKypgxYwZ1dXXodK7fdHv27OGuu+6ic+fOAPz4449MmDCBgoICnnzySQCWL1/OmDFjSEpKYu7cuQBERUUBUFdXR3FxMQ8//DCZmZnU19ezcuVKsrOzmT9/PjfeeGOIV1AikTQaIZFIJCEyf/58AYgff/xRmEwmUVFRIT777DORnp4uEhISxMGDB4UQQqxevVoAYvXq1UIIISwWi+jUqZMYOHCgsFgs9voqKipE+/btxVlnnWVf98ILLwhA5OXlBWyPrd6TTz5ZWK1W+/o9e/aIiIgI0aVLF7/7mkwm8dRTT4nU1FSX/fv37y+GDRsW8Phms1mYTCZx2223icGDBwcsL5FImg45RSWRSBrNGWecQUREBAkJCVx22WV07NiR//73v3To0MFr+T///JMDBw4wfvx4l1GV+Ph4Ro8ezY8//kh1dXXQ7bDVe91116Eoin19ly5dOOusszzKf/vtt4wcOZKkpCT0ej0RERE8+eSTHD16lMOHD2s65pIlSzj77LOJj4/HYDAQERHBO++8w7Zt24Juv0QiCR9S4Egkkkbz3nvvsX79ejZs2MCBAwf4/fffOfvss32WP3r0KIB9ysmZTp06YbVaKSkpCbodtno7duzosc193c8//8wFF1wAwLx581i7di3r169n6tSpANTU1AQ8Xk5ODmPHjiUzM5MPPviAH374gfXr13PrrbdSW1sbdPslEkn4kDY4Eomk0fTt29fuRaWF1NRUAAoLCz22HThwAJ1OR7t27YJuh63egwcPemxzX/fxxx8TERHBZ599RnR0tH39J598ovl4H3zwAd26dWPRokUuI0Z1dXVBtlwikYQbOYIjkUianRNOOIHMzEwWLlzo4rFUVVXFsmXL7J5V4DDo1TKicsIJJ5CRkcFHH33kUu/evXtZt26dS1lbYEK9Xm9fV1NTw/vvv+9Rb1RUlNfjK4pCZGSki7g5ePCg9KKSSFoBUuBIJJJmR6fT8fzzz7Nx40Yuu+wyPv30U5YsWcKIESMoLS3lueees5cdOHAgAHPmzOGHH37gl19+oaKiwme9Tz/9NL/++itXXXUVn3/+OR9++CEjR470mKK69NJLqays5LrrruObb77h448/ZujQoXZB5czAgQPZtGkTixYtYv369WzevBmAyy67jD///JN7772Xb7/9ln//+9+cc845XqfeJBJJM9PSVs4SiaTtYvOiWr9+vd9y7l5UNj755BMxZMgQER0dLeLi4sR5550n1q5d67H/448/Ljp16iR0Op3Xetz517/+JXr16iUiIyNF7969xbvvvituuukmDy+qd999V5xwwgkiKipKdO/eXcyaNUu88847Hl5be/bsERdccIFISEgQgEs9zz33nOjatauIiooSffv2FfPmzRPTp08X8vUqkbQsihBO47gSiUQikUgkxwByikoikUgkEskxhxQ4EolEIpFIjjmkwJFIJBKJRHLMIQWORCKRSCSSYw4pcCQSiUQikRxzSIEjkUgkEonkmOO4StVgtVo5cOAACQkJLpFHJRKJRCKRtF6EEFRUVNCpUyeXBL3+OK4EzoEDB8jKymrpZkgkEolEIgmB/fv3YzQaNZU9rgROQkICoF6gxMTEFm6N5Fij3mzh/KdXUVVnJjE2gq+nnodBL2eBJW2XgwcPMn/+fD744AMKCgro2bMnmZmZfPfdd1x//fW8/vrrjT6GEIKiqnr2llSzv6SGA+U15JfVUWuyaK4jQq8jNTaSdrEGkmMiOa9XGqlxnik32irF1fWs+usIJTX1lFSbKampp85sDaqOuEg9nRKjyUiM5oT28fTr6L0PXLp0KbfffjtTp07l4Ycfts92WK1WdDodTz75JG+88QYLFy7k/PPP93k8k8nE1KlTWbZsGUePHiUqKorTTjuNiy++mIsuuogePXoE1f7y8nKysrLs/bgWjiuBY7tRiYmJUuBIws7a7YepJRJ9VCTnDTaS0i65pZskkYTE/v37ufPOO/nqq69ITExk2LBhPP7445x55pns3buX3Nxc0tLSiI2NxWAIrhupNVvIO1rNrqIq8oqr2VtSTVW9s5jRQUQMkRGu++kUaB8fRcfEaDomRNE+Por0+EjS46JIjDYc02YHiYlwW8c0l3VV9WaKquo5UlnHkcp6jlSpPw9W1FFZZ/aowwTsrYa91XVExydwho8+8D//+Q8AV155JUlJSfb1FosFvV5PXl4eJpOJjh07kpiYaBc+3njmmWe45ZZb2L59O7/99hs//fQTs2bNYurUqXTo0IFLLrmEadOm0a1bN83XIpj7fFwJHImkKVm99ZD9/+cN6OinpETSuklMTCQjI4Pp06dzzjnnMHDgQNq3bw/A1q1bsVqtxMXFYTAY7B2fL2pNFrYfrmRnUSU7i6rYX1qDNUCCoOSYCIxJ0RiTYzAmxdApKZr0+EgMGm0vjgfiIg3ERRro0i7WY1tlnZlDFXUcrKilsLyOA2U1FJTVUt4gfDKTor3WWVNTw+bNm+nSpQu9e/d22Wa7x9u3b8dgMNCvXz8Av/YwaWlppKWlMWTIEK655hoOHz7Mnj17mDdvHh9++CHffvstkydPDun8tSAFjkQSJnK3HwbAoFM464T0Fm6NRBI6SUlJvPjiiyQnJ9s7MNuXum30u7CwEMCvuAE4VFHHWz/s8bk9PlJP15RYuqXE0TUlhqx2sSREya6pMcRHGYiPMtAjLc5lfXmtiQNltXRM9C5wDh06RHl5OWeeeSZRUY4pPtu937p1Kzt27GDAgAG0a9cOIYTfERVbqktFUYiOjqZz584cOXKEzZs3ExERwQsvvED//v0D1hMq8imSSMLA/qIq9hVVAXBStxTioyMC7CGRtG5SUlIARyfl/KWu1+vp2rWrpo4pq10MsRF6qhtsajISo+mZFkfPtDi6pcSSFhd5TE8vtSYSoyNI9PNuqqqqwmw20759e0wmE5GRkYBD4KxatQqASy65BFCnrfxNUbrfV7PZzAMPPMDvv//OzJkz7fU01f2XAscLFosFk8nU0s04bomMjNTsBthaWPvnEfv/z5GjN5JWyLZt2ygtLaVfv3522wotAsV9e0FBgf0dqShKwCkqnaIw5sROxEbq6ZEaR7wcnWm1pKenU1ZWxoEDB4iOVkd5rFar/f5++OGHgGqfA/6np2zYnrHKykrmzZvHDz/8wJlnnskTTzzRNCfhhHzSnBBCcPDgQUpLS1u6Kcc1Op2Obt262b8e2gI//OUQOGef0L4FWyKROKivr+e9997jxRdfJC8vj+joaLp27cpNN93E5MmTg/pytn3FZ2ZmAqpXCwSeogI4s2tKaCcgaVYiIyM56aST2LhxI3V1dcTGxtpFzJIlS1i/fj2XXXYZp512GhCcwPnkk094/PHHGTRoEE899RRAQHHcWKTAccImbtq3b09sbKwcNm0BbMEYCwsL6dy5c5u4B0II1u86CkBiTAR9jUkB9pBImofFixczYcIEzjjjDG688UZMJhPz58/n4Ycfpr6+nnvuuYekpCRNIzm2zsz2AWiLKdZU9hOS5ic5OZn777+f2267jbFjxzJp0iSMRiO//PILd955J507d+aRRx5Br9drvu86nY6dO3fyf//3f9TX1/Pkk09y3nnnAdrEcWOQAqcBi8ViFzepqakt3ZzjmvT0dA4cOIDZbCYiovXbsuw8WEFJVT0Ap3RPQa+TL3tJy3P48GEmT57MCSecwBtvvEGfPn0AdXrhkUce4cknn6Rdu3bcddddLtMQvrB1aLaRG9tPKXCOLW666SZ2797Nyy+/zNq1a9HpdJSUlNC7d2/+7//+j3POOQfQft+rqqp4/PHH2bp1K3fddRfZ2dlNfQp22oyhw6xZszjttNNISEigffv2XHnllfz5559hq99mcxMb6+lyJ2lebFNTFov2QF8tiW30BuC0HlIcS1oHubm5FBUVcd1119GnTx8sFgtCCE488URmzZqF2WzmlVdeQQih6UvaZmxsiyLrzfhY0vbR6XQ8/fTT/P7778yYMYPrrruOd955h1WrVjFq1Ch7uf/9739MmzaN3Nxcv+/qDz74gGXLlnHuuefy9NNPN8cp2GkzIzjfffcd9913H6eddhpms5mpU6dywQUX8McffxAXFxe4Ao3IL5GWp63dgw15xfb/n9YjzU9JiaT5sHU6tg8Gm4gRQnDyySczatQoVqxYQU5ODqNHjw5sLNwgZPbt2wdAp06d7PW1tb9ZSWB69OjBxIkTfW7/z3/+w0svvcT+/fs5/fTTXZ4d27O0ePFinn32WTIzM3n88cdJS0trcrsbZ9qMwPnyyy9dfp8/fz7t27fn119/5dxzz22hVkkksHFPCQDREXpOyJQRsiWtA5s3Yl1dnUunYpuOuvnmm1mxYgXz5s1j9OjRAUWKzcjY5hZ86JAa2FIKnOOT8ePHY7VaGTJkiEvMHFDFdHV1NU8//TT5+fl88MEH/O1vf7NvC8S6PcVEG3QMzEgkohHpbtqMwHGnrKwMcMRq8EZdXR11dXX2321zxscjiqKwfPlyu3ufJDwUVdSSX1wNQP+spEb9MUok4eSEE04gNjaWn376yUWA2DqYiy66iM6dO/Pdd99RVFREWlqaX7FiW2+bxo+JiQHkFNXxyqBBg3jppZe8bjt8+DD3338/W7duZfTo0Vx33XWa67UKwYrNhZTXmYmJ0HOKMYkzuqSQFhEg/LUX2uSTKYRg8uTJnHPOOQwYMMBnuVmzZpGUlGRfmiWTuMUCa9bARx+pP5vBjuTgwYNMmDCB7t27ExUVRVZWFpdffrk9KFNLk5OTw4UXXkhaWhqKorBx48aWblLY+H1vqf3/J3Zp13INkUjc6NmzJ926dWPVqlUUFBS4bLNYLERFRXHuuedSV1fHunXrAIddDUB1dbXLPjaBU1JSwq233sq4ceOa+AwkbZW4uDiGDRvG3XffzfTp0wF1BFALfx2ptKeUqDFZ+F9eMS+u2cmzK/8Kuh1tUuDcf//9/P7773z00Ud+yz3++OOUlZXZl/379zdtw3JyoGtXGDECrrtO/dm1q7q+idizZw+nnHIK3377Lc8//zybN2/myy+/ZMSIEdx3331NdtxgqKqq4uyzz+a5555r6aaEnS37S+3/H9RZChxJ6yEiIoJx48ZRUVHBihUrXLbZxMqwYcMAx4i4TqcjLy+Pe+65hyeeeIKSkhL7Pjbx8+CDD/Kvf/2reT4YJW2SuLg47rvvPubOnWsfhNA60tcrLZ4J53Tj9M7tiHQaEQ9lErTNCZwJEybw6aefsnr1ars1vy+ioqLsmcObPIN4Tg6MGQP5+a7rCwrU9U0kcu69914UReHnn39mzJgx9O7dm/79+zN58mR+/PFHn/s9+uij9O7dm9jYWLp3784TTzzhEr1506ZNjBgxgoSEBBITEznllFP45ZdfANi7dy+XX3457dq1Iy4ujv79+/PFF1/4PNb48eN58sknGTlyZPhOvJWwLb/M/v9+Mv6NpJVx3XXXkZKSwvz588nLywPUL2mbwCkuVg3kbQbDADt37uStt95i/vz57Nmzx75e2tlImgO9TqFfx0RuOb0z/3d5P24+LYu+7eM5PYQPyDZjgyOEYMKECSxfvpw1a9YElV69ybFYYOJEEF7mCIUARYFJk2DUKAij9XhxcTFffvklzz77rFdPsuTkZJ/7JiQksGDBAjp16sTmzZu54447SEhI4JFHHgHg+uuvZ/Dgwbzxxhvo9Xo2btxoj0lz3333UV9fz/fff09cXBx//PEH8fHxYTuvtsQfBarASYg2YEyVIQYkrYusrCyuv/56XnvtNd566y2mTZvm8rf6008/ERsb6/Lxd/LJJ/POO+8waNAgBg8e3BLNlkgAiDboGdIlhSFdUuyjjMHQZgTOfffdx8KFC1mxYgUJCQkcPHgQULPe2ozdWozcXM+RG2eEgP371XLDh4ftsDt37kQIYQ/gFQzTpk2z/79r16489NBDLFq0yC5w9u3bx5QpU+x19+rVy15+3759jB49moEDBwLQvXv3xpxGm+VoRR2Hy2oB6GdMll+4khZBCMEfhypoHx9FeryrN4uiKEyZMoUNGzbw/PPPExUVxS233MKRI0f45JNPyMnJYcKECfbQ+wCpqanccsstzX0aEolfQnm/thmB88YbbwAw3E0gzJ8/n5tvvrn5G+RMYWF4y2nEORV9sCxdupTZs2ezc+dOKisrMZvNLl9xkydP5vbbb+f9999n5MiRXH311fTo0QOABx54gHvuuYevv/6akSNHMnr0aAYNGhSek2pD7DxYYf9/74yEFmyJ5Hhl99Eqcn4vZNfRKoZ0acfNp3X2KGM0GvnnP//JXXfdxdNPP80LL7xAu3btKCws5OKLL241tnoSSbhpMzY4QgivS4uLG4CMjPCW00ivXr1QFIVt27YFtd+PP/7Itddey8UXX8xnn33Ghg0bmDp1KvX19fYyM2bMYOvWrVx66aV8++239OvXj+XLlwNw++23s3v3bsaPH8/mzZs59dRTee2118J6bm0BZ4HTs6MUOJLm40hlHfN+3MMLq3ey62gVAD/vLeFwZZ3X8oMGDeKrr75i8eLF3HvvvYwdO5b//e9/fP755/Tu3bs5my6RNBttZgSnVTN0KBiNqkGxNzscRVG3Dx0a1sOmpKRw4YUX8vrrr/PAAw942OGUlpZ6tcNZu3YtXbp0YerUqfZ1e/fu9SjXu3dvevfuzYMPPsi4ceOYP38+V111FaDO7d99993cfffdPP7448ybN48JEyaE9fxaOzsPSYEjaV6q6s188cchvtt1FIvTu6ZDQhRX9O9IWlykz30TExMZM2YMY8aMaY6mSiQtjhQ44UCvhzlzVG8pRXEVObbpo9mzw2pgbGPu3LmcddZZnH766Tz11FMMGjQIs9nMN998wxtvvOF1dKdnz57s27ePjz/+mNNOO43PP//cPjoDUFNTw5QpUxgzZgzdunUjPz+f9evXM3r0aAAmTZrExRdfTO/evSkpKeHbb7+lb9++PttYXFzMvn37OHDgAIA9h1jHjh3p2LFjOC9Hs7LbSeB07yAFjqTpsArB2rxiVmwppKreEVsrIcrApf06cE63VJnkVSJxo81MUbV6srNh6VLIzHRdbzSq65sog2q3bt347bffGDFiBA899BADBgzg/PPPZ9WqVXa7JXdGjRrFgw8+yP33389JJ53EunXreOKJJ+zb9Xo9R48e5cYbb6R3796MHTuWiy++mJkzZwJqkLD77ruPvn37ctFFF3HCCScwd+5cn2389NNPGTx4MJdeeikA1157LYMHD+bNN98M45VofvYVqVMDyXGRJPv5cpZIGsOe4mqe/3YHC3/Lt4ubCL3CRX3a89RFfRjWI02KG4nEC4oQ3uZUjk3Ky8tJSkqirKzMIyZObW0teXl5dOvWjejo6NAPYrGo3lKFharNzdChTTJycywTtnvRhNSZLAx+9HOEgEGdk1n0oMyHJgkvlXVmVmwpZG1eMc4v6dOykske1InkmIgWa5tE0tz46799Iaeowo1eH1ZXcEnrJL+42j4T2TktfNnsJRIhBL/ll/HxxgIqG0LWA2QkRnPt4Ex6px+fMackkmCRAkciCYGCYkeenkwZ4E8SJspqTHy0IZ9NBxyJgaMNOi7t15ERPeVUlEQSDFLgSCQhcLC01v7/TsktHGhS0uYRQvDD3hKWbjpAjclhRHxSpySuGZwpp6MkkhCQAkciCYFDpTX2/3eQAkfSCMpqTbz/y362OsVVSogycO3gTE42JrdcwySSNo4UOBJJCBwsc4zgdExunYbQktbP5gPlvPfLPiqdXL9P79yOq0/sRHyUfD1LJI1B/gVJJCFQVO4QOOmJUuBIgqPeYiXn9wN8t+uofV1itIEbTsliYIY2D5GQkZ6ekuMEKXAkkhAoqVLTWigKJMXKGDgS7RSU1fDOT/sodBLJgzISueHULBKaetQmJwcmTnRNDmw0qoFKmyhWl0TSUkiBI5GEQEmlKnCSYyOlZ4tEMz/uLWbhr/mYrGqMgQidwpgTOzG0e2rTZ6PPyVGjrbuHPisoUNc3YUBSiaQlkAJHIgmB4io1qWE7GcFYogGz1cqSTQf43mlKypgUza1DupDRHFOcFos6cuMtrqsQ6lDkpEkwapScrpJTeMcMMlXDcYKiKHzyySct3YxjAotVUF2nGoUmxkr3XYl/SmtMvLJml4u4OadbCo/8rVfziBtQO2znaSl3hID9+9VybRGLBdasgY8+Un9aLIH28E5ODnTtCiNGwHXXqT+7dlXXS9ocUuCEmXD9nQXDwYMHmTBhAt27dycqKoqsrCwuv/xyVq1a1fQHD4DJZOLRRx9l4MCBxMXF0alTJ2688UZ74s22SLVTdNk46eki8cOOI5X8Y+Vf7G4IDGnQKYw/NYvrT8kiQt+Mr9/CwvCWa02ES5TYpvDchaBtCk+KnDaHfDuHkZaw39uzZw9nn302ycnJPP/88wwaNAiTycRXX33Ffffdx/bt25vmwBqprq7mt99+44knnuDEE0+kpKSESZMmccUVV/DLL7+0aNtCpbLWIXDio+WfkMQ7P+wp5oNf99NgbkNKbAR3ntmVLu1aIPJ1RkZ4y4WDYKeCvJVfsSI8dkVyCu/YRBxHlJWVCUCUlZV5bKupqRF//PGHqKmpCanuZcuEUBQh1L8Gx6Io6rJsWWNb752LL75YZGZmisrKSo9tJSUl9v8DYvny5fbfH3nkEdGrVy8RExMjunXrJqZNmybq6+vt2zdu3CiGDx8u4uPjRUJCgjj55JPF+vXrhRBC7NmzR1x22WUiOTlZxMbGin79+onPP/9cc5t//vlnAYi9e/d63d7Ye9HU7DxYLvpMWiH6TFohHl/4W0s3R9LKsFqt4tMtheLuJRvty5zvdoqKWlPLNcpsFsJo9P6Ssr2osrLUcs3BsmVqe5zbYDT6flF6K5+ZKURqqvfzCfacVq/2XY/zsnp1OK+CJAj89d++kJ+fYaClxH9xcTFffvklzz77LHFxngkfk5OTfe6bkJDAggUL6NSpE5s3b+aOO+4gISGBRx55BIDrr7+ewYMH88Ybb6DX69m4cSMREaq9yX333Ud9fT3ff/89cXFx/PHHH8THa08AWFZWhqIoftvXmqk3W+3/jzTIWV6JA5PFyge/5vPzvhL7uuE9UhlzYmbLetvp9epQ8pgx6gvJ+WVl896aPbt5RieC9ebyV94fznZFgRIgH8tTeMcxUuCEgWDs98KZaHznzp0IIejTp0/Q+06bNs3+/65du/LQQw+xaNEiu8DZt28fU6ZMsdfdq1cve/l9+/YxevRoBg4cCED37t01H7e2tpbHHnuM6667TnPK+9aGyVngNKcdhaRVU1Vv5q11e9hRVAWAAow+sRPn9Upv2YbZyM5WxYO3efTZs5vHRTzYr0F/5bWiRZS0xik8SaORAicMtJT4Fw1/9KHEz1i6dCmzZ89m586dVFZWYjabXQTH5MmTuf3223n//fcZOXIkV199NT169ADggQce4J577uHrr79m5MiRjB49mkGDBgU8pslk4tprr8VqtTJ37tyg29xaMFkcAidCjuBIUD2lXs3dRWG5Gj4gQq9w6+ldOCkzqYVb5kZ2tioeQnGDDof7dLBfg4HKa0GLKBk6VBV6BQXexZSiqNuHDm1cWyTNinw7h4GWEv+9evVCURS2bdsW1H4//vgj1157LRdffDGfffYZGzZsYOrUqdTX19vLzJgxg61bt3LppZfy7bff0q9fP5YvXw7A7bffzu7duxk/fjybN2/m1FNP5bXXXvN7TJPJxNixY8nLy+Obb75ps6M3oLqJ29A1dXA2SavnaFU9L63ZaRc3CVEGJg/r2frEjQ29XhUP48apP7WIlECeSlrdR4P9GmzMV6GiQFaWNlFim8Kz7edeDzTfFJ4kbEiBEwZs4t9XXxfM31kwpKSkcOGFF/L6669TVVXlsb20tNTrfmvXrqVLly5MnTqVU089lV69erF3716Pcr179+bBBx/k66+/Jjs7m/nz59u3ZWVlcffdd5OTk8NDDz3EvHnzfLbTJm527NjBypUrSU1NDf5kWymNGDiXHAMcrKjlxTU7KWpI3ZEaF8mUET3pmhJeT6nmCD/h8xiB3KcfeUS7m3awX4OhfhWGIkpsU3iZma7rjUYZ5bmNIgVOGGhJ8T937lwsFgunn346y5YtY8eOHWzbto1XX32VM8880+s+PXv2ZN++fXz88cfs2rWLV1991T46A1BTU8P999/PmjVr2Lt3L2vXrmX9+vX07dsXgEmTJvHVV1+Rl5fHb7/9xrfffmvf5o7ZbGbMmDH88ssvfPjhh1gsFg4ePMjBgwddRozaEs5TgqIxtgGSNk1+aQ0vr9lFaY0JgI4JUTw8vCfp8VFhPU5zxJ7zeYylAWxmhIAXXvAufkaPhqeeclVMwX4NaimfmqqWcSZUUZKdDXv2wOrVsHCh+jMvT4qbtkpTuXS1RprSTVwI756MWVlN5yJu48CBA+K+++4TXbp0EZGRkSIzM1NcccUVYrWTSyNubuJTpkwRqampIj4+XlxzzTXilVdeEUlJSUIIIerq6sS1114rsrKyRGRkpOjUqZO4//777dfm/vvvFz169BBRUVEiPT1djB8/XhQVFXltW15enkAd6PBYVvtwuWztbuK/7Cqyu4n/3ydbWro5khZgb3GVmPzJZrsb+DPfbBflTeAG3hzhJ/weA6tYxlXaXKgDLTY3cNsB3Q/q66S0lDebVRfuhQvVn83l7i5pNkJxE1eEOH4+QcvLy0lKSqKsrMzDBqS2tpa8vDy6detGdHTo4dNlGpPGE6570VRs2lvCtbPVkPbjh3bj79kDW7hFkuYkv7SGV77bRbVJncPplhLL/ed0JzYyvH/oFos6iuLLxtZm95qXF/o7JuAxEBjZTx7d0GP1XshWFzpyGUohGWRQyFByXfexjcIsXar+dPfmSk+H11+Hq6/2rNxbFNWsrObz/pK0OP76b19IL6owY7Pfkxy7RDl5TtWa/L/0JccWhyrqeDV3t13c9EiN4/5zuhEdEf6vmCYNP9HwJZa7ykJ+/nm+j4HCfjqTy1CG853PcjlcxUTmkE+WfZ2R/cxhItksdzTY5gaelwdWK9x7Lxw5om4/cgQmT1Zfou6ipTHeX5LjFmmDI5EESZRTZ1ZnboZkY5JWQXF1PXO+30VFQy6yrimxTSZuoAnDTzgZ3BQ+8y9tx6CT7+q4ijEsJR9X49wCMhnDUnK4yrHSpsqefRbGjnWIGxv5+Q7bHXdL6lC8vyTHNVLgSCRB4tyh1dYfvwKnJRLLthQVdWbmfL+bkgaDYmNSdJOKG2ii8BNuHlEZaFNHGRR6NfS1oGMicxq8CV27E9Hw+yRmY3HvaubM8R+8b/p06NJFJriUNAopcCSSIHFOsOmcePN4ojk8e1oLtWYLr/9vN4cr1Tg37eOjmDC0O3GRTTvDr8nhyCgYalmjTWV6iQo8lFyM7EfxYV9jd2paPMHTfRrIZWjDtJT3rkSgs09xuVBc7LudNmQWb0kjkQLHjePI5rrV0trvQVyUwd7plDd80R9PBAqLciz1Rxar4O0f9rC3pAaApGgDDwztTmJ0RJMfO2D4CSGYXXMn+pEaVaYXox49VuYwUa3TTeS4hLi42sl9eswYe5lCtA0f2cvZ3LqDYdKkY3t4UNJkSIHTgC2RZHV1dQu3RGKLj6NvpXPsOp1CYoz6vBxvAidQKiEh4I47YNWqY6NPWrSxgG2HKgGIidAzYWh3UuMim+34PmPPpVSzlDFkH3WzofGnMn0Y62SznKWMIRPX5JVpabBokZO9r16vDit9+629TEhTXA88oGkfwNWSWiIJEulF1YBeryc5OZnDhw8DEBsbG1KOJ0njsFqtHDlyhNjYWAyG1vt4JsZEUFZtoqy6bQYrDBUtqYGKi2HkSHV6Zc6ctuvF+92uInJ3HwXAoFO456yuZCbFNHs7PByI2lsYelNf9OzzLOwtYaUNP8Y62SzHio57mcsR2gM+nJpyc12ml2xTXAVk2m1unFGwYiSfoeQ6knqOGgXz5vnO++QNZ3EmY3FINNJ6e5AWoGPHjgB2kSNpGXQ6HZ07d27VAjMtIYr9R6spqzZRb7YQaTg+XrDBeOzYBhPaYpT7Pw9XsnijY0TjhlOM9EqPb7H2uISfWJMLBV7EjQ1f/uN+EkrmcBVjWeyResTjHro9ALYprjEsRcHqInIURYBQmD1pL/pRq1yFyJw5LlNdAbGJM2/xcNq6kpY0GVLgOKEoChkZGbRv3x6T6fiaemhNREZGotO17tnTtERHAMKjFfVktGv+L/uWIBiPHX+DCa2Zoqo65v24B1tO1ZG90xnSJaVlG+VMqP7jNqOeMWPUG9Mgcvx6QrnfQy8PgG2KyyMOjlFpiMPnJQmfbe7NXay445zF22b85T7q05aVtKRJkQLHC3q9vtXaf0haB2kJjnxDR8prjxuB42cQwCuNCkbXAtSZLby5bg9VDe7//TokcNXAEBM+NgK/szCN8R/3IiwcnlDecbmHPh6AbJYzihVqJOP43mR88gZDh+v9i1rb3Nuzz6pu4e44WzmDf+OvtqikJU1O6/5MlkhaKe2THCM4B8tqWrAlzYs/zx5/BB2MrgUQQrDwt3wKymoB1R38tiFd0DXzVGlAF/xgE1a645ZQsnDaXE3tKizE7wOgx8pw5XvG/fsihp8XQNzYd9LDk0/CsmX+E2YGE9ZZImlAChyJJAQyU2Lt/z9QfPwIHPDt2eOPoILRtRDr9hTz875SQE3Hcc9ZXcOeXyoQmlzwA/qP0+Db7aftTlGBM87rp6ltGe0b3OJ8PQBZWcFNEzlHikxJgV27fGfxbrKwzpJjGSlwJJIQyHSakiooPv5CC9gGAVauVPsmXwQaTGgtFJbXssjJqHj8qVl0TGzeRK+BXPDBKSSMT/9xY9C2KAEHhLCSxT6G3tTdMYzkNgrkIUgC4W2YqkcP1UPLWyoGrQp5xw5t5STHBVLgSCQhYEyNs/8//+jxJ3BA7X/OO0/1+FWU0AcTWhqz1cq7P+3FZFFVxNDuqZxiTG72dgQ9C9NYkdGAY0BIoLj5UdmC/81mEvoD+11j7ISaGyqUSJGBVJiNGTOOrUiTkkYhBY5EEgJpCVHENExf5B2pbOHWtCyhDia0llxWn209RH6D3U1GYhRjTvSdWLIpCWkWJkwJKLPJYWnKnWTiKjqM5KsBBVnuZRgpAN5ucFDDVE7YVJgWy3YZ+VjSgPSikkhCQKdT6Joez7aCMvYXVR1XsXC84RGMLkD8tdYSzmRXURVf/6nGvdIrCrec3oVIffN99zl7S33zjbZ9wm7P1DCiki0Eo3hX9YQigwwKGUoueucUDlrd4nzd4Dvu0D5M5V5/djbMnOnd4yrY9kmOC6TAkUhCpHsHVeBYBew9UkWvjMSWblKL4hKMzg+tJZxJndnCv9fvs0/KXNqvA1nJzefu700D+MM5JEzYcBtR0WNlON8F3s/fcJO/G+xPnGipv1evxu0vOa6QU1QSSYh07+CIbLvzYEULtqTtEOoMRVOwYstBjlSpqTa6p8RywQntm/6gDfgyQ/GHEE1gz6Ql94Y3fA0jabnBjam/vcZ7pLWc5JhGChyJJEROcBqx2VZQ1oItaTu0lnAme4qrWbOzCIAIvcJNp3VGr2ueeDf+NIA/Zs4McWTLn7FTsCMdgdziQhVMWuuXSIJATlFJJCHS15hk/78UONpoTDgTf9F9g8m/aLGqAf1s+uLyfh1p7xSZuqkJVQNomZ3xuA5HctBP9mPsFIxBjxa3uMJCLOj82/E41+es8rTUrzVPoMwnKEEKHIkkZDKSY0iOi6S0qp5t+WUIIVp1gtDWQKhZBvwZJUNwBsvf7Spif6kanDEzKZq/9UoP4gwaT6jmIYGunddrxGnM4TSynb2jnI2drFZVTGiZE7RlA7ddVC+qMmfHQCayxzUnFfuZw0TVE8vGzJlqfAH3m+Zcvzcak6ZCctyhCBHsQGnbpby8nKSkJMrKykhMPL4NQiXh4bY3fmDdX0cAWPXkSDq1iw2wx/GNxaLGd/OVy8pmSJuX5/iI92Wz6j4A4L4NPA2WK+vMPPnldmpMaoc+ZURPujvFNAr2XLSOGjmzZo0a104r7tfE23FXrPBxjRpGTuyu3s6VpqSogfUCdQG2HE/OJ+hFTeWk3s6Yo28jEDhbP3i0ITUVDh1SNwZ7AUN5gCTHBCH13+I4oqysTACirKyspZsiOUaY88U20WfSCtFn0grx2a/7W7o5bYJly4RQFHVReyl1sa1btsxR1mwWwmh0Lad1URQhsrLUOmws/G2/uHvJRnH3ko1iwc97G3UO7u0yGl3b7ou6OiH0eu3n4HxNfB03NdVPHVhEFnuFGV1wF1CvF2LxYt830KmsGZ0wsk+ARVsbtFwofxdf6wMkOWYIpf+WRsYSSSM4uZsjT8GmPSUt2JKmI5iAfFrKBhMYsDE2q+4GywVlNeTuOgpAlF7HqAGhTWOEEojXmXXrtHuJOV8TX8fNz4ejR33XIdCxn87kEqThrsUC6eme67xYSDsyknvvUlzaYMv8HaqrXHY2LFqkjgQ5E0KaCsmxjRQ4EkkjGNg52f7/jXtbj8AJV5TgnBzo0sU1ZVCXLt478YBZsJ3wlmVg50511sS5zeEIZ2KrI+f3Qrth8YV92pMcExF0XeFwc9d6TjfcoF6T7OzQPa9cjksIgs69sT4Up9a6C8lovKtcTg5MngxFRY516enw8stS3EhckAJHImkESbGR9OiQAMAf+WVU1ppauEXBCY1A9YwerY5MOFNQoK53ri+UUQ3nLAPFxWquRfc2hyN3YkYG/HW4kj8OqbGKUmIjOK93YMNibyIxHG7uWu1fP/hAvSY5OY33vgbIIAS16N5YH+pMa90u5YJVrxYLPPWU+vC5X4yiIhg7VuahkrggBY5E0kiG9FKHyi1WwS+7ilu0LY2dPrFhscCdd/ovc+edjUsvpKXNM2aoMxGhOKfZQqqcc47gky2OzvTy/h0DpmPwJRJXrNB2bH9999ChntNzvrDdt08+0VbeG/Zs4AQxYuIrHo0PdTaUXIzstxsUa2pDMJ5OthviKxKyvwettSQ9kzQ7UuBIJI1kSK80+/9/3lnkp2TTYrHAAw+EJ0rwmjX+7TpA3b5mTeNGNbQGvhXCd7Zy9/87/z57Nmw/UkFesZrxPSMxmtM7t/PdWPwLrtmz/e5qx1/frdcHFo82bNfg3//WVt4dW3bw2UzyHovG605+4tH4yOqtx8ocJjYc0/U4LhnJbduCCeanNeyztwctXMOZkjaJFDgSSSM5rUea/X3/Q4PLeEvw7LOe00nOBGP6sGaNtmOuWdO44H1axNHRo2rYFG9GycuWqYsvg+WrrhL8d9sh+/rL+3dA52c4KJDgUhT/3sdaAvFaLGAKYiZTCCgt1VY2Odn1d2OWwqKPBSmvPMlH9+ayJmkUlkCv/cxM38a6tqze4CFyspVPWMrVZCoHXNvgnJHcxssva3PjDsX4yG50FabhTEmbRQb6k0gaSbu4SPplJrE1v4ztB8o5VFpDh2ZM2gjqu7qxeQxDpTGx17S2pUcPWLDAIbyGD1cXWx/pnMnclobo8GFYurKK3WXq6E2nxGhO7JTk9zhaBJdtBMxbHB4hVBOR3FzvYV2CTbAZLAaDKgZ79VKvd1ERPPignvz8kxpKnOM98J4zCxbAeef5PojNDc79RFJSyDatZFR5l8CRjPPz1QsZSOSEYnyUkaFNqdri+8h4OccuTei23uqQcXAkTcWrTvFwFq3b06zHDjZWzOrVgetcuVJbXStXOo7vHpbEXzwaG6tXaztOQoJb7Jf0GrFs0vdqBU4Vu8eJuejR3fa4Nz/uORrwvBcu1NaeSZM8r7l7bBv3uDhewscEtSQlBS7jHArG1/EULELBIpZxlfdKFi4M/IAIoV731avV8jNnBn9CWgIHab0h7g+a1gdLyx+DpFUg4+BIJC3EsH4d7P///o9DfkqGn2A+crWaPgwf7hlmxJ3UVMcoio9Zi4Dphc46S9sHdIVbsvaCI5GMmX02OSNetdtUuM9ItDPW0vXUcnX/ogj2/+Lf9ga0j0aNGuVwc580SV3nbtuUn6+O5jz1FNTXN97Nu0xDujNb/RMn+rHHapiimsRsLOiwoGMNw/iIa1nDMCztNV4Emxvc2LFq2oVgsbnjPfWUbwPgYFMu2B60xsybSo4dmlBwtTrkCI6kqbBYrOLsaf8VfSatECdN+UxU1Zqa7djBfOQGE+R12bLg6vIWZTcry/8xtX5oe/1gt0fH1QszemFMrXLZPvQOR9TiQZce9jmK5Eywo1FaR8/8RRpuyWUm0xoiEDsPrFiDCwbcmJvovriP6gS6Ib72kyM4xxxyBEciaSF0OoW/NUTGrTVZ+H5b82Uz1vqRO3NmcHHQsrNVA16j0XW9zbjXvS5vwfvy8vwfszEf0I7ouOeokXSPOvKARcZa6H1uCQD1NTq2rUrRZGAd7GiU1tGzQB5pLcV0ZpKPq4V2QYHisMHV4mIdzlEQdwNgfzfExsyZ6oPn/KD58Payo8UaXNLmaVMC5/vvv+fyyy+nU6dOKIrCJ40JDiGRhJmLTupk//9XGw/4KRleAr3LQd0+dWrwdY8apdqcTpumLitXevYlzjgH73M2AvZFOJI+F5JBIR1d1vU6t4SIaNWw9a/v2mGqVRuipS8OJpXEsTHD4doN2Ka1Jt1ZjaVL98Au1uHM3G0/+CSHmPJ3Q2wW1bm5ruKrMfOmkmOGNiVwqqqqOPHEE/nnP//Z0k2RSDw4vWcq7eIiAVj31xFMFo1xRxpJoHe5oqjbg32X20KIjBwJzzyjLjff7D/YncUCq1bBE0+oy6pV/uPuaBFngchokDgOBP0vdMQj2vq1w5hIa1+sZTTKYnEkxW5tGI2qHvB9XRuEhK/cUQL2H40lt6Cb6wZvLtbhuIkeB98Pr73mGDlyNnpauFAVNqC6DvoSX8EoVcmxSRNOmTUpgFi+fHlQ+0gbHElTM2/lX+K973aJ4oraZj92KDYw/ury6oHTsG7MGCGmTXN4Udn28WZrkprqvQ02Z5dJk1zr1r44MlTbslkrWEX7npV225srZu7wajvTWLxd69a0TJkixJIl3rcFc52vZKmYxkyxkhGOTODeLmZjXcSCsc3x93B6yybu7O3l5nUnaTuE0n8f0wKntrZWlJWV2Zf9+/dLgSM5pgnHuzxYt/PUVLVDDVTO3WXa/Rg6XTB9ntXV1VlRxLLUO4SiWMXQ2x3GxScMP+qz3wuVpu7Lw7GkpgqRmel9m14vxDXXhFAnR1xdy90NdKdM8fSVD9diu4mLF/t/OMOtZCWtBilw3Jg+fboAPBYpcCQS34TTKcZ5SUsT4oMPQguZ4tnZHnYRNzYFs2SpRdy6YLO4e8lGcdsHm0REtDnkUSxvBCv+vPW9H3+sXoumFjmB2pKa6k+oWX2scxKVzvFymkP1KYoQ6enaykrvqGOOUATOMR3J+PHHH2fy5Mn238vLy8nKymrBFkmOd0pKSjh06BBbtmyhc+fO9OjRg9RAAWc0YHN28RXpNxiaynC2qAhuuKFxdSRE1/Nw5Bymlj/miI5rNKoGo9nZ9D5Qxqp1qtFPp4gkvv6v3mtE4VAJNau3s11rdrZqxpKbq9ozvfsulJeHp31acVYDXrYC3uxp1HUTmc0oVqC3GTQFSKdgQRc4srHWRh/RmArl2LD+ljSSY1rgREVFERUV1dLNkEgoLi7m9ddf57PPPuOPP/6gqqoKgH79+nHvvfdy7733hlx3To6avNHZFfmZZ9RAfG+/HbwtZTidYsLNG/+K5PprJ0PuaWonlpHhkhNhQ4EjGt7V57RjYCdfNYWG1n4zJQWKnRLL2zTYqFGO/F07dmhP3tkUOLfPFX/Gwgr5dCY3fTTDbS7WflRfDlcxkTnk4/iwNLKfOalPk31rsmpE3BR5K1rzQyxpNo5pgSORtDQmk4n333+fGTNmkJ+fj9FoZMyYMWRlZZGWlsbbb7/NQw89hNFo5Iorrgi6/pwcNRisN44eVbd5i1njD60fyS1BZiYOX3Q3LFbB5kJ1KCTKoKNPh/iwH19rv7l4sSOgrk2DrVihOvo0VR6q5qTw+ocdw2I+VF8OVzGGpbiP6xRgZEzxWyw9QyF71ixHErFDh+DBBwMfPD1dHQ70NmKkKKqalPFtJABNOGUWdioqKsSGDRvEhg0bBCBefvllsWHDBrF3715N+0svKklzsnfvXjFmzBihKIpITEwUM2bMEDt37hTl5eX2Mhs2bBAjRowQffr0Cbp+s9m3IanzYjRqt7lsjI1J0y5Wocck6qp9n8i2Q+V24+J5P+QFfT2DuT7B5t1qC4bJwSwuJi5ejLZsXm1g0X6dtF7cJUscdlfu28NpTS5pVRzzRsarV68W4Gk0fNNNN2naXwocSXNRXV0trrrqKqEoirjqqqvE5s2bPcpYLBYhhBDTpk0TcXFxYt26dUEdIxhjYK02l01lYBy2jvWVDT7bvmhDvl3g/LyvOKhrGQw2saK1f229ojGUxeopmL0Ik9UMC+251HpxwxkTQdImOOZTNQwfPhyhijKXZcGCBS3dNInEjtVqZeLEiXzyySfcc889vPbaawwYMMCjnNJgeRodHY3JZCIuLi6o4wRjR3ms5B4s3FXtc9v2w2pGTgXo3yHRY7uWrANa0Bo/zna8GTOOjWkpGiab5ozJRZ+7xnEBvUSaLETbXJ7H86b14oaSF0Ry3CFtcCSSMFNYWMiyZcs49dRTeeihh8h0f1kDFosFvV7Pzp07effdd8nMzCQtLS2o4wRjR6m1bGu3zczoEet1fWmNicLyOgC6psQSG+nqNpWTozr6OAsNo1Htl0PpE7OzVYPh3DUWCtf8qXoHDdejHz4U0Hs9XlsiMdHTsytVV8zb1jvInr0cZuN6AW3CpOGkXSNL+8br82a/uLleDcnt+LDFkkhsSIEjkYSZ4uJiSkpKuP766+nevbtdzIBD2Oj1esrLy5k2bRp5eXk8//zzdOoUnMvP0KHqh25Bgf9ywdhc2qLuFxSo4/7uKAqkpcEdd4BOp5a/7bbm6MitZOkLGXrvQEAdPHDu/yK7VNhL9mkf77J9xw41or87tqwDoUbt16/IYbizinkGMBrJGbeEMS+e4fX6tXZsNro7d6rXb80aYNs2hi+7n+HWNa7u3e4X0EmYDC04iPHBWgqKohDC0ysroC2wFC+ScNA0s2WtE2mDI2kO1q1bJ2JiYsTf//53+zqLxSKsVqv996VLl4oePXoIRVHEDTfcIEpLS0M61rJlge0cgjVLCNbGREsbvNlyBFfWIpZN+UGYzWqgwJQU1zKXPLzXYWC8tEKzzUvIgW99WA2b0TcY1wZzfq1j8Xp/AxkQ+bmAwT5HEok/jnkj48YiBY6kuTj55JPFaaedJnJzc4UQQphMJiGEED/88IPd+FhRFHH77beLAwcONOpYweaA0lpnMDacvtrgX7S4iwBfokAtO2WK72Nc88o2VeAs2iQMkd49d/wtQQW+9dPpazWubQ2LezRlr/dXq9X56tVe04RIW2BJuJCRjCWSVsLs2bO59NJLGT16NJdccgkJCQls2bKFNQ2hhnv16sVLL73EZZddBoAQwm50HCy2mYFAkYzdp3WczRrct40apc0Mwr0Nzz4LL7wAlZWBWu15rjosWNF72ab+/sIL3muKjDPTzqja3xzeHY25PnjfCQ9jVz8Xy7Iml9z8HhRyjkdkXq3Gta2B2bPVKU6/91ej1XnOCj0Tx3u3cdqzR/tzJJGElabTW60POYIjaU6WLl0qLrnkEhETEyMURRGdO3cWI0aMEG+99ZZLOXMzJAb09iVtS9Dsb1soxwl2JGEaM8UrTAx5JCLrpDL79NTZt+SHVMcrrziNPCz2fUGWLRPCmFLpuol99vxMWkdwpk1Tj/XQQy03gqNp1ErDCM4yrhKKl9E3ORUlCSeh9N+KEEK0tMhqLsrLy0lKSqKsrIzERE83Uokk3JjNZjZv3kxcXBxCCIxGo90d3Gw2YzA0/SBqTo5qC+r+l64onuvcWbJE3VcLFktokXoXMg4rOm7gw+B2bGDwVYcYct1BAFbO6czO/7ULan+93tVl3Mh+5jCRbJY7VioKOcI5Mq9jlElpGL1Zyhgu4z/EUoPF60iUvSq+/FKNNL1jh9YRr/BiNKojKwFHUmw31YfVuQU9XfX7yLdk4O18bcbEeXmuxxJCeEQ4Vuz7hDaSKTm2CaX/llNUEkkTIYTAYDAwePBgl/VWqxVFUZpF3PjLg6jl0+baa9W4MVdfHbhsqIkoMyhkIycFv2MDKZ1r7f8/ujc66P3d4+EUkMkYlrKUMXaRYxEKE5mN2i27ToEJdChYmchs8uiCJcBrVQi48MKgm6mJ1FS4+WZ46SX/5ebM0Th9aYtxM2aMpyJWFHJ1w6hN60CfDmbSOlhJ72glJd1KYpIgIclKQrIgIVGQu8NKVDRYhWhYfLdNAXSKgk6n/tQrCgadgkGvEKHTYdArGHQ6Ig06ovSOnxF6RYojiQtS4EgkTYT7y9ZqtaLT6dDpfNuIiEbY4ngjVNFhw2KBsWPh/vuhRw81DVBmpsO917lTDOSu7okgkXLqiaAdRwMX94FN4FjMUFaoPbmu+8iNo1WqYJlky5qNlVyGuiSM9LZPPp15mFeCbn+4SEyEW26Bjz/2XcZbAtZAMYLEVVdR++lnVL/3ATWxCdR07a4uPXpBx0yW6ko0ta9eY2BFAViEaLg32icYFNQcZNERemIi9EQbdMRE6ImNVH/XSfFz3CEFjkTSDPz666+sWbOGUaNG0bNnT5/lbOLm559/pra2FqvVyvBGxAMJV2Tif/7T9ffUVPWncwbz9PRga1UoJ4kL+YbYSDPUB98unV6Q3Ek1MC49EI3V7F082vq2GTOgV6/AeR0FOvbTmVyGMpzv2oTxcHk5vPii7+3Tp8MTT7iO3LhPXxoiBD37mukz0MzaPywk9jQTHW/B0vd0mHW6R51aJIPFAjoUoiIV9A2jMjpFQVHU/YuKYPduqKsDRQeRUYL4eOiYIYiOFVitquAJhABqzVZqzVZKa0we7bSJnbhIPfFRBuKjDETpdXLU5xhGChyJpBlYuHAhr7zyCgcOHOCFF16wj+J4G7GZN28es2fPZvfu3dTV1XH99ddz//33M2TIkKCP21SRiY96GXApKgq9vur60F5F8Wn16A1q51da4Bi9SUtzbY/RqHoN2UYuPvpIW/02YaM1Mm9rRVHg3XdVgWPDYoEnZlg59yIT/U4y0e8kMycMMBPpNghm8aEt9DqFaIOO777Vk79Hx5FDOooO6ig6rKO8VEdFmUJlmUJKO4XduxWv9j7+7MPAEUPQKgRmq8BssWKyCswWQb3FSp3Z2vDTQr3ZSo3ZisXL/JcAqk0Wqk0Wiqoc6yN0CvFRBhKiDCRGR5AYbSBC36YyGEn8IAWORNIM3HTTTWzYsIHBgwdjsVhQFAWr1WqPcGxj7969PPfcc1RWVnLvvfdSVFTEF198wdatW/nhhx+IitI+BQOBIxOHk+ZwV0hNdRVXCe0dwz4VhyNdIvGuW+fbJf7QIW3HswmboeRiZD8FZCJoex2gELB/P+T+TzDwVBOlNfXsP2Ji7vLA80aHDujY8YeBvL/05Ofpqa/RM+EePVdertq87EuAKTMdx7FhEyn/XuDdmDmQfZiiwKRJavgBvV4hUq8QGUB8iAYhVGOyUGuyUmOyUGUyU11vobre4jHhZbIKSmpMlNSYgBoAYiP0JMVEkBRtICkmgmiD9Glvq0iBI5E0A4MGDeLjjz+mffv29nV6vZ7KykoWL15Mx44dGThwIDt37iQvL4/XXnuN2267jejoaObNm8djjz3G3//+d14KZD3qhj8b0bZGVpYqXJ57zpF6IdFN4IA6UhMZ6T3Sv9YcUQpWjOQzlFwA9IpgjpjEGJa2uevYPsPCWefVc/bIeuhkYovqcIYS6Vk2f4+OPzZGsG2TgR1bVVFTVekqKhQF1nzlmqHBKQ2VHfdRM3cC2YfZRVmu9qwNiqIQoVeI0OtIdLM3F0IVPlX1FirqzFTWmamoM2N2G/GxjfQUNuTiionQ0S4mknYxESTHRGCQIzxtBilwJJJmwiZubMbGH3zwAZMmTaK4uNi+/bLLLmvILt6f5cujyciAyy+/knXr1vHhhx8yefJkr8k7/eGrA2or2ATF6NHqqMzUqTBggHo+8ekOgRMlIv3mlfI1HeJxvIbv/NlMcuReMhrJnn09S1G4807vU3StiW69zQy7uI6zz6unVz/vozRCwJ+bDfz2QwSbf4ngj00GyooDd96eoyva82M609wZ7hVFITbSQGykgfR4dSRUCEGd2UpFnZmyWhPltarocabGZKXGVMuBctWYPTHaQGpsJKlxkcRG6KUNTytGxsGRSFqArVu3MmLECIqKihg/fjwTJ07kt99+4/33v+P77z8E/gE8BERgNML558/m44//zpIlS7j00ktDOqbNFbigQO2cGmMz05zodGB1yvFo8/AZNQpe/mI/u+tUgfjYiN50SY3xWkcwMXqysmD2yxay0zx7a4sFunSBggKBNhPb5iO9o4W/XVbH+aPq6NnXu6g5Uqhj3beR/Lougg0/RlBdqfPqSaaV1atDz4m5Zg2MGNG0xwgFi1VQXmuirFaduqqoNfv05Yo26EiNiyQtLoqkaIMUO02IjIMjkbQRPvroI4qKipg+fTrTG+Zb9uw5me+/vwnYBCwCzgaGkp9fy/z5u1CUWpKSkkI+pi1B85o1bUfcgGgQN46OwzmJdUJHMxxQ1yfF+n6daXWXf+UVmDCBBtuo4V7rUd3hg+vILrpIDe4XbiIiBcMuquOSq+s4aYgJbxEItm0ysO7bSNatimTXdt8BCEOhMaMrWjLX+8043kTodQrtYiNpFxtJV1TBU1pjoqSmnuJqEzUmhyKsNVspKKuloKyWCL1CelwU6fGRJEVHSLHTCpACRyJpAUwmEwaDgQsuuACA6uo6HnggAogAZgKjgRuBW4CDwL9RlHTi4/0LHH/5pmyEa8i/efDsJJynSCYscEwnxEX6ng/Res4dOoRnWsWdRx+FO+4IbprQn61PZhcLl46t5aLRtaSkeRba8puBlZ9Gkft1JDqhx2ptmmm1xnjpBYghCKg2PC2dt0qvU0iNU6ekAGpMFo5W1XO0up6yGpN9dMdkERwoV6eyIvQK7eOj6JAQTXyknMZqKaTAkUhagMjISKxWKwcOqMMPP/0USUGB7SXYGTgFSAAWAHuAU7Bap1FaOtBnnYECttloKtfx5sRmgFpUpn5NRxl0ft17tZ5zoHLBXztBerpCQYEaIHHXLlfvrqIiuPtu7+LD1uHHx6upHBRFcNZ59WSPr+WUs00e5fft1rPy0yhWfhrFgX2qKkhMVNNthDtycrhGV0I1UA4nWj4KnImJ0GNMjsGYHIPZYuVodT1Hquoprq633zOTRdhHduIi9XSIj6J9QhRR0iOrWZECRyJpAW666Saee+45Vq1axbBhwzh4MA0woY7grAN+R52qagcUAh2ANJ8jCL4MaJ2nc2ydxdChjk6zrVNntoICUQE8W8I1HTJ0KBhTq8k/Go17ygZPVDudI0fghhvUNTbBOW6c+rvFoo5E+UJRIDZWcPb5dYy7s4auvVwNZkz1kPtNJMvfj2HzLwbcR7weeij8IzfhHl0JxUA5XGj9KPCFQa+jQ0I0HRKiMVutFFeZOFJVx1EnsVNVb2F3cTW7i6tJiY0gIzGa1NhIOarTDEiBI5G0AD179uSOO+5gwYIFpKamMnz4dNQ4HD8C7wBJQAyqsOlg38/bCEJw8URgxYrgxU24XKN9pUcIGZ0AQcDgbOGaDtGvyGHc0Z28wBRsAsaB+++eHZhNcC5apEZ+XrXKd4qLqGjBJVfXcs3tNXTMtLrWs1fHfz6O5stl0ZT68HxKTVU9znJz/Z9TsDTF6IrNPqw5CeajQAsGnY72CepIjcli5UhVPYcqaimvdUyjFlebKK42EanXkZEYRUZitBzVaUKkF5VE0kLU1tZywQUXsG7dOjIyMjh0qD0m00bU745ZwGR7WV9ZmSE4b5ShQ0PL+B0uwjlylJ4OY1/fjEWx0jEhiukX9gH8Tzl4+2LPytLYYVssWLp0p2vB/8gnE+8jOFbaUYo+MY6ict9BGf0JPUOE4LJrahl/bzWp7V1fz5t/MfDhm7H89F0EQvgfAVi2TD2nAAnBURRISYGGaAVexZ8txUVzjq40JYG86vz9vQVLjcnCoYpaDlbUqSOObqTFRWJMiiFRemH5RXpRSSRtiOjoaObOncuSJUt49dVXad8+noKCYcDVwE32coFGGIKJJ9LY5JuNparS1ns2/kV+5AjUmQSGSPhru0JOQwh+zykHwZw7tpLdazPZGRmM2jWU3HX64KdDcnPJLejmN+km6CghBcr9V+VN3Oh0gvNH1XHzA9VkZLl2hD+sjmDhm7Fs/jVCQ0Nh5kyHYNMyevX22+pP92vXrp26burUti9qnGmKIIO+iInQ0zUlji7tYimuNlFYXsvRakf8pqKqeoqq6kmIMmBMiiEtPlImBg0TUuBIJC3IgAEDGDBgAPfffz9Wq5UvvlB48sn2DS9fdcoj0JRAMAa0Le1BJVAAK+FyVRZWBRCYLGogQG8U5AvGTO/HUp4km+XojUaGz5kD44KcYyksbLKkm2f+rY67Hqmma09X5fPdl5G8989Ydm3X/qo2GlVB4oxWY95Ro+DZZ1VBVFysLtOnw7x52u1S2gLNHWQQ1ECDNm+sOrOFwvJaCstrqW9I9lVRZ2bb4Qoij+owJkWTkRSNwZvfv0QzUuBIJK2A9IZU3LfcAjfeCN9/Lzh4UNE0whCMAW247TFCw/bSthLYUNeGlZQ4E7qYKJcYPrbzVRTfM+0CHQpWJjGbUaxAH6qRRUZG2JNupmdYmDCtinMvdE2l/tN3Ebzzchx/bQ3+FT1njmveLefpOncvLvdna8UKdToqXHYprZVwedWFSpRBHdXp3C6Ww5V1FJTWUFmvitt6i5XdxdXsK62hU2I0xuQYmQA0RKQNjkRyDGAzmATvUxC2jimQPUbzYhvJ8T+aozSUmzFTseegsnHLvzcTFWulpCCKRZP6BDziaoYznO+CNrKwWCB3jYWCUfcyqeppikjDmzhTsJJJPgKFAow+zy0ySnDtHTVcd1c10U7Blzf/YmDeS3H8vl7bVJQ7iYnq1F1kZPAeQs1pl9LSaLFLas5zFUJQVmsiv9R1+gpAp0CnxBiykmOINBy/QieU/vv4vVoSSRvCahUcLK3xud02BeGepspodP3qttljgEP8hJP77oO//11raYV4KgKWMhoVli5T6NXLc5u5Tn2FGSI9jTe9YZ9icjayCEBOjtoZjhip54aqtyiiPdin2hwoDb/PYRLn8D98iZuzzqvjva9KuHWSQ9wUH1F4amICE65NClncAJSXq8/AI4+ogtddrNhGYnJyPPcNxi6ltWCxqEb2H32k/tTqoefv76AlggwqikJyTCQDMhI5LSuZjglR9qfHKiC/rIaf9hWz+2gVJou2Z10iBY5E0uqpN1t45MPfGPvK9xwoqfZZLjsb9uxRvaUWLlR/5uV5fq37EkONme5XFNUbac4c1WYjLQ3wmcHHhiCZUqZn/05KiuuW9HTVtX31asjbo5Cd7X26wFSj9kCRMdpe+h5TTAGMLGwjY54dv2cuKiP5LEUdRlvEtR51xcRZmTKrgn+8VUFHo9peswmWzI/mxgva8e3nUR51hkJREbzwgu+wAaBeW3cx0BJ2Ke4EI1jswnMEXHed+rNrV+/izRtaPwqam9hIAye0T+D0zu3olBhtF1xWAftLa/hpXwl7iqsxW6XQCYScopJIgiDYqKfh4LlPtvDv73YD0CsjgQ8mnENiTOhf+Tbcz+XQIbjWs18OiPM0GASftXz1KxsZOuEk1qxROzVwXNfDhx3XGTynFbJn/UX7njVYrfD2NYPwJRAUrBjJJ49ujgzh4DeTY8ApG6ykcYRXeJCOHATgIB15kNkcId2lLQNPNfHY/1WQ2cVx7PX/i+C1p+PYt8sQtjhDweB+6i2d/DLQlJrz87pjBx7TleA5JauFlvibDoY6s5X9pdUcKKt1+WQw6BQ6t4shMynmuPC6CqX/lgJHItFIY6OehkpJZR3Xzvkf+4pUP+iTurbjX3efSVxU+H0EHnlE/fr3RXw8REW5Rse1xZEB74HTArHwAytRMTq/wshohDmvWOCPbYyZ3h9QPbIue3IXxoFqYJ13xg/AVKv3dIVuEDRLGUM2yx0b0tPVA0ZGej2m1g5/Jk8wjzu9uo/HxFm58+Fqrhpfa19XXanw6tNxfLnMMWKTlaWKyxdfVMs0x1t54UJHRGVoWbsUX0H3bP32ww+rozpahPOxZCvkTK3Zwr6SGg6WuwqdaIOO7qlxpMUd29GRpQ2ORNJE+Jqq8GfTEC7axUfx1p1DSIlXO+KNe0q4718/U1tvCdkGwRfPP6/mLvL1/qiqUsXNzJmu02CjRvmOphyIHbt0PqaBHBTkC8ZcrcD0J1nKaDJRC9eUOUTeOx+YWbbMy5RDw9SRi7gB1Rq3Rw+fN0/rVMx0nmoI/OdKt15m5q0odRE3m381cPsVyXy5LBqbuJk5U72Gzz+vjjx06qTtuI3FfcqvpexSAkXiFkIV3VpHBVujrVA4iDbo6Z0ez2md29Eh3hFEstZs5Y9DFWw8UEZ5rWeOsuMZOYIjkQSgtXiXbC8o46bX11Feo77Eerdrz2//Op38/Y7vlHCMKFks0KWL7xQC3s5X62iHK+qrJyVFsUfR9YfzNBNALueydvw48q8YAsCDSRX0/tuZWNA7phx2fM/Qt8ejL9jn+2TA65zGmlUWRozUckM97XGGXlDH31+oICZO/b2mGv71UhzL34/GanWU9XYtV62CkSM1HDZEAj2v3kYq09Nh7lyHp15jcZ4WOnQIHnwwPPU64z5CdaxRWWdm19EqSmtcRU37+Ci6p8Yecykg5AiORNIEtBbvkj6ZSbx91xnERqkvrr9KDiNO/AVF7xi2CceIUm6ub3ED3s83NMNTtaPXIm5AjWezn87kMhQ9VoazhiEljkaUzXwWunZFvyKH4cPVzm34k+ei371D7aF9nYwQnla3OTkMvak7Rvbbp7g8sa13CBa9QXDXI1U8Pdchbv7aque2y9qx7N8xLuLGdnj3a3n4sKbLERJaRmKys+Hll22G4ipHjqgiJBwjle7GwU0hbqDpYti0FuKjDAzKSGRAx0RiIxw383BlHT/vK2FfSTXW42f8witS4EgkXnCe+lm1Sts+zREl+MQu7Xj91iEIs/pCS+h5kMwrfkYxqAn9/HnJaCUUb5rQO5PgbQacowknFzvUQHFaR+8Kb906tYf2h7PKaJiP1BfsYw4TG1rp3SXcmY6ZFl79qIxxdzrc+b/+JIr7xyZzYJ//r+nwXMuGtinqMmWKOlLjjBYPoZwcuOYaXAIqQnjEs2+vtPBh8+gLlBn+WMAWHfmUrGR6psVh0Kl/T1YBecXV/Lq/1GOE53hCChyJxA33L8xnntG2X/v2TdosO7UH0ti/4nSsJrXTjO9yhKwrf0IX4RA5jRlRCiXKqy2acnPg7OqdfnC//f+HMzp7V3j+hqOcKSjwMAjJZjlLGUMmrnUYyWcmDjeeXv3NvJFTSv/B6j0w1cNrT8fxj4fjqa8LLOK8XUt/9qIpKbByJSxe7FvEPP+8trABzgSyh4HQxbO/usNFS8SwaQ3oFIXMpBi7a7mNapOFTQfK2H6o4riMnyNTNUiOe9zdT72Fqm9NFBZC9f509i8/A+Oon9BHmYk1HiVr9Dr2Lz8Da12kvVwoBJP6wYZer05rjB0b2jG1YLPBGYpDubUvdNjWHMnorP7HPVNioNEbewVHvM5HZrOcUawgl6EUoqZrsLVhHneSenp7/vF2JXHx6sUq2KfjqQcS+HNLYFd+X9cyUHLMefPgvPMa2pft281Zrw/Opbspk1CGkug1WPf5QHnbjnUi9Dp6pcfTMTGaHUcqqahTBfehyjqOVtfTMy2O9vFRx7S3lTNS4EiOa7wZVIbK4cMOsVRQoPaX6emqV084Y2vYvvZrDqSyf9lZZF31A/oYEzEdS+kydi35K07HVB4X8lRHoA5WCLj9dnX0wLlD9WXm4g9FUUcjjh7135nZpoRmM8kljk1MTRWJpUWUJ6dxKKOL6042hae1YenpPlWhavPzncf6Ny55n6gX7iSywanl9/UG/n5nIpUV2gfHvY02aE2OCcGLGH80ZbC/UPYxGmHwYPj0U99lrrlG9eJrjTFsWoqEKAODM5MoLK8lr7gas1Vgtgq2H67kcGUdvdPjjzkjZG9IgSM5bvEVeyNUduzw7W0Vzng5ziMstYeT2bf0bLKy12GIqycqtYIu1+Zi+ul0hg5NCVyZD0aNUkeybFmlbdgiDjsHWbOdW11dcMewfUS+/bb6028cHPKZzSRPV2+gQ8EeypPTKG+XTkViOxLKS9QNNoXn7jfuC63lUP2m9t07mYTJd9rX/bA6ghkTEqmr1fZ1nJKijsT4eiays9X70JxB6JoyCaXWfV55RdWaR46oyUHnzvVfft06+PBDKWzcURSFTkkxpMVFsbOokiNVao6r4moT6/eX0iM1Tk0JcQyP5kg3cclxSSDX72BwHoUIVC6UEPDeIq2uWOGaXDMiqRLjqJ+JSlGD3hkUHf83fjCXDNbeadvwNqqVkgLnn6+O2vgKxjZjhvfosr6wBQi0XQ/n82yfaoHNmzm8p5qM0m0M/eBO1wjETiy96SFWXTYegAlP30O/zT+6+kFrudlZWWp5CBjtTqSm8teDf+fgNePtqz9fEsXLT8RjMWvvLFaudEwz+aM5I+1qDfa3c6f/rOSNqfull2Dy5CCjYTdRdOVjiaKqOnYcqaLeyRYnJTaizYzmhNR/i+OIsrIyAYiysrKWboqkhVm92uYf3LhFUdQlNVVb+awsIcxm7e1ctkwIo9G1DqNRXe++TRdVJ3pdt1b0mbTCvrz59Z/CarUGdTxFCe06GI1CZGb63l9RrCI9XYgPPlCvv8/r4O2k/Sw/nnupuHvJRnH3ko3iyytvVRswc6YQCxc6DuTvxBRF3e5+EdzLK4qwKorYue43sWbnEfuSd9dEYWSfULBou1ZYRZbRquk58Hf/mwo/py8URYgpU0Jvk5a6Q3n+Fi5suutxLGEyW8T2Q+Uuz+//dheJQxW1Ld20gITSf0svKslxSagGuO4JKY1GdeQi0OiNjWC8mwJFTwZXL5lVX0by+7tnkH26I2XA7C+2M+Hd9ZRV1wc8XmO8XIRQ23nGGer/FbdEmwpWEII3b/6R669Xv7a9fvGH4EectXu7/f97+56kDjdNn+6agfHHH/HI6AmQmuo5rJadrQ5VOQeCATAa2Zv7M/ntHde3b9F+ug4bwpyZZaAoATO0q7ZEgtk1d6Jf4d/fuqWiZ/tLQvnww2o6iWDa5BxyISUFFi3yXvfixWqZUJ6/Yz3mTbgw6HWc0D6BAR0TidSrD6vZKth2qILthyqOuQSecopKclwSWuRdV9LSVPsAs1ntS7UyaZJqZ+CPxkRPFkLw9sodzP7C0fEbU2KZffOp9M9K9nnMcFwTG3rMWJxM/LLYx2weJFtZ7nueLpR5Q0XBqig89OGP1BoiSSg9yv/dcV5w0XWWLXNtj7c5urQ09i37jLzMHvZVvfP+IMNgtc/PeNtNwYLAcYPU6zCJbOUTdYWPa9Eaome7T42ddZaa2SKYNvm4lFx/vXp+qanqx4HN5iaUoH9Goyr0pQ1OcJgsVnYccdjmgJrXqm+HBBKjG5/MN9zIZJsBkAJHYiOQPYBWFCV42xNQ8w/16uVqvxBK+Hp/tgff/XGIRz/8jbJqNdBXhF7H1OwBjD2zi1fDwo8+Ck6o+Uf9EpzEbEbxKUPJVW1o/PXMoSisBkOe19NPZMsh1f5o+sQr6Xhgj7b93dvjw/I8/5a72TX1afvvPZ6ZhnHBW+ovThbkliU55I59jUI6kkEhZ7GWdZzt4l5utyXycy1aOrO3N4JtkxYjfttz3xhmzoQnn2xcHccrQggOV6q2OZaGG6UA3VJjMSbFtCoD5FD6b+lFJTku0eIKPXOm+nX56qu+6xFC9YTJzIQDB7SLJXcvpHHjtGdLdsbXVJvFAuJwB27uNYxP9/1KXkkJJouVGUt+Z/2uozwxeiBJsa5ZtMM7zK9DwcoyxvAiUxydur9AKloD8oH66b9okX2uq9cXP7CFWAD+6n+qdoHj3J6hQ73O0RWMv81F3HR7/imHuLG1e8wYWLwY/eQHGY7rTfTmXu5xbLdr0ZTu2t7QYsgcTJu0Tnc2VtyA+qHgrd7m9D5rqyiKQoeEaBKjI9h2qIKKOjMC2H20mtIaE33aJxChb7uWLFLgSI5bAsUaGTUKOnYMXE9+Ptx8MyxYEFo78vPVbMmhcOiQKoycX+Ku0wKxoDub7hf/QWSv3QB8/lsBP+8sYubYExnR33GCgQL82dAafM05d5RHJ2/rBe1uU+1h7VrtJ370qHqyDb1Wr/JDYFCTcG4bdAbnfrNUe12gTlNt3OihMI+OOJ+dT/zD/nuX2c/R+e3XXPcVQr0o997rNaigBZ1HkEAXjzAvyiEod+1G9ubeppGMRjVwoy00UEaG9kjdGRnw7LNNm47B/XjO+DqfcIVpOBaJidBzUmYSe4qr2V+qphoprjbxa34p/VrplJUmmsTcuZUivagk3jCbVWcbZ6cbIYL3tEpN1e5NFY5Fr/f0ZPHlhaIoQiT0KhAnPfS5i5fVYx/+Jsqq6+3XIhQPmoAeLlzruXLmzOAr8uM6Y/l2tXj4nW/F3Us2ionvrxN1kVGNvsBV3XqI3A277N4mux6aKqxB1rGMq4SRfa73iX1iGVc5Vqxe7fWZNBr9O35lZQlhXqzNzcrXMx6M11xmpvp8B2rT4sXN9zfg7pXo63xsz29Tep8dKxytqhNrdxfZn/vvdh4RBaXVQXljNgWh9N80YXtaHVLgSIJh4cLgXra2l+gttzTfC979+IG2d+5VLe548wcXkXPu9K/Emq0H7eftzTU5K8vROdg6y2nTtLVrNcNcGxEuFegsDMxm8e+HX7C7i/9+8tBG1W2Kjxc/f7nW/pLf8to7IYkb1XXc1X1cwSIULGIZ2X7jBvgSm7Zl5jVbhRm95wa33tyXq/mSJY3XmO6HDGedWo7pLFhsotBf+WDDNByv1JrM4rf8Ehd38m2HyoXZ0nIiJ5T+WxoZSyQ+CMXmVVFUe5zqatcIwOEk2Pw87nz7raA4dj/PfbKFylqzff3Fgzsx5fL+ZLSL0TTrETBwW0PuqDy6OQyMQVtUxEDYAvM5NWrTsi95U6jzFWetymH8m0+FVLXQ6dg6dwFHR14MQNyffzD46kvQV1cF3jktDY4exSIUurKHfDLxltPYfm0W/4L+at/zJoFSiRjZzxwmekZ4bjBgXvpiHldf4zld1dhnyB1b0MaUlPB54t18M1xwAfz5J7z+umt2c/cgkdA0htnHsy2PVQh2H62ioKzWvi4+Uk//jESiWyAwYCj9d9u1HpJImphQMmQLoXZG4RY3N9/sCOPS2I7p4EGF0UM68+kjIzinjyNP0383HOCSWd/y+pd/Um8xM3y4avzsK2aNzVAbPDNf2+LguOSOCjZokD9eftmjUX2vvICohmP9etaF1MTGqxuysmDKFE03UwA7n5xlFzeGslL633tzYHGjKOpxGvIK5HIu+WTh6xVrt09K928Ukp2tukDPnOl9ewGZjGEpOVzldgDBkv2nc+04H8cPo7i5+WY1snF2thphO1ysXKkmb33iCfj4Y5g2TV1WrvSeFT3chtk5OaqAHzHCNaRSU8Ufam3oFIWeafH0bZ+AruHvu7Lewm/5pZTVmlq2cRqRAkdyXOIcfGzNGu/eHLYOvDV4Si5YED7RZDPKzGgXw9t3nsGz155EuzjVo6rWZOGfX/3JJbO+5T+/5mO1+u8JfQaFy1JYukSQvfoBNQrh6tVqr+TN5SUU3IPwAZF6Had3VwVbXUwcP7y71HHc5593REW8/36f1ebffh8HbrgVAMVkot+E24jZm+e/LbYHZPZsuPpqWLqUwpT+mk7Dnxec8/M5b573cqLhFT6J2VicXuc5XMVYFmOxNv3Du2CBaow/YwZ88EH46s3PV42Vu3aFkSPhmWfU5eabvQupcObRaqkgi62R9glRDM5MJtqgPl8mi2BTQRmHKmoD7NnyyCkqSZukMUPHwXpZLF0K114bHpfWlsbLzA4A5TUm/vnln3z0vzzMTqJmUJd2PDaqP4O7+U/cqfl+hCua4LRp0K+fx8EKS6p4atVOALobzEy5/CTN8XYqT+jHr59+ay/f5+F76fDJEs9j26LSOf8+d64jvDSwZpWFESMDP5DepktCzXC/muEM5zss6PxOj7V1bHrSOUaiTRCOHev7Q0BrcMTWEGSxNWKyWPnjYAWlTqM3WckxdEuJbZZ4OTIXVQCkkfGxQWPy84TiZRGuvFWtYZkyxf/12XWwXNz19o8uRsh9Jq0Qt72xTvyyqyik++VCIPegUBa35FwzX1oq7l6yUdyz6DdRfkI/z5vqpQ1WRREbFn5qN6jcPekx78dKTRXi44+FSEvz+wBq9oJyM3gNNRcYOLzVVjOsxZ8zf8vo0doN1H0tztdPS+qyYLyotP69e3F+O+axWK3iz8MVLsbHWwvLhKUZjI9lLirJMU9jho79BR+zrZs0yXOkJlzB1JoaLR9RH3/sfySqe4cE3rxjCPPuOoMeHRLs69f+eYQbXlvLza+v4+edRQhvF1EL/gx3QqWgAEaPVpf8fAb9qsbcETodv3Q/0fPB8NKGwrE3UHb6mQBE78ujy9yXvR/r6FF1OM/Z4tXWBqfj+LVPcprRch4BaEwuMIAM1Ae1kE6hVdBMnHMO9OmjDnyF+ggIocZIfPZZbanLjEbfGULcae4gi20JnaLQKy2OHqlx9nVHqurZVFiGySlLeWtBChxJmyGQQBEC7rwTVq3y3onn5vp/EdpemmvWuK7XOrefnKytXFPhxSzFA63JPs/p055Ppgzj6WtOJCs11r7+p51F3PT6OsY/8wXfbyl0FTpaDJtA7WUWLYKEBO/bg8XtgTgt9wtHe8+9TN1+xx2uD4bNeCgtjbr2Hdj96HT7Pr2emIKuPnByUq9tcFLI/pJWeutsAz2fvlCwksU+hqLe2Ix0c4A9HKSmNr+N2YMPwg03qLN8oYo5Gy+84L+OlBTVKHnnTvX/gR5NCK8tz7GIoigYk2MY0DHRbnxcXmtmQ0EZtabWNY8vBY6kVaClb9TSARw9qhokevN20PrFNXas677uH+veSEpSP+xbgtRU9SUeKIGnDa3XwaDXMeaMLnzx+N+Y1dtCl4pD9m2/Flu46531ZD/xKct/3kfNkiBcTnJy4J57oLxcW0OCJHP/LrJ2/wHA3p79KTR2Vw0z3B+M7GzEK6+w88nnsCQmAdBh2cekrPWRWiEQNoXspCBtXlC2jO82m2dvIwmhjAgoivrP7Jnl6Bd+AKtXMzT/I4xG/8JFr4clS+Dtt53qcatXUcKnQZuKykr/24uL1QDZPXpo94ayeU/6u37p6eqgXSCxdCyTGhfJSZ2SiGjISl5jsrChoJTKOu0Cu8lpsgmzVoi0wWmdaLWpCSbwnvOce7CB6Wz7z5wpxAcfCJGYGF4bhKSk8NXlbFfQZLYDDYYhZkUn/nPCueLS8f/0sNE57Z6F4unhd4gt6d0dAfG8GT40xsgkiGXVJdfZg/4tuekhn8YYBQuX2m0J1v70h6hPbtf44ztFWA6GUGy9nAMwerllPi/14sWuZd3//lJT1ed/8eJmuV3NvviyybG9KyZN0l6XVvu/Y5WaerP4aW+x/e8od3eRKHWKjh4uZKC/AITDi0oI0SiLcavVitlsxmKxoCgKiqIQERGBTnd8Dqb5yjjszVMiWAccRVGHpaOjg8vj2JRkZanD5evWOVIw3XRTaO3T69URr6uvVn8PGHgvFO8PLy4lVhRW9RjCv04bze8de3vs0vvIHq76YxWX/fkdaTXljoOCf/eUMFKZkMzf3/wSU2Q00dWV/OPuC4mpaYhl03AhSt/8F793H4iIUPPs9JtwG+n//bTxB9cSSc6L25kFfcD7l5mpumUfPhzYe9CbN5a3AHm25jz7rGo35OyF1JhEsK0db4nk3a+Xlmzn3t5Vxxsmi5XNheVUNIze6BTo3zGRFLeEvo0hlP5bChwN/PDDD0yfPp2ff/4Zk8nEwIEDefDBB7nmmmuCOv6RI0d47rnn+PTTT9m7dy9RUVF069aNUaNG8fTTTweuIEw43/JgxZpNnFmtVoQQ6HQ6DAYDBkPweVuDdccM1IG3Zvy9BKdMgRdfDK1e977UJhjB9RqF/BL2oyoFsKljbxYPvIj/9j6H2ogol+16q4Vz9vzG5du/Y8TrM4k1KOELc6uBD+56krUj1ZO9ev7z/O2LhfZtlqhofl7zK/XpagZJ479ep8dzMxp3QK0K0k+cghyyw3r/tLrvB/rQWLxYtfEqLIQdOzyFUFtm9Wr1XHydvxDwwAOqyPOSS9Ve7nh0HXfGYhVsPVhOSY3qRq4AfTok0D4+yv+OGpECJwChXKBvvvmGiy++mMTERMaNG0d8fDxLly5lz549zJo1i0ceeURTPX/++ScjRoyguLiYSy65hMGDB1NbW8tff/1FQUEB69ata8ypaeKHH37gySefZP369ZhMJgYMGMCDDz7ItRqNR7788kveeOMNtmzZwsGDBzGbzWRmZnLmmWfy6KOPMmjQoKDaE0podV8deGvH15fzkiXqF3Ko8/gLF6r7OxPMl3tAPvoIrrsuYEbs8qg4vux1Nsv7ncfGTn08qonVCc5MsjBs8ZsM2/Mr7auavnc8YOzB068sAyD1cAGPPXod8ZVlABy+7Cq2zVYNUJLXfc+gW8aiBHMTFAWhKNChA6JTJ0Rmprrcey/ihBNACDWWs+2njepqRGkpKAqKyQT19VBXB/X1KDU10KMH++s78s1KhX17FYqLIT9fod6k8MAEhcsuC/6jJBChxH2xjfjMng0lJWFtTrPz3nvw97/7P/+0NN/ixplg0kAci1iFYNuhCoqqHEb6J6TH0zExutF1S4ETgGAvUF1dHb1796akpISffvqJvn37AlBRUcGQIUPIy8tj27ZtdO3aNWA9p5xyCoWFhXzzzTecfPLJ4TidoPj666+5+OKLSU5OZty4ccTFxQUl1IQQTJs2jaVLl3L22WeTmZlJREQEW7Zs4dNPP8VqtfLll1/yt7/9TXObGvrOgLh34qEGQmsJ7r1XDd6bnq5OLzh/QefkqJ7NjcHXCzVsOXSeeoqc6RuZyJyG1AMqPnMgAXnJnVjRdwSf9h1BYWK6x3aAnkV7OXP/Js7c9zunFWwhvr7Gs1BmJtTWqp/X/l5TRqPqJTV9usem16a+zh8nnQ1A2sH9THz6LtIOH+D3BYspOUdV1ydeP4rkn3x/YIjUVKz9+yN69cLauzeib19Ejx6Izp0hhJHLxmKb2tYpCopOp/5sWEJB64fGypVw3nmu61atUm23W5KEBEhMDH0aOjExfPbu3j44jjeEEPx1pJKDFXX2db3T48lopMg5LgL9vf7666Jr164iKipKnHzyyeL777/XvG+wRkqff/65UBRF3H333R7bPvroI6EoinjiiScC1rNgwQKhKIp4/vnnhRBCWCwWYbFYmi39fG1trcjKyhKJiYli27Zt9vXl5eWib9++IiYmRuTl5QWsx2KxeF3//fffC0VRxGmnnRZUuxpjFGs2C/H110IkJLS8waK/xVc8OLNZiMzMxtXd5JmRly3TlhFb7yWjNQgLivjJOEA8ecUUcebULzwMk21L/wdyxDXX/J94bugt4oteZ4v8hHRhnTHTEcXNb0rthnI+IusdMHYXU+atshscv/nwS6ImI1Os+euQWLPziPjx2/XC6hzwLz5emC+4QNQ/+qio/fhjUfPnn6K6pqZxS2Wl+rO8XFTv3i1q/vpL1GzfLqp37xbV+fmi+sgRUV1W1ujj1NTWirq6OmEymYJ6v2g13k9JaZzhf1MtqalC1NWp74n772/ZthyPwf+8YbVaxY4jrgEBD5TVNKrOUIyMm//zoxEsWrSISZMmMXfuXM4++2zeeustLr74Yv744w86d+4c9uOtWbMGnU7HBRdc4LHt/PPPByA3QFARIQSLFy/GYDAwatQo/vzzTz7//HPKysro1asXl1xyCSm2LIpNxKpVq8jPz+fuu++mTx/H9EFCQgJPPvkk1113HfPnz2emr4x+DfgyhD777LOJj49n//79QbXL5o4ZyCh26FDPbStWqKM4FRVBHbLZ8YwHJxgzBq4eCwUHUCeqbZMYHq663i6KY93zL0GtGWiInG62CNatg4MHoUNHOPNM0OnU8kLgmCppuNjC8V9sEym2VzVWC5ZHpjMpbgV6akBxu/eKQMHKg/wfp97RH/2/3kY02CsIpSHVpqLQvqqYm68+gxuTItn86At81+1U/tdlMJVRjkBhFp2eTRl92JThNLVVChlPfEGfKOj36JvcsOglkvP+cmz3Nuf28suqj78TGfm7eezx65n13EIqk1LYMvgc9tUdgoZnuWPOxxAZifXEEzHfeCOWa66B+HjP6+5ORQXKrl0o+fkoBw6gFBSoPw8eRCkpgdJSlNJSKC9HMZtV45m6Or9DlsJggORkxLvvIi69FIRACNFwn4TL4nV/IbAIgcXqmDrUKQo6nc6+eBvl0RrPxWansnQpjBqljhD+8Ye2fZuSo0dVo33bSOY//9k0x0lPV/+eg31XHY8oimIPBmjLRv7XkUqEEHRKimm+dghffy2tkCFDhnDyySfzxhtv2Nf17duXK6+8klmzZgXc3zbEdcPL3xAZE9fw0nc6/YZOwLZmzduPs3/zWi599F2SO/VwqctqtbL08cvQR0Ry1VPLbbu7vHwEIKxWPp0xmvqqMgZdfg+bPp2LouhAUbBaTERExzHkxpl06H2qoy1OP1zqc93ssg6nDsp9321fvMWetUs5adx0OvQ/x6V8XWUZq2eNoV3XQZx220v2ut0fC+HSLoGwWhECrMJKye7f2PzhE6T2Op2B1z3lVIfzvq7Xxbaqvh7Ky3Dq3F2PG58ADU4u9naZTFBlS+7sbT9FteJ37sC9lbEZEHrd7lwuwHaf21y2SxpDNBZu660nITUJEROLSE9XbWBQr7w1Px/x+2ZEXR1WRYfFEEFtTCx1MXHURsdSlO6I7nuOMR5jehIIwUl1RUR27ex7/q6iAt2mTSibNqHbvh3lr7/Q7dgBhYXab62t95s/X9t8TgBDDmehY7X9bDD6D4ROUdDp9egbxI6iKEEZ79s8E2NiWtcUsW1qKJA9USjYbp+zfna+TtKLyjdCCHYfrSa/zDEF3Ss9nk4hTFeFMkXVZkZw6uvr+fXXX3nsscdc1l9wwQVBG+j+tKMIfVR1wHJFxaUIYNuheiJqjrpsE8KKVR+Nqbqc3/J8G0wKYaW2sgyElY2fziXtjHG0G3Qxit5A+fbvOfTdv1g3/wm63/Q6hrimGck5cmAvAoUSJZmaw66RsYTQoYuMo+JIPvuKqnzU4Erlnt+o3rcJYTVRX3KAqn2biEzOIOnMmzhYGnyGWcXPs15VD3gJKqsE8D60vX8CdUJSf7QNatHzQ3RHEnVRUAfkl7mViIeBZ2qqy6JXFXNStIFIYzfXjRUV6D/9FJ1ej+4f/0DZuROlsd+AQqgBACH0IUuXYg57G2dZJpzEjtVqtYsfZ6xCYDWbMTfUo9fp0On1zJmjMGZM4L8GIdQREy14fkA0HbZRKFuKDG8eUaHgnFbDFpnamwNcSAb8xwGKotA9NRZFgf2lqsjZcaQSvQIdEhpveByINiNwioqKsFgsdOjQwWV9hw4dOHjwoNd96urqqKtzGDqVhzVyqhJEiHP1Ly2x19mknX61fW3ywAswVRzh6PqllG5ZSdqQsb4q0NYiH+2xNhhwGqLj0OlcCwmhoI+KxVxTjk6nODp8H3UpKNQf2kHJps/tB4xKycR4yWRi0rMC7m+rAz/lPFa5T924/+Z3u+cGxX2F121B7OtczN95+7k4gZ4l1109roD/6+jluIrXgt62BThjX9dNUYIor7gfCoDaWjOV1fVYrYJe3VNITAje3VQBDHqFCJ2OKIOOzu1i6J4aQ0qMgczESKiqQtm+Hd1ff6HLzUW/ZAlKVRXMmKH6Q4eTw4cdva977+8rQVUQ2I2PnaaSRcOUlU30uI4IC8wWC1gsXHwJbN2qZ8IEPatWKYRD+mdmqgNWCxY0uiq/6PVw1lmO37Oz4eGH4aWXwGm2ThPuieLdxUt2tmN6rtEG/McJiqLQLSUWAeQ3iJzthyvRKQrpYXIh90WbETg23OeQhfAdeG/WrFle7Uqu/Fs3omLj0aEQF6Ej0mB7ITheyooO3vmtI1sKtjLm7A5k9ujjyGuhqMd9bH49cclJPJDdv6Ft9s3234UQPLQgkdrKCq6+5krOuHCAvb0KsLPv1Ty3fgldoo9y3zUDcdYfarh0hbJaC3UWx4vJlwDwdhUUYO53cew6oDBuZC9SOma6lLdarTz9UQTVJj13jurnWo/Xjk+BUU8DatyeA7v/5Mv5r7Lpw4e47tFZnHHJGJ9t9NapefvNr0jwWq8PseHjF3+vbsVHx+x+TbxdI6+ddoCyHiHy3Qq5t8WbOFS81uOJ4/n0fgVctK/7s+xNHDkpK49n360RittPbx8Ivo5vu5K2Ou2/K479lIbf1WuhoFMgSq9g0OvQNwh3g04hKUpPoqUKXfVBlP3FKJWlKHk7UTZuQrfqa5TKSvVk/vlPaNcuvD7QGRnq1FMzDgMoioJBr7f3wFar1UXwONO1m4X/fGZh926FJYt1vPeenry84AOQ3nAD9OwJ8+Y1vbgBdVrK2QYnJ0eNKxXMCI5t8Mw5CKcv8aLX+51BlHhBURS6p8RitQoOlKuj/NsOVaBTFFLjwhcM0J02I3DS0tLQ6/UeozWHDx/2GNWx8fjjjzN58mT77+Xl5WRlZXHHKR1ISFDn8GINEGH7G3b7g9h/Sl+2/G8lgwxFXHWS0/SRgKPFxTxQUcbQQWdx88AktyO7VvRe3978tP4XLjohjcv6uhowbrGm8RzQPsLMuN6xeKPOKrA4V+liSuL7r9g2tP5tRjt2bxJc0L6eAX1icDZUFULwVG0lqckJ3HyCbcjQtU43SelapNcA/n7h2wy78FJyXpnOM9dfQGqq51Sb73a62kB573qFl3bYT8Bju+LlHBDehJOPNvl9M3rWq6lO520+i3jZIPxsa656wfs18bvO9b56W+dhlRWoR3KxlHatzvP5sqsd9YNCAMKCUlOOYnWKd6PXQf/eiP69sVxzObqZs9F9/Z22oCdu+I0VlJ7umHpqwWEAm7ExOEZ3bIE7bXTvLnj0MQuPPmbhm290vPMvPZ9/rsNi0Taq88EHTdJ0v9jyeIWSkd158CwyUoqXpkJRFHqmxWERgkMVdQjgj0PlDMpIIikmIuD+odBmBE5kZCSnnHIK33zzDVdddZV9/TfffMOoUaO87hMVFUVUlOcQWNSqD4mOVTtzC27iAbC9fYcYVKvvL+e/ycW6Ipw7kv+u2wDAGWnR1H76juvLWtg+PwWgMLRjAj9Yrfy27H3+Zj7gcqRfG+rJoobqz/7t6KW9/YE6b2v4vxCAohDT/1Siuvd120F9kfXr1YPP/ys4sG8PZ53sGozvaHExZWXlnDRgAAnRTkra3xwGeKiFYUPP5vetf7Dtrx1cNPI8r0Mt1Vt+oi5vu5OBjAIIKirg0CEFk0mdMhMIIiKgYweFxETnw6v/2bYN6k3qdRBC8TC+NhgUBg7yHEVw/qVqVx7mykr7GLbL5bYK13XCRwdsX+1Uh7uFtVN9dkNwj/VuhtheDMudt3m2y/bD2zEd5YRwKdywzsu5ee0chO9OQ/j8xa193vb1XumO0nLWFR6mymzmlPRUeiYnkhrtYzjbaUTU9o+iU9AZdCh6HXGYicWKNb0ddOqIPjYOfWwscXsPo3TrDL26qfXERGN97jFE/17oXlugej9pJIer/McKuv56VwHTCoYBbKM7Br1eFTsWCwcKraSkWO1NPf98K+efb+XAAVgwX8877xgoLGz89FW4OXRIjat16FDwBsbShqb5UBSFE9LjsVoFR6rqsQrYcrCckzolERcVfjnSpryoFi1axPjx43nzzTc588wzefvtt5k3bx5bt26lS5cuAfe3WWGXHDpIYpLNCltx7UBtUxSKgtlspk//ARQWFrLuf7mceOJJAJSVl3HmWWezZ88etm7dSrdu3VAUhSNHjnD48GHat29PerojwNm+ffvo27cvcXFx/Pzzz2RlZaEoCkePHuXcc89l165drFmzhrOcJ5LDyJdffskll1zC7bffztu29MENfPjhh4wfP57p06cz3UugNC0IIcjOzubTTz9lzZo1DA3CVzKYXFTu+6jH1raPO6byCrY8+gRHv1/rsa0s9UT+uWks1VbbiJripFsUp3W2sSLnMSPFpZxwKmfDfT9HedexJ4HzOvd6nfHWFsVeqENHhS++AJ3eNp0UaK7MKWicooDVgvXH9fyyooAj1XGkx1Ry6n+edIxO+BBEAGR0Qvn+e3WkxKXJCnz5Jcycqfq1A5VC8Kmw8qbVym9CENlQSz1wlqIwXafnHJ0OqxDoFNdrLhqEzY4OmRQltLMfJrauhiG7t6FvEKHlI0+ncpgaaDPlvc+J3rEf0S4J68RbEVc4hYMoPIzu/WUon3yFUttgxxcdrbr9uU3t5HAVY1ja0BbHeSoN12cpY8he/UCLC5pAPPIIvPACdOwouO56C7fdZqFbN9ebW18PixfreO1VA5s3t478eVryRbkzbRr06ydtaFoKqxBsKXSkdYjU6xicmUR0hO8bcdwE+uvSpYuIjIwUJ598svjuu+807xtKoKDVq1eLqKgokZCQIO644w7x4IMPiq5duwqDwWAP3Gdj2rRpQlEUMX36dI963nrrLREZGSmSkpLEDTfcIG655RaRkZEhIiMjxeTJkzW3JxRMJpPo3r27iImJERs3brSvLykpEX369BExMTFi9+7d9vWHDh0Sv//+uzh8+LBLPV999ZWora21BxIzm82irq5OLFu2TCiKIjp16iTq67VnkbXFZsNH0CxF8R3MzlsGZF+Zlf1RU3hQbH3iafHNgNPFN/1OtS9Le58nxqW9K+J05T7b15glNbX5sjQ3OtOxt4sdzOIv+pnZLMTMmaISxJMgUkCcCuJFEF+C+B7ETaghds4EUR/gWDvSO9nv4Y/d+op6nWsQwspT+oiCp+8WBU/fLSpP62dfbwVhufpSYfrpP8L0238dy6qPhWXMpY4M6W6LGZ0wsk+4B0K0P8NYRJY+X5jrmjIiY+NZvNjb359VjBxpFh8vqhMVlZ6BBT/7vE6ce65ZqMOFzfMsh2uxPZK27OELF6o/mzRwpsQFk8Uift1fYg8E+PPeYlFv9h5MVojQ+m/C0dC2QigXSAgh1q9fLy699FKRkpIi4uPjxZlnnikWL17sUe6pp54S0dHR4tlnn/Vaz9dffy0uvPBCkZaWJtq1ayeGDRsmFi5cGNK5BMvq1atFZGSkSEhIELfffrt48MEHRZcuXYTBYBAvvPCCS9mpU6d6FWqJiYmic+fO4sorrxT33XefuOWWW8Spp54qFEURKSkp4ssvvwyyTY3rH8P5cqrcnSc2P/qEh9D55IRh4u4OL4mOEflhFx0TJzb9i3zmzNCviRDCEUm4MY3w94w3qNzDIGJB3AjiTy91nIUqcr5r+N2X4Kg1RIg1vU8S2ztkiXpjloeSrOlhtAucsvOHeOxv7dNDmOfMcBU5v/1XmB+/X1j1Oo/yqxnWqGe4NWA2e0bcdl8yM61i5sx6kV/gKXS++rpWnHW2d4HXlIuPANp+F+ePJm+63RZpXNI81Jkt4qe9R+0iZ0N+ibD4iMAtBU4AQhU4xwpahdrTTz8tYmJixD/+8Q+X9W+99ZYYPXq06NWrl0hOThYpKSli8ODB4qGHHhJ79uwJuj1aw7w3kwYUQjiEztf9XYXOl31PF08ap4j+MRtEY75YExIcL1CtAi/UxWhs5BdpoCE2rYs/lbV6tV2svAei2mk/K4jahv//HVXgLNRwPMu99zoUr1uqB1Nakl3gFI85z2cd1l5dhfkfj7oKnQ9fFda+vVzKLeTaVvcMB0swz2FsrFXceadJ/L651kPorPi0TpxyinahE6pubtdOiBdfDH4/22OwbJlv3e5cRtI8VNebxdq8IrvI+eNgudc0I6H0323KBqexhDSHJ2kyQskm3lxU793PmukfwM+fE6nUuWz7s6Yv/ykew5ryC6gTwQWrMhphzx6H3UCgqKs6XfCxPMIWWVXrDfLBZiAW6OF80u54ybhqxdmSRbXBuQlYCuwEugQ6sPsD45Sd1Rph4OCTtwMQmXeAtHc/dZTzknXReunfsD45yRFO22JBWfwZurnvoVRVs4ZhjGBNoBa16izTWpPeOqPTCcZcbeXxx82ccIJrF/Kf/+iYOcPAH380jY3ONdeoTmjBtjk1Fd5+W9032OzpkqalvNbEpgNldt+ILu1i6JoS51omhP67dViJSY5LbLmofMW9URQ15VBL5HeJ7ZLFJQseZ8T3n6G7/B4s8Wn2bSfEbOPhzKf5qPclTOj4f/SK3gZ+3a0d5Oer3sHgiLrqD6tVQIOpsi/cU4QZjWEKG2/zvQ2Cv4C7gFTgDGAkcGd+Pr/7CojilghJ4PlSMgFrgMFAwFjfSUmwZInqFlPfEAI7Oxt27YK0NHQmM0q1GofDkuQUskFRHCLG6YHUff4t+lsfhh156gq9HjFuFJalbyH69GAouRjZbzcodqfFnmGLRRWoH32k/vRjhas1F5UzVqvC4kV6Tj0lkjtuj2DPHsc1u/xyKz+vr+f1102kpYX/+/mbb6B9++D3i4lxeOf7+6gQQg08HSDNoCSMJEZH0Ld9gv33vSU1HKoIPiq+O1LgSFoM5w7eI1CdU2yKlvyKiklL5m/P3cr5uZ/Sf9ZMEvo5JSvVV3BFylLmdr+RN7tfz9jU90g3eI+q7Yyzbhg1Sv2y9IWCIJEyknFNB+IsaqxWNczKpEnqSEFeXphcXoPs+UqBh4APgdHAZGAQ8C/g5meeYfv27Z472VRuA86Pga1r/BA4BNwBJBCAsjKYOxcefBBiY1XXIFCjtzVkPtVXqClJLAlOcaeEUHMQzJyphuB1QimrRj9hBrrZ70BNw0u3QxqWt59HOetk5jBJLeeWHDWsz3AQgoWcHHWIYsQIdZhjxAj195wcr8XdbkGQzVL48EM9Jw6K5IEJBg40RMHQ6eCWWy1s+r2Ou+8xo9eHT+gUN/wpODmqaiI/H157Tc2UoYUQ9L2kEaTFR9E91fE3+eeRSsprTY2rNHwzaa2f490Gp7USLo+o5mDZUqsY2nmzmNJpuvhPn7Nd7HRsy8tdbxeXt1sskvTFXm0BnA1OZ84Mzo4gnnKh2gC52gE1ie2A2SxEZqZ/o4b0dPvvrzXYyTwPotyp3HMg9DqdGDVqlKioqHDUbbMQv+UWn8coB3FCw1IyaZIQiYnBG19MmeJi8HXk1ivsdjhWg5ul6sKF3q3XG4w2rJ06CNM7L7oaIE+6TeQ8uq7pnuFgrGFDNC4Jhy05CBEdbRWTHjSJwoOu9jk//Vwrzg6jIfLChUJMmhTavoEMqr39nUqaB6vVKv48XGG3x1mXd1TUmlRDQmmDEwBpg9N6sVhaf36XnBwYPdrxe6yukmGJ33BR8n/oF7vZo7xF6NhcPZh1FcNYVzGMQ6YMUuPrOVQahV7vWZ82bH+unvN6YbcdyMmBO+/0nl3RNjyxeDE8+CAiP5/Tge0YmM0V9OAoZ/M9BgRKVhYX9e3L1998w8cff8xYgwHLAw+g1/Ap/T6q/c2/gFsTErBWVAQ/7KzXwxdfwIUXAlB83YXU9lWD+3V4/j30FU6Jd/0ZyzTY8oiiI1ifeQTxN6e4VbFJkHUi//s5NrzPcDCBogIZdQV4QJxMleykp8O996oDW8HQoYPgqafMjL/RdaTp/ff0TJ1qoKioccECV69WfzbCRMwn0ganZbEKwe8HyiirVQNtJkQZOKlTEpWVFUH331LgSFodTSl2Qq3bYoEOHXxnUu4UsZ/hSV9zftIXGKP2eS2TV9uDnyvP4rLJp3Lx/SfTo0900FFXtRAWg1ZfHasNm8Vmdjbk5LBs9AeMYzsmIoFPgc4Y2c/LTOTqZTfwRXQ0l112GSMHDeLr339Xg/X5ObwVMAP9gUhgNeBsdlELBGXe/eKL6lxRQQElVw6j5mR1qjH91UVEHCnR3qs1PECisBDRJR1rrMVxjfQG9D1PQ4lNCqZl/o8VjGAJg9W+t78Pi0WdtWuY4dNMbCz0H2Dl5ZdNnHKK4zk6cgQmTozgk+Wh/VGnpKh5SyGwkX4gfOU8ddeNrf3j61ij3mLlt/xS6syqbVuHhCgyoqwkJydLI2NJ2yVI84Fmq3vNGt/iBuCAKYuFRbfRcVcZj+x6nY+LbqSgLsulTLfoXVyT9j5x703k+zP/xkT93VyX9g79YjYR4eap1Rj82Q5oMuXQktDHZrEJ5JDNGP6NiXTAcR4FZHINy8ghm/PPP59evXqxctMmShrEja/aBeqL6QNgF/A4qrgpBTYC7wATfbfMO3v22A2+dPWOeX0RGRGcsUxDigVl3Dh0Z41E3/sMiGqwG7CYsexcj6gsCbZ13gnWGlar0YifcrYMEuPGqT9XrIAePYIXNwADBsD6n3UMOzeSiQ8YKC1V16enw8KFJv79Xj2pqcF/X0+cqLbTZsOnJiUOvn3guZ+7gX5Tvo8kvonU6xjQMdGegPdQRR0Hy0MwOm6CabRWi7TBad00ZWyKxtY9bZq2efsbWOD0u1V0idoprkt7R8zpeov4qu9pXm12vul3qvi8z5ni1a43i7s7vCSGJX4lOkQUiFDj7fiyHdBsyhFEBEZHqByrgCsERAjY5HJ9jUaLMJuFuOeKK4QC4sOGjWY/dVeA6AwiE8R/QbwDYjyILqh2PgqIzcFclFdesV+E0jHn221wajt3bLSxjNVsEqa/fnTY5Wz4UliO5odcn51gA0U1NnKmG8uWmIXixd5L6+JuLtWhg1Us/KjOxTYnL69GXHSRWXOdqamesZ0aG2gbVHse92ChMlZOy3OootZuj/PFht0i2P5bTlFJWgWNNB9o8rqfeAKeeSbwsa5kGZ/g3bAmSV/C4Lj1DI77mRGZ64mpPuC1nI0KSwJ5tT3ZVdubvLoe7KvrTkG9kVJLCsHa4ASV80trYJSFC1mTMc5pVuR1YELDz3ucClpYvVpP6aeTyX7lFe4HXgXnnLEezAXuB4wNgYDygXbAKOAWICiva70eqqvVVNFA+cocKn/8BoDUgSOIumx0o+cchMWMNW8DosIxzKe074auU2/X3F/OBJr7CHbKyfagFxR43mgI6o/IsiSHruPOIN/SkVAG+n3Hb1Lj57zyisnFe/Ddd/Q88oiB6mrfQzGK4jv8gfOlbN8eNm6Ehx/W1lZvl6Up30eS4NhZVElBWS1VFRVcOrh7UP13m8kmLjm2CWY0Plj7knDUPXy4NoFzTuIWPin3LnDKLO1YU34Ba8ov4NL34f5xBXQzrad/7Cb6xmymc9Rel/IJ+goGxW1gUNwGl/XVllgKTZ04UJ/FQVMnik1pFFvSKDanMX1qGtaKdijxcegM6p+3vxknIdSX9aRJ6oyTXg9kZKg2MHoDZr2een0EtRER1EVEUmuIxKrT0bdwL2RkuM12XAI8CCwHbgca4sqgp7AQzjr9dECdajKjvnwEsA/YCwxEFTEAfzb81CUlcVFJCeOBs31fdv9MnmwXNwAYnF57AweGpYdS9AZ0PU7Bmr8NUbQfAHE4D6vVgs7Y11XkWCzw7LPq/Eqxk/u/0aius/XgQ4eqxi/+jLGdg+zY5mzGjPFtXKJlGi4nh9yxr5FP6LEGLrkEPvvM2xaFpUv05H6vY+5cExdfoqqgW2+zcMaZVm64PoLt270LqhkzfIc/cE/OPny43eTK63PvjLe//6Z8H0mCo3tqHJV1ZtINMUHvKwWOpFUQBvOBJq17+HDVrtafHU5qKkx4fSCzr91PAZl4M6NVEBizFIYOhSPWTLaXZvLf0isBSNCX0jdmK31jfqdX9J90j95BesRhjzpi9dX00O+kR/ROz0a8Ct+9qv5XHxODPj6Oel08j0fEYO5qwIIei1AXgAjFRIRiwqCYWH1uOdGxAlN5BZZ+p/o8T0UI+hisKEOHkuESDK0LcBbwPbAWGN6w3kpGho4jMd2IRLWnsb149qJ6Sa1FjVR8ZcP6qQ3rT37tNXjsMf89VXo6jB+vdu7ORkU6HVx9Ncya5VreeWjBPUpiMLiNwChDh6LP6o81OgFr/h8AiKJ9WPV69J1OUPfx55mWn6+61c2cCVOnqgYwtQHsDq691lWwZGerwxzu7lBGo9rjBwqQ1KCGCzkn8PnjqaP0elVP+hY4KocOKYweHcE991qYOdNMfDz06yfI/V89D0yI4KOPPEWYyaQ2T4seddZ6WnH++2/K95EkOHSKwomdkqioCP5vVQocSatAa0y5UKKuNqZu5z7sgQdg+nTf+7/5JqxLy2bMZTuY/ZmCgtVF5CiowyWzZ6tx59z7uApLMj9Xns3PlY6xigR9Kd2jdtI1ahfGqH10itxPp4gCOkYWYFD8BHsDLDU1WGpqgCJ6a/n4KYXa0sDFhKJQ//QzROn19iBxBfmi4VzvB35GnabqikJnjOQz9LPXWXzKSdQrChlOPWI7oBPQC3CO29a+YSEzM/CoxJtvqh33rFlw111qJOOqKlXILFoEa9e6jIwIs8PIWDFEEBJLlqj+087Wtw0jMLrsbNDrse5VQweIQ3lYFB26Hzaj+PNMszF9Orz6qn81bePFF+GMM1yFS3a2I2RvsK4/DUMXGWjrub/6CrZuVQNF9+ihXpLISPXvxmj0r0vT0hTemGtg5Tc63v/AxKBBgrg4eOddE2edZeXhhw3U1TlGvp55BhYscB3k8odN6911lzYjaee//6Z8H0mCx+c0b6D9pA2OpDUQRvOBsNXtLS5Iaqpah/vMwrhxqumKc1m9TmCxOv4ws7IcH9Gh5P9xRoeZ9hGHeO2pg5wz6Ch1R4qoP1JEXVERpuISzFVVmCsqMVdWUVJYRZRSg17xndSq3hpBpDARKUwYLBYienbHEBNDxJYtRFSUE22uJ8pUT3RKO6KemEb0TTeiNFysnMUmxlyj/l9QA0xC9XW6DXiet7mOdN3XTMrKIjo6ms8mTKDnrFn2qZdywOOv0f2meLsZzhcUNBsalX6xkOrf1KGntNseJzKjc3AX/5FH4IUXvG9zMhSxHtlnH8kBUP6zCt30F33aHYVEqH8Yvux/Gh5MCzq6sod8MvFug6NaUC1erA6SecN2O8C7Lp04Ub19ANHRgpdeMnPLrQ7RvuE3hWuujSR/v+KxbzCpSOrr1Ut05Ij37f5scJrifSQJjZD67yYyfm6VSC+q1o1b4uewei0EW3cgD4qZMx2Bbhcv9h8F1puHRrgyiQdyiHE9jlXoMIkIpVZEK1UiRlcpDNQLsIp0DgozOsdJZmWpDfYW1dedV14Ry7hKGNnXcJwiAecLUEQcEWIwiBQQHRMSxJIlS9R9zGbfYZx93RR/bQmU+dzpnIpXLLB7UdUfPhDw2XFhyZLAN8V27YQQlkN5LlGPLXdeF54bH+yD4Iw/dzqnB2YJo4W3qNk+TlXzoWxOa95u//XXm0XRUVcvq9NOd42A7Px4BnPKwb5bmvJ9JAmeUPpvKXAkrYqmTNugte4g+sqgyno7hj9hpNdZhIL38PZaX/JaPY0f4OXQO8377xcChBmdWM0wsZBrxRecI95DEbeCuBDEsyD23XRT6DclEEG4SBd9/Lpd4JgrSrUfw2x2SU2hVXBYivJdRc6oC8IvcKZN8y9Cna+3P+W+ZIn9wVzNsLBoK2+61GwWolMn7/UNGGARm7fU2kVOcUmNuHqspyt5sKkUQnnU2lIamWOdUPpvaYMjaVU0xnwgXHUHG18tFG8LLQ4vkx/S8eKLAkUIBJ7D9FocYrTaCHzE9Qwjl2yWO1ZqtaDs0QMAPVaG853LpnE4GfqddJLnvuG64UFYhVprK+2/6mLi/RR2IzfX9zyHn/boUjPhz21YY9TQ89apD0BRCbq167UfOxDOLn7u3lg2tLjTTZ4ML78M11xDIZ00HTrQpXf3cALVA/6AjygJW7aowQE/XGhi2DAr0dHw73+b6NVT8I9/6LEFFwjWwDeUR60p30eSpkdGMpa0OtyjqYbzZaKl7mA8KBrjbWEzgnRLXm2Ppvr887B0qUKmUfG6XYsNgmoELFQDZz8UkcYYlpLDVY6V/tSRc0jkfv18eiPZxY1er1qgeiMcNzwIq1BrtSpwlOgYux2RJoLpUd3aoxsyAmWFGnsHgx7r839H9Oqmvb5gKChQlbN7uF2tyj09HZYuJSNNWybnUAxtA6UhKy5WuOLyCN59x3F/pj1hZt6/TBgMIuTjhvKoNeX7SNK0BCVw5s6dy8iRIxk7dizffvuty7aioiK6d+8e1sZJJC1BMB4UjfW2yM5WswisXg0LF6o/8/Ic4iXQ9kDoV+Qwp+ZOQKBmePKOzdtrErOxoHeNr+KMxQJPPaVGU7PFr7/wQjXxkD/cY9GEG5s7ly9vC0WBrCzEOedgrSoHQBebENwxtN7s9HTPa6fXoxt0Nso3DcN+MdFYXn4SkZwUXBu0YBuhmTTJ1W0+GDWenc3Qgo8xptf6FMcNl9TrYxIILQNhJpPC/fcbeOwxg92z//rrrXz8sQmjUYR0XMnxhWaB8+qrrzJlyhT69OlDVFQUl1xyCbOc4ktYLBb27t3rpwaJpG2gsa9k6NDgyvoi0BeifftYC8NZg36xv0RSTjS4sWQf/RdLGUMa/n1lBTr205lchnqf/8rJUTOOTp/u6kYGqls2eI7k6PUwZYo6HNWU2Ob8wPNmOM3pWetrEfVqvixDcipBYbvZgXj9da+f+Up2Nrpep8COPeqKzI5YXpiKMHixFLC1OTXINtpwn0eFoNW4PlLPnDejQVH8XdKQRjTS0wOXaTgSr84xcN24CHtIoEsutfLV1/UIIQLnVZMc32g11unXr5/48MMP7b+vW7dOtG/fXjzxxBNCCCEOHjwodDqddouhFkAaGUu0EowHRbN4W2hOJNWAF+vnD7hOk9Howkk/+b4g/nZUFCEyM4V48UXV8PiVV4SoqwvDyQdBAKvQuoI8u4FxyecfBqjMR/3+rsOUKQE9z6zVlcL0i8Po2Lx6hbD6arPZLMTKlaoR8d//rl7bDz7QnhzNlqdKCLWu1FT/5b0kewrGOD+Qw52NULwIzz3XLA4ecnhY/fJrrWjf3uHh5e/PwZ1g2ippHTSpF1VMTIzIy8tzWbdlyxbRoUMH8dhjj0mBIznmCMaDYtkSszCm17iUTU9XXchDxvYWnjTJ/5vfW4O89CBBecU49wArV6rCRWtPFKx7S7jx03tVb/7ZLnDK35sbWs82ZYoQer3rOet0Qjz8sHrD09ICClFrZYkwbfjK4Vl1aI/3NvsStr5c7N2XV15x1FlXF5LACXBJ/TYzCP2taRl8skXsz3eInA0ba0WHDla7vnb+oPDVZq1tlSKoddGkAicrK0t8//33Huu3bt0qOnToIMaPHy8FjuSYQ9NLruGNuYTRIp1Dml/yfgkmRbK3TsmLf7gZnTCyz7frOVbV9XxxI9MzO48atCaWLRPlV4ywC5zq/t2Dv0FaRrK8XlzPoTzL0QKH+/jGr4W1plLbsWw9eWpqgDgDbiIsBBf3xlySQCOYU6YEfwlBiNNOs4g//3KInF9/c4zk2MIn2Lzd3f8Wp0zR1tZgBZuk6WlSgTNu3DgxceJEr9u2bNki0tPTpcCRHH80vN2XcVWDcHALSoY1+GmqUDrRlStd6/AxB2Brp7vIsa1bdsX80DrwUDvI5vpMbrimR6893y5wTGlJwc0jhjrsYFu8BC4y79viEDnb1wmr1aLtWDaB49zzh2sJQqCGGgcqlEc8PV2IGTPU/3fubBHb/3SInPW/1Ir0dN8BCbVcJltbfQXulAH+WpYmFTibNm0S7777rs/tW7ZsEdOnT9d84JZAChxJWGl4u9tGRtzFjePFaBVGo6pBAvbjoXai06Z5r8fLm9o16nBD38tesYyr1KmWxnSOwYSY9faZnJbmPfRzY3C6pgcnXqsKnCdvF1bbtdEaNTEc4afdxJ/VbBKmrd857HEK/gzuWDNnel5D95GbRrYxHJfEucpgHnH3xzElxfH/Ll1cR3J+Xl8r0tL8ixwti/sMo+vfcvBRlCXhIZT+W7MX1dKlSxk/frzP7QkJCaxdu7YR5s4SSRujIa5ILkPJJwtfTolCKOTnw8iRqlf1iBHQtX0VOU9t8XT9CBSrRCt+vIqyWc4eurKa4SxkHKsZTh7d1CB/Vt+u5JrQ6lZjS1Tkfq5FRWodI0aoyYDcY7mEQsM1tUYYsKSoOWwiDhejCKFuF8LT48gb4UgdbaujIY6QsngJ+qNmbMHrxKHdiNxvAweKsdGrl2scgVdeCd2dKAS/71DiQAXziLs/js7Oe3v36rjwgkh7XQMGCJZ/Uk98vNBWuQ/8JebU+qhIWgeaBc6CBQs4/fTT2bx5s8e2t99+mwEDBmDw5u4okRyrNLy1Cwk+4lhBcQxjpvcjp8Pdrp14qJ2oe6hY8B1JEEfU4XF8zHC+Q+8nRo4mUlNh2bLAAXosFli1Cu64Q+0t/JGfD6NHq3F3GuMD3HBNTZnpdjf2iANeerFA1z4cqaMzMtT73bWrPY6Qcu556F59117EUpyHeOxR7fU5xxno0CG0doXo952x43tt5ZwuXTh0oo09e3RcdGGkPSryKacIPl5kIjKycSInEOE8B0nToVngbNmyhQEDBnDaaacxa9YsrFYr+/btY+TIkTzyyCO8/PLL/Pe//23KtkokrYuGt3YGwb/t7IH1jj6BZfRYh8gJpRNNTPT91W2LFLhyJSQEGdhOC6mpMHMmHDoUWNzYOvaRIz3j6Phj+nTo0iX00ZyGa1rfuaN9VeT+Qz7L+eSssxoXxlang88+8zpypby3FDY1ZB7vnIl17CX+6/I12hKqCEtJ8R4e2zlitXuwGYuFoW+Px8h+FB8CWcFKlltQvnDoRGd279Zx+WWR9kfqb3+z8s67JnS64EWO1iTV4T4HSRMR7DzYJ598Ijp06CBOPPFEkZiYKC688EKxb9++YKtpEaQNjiSs2G1w9H69kwLaJzDcM3tnsBaYWlw8tGTC1ut9H1tRhHZjIjdC9T5yP34oFp4N17To+otcDYyDNawIVwp4H4u1q1GYflih2uP88rmwDurj+zr4uhaNeX6C9f1uuB4BDdfHuD4roTbRfXG2xwEhLr/cIioqHTY5r7xSL/xlQve2JCT4b5u0wWk5mtQGx8aQIUMYOHAgv//+O1arlUceeYSsrKzwKy+JpLXTYOeiV6zMYRKAzy9ZfxTSUZ3YX7NGndwfM0Z9n/oKj+wN9/xD3r68x4xRMwf647LL1J/eQtcKodah18PYsdoT8/hL8hgs7ukH/B3Tdv65uYj/b+/M46Oo7///mt2cXIEcHLJBTi0e1Yq2SkVBrT2+fqsiHvWuR4t3qmL9KQXSam09ilitWq1oi4CBhKL260E0IPUCVEQ8OYUcQEIgkGuzO/P+/TGZPef4zOxsdje8n4/HPCC7szOf+czxec378z4eeSRkwZHaO+FtalHX047ruuuAigrzlLhJnpeQttfC89S/1D88Hsi/vQnk8QDFxdErmhUjM8vobEZtbbRjiZGPVOR11t0fU7EMd+IheGKufQ8U3ImHMHXppVE+VSJJp0WoqIguYbJsmQf5edkhv51fT5dx4032pjYPHlRnT83a5jR7M5MC7CiohQsXUmFhIZ1xxhn01Vdf0YwZMygnJ4duvfVWam9vt63Iehq24DBJoftNVy86SWSpwen6r6Sx0TDDhxMNGGBu4dDiXGPfvIcPJ5o9O34fsYtREpHYtthJCuK25cMqykfH8uD/3tEh683eS34U/q6oKD75ndGxJdmCQwApXg8FFj0eTgB47tlq5mK7ofR28ihpixYeLhr7PXs2EWCYIgGaBQfnh38XYXkySqSpXX5OrSiBYDBkxTnY2kFnnx203Q12knwyPUNSw8QvuOAC6tevHz322GNRn7/33nt0xBFH0Lhx4+i9994Tb20KYIHDJI3ufC7BBYuoZu4ntHCBTNXV3Q9qAzO5BJlK8S0FYRGarYVNV1cnfYANCQirLMp2koLoJB1MaDHL02IwFXbg9BNCAqd10bPqNsrL7SU8cWtuxWKRJxwbzo3z5ouk1Lzt/JosL1fnXUTPO5EtIWeZIiH2Go9RJ2bZhhMpf9LV1RUSOS0HOui992TLJM7aoqWU4kzG6UVSBc7EiRNp06ZNut91dHTQrbfeStnZ2cI7TgUscJieRn1QK8b+CdrbrdESOSC4LRSsBITTLG6xuC3MjCw4Ju1tvObnYf+bpl2JZ6jTG3ldPMbgQ/eGc+Ps/DKRi896f7HHauM6Ey7/oVkprc5hTPOdWlEURaFOvz8kcjo6O+lPfxLzx4nNmcmkB0n1wVm9ejXGjh2r+11eXh7mzZuH6upqFybNGCbz0VxA/H5gzhwJhxV2Rn3vQy2WYpqae8YMonDiDZuhGzI8WInTsQiXYCVOhywaNPnFF2rjV640T1gS2baewCpPi0GCFSU/N+R/423aj6zPvrROxmJ0bEah9z6fGk3WjeO+78Yz7zmgK6A2pWkHqKvT4hcxiPo96TmW2LjORFMkxK0n4M+kBQBG+tls22YdrAcAkiQhJzsbUvfxEREuvlgssmrPHuvtM5mBcOIaj8f6Bj3ttNMSagzD9AaqqtSxJXL89Pn6oPxHn2PciicxrHkjJmG1mnumqAjYu9d6ow0NqmOvz6c6eloMXFU4H7dhXncCwu42YCfm4TZrUXXffepSWGjdLgBYsULNbQOojsexzsdujRhE5h6eBoNmx/hRgFd9fuV9/a09Z+Hly+NzDE2dqjprr16tbmvYsLDoeuYZVNWeZL/vPZ6orHaSlAWpA6AcAKRA2bUF3hFHi7dbNJte//7As8+q53rRIvVYJk4Uvs5EUyTErScoorQUP06QJAm5OTno9PsBAEOGKvh//y+I++/PNv0dh4D3IpJnUEo/eIqKSTaWhQeXxEzsi07faCZ9oymSiMW4LpbgtFiiS1FR9DyCW8655eXmnW+wn8arzglNT/mHl6jribappMSW80XljPed9b1O+L0S8FNg/ZvdU1WvxxfjNMPOdKaek7VgJUzrAq7mPjg9QTDC6bi1rYN+/GN9p2MOAU9veiRMnGEYfcxmBbTPym73Qp40Wc06O3kysG+fecypJKlv07KsvmEXFqrxsXrWlaIiyIOKcRvmQd1d9O0dSi6IR21Pmdhi7141A7EWsj5pknoMdmKA9Rg3zvx7nf3IffPRNfowAIC3uQXZnjx1vUmTgJIS6302NgJ//atQaLosA7ctOhkECbb7vrZWvQ6068LrhZSVA8+QUaEtKLs2W7dXw44ZItaCWFcHPPxwOGWACV4omIfbAMSnSND+fhRlqrUyRXHWXq83lGXf4wH+8VwAw4fHtJVDwHslLHAYxiVsu3VUVanTTmaDJxHQ0RFdyGr6dP1preZmrN53tHldLHiwEyOwGuL1hhxz223qsbmV+EQbtI2y6+rsp+P4I0LlGfI/2wJJG8G8XuCyy8T2+5vfCNXFCp9//WOy7Pu33oo7JqlkJJClTqnQvgaQv12szZrYc4Kmxj/8UGj1qViGpZiG4Yiun+XzNkT7mZnl70kyWV4vPLt2AVDfDf71j9Yof5wUNo1JIixwGMYlbBUetJP8LlbMGPnsEDl3+rRC1B8nksjkcWbOuRUV5haeSOfimDpOcUU5I/ZDANonfCe0mT7X3hI9glklPYwkNpGiDg11YkkeDfv+vvvijknyZsFTcnhoFWXPdrH2Roo9JxCp1qviYnMR2m3uCBVwLbkIC8vWqA7B7UMxteZW+x7CSUBatgw5J5wAaedOAMDJp+fgyzuewkJcihpMwbZHqljc9EIkIpEnbO/gwIEDKCgoQEtLCwaIFh1hGEFWrlTHJitqaoDJEFzZbhtwOqZgpXUbMBmTsUp8w/fco07plJQAX32lDsYiLFyoTrtoyHK8c67XG86cC0SLPm1wXbpU/VfL8hxJ5DraKCXL8P+nCns3vA0AyBkxFsVX3hH9O1lWhYSd6u0lJer6OTnRn1dVYeWvF2FK0xLLTQj1fcQx0c/Pgfz5KkCRAckD79GTIWXnmP9e4/e/V2t5OaWsLCyU9M5LRYUqgmLPZzoRcZ7lH/4QXW+8obYxGETumWfCs3atKrC3bUu/tjMhnIzfbMFhmASInC2RZdVAIWKISFba/0lYbV38EDswCTZDu//4R3Wq5u67gWzzKJQoYn1BIitfR0ZbmVl4li5VrS2WDk5lQFeXekIqKtDesCW0Sp/jfxj/O83KIUniU2WNjWobIy05VVXABRdgUlOVe30fcUyS5IVU1D3dRApo3Tv6xS/1GD8+NEXniHPPNT8v06bpn089zAp3JpOIuWPvu+8i689/Vj/PykLXs8+C8vJ6Nt0B03MkzeU5DeEoKsZN9BKRaQEplhlYk5j237L4oRtRVGYlI7Rl0CA1MsgsLCU2Xazfr58+1k7UE0DBfvlUN/t6qvvDdKq//2aSu/z2TqTVop3MYDAqCsmw7yWFJChUWXSd/b6uqSGls40CH/2fGlH1n+dJ8XjCEU9Gme8SKXAaG1KUaFpfq8KdRriRTjgmokzxeqlj5cpQZNWWPy1Qo7zMMmQzKSepmYx7AyxwGLcwCwePFDraEpeBNVlp/7sHpso73yWfty66DfjWXNxY1alyuhgNZHYGPZtZnA9MmRAKDW85+2TrgdTvJyoosC8A3ngj7ju9mmSlJR1qEyIH7JkzxfbVXRwp+Fh5uEbVD08Kt8OsrITVMZh951bhJcvcCSYCzYkoiiVGHFfifDrtiDpq3hcOHf/5KfVUWf5ZwofKJA8WOBawwGHcQCTLv05ak3gSecM22nHEgBFcXEE1OJ0W4hKqwenWNa9mzyYqLnZf4OgNZHYHPRsWL8XroYa7rlQFzpxfUWBgf+sEJ04tapdfrvt5EJ7ovr/nd8732V3UTD7tB+HyDY/Oie6z2ONL1EI4Y4bIrWBNoiUx7IoiszZIUlSOqNvKAiErzqcbOig/X+ZimmkMCxwLWOAwbiA6dgiU21Ef1KJVACMf8oC5mUjkDT5ye3bbELkMGEB0993mRR0jBzIng54Ni1fbCUeGK4dfHFE53OyEOK3zdd55YuvNnBm/T5Fj8nqJ5qhiRvF6KPCfF1SRs/ZVUkpizlnk8SVSt8zNjHdObha3aqBFUllJQXijCoN6PArVrOwMiZzfzeriRH9pDCf6Y5geoGH5GrH1RP2IRUo1ROLzAZWVwO7dxoV6RFP1u8GBA2oY+cGDxusQhR05ndSBMsulE/lTSULrqceH/u737qfhL81OiNP8/KeeKraeXr0BkVBuWQbmzAEASLIC6ZUVod/ST2O2GXl8idQb0Ot/p9jKndCN0zphZkyditXlb0fliFIUCTdMz0ZALfmFO+6QkZ+vsK9xL4IFDsPYoaoKwx79rdCqlmOMlgvHDnPnhoWMUUQSID6wFBaqA6hdkRXLrFli6zU0OBv0AONIqwg6vjsWwZJBAICcbfXIqY2og2V2QpxkWy4pAW68Ua0nZkZRkbp9vQiiqVOBl14SDk/2/Oet0P+V/zmzO2N1N8OGhSOV6urU8O1EcCPST1RoRa7n9PqwWn1cfK3Er77y4NG5at/n5ACP/TWAhgaKW4/JTFjgMIwo3YJkEt4xDweWyLTodQg7VhYtxvyWW8QGQ9GBZfp0hF5hE6FTsNr1sGHOBj2NyBLTM2dGfUVeDw6ecVLo7/4168JfWp0QQQtRFI2NwBFHANdcY77eNdcAY8boJyeUZVWMCIZMSzsbgPWfq3+MGwUcMTp8bTQ2hpMgXn450NQkdhxGuFF10ko46lWIT+T6cLD6n/6UhW3b1Paddhrh2GPFEjYyGUASp8zSDvbBYRIiwp/APBRbEXNWtOMnkYBjpWNfDDcXPR8co7aJ+ljE9N/Bid8N+d40XfU/0dsU7Tu9yB2v1/y4JEl1yh0+PPo7rWilnXA7gUW+4GdhZ+MbrwzvX/Rc+3xE/fr1jA+O1qd6BWKtIsASvT5iMNvs2WeHC3K2d3SQsmpVYqHpjOuwDw7DuElsYrK6cK0dw/o7qMXSsv+KpX0XfQMtKbFfKMeJRSJZxFYyFKlNJVL1MKL/lPxcHJw8ofsPwoA3PgivV14u3neRFiLNt+ngQePCnNQ9nbF4MbB1a/TvtmxRrx1tHb3fOZgalFa+DyiqlYGmTFT3bbQfQO3TkhJgwQK1XXPnAq2txjsgiu9/u0n6ItcvLFSn4YySBcaeG7eujxjMNrtihRevvhoeDoPV1fqlQJjMIomCK+1gC06a40ZSL7fQe5PXCaGOCweGRzB8isSsLCUlao4WN48j2UtsPp24JEAmbTNa16L/9v90Ysh603z+5GhrRaLXkdOwuSQmcww893DIiqM89bh4+0Si64qKovvMbj4ao/WXLLF3fyd6fdjc7GsPfETtLS2qFae5meTSUnNLE9OjcJi4BSxw0hi3knrpYVc4Oc1P4zB81Zb53gna8Ysmlkt0qa4O93d1tXlCIBcy5HaVDKK6Od1Zi393LQX797XuPzv7FZ1KjM2Em0iotsUiX3lBOOnfU38Rb59dsWY3H42b+WvsnqdENutXhV/XAw+Epqn8zz2X2L3NuAoLHAtY4KQpbj8UY7dtRziJ5o9xU5Ak6U01jiQOuLqDQDJFazeKolDjX+4JZy2ecqJ1/9ltVxpacJSRvrAfTvUSsd9VVxPdc4/YugsW2M9Hk4z8NT1F97lSCgqofedOVeS0tZF89NHm55jpMVjgWMACJw1J5kPRiXCyWfPINUHSE9NziQy4Wj/m5Rl/L0nhaYiyMvP1RPpKoE/aP18XEje7HryD5AX/Mu8/J9eE06nEJDp6KwAF/u+fqsj55A1SRo0038+AAfFO0GbL3Ln2hZ2rGTBt4HK9qq5bbw1ZcTqXxIjHm29O/fT5IQoLHAtY4KQhyXooOhVOolaOBQvSx19IlEQGXKton6IiNZJH1PplJVoFrCxyRzs1PPrbkMDp+Hq92PE7aZfRVKJJ+4hIFXwuixttCc75TXia6j/LrNtnZ1mwwP7UnNOpvERIQr0qJS+P2jdvDomc4Mkni51rJqlwFBWTeSSa1MsousNpNlTRyKbhw42T7KUDev2SzMgqIuDhh8Xy+hj1vUZVFTBtWvy26urUz7sjWlreeAnKwRYAQO7YY5A77rvm+7W4JmSSsHLnaCya83V8oJBAksHY9qGqCvjNb8zblADSh+tD/6fjj7Zunx2GD7efjyZJ+WsMEbxOhIjI1yN1diL7/vtDXwX1klg62QfT8yRRcKUdbMFJQxKx4Ji9vTl9m0xSDg5XEDXFW73VCkZW6UaIuWmF0HuTF7SytG9cG7Lc1D9YRoF9Tdb9Z3JN6FUA131J9/vjpyf1ro+KiqTnIFKGFIf9cL5ZI9Y+kcVpviK/3zxnEEDk8SQWFWjzOnHq8K94vdSxcaO5FSedfYp6ITxFZQELnDTEqaCw8qUoLxd7mBsJp2RHNtlF1BQv6mNiEVmlO+BjB1XifPcGab2+FxC8wX59qP6BW0MCp+3T98X60GDbkRWmLU+3qCBPRlV2nSWwThU4gfVvkqIoiftZGVV9F7kXRPddXh59Xuz40NiNCBSZ2o7c/5w5oXMXuPLKsC/Ov/+d2D6YhGGBYwELnDTFaaZTswe1z6c6VTq1xPRUZJMIdkSLlWVGoEq34YDfnbk5YZFj1vcWljcFoKYrfhauFr7kaXVgF0HnWIPwRFWYtmxqsiPR7Cw+HwW3fBTOh9N+ILH2FRUllq9IdN+ReXb0tl1YqIogvZcauzmdrHzlTLapZGVRx1dfhUSOPGGC/j7c9CliDGGBYwELnDTGjqCw86Zo5nhZUWHepnRIPGjHFO90ui9CYFoO+JCpFN86n66ysoJZHMPBU8LlGBrm3kXBtoP2+jNGTNfgdHtdlsTQb9tLZSXJu7aGHY2bdibevsjzEnn9a/mMFixQI6w04RB5T9jZd02Ndb6pSMHlNDdVrCUtdqrWYpuBa68N58VZsEDsfmKSQq8WOPfddx+dcsoplJ+fTwUFBY62wQInzREVFHb8a8ze+noqEiL2uPx+ceFkR7SI9ktZWfx+uvtJeMDH6ep/ImsqiQxAVlYwkylL/2HFVDf7+nDU1OaNzs5HxDWxEJcIX0pW7Qv1R6I+MFZLxMAvH2gK++Hs/CKxSLlIsax33xQVxUfTRd5DwWB8FmujZcEC8Yi7JUvcy8atiduKCqFtKjk51L5tmypyWltJPvxw/f5ikk6vjqLq6urChRdeiBtuuCHVTWGShdcrFplkJ1pj6lS19o4eyYiEiI1eWro0XOFZq23Tp49+ZWk97ESZDR4stu6LL8bXEuquwdQw829iu0P3OfD5gMpK4O9/V/82is4qK1PrIG3bZl4XyiDSS8nLwb6LfgRkqddE31N+hLwxRwu1NY7uY5Wra7D78juFfhK65ETqJP3tb+YVtGMpKtLfnh5z5gC7d4f6UMrrF/6u42BikXJEanTb/ffrRyft3RtfOyvyHvJ6gdtuE9tXY6N4xN2NN4qtq2F23ETqvzfdJLRNqasLWU89pf7h9SJ4883R+3BQE4vpQZIouJLC/Pnz2YJzqGPHMbkns6smWvdJz+/AjgWnulp8XwZmdeHd4fT49rrptxSxLcUjUdOVYb+bPf94gJRgwP429TcvbNTQ20BUpFnJhRSsiJn+MPIrKy+PtuBZNEjdz2RaWHgT1VQHo9oT2PCWasVZ8x9Sat423p6odUV0PaP7zSxnkrau0XSPG0v//q5uTykqova9e1UrTmMjKQUFqfPHO4Tp1VNUGnYETmdnJ7W0tISWnTt32u4gJk0RdUzuqeyqTn0EYpfYaTORKQdtcLHjYGrgGBnenaI/PoV8cLz6I7+bfkvd29r/2O/DfjeP3CkWEm6C6KmycheqXBIkX0mH8emzK/gMBKppCHtlJQWfeyQ0TbV04FVUUzxNFVqx58KOAHay2KlflU6+TAKLf968kC9O4Isvem5aKh38ANMEFjgxzJ49mwDELSxwegkiA0hPZFcVrV8luhiF6hqtP2OGup5dJ0+TbpWgdEdSRTRLL4pKQBgm8oxuXbcyJG7q7r+JOrd/I/5jg7aIniqrclZCFR8SLOppGsIOhSoxlb65O1xZ/AfH7iOgO6R/Rkz4vIjl0ypjteg9ZHVv2r1nkhF2X1IirHTlM84ICZyOzk7xyL1E6IFabplExgkcIwESuaxduzbqN2zBYaKwGkB6woLj9tuo3rzIjBnmv1myRF1fpN6Qz2epMirL3omzGpTi2/gQcQthmMgzunPLF1R3343hfDefvCtwMswRPVVz5xp3UdJmPWMaJxLRVoQ9VHbZlpDAuex/akPfSZCpcolOWLTV1JnT6zb2HrK6Nysrxe+Figr3768lS6zLW0Qo1s7Ozp4TOMksQJyhZJzAaWxspC+//NJ06ejoiPoN++AwtuiJzMTJyo0yd244RNfqbdfrVQcBkUFD5OFYUyOWydjKEuTwGd3VWE/1D/4mJG72r1hq+7ToYVQDNHYx021J08wx16poRNs5p+0KCZxZ078J9zNkKi3pMPQfitqIZl1xGoWlXX92qaw0thrFXiiiJ89K3ERu08oZK8LqJMsyybJs/xjtkslV2ZNIxgkcJ7DAYaIQmQJIdmbidPInqKw0HjSMErkZ9WsCwjCRZ3SwpZl2PXZvSNw0LX6CFBcGlmBQPILbTJwkddYz4loVDWE/dtyBkMD5+6wNYsdidt+IFBY1OqlO7qVgULUcxTo3x84RunGf6c076uX7SbbPi1n/p6oqe5rjZPzO6rl4rcTYsWMHmpubsWPHDsiyjPXr1wMAxo4di379+pn/mOmdVFWpYamR4Z4+nxomGxmKrBVK1Fv30UeNw5ZlWS3Q2NCgxglPmhQdEqp9X1cHFBcDTU2uHp4jysrUUOxzz1XD1FeuVD+fPNleUVAt3HjaNDUklij8nVGIbER/rd49HrW1xxtunihcb3Py5IhNtB5A04JHIbeo4cjZQ0sx6PxrIHkSz2ixerUanWxFSYl6qo1Iak3JiGt1WK1YioD6xtzQ/4cW++O+b6hTgJXvxF/HkR2v14Zbb1WvbY3CQmD/fkBRjBtTVqZee3ZCp71eYNYs4N57ze83rSBmXV309SjCzJnAmWfGb1Pbv1FfJAOr51aiBYiZMEkUXK5y1VVXERDvo1NjQ8WyBacX4WT+w+ytKfY7veRiDgpW6i5WBQkTXSLvCVEnV6P1RCOBYtazm0AvGCRa9eZB+vrPfwhZbnY9UEbB1//PtbfoRPIgxnZV0uuxBoMUrK4hX2ErSdCPaAv75ijU9uHrFPj4NVq36L/xl0PxNOPr2Ai98y7q6JtMy4KVL1pSToaLuBlhxhYcSzJG4LgBC5xegttz1HbFysUXi5vvNQfJ2EzGiThziqoGOwU6zdYTcRaN6Q87JRAqK4nGj26l12+4PyRuPrrjLnql4ALxAVmARMaNYFCduZg5U13mzDG+9Nz0ATWbXQUUKkIjSZBpy39qKPDxa1S74q3wOlD0y2pYxr8nmPJg5szkCAq77bI6ziSEYHd0dNCePXuotbVVf38izy2/vwcUdObBAscCFji9BDfecLSHmxuOi7EPn5ISonvuUR/01dXGuWLKypIT/mpW50fPyTKRaA2Dh7YWARQbah77jK6oIBqQ106v/vqBkLj5+K67aHRhQzgk3SXVkEjhetHoaY8nHLXvFnr6s6Sk26d8xvskQaa1C/9LgY9fo7YPXydACYX5GxZGNTpYt1IeuB3O7KRdejmlNEFTXh4fceigze3t7fTpp5/So48+Sj/5yU/omGOOoe985zs0fvx4+t73vkfPP/88HTzYXS/NznMr2X6DGQgLHAtY4PQSEvXyTDTjcCIPe719FxQQ/fSnRDfdlNh+Yt8A3VrPolaWUbSVlsMlLp9O9zN6yRKio8a00iu/+lNI3Kz/7QwaU9ygrhdZ2NOlt1a744ZIUJre4vb4s2RJvIO0dllVLgnSO/PfDzka5+UGqbSkQ6zqe+xLgFsO824PxE7aVV0d/r3IPW+zzcuXL6ezzz6bcnNzSZIkkiSJRo0aRaeffjqdcsop1L9/f5Ikia6dPJkaKivFszebWWAP4QzKLHAsYIHTS0jEguNWxmEnD/sZM6z3beafI0nGafSdzOHPneu8H7Xu1MmX48OO0OCql4VXe0avfn0fvXVzeUjcbLj7TjpicF387rXCnhZtiURv9iHScBYrFoyCa0TSCuktAqmGhBExsgU2fxQSOP9d2UHBBYvsDaYabqY8cGsqJRhUraF29x8pFETveYE2+/1+uuGGG0iSJCouLqaLLrqI5s+fT1988QU1NzdTU1MT1T/7LO0ZNozuBCgLoD8CznyYOJNxCBY4FmgdtH//vlQ3hUkEp3MNbmcctrsk4lwcOZpZvdmJDlLnnedsEOxGy3gcl2U3JuNxEB6qmftJ1DO6q7GBtjxwT0jcfHLXXTR+yE7d3c9EediPRCD2WrQQdnGxKnaMxo1EjRlu+ICKum0Etn4aEjhKR6vzl4BkpDxIpCMSsbZqJ9bJ7yOtPzE899xzJEkS/eAHP6CXX345fsCNEFS1AJ0N0PGi+y0qOqRFjBkscCzQOmjL335PXbt2pro5TCI4maNOp3w1VkusGIo1MbiRRyOBASo8btivWeXfuYXqH74jJG7e/c1MGlm427QJIauQxWBp92XdbEYiUWNGItU/NERP5c4PPgsLnPYDib8EuGnldNoRiVhbNROa03uhsFD3wmhpaaEzzzyTRowYQfv27Ytvc3f/Kd3bUQC6CqChAAVF932ITkFZ4UTgJJ5cIgMJ1G1H47MPoGXFUij+zlQ3h3GClqtj+PDoz30+9XO93DaZlDdCloG5c4GFC4GaGjW3TeQxabk7fvGL+Pw2Wr4QLV+NUyQJKC3VTQqzerWWxkN/HwQPdmIEVmNSVL6czs0bsXfBo6CONgDAV42lOO+ZGdjePNi0KXUYjmlYiqom4wQ1sqymFyESOrrQemVl6m9jcZTHxsXfA+KXbGtb+Dy8/pqClau9kP8yT/0g9jowymMEhPMf6f3OKZEdIctqbqZFi9R/Izs+8ru33rJ3MmO5/nr1WJze883Nag6oqqqoj7u6uvDpp5/ijDPOwMCBA6EoCiiyjatXQ6mthfaJBGANgCMBtIjsV5KML0jGNoekwAEAkIK2D9/CnqfmoH3j2uiLlEk/9B6MU6cC27erAsBICETixojTkwwZAvmiX2AlJmNRhTduPDAkcpByitkgCBu5yMr+DEydCiJC64dvofmlv4GCAQBAzsgjcfDU36CpbYDlWErwAJBQdrvXsA/CoksconDCwVgmTYrXz6L4fObJAkURvWSb94Uf5eVzCFOmACNvn4qqO9+LP4jCQmDOHDUhnx5GLw92iRXIVVXAyJHAlCnApZeq/w4dCixZEv/dWWfZP5mRNDer/yZ6z8eIjeLiYmRnZ8Pj8aC1tRWe7gSUwWAQiqIADQ3wQB1YmwBcDeArAOcDKARgOcqYXZCMbQ5JgdNv0s+ArGwAgHKwBfv//Rz2vvAIuhp2pLhljC5GD8bf/EZ9EEyapG/JiEWzbLhNYSHw5pvqgGA2UtvJ7gqgatOxcYc9cmTcS6U+U6cCd95pa39RmFnCYCOb77nfBwUD2P/yCziwYmnojTxv/AkouuQmnHthvvBYSpBMn/2JGOj0fuv1Ao895mx7118PVFTEGylE0fS8liTbCEkCioqA994Lf+b1qH1cVwdMe/hkVM3dDpSXq9cpAOzdC8yebX4xxb48zJxp7wBiBXJVlWoRiRUtTU3ARRcBF1yQmKCJ5YUX1E5MxJppIDbOOussVFZW4u233wYASJKErKwseDwe+IuK8BGAPwCYAuCfAM4BMK37t8KtyCRrczqTrPmydCRyDi/QvIeaFj8R8gNQlxto3yv/omArR1mlDSLz8HbyVyxZ4p5vgebLEJtTxsgvyEYW1soBV5Mkxfu3CEeyOnGunDlTOFpjyRLrgK/SUiL/vn205x8PRN1nLW//mxQluraUnUAZI5eORFyPzFx7KiqI8vLEttOvn31n5lhE/Wq1a6GoSKG5Mz4P+eCcdPS+6PNQ1Kb6Qjm+mPQ717QYa6TPWKqc+7WTmmjkZMwF9+GHH9KECRNIkiQ688wz6fe//z3dcsst9OMf/5hGjhxJEkASQIUA/Q6gTrcvyEMUdjK2QK+DOjZ9RruemBX1AK5/sIwOvvcmKYGuFLY2A3E7pFH0wZjgg1p4KSiI/lugZEHcehUVltFUQXjI16+ZDB14RaJvnRyn4ENVZLyQJKI3/rWFGubeFb6v/nQrtX++LuEmGzXTiX+sVV+KJPmbNk0VZ7NnJ67F7YzFpaVE5RdvJIDosbvDAueE8fvj+ywy1N72xRTfuXrh/z5PLVXetir5Tu+iS6QwSTQaK4aNGzfShRdeSPn5+ZSXlxfKg5Obm0sTjzyS7gfoE4AC3dtQtL4G1AuKsxTbhgWOBUYdpAQDdPD9FVT/YFmU0Nn12D3UtuHDuLdNRgfRsgB2sPNgFH0wJBIas2BBYnWdNCysSHbKHLhynDYeqiKaMytLpneffIPq7r8x6l6yilx0o8aTnULYIhULRLvOKmeiyD7tGjpu/dk3NBO/J4Dob/eGo6iOO6IlfqzHJc4vppis31oCx/j0AGoG5cqyd6Kve9EEd24vscdUXW3v9xYXXHt7O+3evZtee+01WrZsGa1fv54CgUD44jF60eEsxY5ggWOBVQcFW1to36sLqO4PN0QJnT3P3E+dW7/s4dZmEImm+zfCiRixskIkax7DLiZvlHYLVSZ8nDbOkdVmS/rtp4VXzYu6fxr/+RcKth0U7pZEn/2ieXDMksLaFRuiORPNxs1ELs0X7lsfEjhjS1vjL10DC05omunmd/U1e0xnaiU4YsVN6Ngis08PH67Wbevf3/mBOV0KC+MPxq7ol6RwHbkFC9ST3P2SE/T7Da8/WZYpEAiQ3NVl/KLDWYpt40TgZKXO+yf98PYdgIH/cxn6nng6Dry1DP6tXwAAArt2Yu+L85A7+ij0n3IucoaNSHFL0wiz2FyicNjjuefadrJ1FAER6Zwny6qDYEODuq1Jk8JOh3V1+m3WQ5LcC4vRmDpV7ZPVq9W2lJWpDpcAhkHMwXDYYBmAQZ+KHqfPp0ZcGUWexWDm+zhl3EY8OvUFFPdr7f5EQr8f/hj9TzsHkuC51wJ4brst2ufU51P9VUWaGdm1kaceiP/MqFl2I7K2bBFflyjsuzp5cvhz536lhH59gqG/DraHH+sSCD7sxCSEHWVleLAak7AcP8cCXI4mDAYeB/A44PMR5l3/OaaO+wzYtEmNtoq4flZjEmpRatKScHqAyXWrgJdecnpQ3QcgqR7ozz8P7NkD7N6tBhdYcdtt6smNfAa8+ab4fn0+4JJLgNtv170QvDH3jaIoAACPxxNaAESf4EiMLlK7z0jGFBY4OmQP8aHo0lvQufVLHHirCsHd6gXu3/oF/Fu/QN74E9D/9HOQXZxhYcfJwGokMHqai+BEjGiiqKpKf5ScN09dpk1TH55W27UIl04ILZfNypUhcQMAk7AaPuxEHYZ3h0fHNAkKfKjFpKsmAY/N1R/1tVBxs+MsLwfuvdfWcelpzrzsLtx91nJcP/Ht0GfBAwEMWbUOuUedZbvf3Hj2a10bS+RnWqSS3j7sig3RyzOS2H04j2iWMKBvhMBp83Z/qgCQ8GjRH+BtJoCAKpyP2zDPUKTU1RKmzT4KSzELU7Esvs0Qa6ToeqZo9968ecCZZ6r/l2XgkUfMnwkeDzB+vP4zwIq+fdX7wudTIzMN9tFZW4vKCy5A669/jcsfeQR9+/a1cWDdGF2kjHsk0aKUdjgxcSmKTG0bPqBd8+6Jjri67wZqXvYcBfbuTmKLMwC7hS/tOiKLel1G2v2tflNWpl9N2O48hpPjEexDw0KVkWUQROZtLEzhdpsf6yfzg8O/of+W/S7q3njpsoepq09+WvsU6HVLcbE6I0HkbLrIbiWO2BnPRJIIf/jiuxT4+DXqXPsaac7ppfiWKss/C90PlZiq6zsTdytFTjPFfCnsH2bk1GxnKSrSv3Z6op6cwMkshhot9dGaNUm+Whki9sGxJJFim0qgiw6ueZsa/jIjRujcSM3/nk+Bpl1JaHEGYCf0xYkjcjCoihGjIpNA9EBqx3nC51O3rVeR0eozIvccq8vLddunW6gS30ZXiRbxvDVov9PmV1YS9c3toPvPWRR1L2yZdTNde3K1/faZkIxag1bj4513JqdigUiX2HGSjly2/l8NBT5+jfa9ubw7bHsyBX2Hh3YSrKgkn7eOrMRN1C2rI1I0H5xY4R06NhNxZHuJrVgaeTHMmUPkcWEfCSy3A3Q5QF/dckviFyVjCQscC9yoJi77O+nAu29E1dIJWXSq/kFdjfUutjhJuDlqiIa+LFli3xFZbwTu149owIDozyKtLHada51YQHw+48rgdq0WFoLMNNdI1GhUY+u0OfIL775uOp59grb+Plro//u6B+mHxR9Hi5sE2qe10UqAObVAWV0ad9zhXGxY5QdycsmZLwq1fvA6BT5+jfYvfkF3J04sUkaRV0LWRYeiQfe68fuJfvlL9d53a7suLC0AdQD6Ds06+P1+2r17N33++eeW6zLxsMCxwA2BoyF3ttOBd/4TL3T+cAPtXfI0+eu2Jd7gZJCMcG6r0JeKCrGSyLFRBkYjMBBvedFwEnlVUqI+RI2Oy6gNdo7HCLdyhNgoaChaoTqq+ZWVFPjOWNp78Y+irved5TfQqpOvpBrJRHzZbJ9V15sVVbe6lO10d0WF8/Qpc+eqM6ElJdGfiwbKREZmFxeb76ugX1cogirwxP26O3EUkGgyzSRkXXRjOeeclFtqhJaZM+OeRa2trVRXV0dr166l+fPn0/Tp02nixImUl5dHu3Ydohb/BGCBY4GbAkdD7uygA+++Tg2P3BkjdNTw2I7NG0lRFNf2lxDJCufWtq032pSXi6eo1d7yHY3A3TgVDCUl8aaBRLOvilgtEi1ZbWdfNrtI26SypIIOnH4C1f/u2ujr+5r/pcCgAWIbq64Wbp/I6TfKleZmhfCSkvgZSrvZlt0wllZUmO9r/KgDIYET/OAN3Z3YMmxaTTN1d7ywdbGXLx0A7dX+jlDYK1asoClTplB2djZJkkRFRUU0btw4kiSJKjRnL0YYriaeAjy5eeg/8ccYfPN9GHDmVHj6DQh91/XtN2he9Dgan7kf7Rs+BMlBky0lGatwbiCxKraxtWvKy9Xtzp4N3Hef2Da0sBLRyKy//jW+vU5rzzQ2RlcPdlK5MRaRUJxEiwGaVPw2QrhQZj2h86v12PPhKzh41vdBOWr9Nqm1A7uq6rFifl/8d9/3ILtc0k7k9O/d6+xSttPdjY1qW7xetXvt/HbTJvVfs6LvIsiyGqlsxihfZ/iP0lG6OxG/LdRw50dRBm/3/+Pw+YAZM+CVCJOld/ALLMZkrDJeP5aiIrVQVzLqwvUwbwMYB2Cp9kFtLeiCC4CqKowYMQIdHR0oKCjAQw89hFdeeQXz5s3D6NGj8eqrr6au0YcQLHBcwpOTi36n/AhDbr4PBT+7DN7CwaHvgnvqsP/l57H7rzNx8L+vQW5vNdlSkrATzu0U7Wmem6vmz6irs/d7bQQRHYF/85v4goGRlbSdFNjTRkY3it3t3m0tGK1GHq2aovb/2O8A2yHsIgP1Cb6tOKlpLpqXPg25oDsEVlbQ8F4bzph3LyZ8shyX0iJMwUqMxHZU4XzzDe7ZI9y+RLve7FKeNMm8eKVeWyJrvYpq9WeeUU+9Foq+aJGzwpsiOnvE0I7Q/6XcfN11om4Lk5rWpajFUkwLh4hr11h5ufriUlMDbNsGPPigfsXxkhL1HtJCrSMpKlI/370buPBCYO5c8wPLAOoBNALQel1Gd0HNX/0KR4wZg+uuuw6dnZ34wQ9+gFNOOQWTJ0/GhAkTsGrVqlQ1+ZCCBY7LSFnZ6HvCqRg8fTYGTfsVsg8bGfpOaW3BwZUvY/e8/4f9r/wLgd0uVs+1Qvi1PcHRxcxSZESsFcLOq3JdXbTlBQhnixMpUR1J5MiYqGUF0BdgsZgJMu3vp55SB4ZBg6K/t6j4rRE7yE6caKypxhTvwrO/eBqv/Poh9DmwKfR5zrZ6bHhyH0567Xl81Tkm6jd1GI5pWGoucmz0pxtdD6iXRqy48HqBv/1NfBubNukXwbaitha4/344rwbfjcjtePhhYYGDHH2BA3TfFhUyhnujN1qC3SjDX1CDydiGUdH5b3w+oLISmDUr3gwVa7WtqVEbPHeuun7sd7t3q59rv//iC6E+SGcOg5pMrr3779CAuncvcP/9OO2009DW1ob169cDAPLy8vDd734XO3bswMGDB3u8vYccSZwySzuS4YNjhaIo1PntN7S34qm4EhB1f5hOe+Y/SG2ffpD8wp6JVjJ0ez+R8/mxThN2Y3SNfHJiaugILwsXuhcnLOrfZJSvZsaM+M8LC1XfJoGQocryz8hXGJ26PzIITDu8EYP20EPn/pN2lEdfo7senkHtR4+mgJ0U/SLnRr+5oaz4JSWJd32sc26kA/Kdd1r/3udzvwi2XXc3kdtp6SMfhXxwlM42yw3a8p1ZskSsoWYn1KhUgZsdm6JlL0AegObofV9URNs2b6Z+/frRAw88EKpTtWzZMpIkiV5++WXnfXsIwk7GFqRC4EQSaN5D+99cQvUP/iZO6DQ8fAftX7E0eYkD3ahkKIJdp1mjsBInMbpm4qyy0jocRVtmz1a3deut5qOUnvhIpG9jB4SKCudO4ZWVVFl0nX5RxIjm//CYBpp3wXz6ds6N0dfj3N9S67p3SOlSq0jWYLLYKYiMvLExmotGK4kUZDb7bWRz7rjDfF2D9EQJL3ZuNZHb9utXVqkC55M3rAMaklSEVeiE+nzqNV1dbZ7XqieX/PyEtzEWoFMBau7+OxDx3bN33kmSJNETTzwR6pp169bRkCFD6J577rHfr4cwLHAsSLXA0ZD9HdS6biXtfvoPcUKn7g/TqfGFR6jt0/dJ9ne6u+OeqGIrasHRCavUba+dV+iyMvV3Rsn63nhDrPCfVVhqpCgLBsUrLdqxjiUSSVZZSUF4TS0uRw3dSS9c/UycVbH+wTI6sPr/oq+9ykpaiF8IHWJU7hTBmGg7iWlFCjKb/T622yoq4nWvtg+3AtwSvRzMjjUnWyb/R2oOnMCX/7XemJMow+rq5GQfT3TJy0t8GwsWWCcSjV1KSogGDQr9/TuAvN3/dnV/5gdoLUDHH3449evXj1auXBnqnvb2dlq3bh19++23YhcAQ0QscCxJF4GjoSgKde7YTM3LnqO6P94cJ3Tq/1xG+175F3Xu2OxeqHmyq9iKTO3YeSsMBomuvlr8wbNkiVgp6USWWLO93XIVIjidUuzuf72U+pIk05lHbKDFV8+Nv9YevoMOvPMfkjv0pzhqyleJNWfuJ7ZiokWi8UtKQkWcoza5ZEl8npnYv0W6zWgmxa0URW5cDka37ZsvR4SIb1tvvSEnU6+xg79ZsiE30itELiedFJ85UZLUKuXV1YlvXzvh1dVE99wj9gK0ZEmUee9LgH4GtWzDFICuBOg8gPIAkiSJ/vCHP4ifaMYQriaeYUiShNzSMcgtHYMBP5qGjg3vo339ewju3Q0AoK5OtK9/F+3r34V3YDHyj/k++hx7ErKKhjrfabKr2EYWeTTikkvE97d8uVpJWITGRjU6I5a9e8V+L8rttwPnnx8+BlGvWDves06dwrvDbhpwauij/Gw/Ljz+A1x7Sg3GluyOWr1LzkPxmCPR54Ir4cnvY7ibSfeeBt8zhLpagBDvnRwquH7L8YD3eLG2QyxKqLERGNp9yVdUqN3Y1KT6cDc2htcrLlb9YB97zHq/kd2mBf9phae1fWjO2FZ1HZWY6OgBA4ADB6zbYOdyMLptpZYDUL5V15Hy+ltvSOT+jKW5OfpvzbFfz8HdjfQKkezaBRw8CDz9tFqyfcwY4MYbgZwc9YTZLcaroV2wjY2q57dom2fMUI9dltULbe9efAfAUwB+DaAGwEqokVRHZGXh8lmz8Ovp08O/j6xuzhXEk45EZPfKyFwOHDiAgoICtLS0YMCAAdY/SAFEhK7arej49D10fPERqMsft072YYcj/+iTkD9+ArwDBvZ8I0W46y7goYf0v5Ok+Iej3o0P2Hv49CQ1NeFKwLKsttPoQas9TLdti3+YGT3wVq5Uw27stANQQ4YuvRQrcTpuHLIAl534X0w9bg0K8juifrZtbwmeff8MXP1xOU4PrAxXWjeJyKqqAi64wLgplZXmAV16h1pRoUYYWVFYGD/OxiJSHF4jttuMis//4hfAww+rf0du22xfVu0wuxxCCA6Ecu2XoEZV4XjGnAjPAMEY+Koq4NZb7adysDqI7uvPVWJPViRVVWGxJnpCtPDBO+9UT67IRVNSAjzxRPQLlLbviN+vBEAACgGMeOEFDLryyuj19S4yi/uOUXE0fifLnJSOpNsUlRWyv5PaPn2fmhbMo7r74iOw6v4wnRrnP0QHP3yLgi3NqW5uGLv+I2ZZkJM5R5DIEjm/oBUENTpWO7W2NPO/Q6dwuXoFtX3vSNrzq/N1r5clv/wL/eg768krBaKjngT8sKwCX5yU9ErGKfZ6zWdgYms4WiX41vMlLypyVkFAyN3NRg2KwDcfhCOoAjrlRiKJnYvz+xM/AbFTpMmY17OayzPz1dObntZq41lNpQ0YoD83GknEPKkSuf3ucxVyLUhmFvlDBCfjN1twMgT54H50fL4OHRvXILBrp+462cNHIe+I45B35HeRVTQUkpNEd25gx/rQ3Bz3FgTA3ut4Kpg7F7jlFnUKLfatLJLSUjURX+wbms7bH4Dw2+XS7tyoem+nketMnQpSFHRt/xrtG9eg86tP4qx+HV3ZeHnjifjHB1PweUMppO6Ms1EJ3bTtGpgWNCOV0WGaWSWsDlWzzvTU6S4qAv7+d/WUiB7X5s3Ae++pBpVNm9QE3SIUF6vTaRpGl0MIkeui+8dECuQNbwGKDGTnIeuYyfHb0yxBy5cDL74YPaenWQ+A+Gu4qEhsanfhQtXMFbk/M2umE8wsOBpLl+pPT2vPkfJyYNy4sDVs9WqxZ1R5uZq7Rw89i0xxsZpoKbItidw8TAgn4zcLnAwk0NiAzi8/QscXHyHYtEt3HW/hYOQd8V3kHfFd5PhGQ/K4cOOIzh+LmqkXLADuvjs9p6BEsBoEysuBe+/Vn5YSfeDpCajSUtDcuQhMPBEdG9ei4/N1UFpb4jaTtWsvNq3NxnUb5uCrznHhn2MHHkVZtLiJRGdAcTpjJnKohYXuu0n16we0GiQMj9QKhYX2jsvqeGJZsEDNNynkcmFzIKT2Fshfv69+NXAovKOOj15fbwCO3R6gdkSsg48sA2edZX2AeuJDZNpIFK8XWLzY3GfIiYAQfUYVFakJCu2q9sgpeKc3DxOFk/GbnYwzkOySYcguOQf9TzsnLHa+/ATBxvrQOnLzHrR9UI22D6oh5eYjd/R45I05GrljjoK3/0D7O7UzfyzqPdnY6I64iX1N7inMRmVJAp59VhU4sdgpmxHhXUr19ejqm43OPELn1x9D/kd1/G5z85A//gT02R9E9tOzMLi2FhuxGKsxCQ39jsCw1m8wCavN6wbpODgn6PNseqh79wIXXwy89JLYPkQwEjfaPiVJrSjwwANi21u+XB177PrQDh9uY8yyc11Mngxq3Rf6SuoXk+XaaACO3Z7WEeeeG69MzRx4Q17lOjXQtEziZuJKFFkGLrrIPGO3aL+tXAmceab6megzau/eUH9Htcmqrt/06UBHh3oBiPo5uVEehomCBU6GEyl2gs2N6PzmU3R+swFdOzeHbjbyd6Dzy4/R+eXHAICswcORO/JI5I76DnJGjIXHoH5NCKOHpVE0hVZfyerhWFIifqBGU1azZ6um58svF99WTxAzGEVhQy0o/k74t38Ff1sdOndvgNKmE57j8SJv7DHIP/b7yBt3LKQstSgmfnElsHo1vA0NmGznrVzn4e80UEz0UFesEFvPLbTTEzljY8aLL6r+qHbGIJs1UG2rSGrbH/pI6hshcOyUSzG6TiOjrYycdM1qoEWGfS1frpqyEnkJ0URY5P40i3Jlpdg2LrpILRI2dar4iQfsq3ZA3b72TBJ9zrlVo4QJwQKnF5FVWIJ+J5+FfiefBbm9Ff7NG9G5eSP8W78EdbaH1gvuqUNwTx3a1rwNSB5kH3Y4cg8/AjkjxiLHNxqevIhwYau3lcg3QO3hI/pwLCwUO7DycvXBpPdQ+cc/gOuvF9tOJKWlarj6okXJnSLTG7RMHmQEIDh4EPxjS9G5fR26HnlX9bGIxeNB7sgjkXfk95A//nvw9OkXv44W/6yRwFu5qGaN/emmTfHr6mEVIZUsSkrUxWq80yqL2xmDbNZAtaUiiQjU2t1pHi+QH3H+nYRq612nRpYYn8/Ckagbr1c9sfPmJTZVFSnCNP8ZPZ8iKzR/v4oK6xLtkThV7RpWws7MGsYkBPvgHAKQIiNQtx2dWz6Hf+sXCNTvAAwrCkvIGjIcOaVjkOMbjZyde+D92f/qZD6JwWguXsd/JPRwtBNevWyZsRMhYO2p6vOp+XT27Il2hujqUs3IyZri0uuXiOMmIgRLBqJr1HD4Rx2GrpGHQelnYFHLykbemKOQd+T3kDfuGHjy+9pvj5F/hJ7vgPBPCSBgadl/MfVcOdS3VmHl2m779jWfUkom1dVqLVPNp9uMhQtVI4CVD63Xq+pmvcvVFBv3A/nbIH/9nvpxwWB4R58QXs9JqLaZ/4eR752VT55dhyUrbrtNtdYksj1JUqe0RYVRaWm886+FT41y1FGgIUOA3Fx4X39dbD9LltjLTXQIwk7GFqStwOnh5E9KRxv8Ozaha9vX8G//ytBRWcPT1oHsukbk1O5Gdl0jshua4DnYHi16YqMpNKyOzcohsawMOOcc4KqrjOeyIz1VjSxGRgO3qAOgXSRJFU4xokqRAwjUf4uuN15F1+pqBHxDjAUNAO++A8j9Zgfypl2OnEt/CU92TuJtsxKedn/qrcOj8i1hp2WfD/Jf5mHk7VMtxyJJUh2C7RZWliT1JfynP1VFhxMLUGEhkJ8v7iKhaQAr9xZtrHJ0WwsKUGX3Nij1XwMAPLvb4ckrtJ9DSduukwgeEZ+8ZN1bPcmdd6r5vCJP5uDBwNVXGwrRzk2bQD4f0NCA/NGjo7808hfkfDiWsMCxIC0FThokf5IPtqBr5+buZQsCu2stTcqe1g5kNzQhe1cTsnY1I/v+PyPrx/8DyckArNcH2huiHfSmsqwG7mQkJusWWcqwIQjkehAYVozA0GIEDh+GYJH5dSd1+pGzvQG5W+uRu2kHspr2q+H+boeRJiCqQz9dvgbDHv0tJuGdaKdlScJKOh1TUGO5rV/+Epg/3+lBqKdcNGQ7EYqKVCfoyZPVbrrrLuAvf4m+RL1eVXQ9+GCCt7WAAJXf+w8oXz1f3qnXQ9peG97BueeKhWoLWO0M2ycSQZSMe6un8XrVl6yXXtIPo9fxDex8913QCScAwSDyBgyAFPn9rbfqp9p2ei4OIVjgWJB2AsdOqGEPonR2oKtuGwJ1W9FVtx2BjZ9AyRcQLpIE78BiZJcMQ1bRUHgLS5A1aDCyCkvg6V8ASfIY/zYyX8ejjzpruDaHYGfgTvAtkwBQfi6CxQMRKBmEYMlABIcPQXBgX8iDrK8xKa8PcgIScv9TjZxt9chuaIp+IEaSTmGkFtMPi/ALXIqFlpu5+Wbg8cedN8MsHDwZFBerxsS//MX4tjVKkGvrtjYRoLSsCvLwbCA7G2jYDe//XK1aU0VyKEUiaLWLa5doSLZorplUkGjkpSZsdFJF+F9+GcqPfgQAyBs6FFJLRAoHs/1yPhxTOEw8k3DivNtDePLyVV+PMUepzamshPyra9DlG4zA0GIEhxUhMKwYSt+YqRUiyPsaIe9rBLAh6ispKxvegcXwFgyCd0AhvAXdS/+B8PQrgLffAEinngrpiiucN3zYsHjHWis071mDBzZJEpR++ZAH9IU8oC+U/n0RHNgfcmF/yIMGIFg4AJSXK7YvWUFW4z7kNLciZ8a9yBkxFt7CwZAWLwbefcT69+kURmrhyLoJY4Q2M0ZsNUN62nenqQl4xOBUabetnviJ/P6224CCgnh3sCiMrmNZBi1+Abj7BgCAtOrD8FRx5HNj2zZ9B+GSEuCyy9TnipOpcDuh7Fae6YBauOvmm4EzzgD27VPrTNlxHHaC16uqVKMTKYLW1/n5qiPXrl1qcbSmpihBQwUF6t8ivj9mkZeMI1jgpAqbOS9SiXTBBciSJGTddhuw4QO1eQCUI8ci8P/uRGDcKAQb6xFs2oVgUwMo0BW3DQoGEGxqQLDJZJD2eOG96DR42jrh6eiEp8MPT3snpA4/PJ1dkLoCkLoC8HQFIPkDkIIyICuQFAUoLoF0zHeA/XsjHqbd/yoKKBgAyUFQMAjIQVDAD8XvB3V1QJlZBlryEpT8XCh986H0yQsvffMBr4nlyajPOruQ1bhPncarb1L/3dOsthkAfnUXUDRE/X8yinUmGxOxVYXzMRvlUPtf3z1de1m98UbgvvsSS/TXv78qdNLBFk1kPrNKpN72kdH6tmakV68GHXtE6E9p1QfxO9DJoeSaf5/dhEjXX28+h3jgAPDHPwL//KfaCXPnJj/lgyyrKjTR5EvayfR6VdGYn69azSItNgMHqucDUNcRsU6n04tMhsMCJ1U4zZyWKmIeltKwYfBOmgSv14u8iNWIFMj7mxFs3gN5XyOCzXsQ3NcIuXkPgi3NQDBgvA9FhjxogNDUji5PlTv7HQBMnmD/N7ICb8tBeJsPIqu5BVmN+5DVuB/ZjfvgOdBmHnkWeV4nTbLOilxUlF5hpAZiS4YHt2Fe91/msXePPgq8+mriWYzPPlud7U336h5GmBXnjoUaGkCnfl/9o7Ud0kef6a+oXV+RliA3ghlERfamTfYiqLROmDPHXnsS4d13gcMOA+rrrdc1Q+vr7rB6KeKYqaAgHFZfWCgmcNLpRSbDYYGTKjLxrV1g+keSPMgaVIysQfFVjYkISnsr5APNkFv2QW7ZC7m1BUrrASitByDvrofSuAtKfh7gsQxMTy6BIDztnfC2dsBzsA3eA23wHnMcvIePhvfBR+BtPgDvgVZIisMRVTRBTLpiMP2wGpNQi1LLn8+ZE/aFTZQbblB9We0kzi0sVGdE0kEQ2ZmRptLBQF/1JUFa/SGkYFB/xdjnhlvBDCIJkQoL1RNsp3O1TnjmGbHS8YmiWV/c8FKP7OupU9XUE0q30/1jjwHHHhsOmnCaHZpxBDsZpwo7OWAOFYez7j6h+jpQbg6UvNzuqaJcUG4OKCcbyuBi0CUXgw4eAPXJB4YNBSkKSJbVqSdSIEGKMB5IgMcDyZulZvjdug3SsmWQAkFInV3d011d6hSYv0udHmvvgNQVjLc/1HRHBbnhOClJ4Vf2TK1VoxPSvAiX4FIssvzpwoXquJBoV0aWCpJl4K9/VV0hrCgvtz8GiyCpl5vtAEANq1Msb/8UtE+1GHh+Uw5P7BSV3nPD7WAGs1B2A8fbtGXhQtVZ+5JL7J80g2d0IBhEsFt45mRnwyuSFiPVUVQ9nKrECY7Gb/eKmac/TsqtJ5XKSiJJUhf1clcX7bPKylS3sOdJZp8Eg0Q+X/R2RRZJIiotVX8fDBKVlNjfhtk2Fy4U+83Che71s1tUVkb1aQ1OFzqUmhrxwzZbYi8H7RTHXj563V5RQeT1Jt6G2Et0xgz9SzjRU6zIQQqsX0GBj1+jwDtLScnJtr5HrK75yA5J4LwToG6nvNy9Du2JpaZGPZ6KCncuQCIKBALU3tFB7R0dFNDrV6O+S9XzXq89Pl/ajT9Oxm8ksT1pR9oJHKL0u9jTgWT1SU2N81Erct9Llrj3gK2uJpo7197DON0IBkOKJVhdQz6fIiQwRE/HxRcTeTzRn3k8qpDQQ1QjO7kczJbIS1TvEk70FMvN9aq4+fg1Cq56xfIeCQaJauZ+QgtxCdXgdArC42zHAuedamrsifVkLAMG2LuvY4Wd3ZNWXq7bB1ECJxAQ77tUoN0sev2TZi/ZLHAsSEuBQ5Q+F3s6kYw+cfLwNRJWF1/szkO5sNDZwziNERUYItaWoiLz742evyIa2a2xuLBQ1amxpyfyEq6uJho+XMyyZERw09qQwJEPNJneI7ov5dhBlThfvwFuWQfdVo3JWMwG72CQaOZMse2UlelaPgLvv28tcNKBZFn3kgQLHAvSVuD0BtJVpEW2S9RSMneu9XH01JtqGr5JiaA3wJaUqDMBsesZiSFAFThOn79Wl6SbY7GIASSR2VfF3xESN4GNK0lRFMv9xPUXZJIg64sct6yDVqo1HRYra3AiF4YkUeDiizND4IgeZ5pYjlngWMACJ0mk6xyuXrusnC68XnUKyoqeelPN0OnKYJBozhyi/v3NL4tgULX0xxqy7LhzOH3+ujkWixpAnM6+yg2bw9ab+k2Wx2Q4/kKmUnwbnq5Kxlu6kZJL9XLeeerLi99v3n6RC8PkORKYNi0kcLrSWeBkmO8fCxwLWOAkgXSdwzVql8gi0m7R+RXt/07aMXeuxZxFelrNKiuNLS+Rl4XeYF9YGHZtEH3+LljgvBusLEiibh12RJbd06YoCgU+qwkJHKWzzXBd4ZdynJ7ce1Tv5BYVxV8YAwYQff/7PSNwtEXk5cvqwjBZAhdckBkChy04vQsWOC6TrnO4TqOl7LZbZM7BaBQXaYfZm1OaWs1EdGWk9tP7Tus60edvbFCb3W4ws6r4/UTFxam9xOV9u8LOxZvXma4r/FKOS5JvHdRTctpnZWXmHSu6aIrYyW9j50tjMbowyspMtxu46KLMmKKyE3KYBrDAsYAFjsuk6xuAW9NHoo4VVnMOsQ/66urE9p+mVrNEdWXsc9XvdzaF5KQb/H7VYHbzzfGzGKnO5hDctCY8PbV/j+m6wrfk3E/SL3LHyVJcTLR4sbN4f5HpaD2RZtHJgUsuyQyBQ5T6i9sGLHAsYIHjMuk6h7tggTsPT9F2251zSOTNKV2tZuS+W1JNjXN3DjvdIGIMc6Jj3TgFSsdBYedirQ1Jeyl34wDdUsGRB5ToNuwO4hadHLjssswROEQZk6qEBY4FLHAcYvRgS1cLjmi0VCrb7fTNKV37nNwPLNP0pVFElhvdYMcYZja+J2vGMPjtZ2Hrza6tQr9Jyku5WweYjokAnSY6NOjkwDXXZJbAIUpbf75IWOBYwALHAWYPtnSdw03UgtNT7Xby5pSuVjNKjgVHI/b5K3qKzbrBLWNYsmYMla5OCnzyhipw1r9JSrBL+LeuvpS7dYCVlfYvgjvucMdXx87FZqdfdDo5sG6deSZjxhEscCxggWMTkQdbOs7hJpjHwna7E3n7sfvbNLbg2Am7tkrg5/OprkqJ5rApLzdurxtdmcwZw2D9N2Hn4tov7f/ejZdytw7Q6dRUURHRP/+ZfIEzc6bzKbeYTu6KyGQcZIHjGixwLGCBYwM7D7Z0m8NNZJ7fbrt7OpopXa1m3Vj5zBQVWetibT2zLhU9xT5f4rkazaxAydKbSjBAgU+ruwXO66T42+1twC3cOsBEXjoefjj5AsfF+7arq4sFThJwMn573KjyyfRCVq8GamuNvycCdu5U15s6Fdi+XS2FvHCh+u+2bampiguoVXDnzVMr9EpxNcGNKS+3126tMnBsP9XVqZ9XVYnvWxTt2ID4Y9P+fvTRlFUCnjpVLYg8fHj050VFavfu3q2uY7ReYaH6b2wx6sgu1Qofn3yydXtqa9V19Rg2TOyYzNZraBDbhuh6ITxeeEYeB/QdCKlwGKScfJsbcAm3DtB2B0TQ3KxW7bZzLzvBpfuWIv9IdpsZU1jgMPrYfbB5vcDkycAvfqH+m6IBNoTRCGqEJAHPPiu+fVkGbrtNFXqxaJ+VlanruY3Rsfl86uepEpbd6Ond3buBWbOiL4vY9aqrgXyDcVzr0l/9Cjj8cGDKFPVQRTC6lCdNMh83JQkoLVXXM2LTJrE26IkkWQZWrgQWLVL/jbxUJEmCZ0AxvON+AE/p0WI7sbF9YdxQgXa2o4fHYyzq3SQJ9y3LmxSTRItS2sFTVDboSV+PZHrwa9sWLaAnejx2+idZx5cBkQ92SFb1C7NTmogLWTCoFtAUmfmIPTXJntl0bftuTYkmUhejutr4oJzkv0nyc63T7w9NUVmF9TPi9FofnG3bttE111xDI0eOpLy8PBo9ejTNmjWL/FY1RWJggWODnvL16CkfFrejj0S3Z1BxmCoqepU4cYNk1C8VyeXm1IXMqaOzGkyk6N5Sbvjlux7V5VYggZOkRkVF0fdGrKivqEhOzasEohBZ4CSHXitwXnvtNbr66qvpjTfeoC1bttDy5ctp8ODBdMcdd9jaDgscmyQ7QqonM/K6bZFy29yQBmUWUk2yLDiipcXs6k3RUPUFC6L3ExZTQd22JvLekLSoLrcCCSorxcxe2qK3fT2R42byQDvPAR06OjtZ4CSBXitw9HjwwQdp1KhRtn7DAscByYqQ6umMvG5bpBKsOKy7/zRIjZ7KWS83q3sn81LSEM0n+ctfhn9TUVFLwI0E7IlYR3ZtfHVFxxtdBG5cHHrPEzsCx8jiu2RJOEHS3LlqaLloNkiXLxZN3HR0dDjeBhPPISVw7r33XpowYYLpOp2dndTS0hJadu7cyQLHCckY9VKRz8Vti1QCFYd7dCS2cTiprt/ptDSDyKK5criFqAWnsDB8So8++gwCJAImEPBqxHrR1hynMyQJz8Qm8yKwW4Mq9n6wa/F1sr8EXzIURQkLnM7OBDqLieWQETibN2+mAQMG0DPPPGO63uzZswlA3MICJw1IVUZety1SRts75xznI3EKkvSlU/1OozHWLDmgqNBIxawnQPT22wp98803JEkSATndIkciYAYBB+KETk0NkSzLSWuT7iWWzIsgkdxU2kuVE4uvUZ2PO+5w5zkQ8/KnRCT567TpI8qYk3ECx0iARC5r166N+k1dXR2NHTuWrr32WsvtswUnjUllRl63LVKxpajb2xNLL9/DZRbSpX5n5Gmpro7PZOyGdcdNsRYMqqLJzim9+eZbKD//IgL+SUD/bpHzAwLeCrVvyJBPadeuRsdtcjQTm+yLIBEHK+0icPq8SNaUm454ks8+OyRw/F3ipTUYazJO4DQ2NtKXX35pukTOY9bV1dERRxxBV1xxhaO3G/bBSSPSPCOvJdrDsawsXswkWjunhy046VD9QXRmRM9HdfhwceuO1WWlnVbNlWPBAvOxT7R25Ntvq86mS5cu7bbibCCgiYCfdYucHALKCWilvn0H0sSJE2n37t2O+9L2TGyyL4JEQuQ0ESIqhnoCA2tX8Gc/CwmcLhY4rpJxAscOtbW1NG7cOLrkkkscp79mgZNmpGMdKxHsOkqKLikSdakeO+zMjBgJoRkz7Fl29MZps9Nq5IYSDMaXlTA7pcFgkL73ve/RWWddH7GvvxOQTYBEgwYNo6ysLHr66aeJiBxH4dieiU32ReDEghPZeemgwjVMrF2Ba68NF9pkgeMqvVbgaNNSZ5xxBtXW1lJDQ0NosQMLnDQk3epYWWHXcdHOwzxFoi7Vs4V2Sp6ZCaEZM4j69RM7lptvjrbMiJxWo9Nj13Vl7ty5VFBQQGvXfhwyTqxY0UXHHXcceb1eys7OpnvvvdeVvhWegRG9CJx6ajsNkSsvD89ZpovF16Svun7/+3Adqo8/Tn5bDiF6rcCZP38+Gfno2IEFTpqSjhl59dqUiKNk7BIbQp5CUZfK2UI746qVENIcke2cBi3CWPS0GvWDiE7XrDEbN26kgQMH0p///OeobQwZMoSGDh1KkiSRJEl02mmn0bp169zvdD1EBcjw4Yk55IsKHI8n3sFJO7mptviaWLv8//xnSODIL7/cM+05ROi1AsctWOAwQhjNg4g6XJgt2sM4zTIZp2q2UHRmRLTShpPTYfc3RkYMOzr9hhtuoPHjx4d8DC+66CLyer20ePFi2rZtG/3whz8kSZLoH//4h+t9boiIF3eiF0RlpXMfNa1dsSq2p18OTFR5xzvvqAKnrY2UlSt7rk2HACxwLGCBw1hiNt/g5KEcm2wsjaff3J4tFBnwRS04yRI4TpZEws01K87rr79OkiTR+++/T5WVlSRJEl133XXU1NQUWnfx4sXOdpIIIv5liZr0RBMIGS3Dh8eH2PUkJtau9h07VIGzbVvKX1p6G07Gb4mIKDllPNOPAwcOoKCgAC0tLRgwYECqm9P7kWVg9Wq1nPOwYWpZ5lRXGTdDloGRI4Ha2sS3JUlqqerNm4H33suYPnDrlFVVqcXWI7vS51OLQkcWO9e6vK5OHSFi0bpx/nzgrLPstyNZSFJihdsDgQAmT56MAQMG4P3338fo0aPxzDPPYMKECfD7/cjNzXW3wXZ46y2xzq6pASZPtr/9lSvVcvCJUF6ulqdPFVVVwLRp6v+7L1waNAid9fUAAE9TE3J9vlS1rlfiaPxOmtxKQ9iC04OkQ1pcu7hZDCmdo8CSjNOEs2bTYyJ+Qj6f+nKfDB9wt40YRESPP/44SZJEAwcOpGeffTaxTneTZEdUuVWTI9X3V0VF1HSb/P3vcw6cJOJk/PYkU3Exhyja202sJaSuTv28qio17bKioUF8XUky/q60NLHX+wxGllXLjZ41RvusrExdT2PqVLW7hg+PXt/nC3ej16taf4D4rtf+njcPeOwx/XXchgjYuVO1djnl8ssvx9/+9jf89re/xUUXXdS93TQwqA8b5u56sZidTDvEXkg9SVUVcPvtQFNT6CNlwoTQ/z3JvgAZMZKnt9IPtuD0AOmSFteqjXrOIaIWnPJy/fTvZWVp4TCcSpKRcDYSET8hvXXM6p5ql6STotSxRgzNx0Y0f01HRwe1trba+k3S6amwOrOaHKInIAVlTYxMlF333RcOET+EnwHJgp2MLWCB0wOkU0IuPcymzuw82NMxtD0N6ImkgSJdH7vOkiViUWLa7+65x9llrCdSRAa7tBE3Gj0VVqd3MisrnSvMZGPyAte5bFk4RDwQ6Nl2HQKwwLGABU4PkOq0uGaIOIdkanblNCGd9a2dKDG7RoyGhgZatmwZlZWV0fTp0+mpp56it956K2J7QV0R46TkTI+RyiScoikZevpCMgsR37RJFTgNDaSk6gWuF8MCxwIWOD1Auo5wdlPmZlJ25TQi3UuM2TG8iWrdjo4OOvPMM8nj8YSS9EmSRHl5eXThhRfStm3bIvYf3qEsy/Tiiy/S0qVLyZ+uladTZalM16lugxc4paAgXEVcC2FnXIUFjgUscHoAt0c4tx6wdoUXT0E5JlEjWDp1vYjWvfTSS6mwsJB+97vfUV1dHX344Yf0/PPP07hx40iSJMrJyaEnn3wytL5mtfnggw9IkiQqKSmhrVu39vShpT/paE01eI4ETzstHEE1d27qpuB7MSxwLGCB00O49WByM9Q8nafOeiFOjWDpmF3ATHC9//771KdPH5oxYwYdPHiQiML+NF1dXTR79mzKzc0lSZLo+uuvp3379oV+u3HjRpo2bRrdddddtvd7yJBu1lSDF7iusrJwkc2yskP0ZCUXFjgWsMDpQRJ9MNlNpmJFuk6d9WLsDtBun/Ke4LXXXiNJkmjRokVEpIoaouipqLfffpvGjBlDkiTRHXfcQZ2dnaHv9u7dq7vddBR6PUbsheP3p5fS03mB8z//fNjB+M03U9u+XgoLHAtY4PQwTl9BkzH/nu7OIYc4Tk95qq0c1dXVJEkSPfLII3HfRToQ79mzh04//XTq27cvrbSoUZSJQs81MkXZxbSzY8MGVeC0tqZfRFwvgQWOBSxwMoRkWVvScU6fISJnpzwdxsKGhgYqKSmh4cOHU01NjWmY+BtvvEGSJNGll15KAYMw4nT1re0RMk3ZdatrZenSkPWmI8I6x7gLZzJmegeiGYXtZB4GxFLmMinB7ilPl2TZQ4cOxe9//3vs2bMH999/Pz7//PO4dbzdxbzOPvtsHHfccdixYweISHd7q1ebl0IjSjyDclriJAV2qvF6gcmToZx3Xugjj4eH1HSCzwaTfiQzVfzUqcD27WqhwIUL1X+3bWNxk2LsnPJ0Gwuvvvpq3HrrrXjrrbdw/vnn45VXXon6XlEUAEBjYyNkWUZOTg46Ojp0t5UsbZ/2ZLCy084vAHhZ4KQVfDaY9GPSJNWqYlTPRZLUek+TJjnbfvebF37xC/XfNK7ufahg55Sn21iYl5eHP/7xj7jvvvuwc+dOnHvuuZg+fTrWrVuHpqam0Fv9smXLsHnz5lAVcT2SXQYqbclgZadEKG224KQXWaluAMPEoRXjmzZNHdkiX9W1EfDRR1mY9CLsnPJ0HAtzcnJwzz334IwzzsBNN92Ev//97/j3v/+NCRMmYOzYsairq8Obb76Jo48+Gr/73e8Mt6MJvbo6fQuVJKnfO9X2aUuGKjsiCllwJEmCxEU20wqWm0x6wv4yhxyipzydx8KTTz4Zq1evxhNPPIETTzwRK1aswF//+le8/vrruOyyy7B48WLT34tUTe+V2j7ZVtskEWW9YXGTdkhk5O3WCzlw4AAKCgrQ0tJiaCJm0gxZVucaGhrUEWvSpF74dGcisTrlsgyMHGlt5di2LbWXSldXFxobG9Hc3IxBgwbB5/MJ/7aqSvUzipyKKy1VxU2v1faa5zigb8JLwxebQDCIYDAIAMjOykJWFk+KJAsn4zcLHIZhMo50GAvJ3w4E/ZD6DkrK9g9JbZ9hys7f1RWaosrNyWEfnCTCAscCFjgM03tI5VhIgS7Imz4AujrhGXU8PAWDk7vDQ4kMUXZEhE6/HwAgAcjNzWUfnCTCAscCFjgM07tIxVhIchDy5rVAe4v6QV4/eL8zEZLEb++HErIsoysQAKDmOsrJzk5xi3o3TsZvnjBkGCZj0SL+ewpSZMhbPwqLm+xceMdMYHFzCBKZ/4anptITPisMwzACkCJD2fox0LpP/cCbDe+YEyHl5Ke2YUxKkDnBX9rDZ4VhGMYCUhQo29aDDu5VP/BkwTv2REj5/VPbMCYlKIoSKrfh4fw3aQsLHIZhGBOIFCjffgo60Kh+4PGq01J9ClLbMCZlRE1PpaEDNKPCPjgMwzAGkNItbvbvVj+QPPCMPgFSv+SEhjOZAU9PZQYscBiGYXQgRVanpTTLjeSBZ/T34OlflNqGMSklsjwDAJ6eSmNY4DAMw8SgORSHfG40cTOgJKHtZkiKF8aEKOuN18sCJ41hgcMwDBMByUE1FFyLlvJ44Rl9QsKWG73EhD6fWnsqDZP0MgYoshz6P09PpTd8dhiGYbqhYBfkzWsixE0WvGNOdEXcTJsWLW4AtZ7WtGnq90z6Q0RRFhzOf5Pe8NlhGIYBQF0dkDd9CLQfUD/wZsM77qSEHYplWbXc6OWM1z4rK1PXY9IbhaenMgoWOAzDHPJQx0HI33wAdLapH2Tlwjvu+66Egq9eHW+5ido3ATt3qusx6Y3M01MZBfvgMAxzSKMc3Atl6yeAElQ/yO2jZijO7ePK9hsa3F2PSQ08PZV5sMBhGOaQRWmug7JjY3iuqE8BvKMnQMrOcW0fw4a5ux6TGjh6KvNgCcowzCEHEUHZtRnKt5+FxI00oATesSe5Km4ANRTc5wOMxkNJAkpL1fWY9CVqeopj+zMCFjgMwxxSkCJD+XYDlIbNoc+k4hFqhmKv+0Ztr1cNBQfiRY7296OPcj6cdCYyuZ8Etf4Uk/6wwGEY5pCBujohb/oQtC/s8OI57Eh4fOOTOuUwdSqwdCkwfHj05z6f+jnnwUlvYq03PD2VGbAPDsMwhwTUtg/y1vVA0K9+4PHCc/h34Rk4pEf2P3UqcO65nMk40yAiBHl6KiNhgcMwTK9H2VsLZefnYWfinHx4R58AKb9/j7bD6wUmT+7RXTIJQkQgzU9Lkjh6KoNggcMwTK+FFBlK7ZegveFENFK/QnhGHQ8py11nYqZ3Emm9yWLrTUbBAodhmF4J+dshb1sPdBwIfSYVj4DH9x1IEr+FM9YQEUdPZTAscBiG6XUo+3dD2fEZIHcn75M88JQeBU+RL7UNYzIKdi7ObFjgMAzTayBFgdLwDWjP9vCHuX3gHfW9Hve3YTKbWOdinp7KPFjgMAzTK6DONsjffhoulglAGjgUnhHHJCW/DdO7YefizIfveoZhMhoiAjXXQan9ElC637glCZ7DjoRUcjhPKzCOCAaDof+z9SYzYYHDMEzGQsEAlJ2fg/bvCn+Y2wfekce5UgmcOTSJLazJzsWZCQschmEyEuVAk1ooM9AZ+kwq8sEz/Ds8JcUkRKz1hq2AmQk/BRiGyShIDkKp+xq0d2f4Q282PCOOhmfg0NQ1jOkVxGUuzuJhMlPhM8cwTMagHNyrhn93RVht+hfBM+JYSDl5KWwZ01uICg33eLiwZgbDAodhmLSH5CCU+q9BTRFWG49XdSQuLuUpBMYV4kLD2XqT0fDZYxgmrVH274ZS+wUQ8Ic/7DcI3hHHQsrtk7qGMb0ORVFCoeEej4dDwzMcFjgMw6Ql1NUJpfYLUMue8IceLzyHHQGpeARbbRhXISIODe9lsMBhGCatIFJAjTugNGwK57UBIPUvhqf0KLbaMElBURQonNivV8ECh2GYtIFa90He+QXQeTD8YVaOGvo9aBhbbZikEet7w9da5sMCh2GYlEOBTih134D21Ud9LhX51CmprJwUtYw5FJAVBUp3Yj9JkuBl602vgAUOwzApgxQF1PgtlF2bo6ajkD8A3tLxkPoOSl3jmEMGTuzXO2GBwzBMj0NEoP27oNR/A3R1hL/wZsMzbByHfjM9Rpz1hp2Lew0scBiG6VGobT/kuq+Atv1Rn0tFpfAcNo6no5geha03vZeMmWj8+c9/jhEjRiAvLw/Dhg3DFVdcgfr6eusfMgyTFlBnG+Rt6yF/80GUuJH6F8F75ER4RxzN4obpUdh607vJGIEzZcoUVFRU4Ouvv0ZlZSW2bNmCadOmpbpZDMNYQF0dkHdshPzlf6Orfuf1hWf0BHjGnAipz4DUNZA5JCEiBAOB0N8cOdX7kEhL25hhvPzyyzjvvPPg9/uRnZ0t9JsDBw6goKAALS0tGDCAH6gMk0wo0AVl9xZQ0w4g8jGTlQPP0LGQin2QpIx5x2J6GbIso6tb4EiShNycHBY4aYyT8TsjfXCam5vx4osvYuLEicLihmGYnoECXVD2bFOFTWRklDcLnsGjIJUcDsmbkY8eppdARAhE+N5ks/WmV5JRT5nf/va3ePzxx9He3o6TTz4Zr776qun6fr8ffn+4fs2BAweS3USGOWShgL9b2OyMFjaSB1LJ4fAMGcU+NkxaIMtyuOYUZy3utaT0rM6ZMweSJJku69atC60/Y8YMfPLJJ3jzzTfh9Xpx5ZVXwmyG7YEHHkBBQUFoKS0t7YnDYphDCurqgFz7JeTPV4H2bA+LG0mCVDwC3qNPg3f4kSxumLQg1nqTlZ3N1pteSkp9cJqamtDU1GS6zsiRI5GXlxf3eW1tLUpLS/Hee+/hlFNO0f2tngWntLSUfXAYxgWo46BqsWluABDxGJE8kIpL1emonPh7l2FSSSAYDIWGezwe5Oaw8M4EMs4Hp7i4GMXFxY5+q+mySAETS25uLnJzcx1tn2EYY+S6r1RrTSSasBkyClI2Cxsm/YitGJ6dlVFeGoxNMuLsrlmzBmvWrMGpp56KQYMGYevWrZg1axbGjBljaL3RQxNF7IvDMImhBCQorW3qH95sVdgUlULKzgE6utSFYdIMTeAEZRlerxeBLr5OMwVt3LYz6ZQRAic/Px9VVVWYPXs22traMGzYMPzkJz/B4sWLbVloDh5UKxSzLw7DMAzDZB4HDx5EQUGB0LoZmwfHCYqioL6+Hv3793fFqUzz6dm5cyf79NiE+8453HfO4b5zDvedc7jvnKP13Y4dOyBJEg477DDhqLeMsOC4hcfjgc/nc327AwYM4IvWIdx3zuG+cw73nXO475zDfeecgoIC233Hwf8MwzAMw/Q6WOAwDMMwDNPrYIGTALm5uZg9ezaHojuA+8453HfO4b5zDvedc7jvnJNI3x1STsYMwzAMwxwasAWHYRiGYZheBwschmEYhmF6HSxwGIZhGIbpdbDAYRiGYRim18ECxyV+/vOfY8SIEcjLy8OwYcNwxRVXoL6+PtXNSnu2b9+Oa6+9FqNGjUJ+fj7GjBmD2bNno4trxAhx//33Y+LEiejTpw8GDhyY6uakNX/7298watQo5OXlYcKECVi9enWqm5QRvPPOO/jf//1fHHbYYZAkCf/+979T3aSM4YEHHsBJJ52E/v37Y/DgwTjvvPPw9ddfp7pZGcGTTz6J7373u6HkiKeccgpee+01W9tggeMSU6ZMQUVFBb7++mtUVlZiy5YtmDZtWqqblfZ89dVXUBQFTz/9ND7//HPMnTsXTz31FO65555UNy0j6OrqwoUXXogbbrgh1U1Ja1566SWUlZXh3nvvxSeffIJJkybhpz/9KXbs2JHqpqU9bW1tOO644/D444+nuikZx6pVq3DTTTfhgw8+wIoVKxAMBnH22Wejra0t1U1Le3w+H/70pz9h3bp1WLduHc444wyce+65+Pzzz4W3wWHiSeLll1/GeeedB7/fj+zs7FQ3J6N46KGH8OSTT2Lr1q2pbkrG8Pzzz6OsrAz79+9PdVPSkh/84Ac44YQT8OSTT4Y+Gz9+PM477zw88MADKWxZZiFJEpYtW4bzzjsv1U3JSBobGzF48GCsWrUKp512Wqqbk3EUFhbioYcewrXXXiu0PltwkkBzczNefPFFTJw4kcWNA1paWlBYWJjqZjC9hK6uLnz00Uc4++yzoz4/++yz8d5776WoVcyhSEtLCwDw880msixj8eLFaGtrwymnnCL8OxY4LvLb3/4Wffv2RVFREXbs2IHly5enukkZx5YtW/DXv/4V06dPT3VTmF5CU1MTZFnGkCFDoj4fMmQIdu3alaJWMYcaRxf72wAABXVJREFURITbb78dp556Ko455phUNycj+Oyzz9CvXz/k5uZi+vTpWLZsGY466ijh37PAMWHOnDmQJMl0WbduXWj9GTNm4JNPPsGbb74Jr9eLK6+8EofqDKDdvgOA+vp6/OQnP8GFF16I6667LkUtTz1O+o6xRpKkqL+JKO4zhkkWN998MzZs2IBFixaluikZw5FHHon169fjgw8+wA033ICrrroKX3zxhfDvs5LYtozn5ptvxiWXXGK6zsiRI0P/Ly4uRnFxMY444giMHz8epaWl+OCDD2yZ1HoLdvuuvr4eU6ZMwSmnnIK///3vSW5demO37xhziouL4fV646w1e/bsibPqMEwyuOWWW/Dyyy/jnXfegc/nS3VzMoacnByMHTsWAHDiiSdi7dq1mDdvHp5++mmh37PAMUETLE7QLDd+v9/NJmUMdvqurq4OU6ZMwYQJEzB//nx4PIe2YTGR646JJycnBxMmTMCKFStw/vnnhz5fsWIFzj333BS2jOntEBFuueUWLFu2DCtXrsSoUaNS3aSMhohsjakscFxgzZo1WLNmDU499VQMGjQIW7duxaxZszBmzJhD0npjh/r6ekyePBkjRozAww8/jMbGxtB3Q4cOTWHLMoMdO3agubkZO3bsgCzLWL9+PQBg7Nix6NevX2obl0bcfvvtuOKKK3DiiSeGrIQ7duxgXy8BWltbsXnz5tDf27Ztw/r161FYWIgRI0aksGXpz0033YSFCxdi+fLl6N+/f8iKWFBQgPz8/BS3Lr2555578NOf/hSlpaU4ePAgFi9ejJUrV+L1118X3wgxCbNhwwaaMmUKFRYWUm5uLo0cOZKmT59OtbW1qW5a2jN//nwCoLsw1lx11VW6fVdTU5PqpqUdTzzxBB1++OGUk5NDJ5xwAq1atSrVTcoIampqdK+xq666KtVNS3uMnm3z589PddPSnmuuuSZ0v5aUlNCZZ55Jb775pq1tcB4chmEYhmF6HYe2swPDMAzDML0SFjgMwzAMw/Q6WOAwDMMwDNPrYIHDMAzDMEyvgwUOwzAMwzC9DhY4DMMwDMP0OljgMAzDMAzT62CBwzAMwzBMr4MFDsMwGYEsy5g4cSIuuOCCqM9bWlpQWlqKmTNnAgBuu+02TJgwAbm5uTj++ONT0FKGYdIBFjgMw2QEXq8XL7zwAl5//XW8+OKLoc9vueUWFBYWYtasWQDUgnzXXHMNLr744lQ1lWGYNICLbTIMkzGMGzcODzzwAG655RZMmTIFa9euxeLFi7FmzRrk5OQAAB577DEAQGNjIzZs2JDK5jIMk0JY4DAMk1HccsstWLZsGa688kp89tlnmDVrFk9FMQwTBwschmEyCkmS8OSTT2L8+PE49thjcffdd6e6SQzDpCHsg8MwTMbx3HPPoU+fPti2bRtqa2tT3RyGYdIQFjgMw2QU77//PubOnYvly5fjlFNOwbXXXgsiSnWzGIZJM1jgMAyTMXR0dOCqq67Cr3/9a5x11ll49tlnsXbtWjz99NOpbhrDMGkGCxyGYTKGu+++G4qi4M9//jMAYMSIEXjkkUcwY8YMbN++HQCwefNmrF+/Hrt27UJHRwfWr1+P9evXo6urK4UtZximp5GIbbsMw2QAq1atwplnnomVK1fi1FNPjfruxz/+MYLBIKqrqzFlyhSsWrUq7vfbtm3DyJEje6i1DMOkGhY4DMMwDMP0OniKimEYhmGYXgcLHIZhGIZheh0scBiGYRiG6XWwwGEYhmEYptfBAodhGIZhmF4HCxyGYRiGYXodLHAYhmEYhul1sMBhGIZhGKbXwQKHYRiGYZheBwschmEYhmF6HSxwGIZhGIbpdbDAYRiGYRim1/H/Afe/UohAhKtDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/882dxcks2c78wz1dfm9f0ghm0000gn/T/ipykernel_24720/4045588789.py:29: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha = 0.0005 # 学习率，试着自己调整一下\n",
    "n_steps = 30 # epoch迭代次数，试着自己调整一下\n",
    "\n",
    "# 这里使用极坐标做线性回归，实际上是用阿基米德螺旋线做分类\n",
    "w, ll_train, ll_test = fit_w(X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha)\n",
    "\n",
    "X_tilde_test_polar = get_x_tilde(polar(X_test))\n",
    "y_pred_prob = predict(X_tilde_test_polar, w)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy:\", np.mean(y_test == y_pred))\n",
    "print(\"Classification Report (scikit-learn):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "plot_loss(-ll_train)\n",
    "plot_loss(-ll_test)\n",
    "plot_predictive_distribution(X, y, w, polar)\n",
    "\n",
    "c, b, a = w\n",
    "polar_draw(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.00000000e+000, 1.00000000e+000, 5.48786966e-042, ...,\n",
       "         2.78706166e-127, 6.01951765e-006, 2.23143248e-014],\n",
       "        [1.00000000e+000, 5.48786966e-042, 1.00000000e+000, ...,\n",
       "         9.69542806e-055, 2.54422127e-037, 8.71561710e-064],\n",
       "        [1.00000000e+000, 1.66590526e-063, 4.87090492e-006, ...,\n",
       "         1.15848792e-080, 1.34679343e-048, 7.34037039e-073],\n",
       "        [1.00000000e+000, 6.42620407e-063, 2.63407185e-003, ...,\n",
       "         1.53051755e-060, 6.77274456e-053, 3.46725564e-081],\n",
       "        [1.00000000e+000, 1.34146659e-049, 6.94194925e-017, ...,\n",
       "         4.05932269e-129, 6.71293461e-029, 1.07700142e-041]]),\n",
       " array([[1.00000000e+000, 3.72434846e-026, 3.45349466e-073, ...,\n",
       "         5.16795389e-091, 1.11772594e-053, 8.41750139e-076],\n",
       "        [1.00000000e+000, 4.99681934e-139, 3.86580289e-030, ...,\n",
       "         1.31495527e-076, 1.20087146e-120, 5.22700341e-158],\n",
       "        [1.00000000e+000, 9.48308332e-016, 5.23201650e-042, ...,\n",
       "         2.33179217e-070, 4.48662591e-035, 8.68725670e-058],\n",
       "        [1.00000000e+000, 2.47081557e-020, 4.99406622e-102, ...,\n",
       "         7.59920694e-246, 5.36499177e-017, 2.55975682e-007],\n",
       "        [1.00000000e+000, 1.30142029e-008, 4.38597959e-029, ...,\n",
       "         2.50602700e-138, 1.81658095e-001, 1.31230135e-007]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 核函数：用于扩充数据维度，常常用于SVM\n",
    "# Radial Basis Function (RBF)： 定义为空间中任一点x到某一中心xc之间欧氏距离的单调函数，若两点距离很近则接近1，如果距离很远就接近0\n",
    "# TODO: 尝试使用其他的核函数，观察效果是否会提升\n",
    "def RBF(l, X, Z):\n",
    "    X2 = np.sum(X**2, 1)\n",
    "    Z2 = np.sum(Z**2, 1)\n",
    "    ones_Z = np.ones(Z.shape[ 0 ])\n",
    "    ones_X = np.ones(X.shape[ 0 ])\n",
    "    r2 = np.outer(X2, ones_Z) - 2 * np.dot(X, Z.T) + np.outer(ones_X, Z2)\n",
    "    return np.exp(-0.5 / l**2 * r2)\n",
    "\n",
    "l = 0.1 # 高斯核函数的宽度，试着自己调整一下？\n",
    "\n",
    "X_tilde_train = get_x_tilde(RBF(l, X_train, X_train))\n",
    "X_tilde_test = get_x_tilde(RBF(l, X_test, X_train))\n",
    "\n",
    "X_tilde_train[:5], X_tilde_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss: 0.8912089458837933, test loss: 0.7931425810991988\n",
      "epoch 2: train loss: 0.8801769092009536, test loss: 0.783299681691163\n",
      "epoch 3: train loss: 0.8701127810973105, test loss: 0.77445495930372\n",
      "epoch 4: train loss: 0.8609044757746847, test loss: 0.7664874124495068\n",
      "epoch 5: train loss: 0.852451579111571, test loss: 0.7592889011775946\n",
      "epoch 6: train loss: 0.844664533272796, test loss: 0.7527632429783278\n",
      "epoch 7: train loss: 0.8374637747460129, test loss: 0.7468252519376054\n",
      "epoch 8: train loss: 0.8307788610059563, test loss: 0.7413997629106928\n",
      "epoch 9: train loss: 0.8245476119866271, test loss: 0.7364206716236614\n",
      "epoch 10: train loss: 0.8187152849905999, test loss: 0.7318300125600612\n",
      "epoch 11: train loss: 0.8132337955329794, test loss: 0.7275770891806576\n",
      "epoch 12: train loss: 0.80806099177563, test loss: 0.7236176652874069\n",
      "epoch 13: train loss: 0.8031599864875512, test loss: 0.7199132219664903\n",
      "epoch 14: train loss: 0.7984985476913533, test loss: 0.7164302813055196\n",
      "epoch 15: train loss: 0.7940485471518274, test loss: 0.7131397957634615\n",
      "epoch 16: train loss: 0.7897854644748051, test loss: 0.7100166004865233\n",
      "epoch 17: train loss: 0.7856879436774621, test loss: 0.7070389248438482\n",
      "epoch 18: train loss: 0.7817373985510256, test loss: 0.7041879588654837\n",
      "epoch 19: train loss: 0.7779176628701354, test loss: 0.7014474699895549\n",
      "epoch 20: train loss: 0.7742146814349765, test loss: 0.698803465476801\n",
      "epoch 21: train loss: 0.7706162380038664, test loss: 0.6962438959591043\n",
      "epoch 22: train loss: 0.7671117163397622, test loss: 0.6937583958012664\n",
      "epoch 23: train loss: 0.7636918908196648, test loss: 0.6913380562321728\n",
      "epoch 24: train loss: 0.7603487433155229, test loss: 0.6889752275133371\n",
      "epoch 25: train loss: 0.7570753033302665, test loss: 0.6866633467385296\n",
      "epoch 26: train loss: 0.7538655086498042, test loss: 0.6843967881830773\n",
      "epoch 27: train loss: 0.7507140840420825, test loss: 0.6821707334355049\n",
      "epoch 28: train loss: 0.74761643579164, test loss: 0.6799810588411725\n",
      "epoch 29: train loss: 0.744568560098864, test loss: 0.6778242380637056\n",
      "epoch 30: train loss: 0.7415669635953713, test loss: 0.675697257823505\n",
      "epoch 31: train loss: 0.7386085944298759, test loss: 0.6735975451029605\n",
      "epoch 32: train loss: 0.7356907825626241, test loss: 0.6715229043155824\n",
      "epoch 33: train loss: 0.7328111880716486, test loss: 0.6694714631221166\n",
      "epoch 34: train loss: 0.7299677564217221, test loss: 0.6674416257421727\n",
      "epoch 35: train loss: 0.7271586797781938, test loss: 0.6654320327565405\n",
      "epoch 36: train loss: 0.7243823635642049, test loss: 0.6634415265248218\n",
      "epoch 37: train loss: 0.7216373975624515, test loss: 0.6614691214569284\n",
      "epoch 38: train loss: 0.7189225309530178, test loss: 0.659513978476946\n",
      "epoch 39: train loss: 0.7162366507581247, test loss: 0.6575753831053681\n",
      "epoch 40: train loss: 0.7135787632341103, test loss: 0.6556527266621387\n",
      "epoch 41: train loss: 0.7109479778116914, test loss: 0.6537454901595996\n",
      "epoch 42: train loss: 0.7083434932385562, test loss: 0.6518532305124707\n",
      "epoch 43: train loss: 0.7057645856245265, test loss: 0.6499755687424512\n",
      "epoch 44: train loss: 0.7032105981297196, test loss: 0.6481121798988512\n",
      "epoch 45: train loss: 0.7006809320710858, test loss: 0.6462627844546822\n",
      "epoch 46: train loss: 0.6981750392530366, test loss: 0.644427140970582\n",
      "epoch 47: train loss: 0.6956924153542045, test loss: 0.6426050398474896\n",
      "epoch 48: train loss: 0.6932325942252033, test loss: 0.6407962980136794\n",
      "epoch 49: train loss: 0.690795142972021, test loss: 0.6390007544131155\n",
      "epoch 50: train loss: 0.6883796577167971, test loss: 0.6372182661805527\n",
      "epoch 51: train loss: 0.6859857599425471, test loss: 0.6354487054047453\n",
      "epoch 52: train loss: 0.6836130933412037, test loss: 0.6336919563948978\n",
      "epoch 53: train loss: 0.6812613210954105, test loss: 0.6319479133773643\n",
      "epoch 54: train loss: 0.6789301235340729, test loss: 0.6302164785598524\n",
      "epoch 55: train loss: 0.6766191961099293, test loss: 0.6284975605092246\n",
      "epoch 56: train loss: 0.6743282476545392, test loss: 0.6267910727965962\n",
      "epoch 57: train loss: 0.6720569988722416, test loss: 0.6250969328700003\n",
      "epoch 58: train loss: 0.6698051810399406, test loss: 0.6234150611205264\n",
      "epoch 59: train loss: 0.6675725348841742, test loss: 0.6217453801127171\n",
      "epoch 60: train loss: 0.6653588096108506, test loss: 0.6200878139541819\n",
      "epoch 61: train loss: 0.6631637620664677, test loss: 0.6184422877829925\n",
      "epoch 62: train loss: 0.660987156012548, test loss: 0.6168087273545195\n",
      "epoch 63: train loss: 0.6588287614975695, test loss: 0.6151870587120358\n",
      "epoch 64: train loss: 0.6566883543128421, test loss: 0.6135772079276888\n",
      "epoch 65: train loss: 0.6545657155206688, test loss: 0.6119791009024221\n",
      "epoch 66: train loss: 0.6524606310447435, test loss: 0.6103926632151024\n",
      "epoch 67: train loss: 0.6503728913141401, test loss: 0.6088178200125572\n",
      "epoch 68: train loss: 0.6483022909534413, test loss: 0.6072544959334762\n",
      "epoch 69: train loss: 0.646248628512597, test loss: 0.6057026150601827\n",
      "epoch 70: train loss: 0.6442117062309941, test loss: 0.6041621008932044\n",
      "epoch 71: train loss: 0.6421913298309786, test loss: 0.602632876344342\n",
      "epoch 72: train loss: 0.6401873083367482, test loss: 0.601114863744612\n",
      "epoch 73: train loss: 0.6381994539150843, test loss: 0.5996079848640021\n",
      "epoch 74: train loss: 0.6362275817348986, test loss: 0.5981121609404751\n",
      "epoch 75: train loss: 0.6342715098429822, test loss: 0.596627312716062\n",
      "epoch 76: train loss: 0.632331059053711, test loss: 0.5951533604782488\n",
      "epoch 77: train loss: 0.6304060528507762, test loss: 0.593690224105162\n",
      "epoch 78: train loss: 0.6284963172992767, test loss: 0.5922378231133102\n",
      "epoch 79: train loss: 0.6266016809667394, test loss: 0.5907960767068647\n",
      "epoch 80: train loss: 0.6247219748518404, test loss: 0.5893649038276411\n",
      "epoch 81: train loss: 0.6228570323197641, test loss: 0.5879442232051051\n",
      "epoch 82: train loss: 0.6210066890432898, test loss: 0.586533953405856\n",
      "epoch 83: train loss: 0.6191707829488238, test loss: 0.5851340128821536\n",
      "epoch 84: train loss: 0.6173491541667, test loss: 0.5837443200191487\n",
      "epoch 85: train loss: 0.6155416449851729, test loss: 0.5823647931805561\n",
      "epoch 86: train loss: 0.6137480998076021, test loss: 0.5809953507525762\n",
      "epoch 87: train loss: 0.6119683651124019, test loss: 0.5796359111859217\n",
      "epoch 88: train loss: 0.610202289415388, test loss: 0.5782863930358595\n",
      "epoch 89: train loss: 0.6084497232342061, test loss: 0.5769467150002053\n",
      "epoch 90: train loss: 0.6067105190545701, test loss: 0.5756167959552506\n",
      "epoch 91: train loss: 0.6049845312980803, test loss: 0.5742965549896168\n",
      "epoch 92: train loss: 0.6032716162914178, test loss: 0.572985911436056\n",
      "epoch 93: train loss: 0.6015716322367491, test loss: 0.5716847849012371\n",
      "epoch 94: train loss: 0.5998844391831909, test loss: 0.5703930952935605\n",
      "epoch 95: train loss: 0.5982098989992127, test loss: 0.5691107628490636\n",
      "epoch 96: train loss: 0.5965478753458678, test loss: 0.5678377081554794\n",
      "epoch 97: train loss: 0.5948982336507632, test loss: 0.5665738521745198\n",
      "epoch 98: train loss: 0.5932608410826888, test loss: 0.5653191162624591\n",
      "epoch 99: train loss: 0.5916355665268411, test loss: 0.564073422189091\n",
      "epoch 100: train loss: 0.590022280560582, test loss: 0.562836692155141\n",
      "epoch 101: train loss: 0.5884208554296894, test loss: 0.5616088488082087\n",
      "epoch 102: train loss: 0.5868311650250522, test loss: 0.5603898152573187\n",
      "epoch 103: train loss: 0.585253084859781, test loss: 0.5591795150861568\n",
      "epoch 104: train loss: 0.5836864920467018, test loss: 0.5579778723650645\n",
      "epoch 105: train loss: 0.5821312652762083, test loss: 0.5567848116618654\n",
      "epoch 106: train loss: 0.5805872847944535, test loss: 0.5556002580515945\n",
      "epoch 107: train loss: 0.5790544323818607, test loss: 0.5544241371251956\n",
      "epoch 108: train loss: 0.5775325913319408, test loss: 0.5532563749972534\n",
      "epoch 109: train loss: 0.5760216464304033, test loss: 0.5520968983128242\n",
      "epoch 110: train loss: 0.5745214839345493, test loss: 0.5509456342534201\n",
      "epoch 111: train loss: 0.5730319915529393, test loss: 0.549802510542209\n",
      "epoch 112: train loss: 0.5715530584253276, test loss: 0.548667455448478\n",
      "epoch 113: train loss: 0.570084575102858, test loss: 0.5475403977914176\n",
      "epoch 114: train loss: 0.5686264335285149, test loss: 0.5464212669432696\n",
      "epoch 115: train loss: 0.5671785270178252, test loss: 0.5453099928318877\n",
      "epoch 116: train loss: 0.5657407502398084, test loss: 0.5442065059427514\n",
      "epoch 117: train loss: 0.5643129991981725, test loss: 0.543110737320476\n",
      "epoch 118: train loss: 0.5628951712127498, test loss: 0.5420226185698545\n",
      "epoch 119: train loss: 0.5614871649011746, test loss: 0.540942081856469\n",
      "epoch 120: train loss: 0.5600888801607968, test loss: 0.539869059906904\n",
      "epoch 121: train loss: 0.558700218150834, test loss: 0.5388034860085927\n",
      "epoch 122: train loss: 0.5573210812747541, test loss: 0.5377452940093272\n",
      "epoch 123: train loss: 0.555951373162895, test loss: 0.5366944183164581\n",
      "epoch 124: train loss: 0.5545909986553125, test loss: 0.5356507938958104\n",
      "epoch 125: train loss: 0.5532398637848596, test loss: 0.5346143562703394\n",
      "epoch 126: train loss: 0.5518978757604937, test loss: 0.5335850415185476\n",
      "epoch 127: train loss: 0.5505649429508106, test loss: 0.5325627862726856\n",
      "epoch 128: train loss: 0.5492409748678054, test loss: 0.5315475277167537\n",
      "epoch 129: train loss: 0.5479258821508555, test loss: 0.5305392035843239\n",
      "epoch 130: train loss: 0.5466195765509272, test loss: 0.5295377521561973\n",
      "epoch 131: train loss: 0.5453219709150041, test loss: 0.5285431122579136\n",
      "epoch 132: train loss: 0.5440329791707323, test loss: 0.5275552232571261\n",
      "epoch 133: train loss: 0.542752516311286, test loss: 0.5265740250608559\n",
      "epoch 134: train loss: 0.5414804983804455, test loss: 0.5255994581126358\n",
      "epoch 135: train loss: 0.540216842457893, test loss: 0.5246314633895574\n",
      "epoch 136: train loss: 0.5389614666447163, test loss: 0.5236699823992299\n",
      "epoch 137: train loss: 0.5377142900491251, test loss: 0.5227149571766613\n",
      "epoch 138: train loss: 0.5364752327723739, test loss: 0.5217663302810684\n",
      "epoch 139: train loss: 0.5352442158948904, test loss: 0.5208240447926279\n",
      "epoch 140: train loss: 0.534021161462609, test loss: 0.5198880443091702\n",
      "epoch 141: train loss: 0.5328059924735029, test loss: 0.5189582729428291\n",
      "epoch 142: train loss: 0.5315986328643176, test loss: 0.5180346753166465\n",
      "epoch 143: train loss: 0.5303990074974994, test loss: 0.517117196561144\n",
      "epoch 144: train loss: 0.5292070421483182, test loss: 0.5162057823108629\n",
      "epoch 145: train loss: 0.5280226634921844, test loss: 0.5153003787008777\n",
      "epoch 146: train loss: 0.5268457990921511, test loss: 0.5144009323632894\n",
      "epoch 147: train loss: 0.5256763773866076, test loss: 0.5135073904236989\n",
      "epoch 148: train loss: 0.5245143276771528, test loss: 0.5126197004976664\n",
      "epoch 149: train loss: 0.523359580116655, test loss: 0.5117378106871615\n",
      "epoch 150: train loss: 0.5222120656974881, test loss: 0.5108616695770027\n",
      "epoch 151: train loss: 0.5210717162399451, test loss: 0.5099912262312916\n",
      "epoch 152: train loss: 0.519938464380826, test loss: 0.5091264301898437\n",
      "epoch 153: train loss: 0.5188122435621991, test loss: 0.5082672314646187\n",
      "epoch 154: train loss: 0.5176929880203286, test loss: 0.5074135805361496\n",
      "epoch 155: train loss: 0.5165806327747703, test loss: 0.5065654283499748\n",
      "epoch 156: train loss: 0.5154751136176327, test loss: 0.5057227263130746\n",
      "epoch 157: train loss: 0.514376367102997, test loss: 0.5048854262903109\n",
      "epoch 158: train loss: 0.513284330536498, test loss: 0.5040534806008747\n",
      "epoch 159: train loss: 0.5121989419650618, test loss: 0.5032268420147403\n",
      "epoch 160: train loss: 0.511120140166796, test loss: 0.5024054637491265\n",
      "epoch 161: train loss: 0.5100478646410331, test loss: 0.5015892994649694\n",
      "epoch 162: train loss: 0.5089820555985227, test loss: 0.5007783032634018\n",
      "epoch 163: train loss: 0.5079226539517696, test loss: 0.4999724296822454\n",
      "epoch 164: train loss: 0.5068696013055182, test loss: 0.49917163369251305\n",
      "epoch 165: train loss: 0.5058228399473766, test loss: 0.498375870694922\n",
      "epoch 166: train loss: 0.5047823128385833, test loss: 0.49758509651641947\n",
      "epoch 167: train loss: 0.5037479636049081, test loss: 0.4967992674067204\n",
      "epoch 168: train loss: 0.5027197365276915, test loss: 0.4960183400348578\n",
      "epoch 169: train loss: 0.5016975765350139, test loss: 0.49524227148574546\n",
      "epoch 170: train loss: 0.5006814291929983, test loss: 0.49447101925675485\n",
      "epoch 171: train loss: 0.4996712406972399, test loss: 0.4937045412543036\n",
      "epoch 172: train loss: 0.49866695786436255, test loss: 0.49294279579045897\n",
      "epoch 173: train loss: 0.49766852812370005, test loss: 0.4921857415795546\n",
      "epoch 174: train loss: 0.4966758995090988, test loss: 0.4914333377348202\n",
      "epoch 175: train loss: 0.49568902065084186, test loss: 0.4906855437650265\n",
      "epoch 176: train loss: 0.4947078407676902, test loss: 0.48994231957114287\n",
      "epoch 177: train loss: 0.4937323096590414, test loss: 0.4892036254430103\n",
      "epoch 178: train loss: 0.4927623776972014, test loss: 0.4884694220560278\n",
      "epoch 179: train loss: 0.49179799581977, test loss: 0.487739670467853\n",
      "epoch 180: train loss: 0.4908391155221365, test loss: 0.48701433211511796\n",
      "epoch 181: train loss: 0.4898856888500836, test loss: 0.4862933688101586\n",
      "epoch 182: train loss: 0.4889376683925002, test loss: 0.4855767427377591\n",
      "epoch 183: train loss: 0.4879950072741969, test loss: 0.48486441645191064\n",
      "epoch 184: train loss: 0.48705765914882737, test loss: 0.4841563528725854\n",
      "epoch 185: train loss: 0.4861255781919104, test loss: 0.4834525152825244\n",
      "epoch 186: train loss: 0.4851987190939527, test loss: 0.482752867324041\n",
      "epoch 187: train loss: 0.4842770370536701, test loss: 0.4820573729958384\n",
      "epoch 188: train loss: 0.4833604877713064, test loss: 0.48136599664984386\n",
      "epoch 189: train loss: 0.48244902744204643, test loss: 0.4806787029880556\n",
      "epoch 190: train loss: 0.48154261274952476, test loss: 0.4799954570594069\n",
      "epoch 191: train loss: 0.4806412008594261, test loss: 0.4793162242566443\n",
      "epoch 192: train loss: 0.4797447494131762, test loss: 0.4786409703132215\n",
      "epoch 193: train loss: 0.47885321652172363, test loss: 0.47796966130020907\n",
      "epoch 194: train loss: 0.477966560759409, test loss: 0.477302263623219\n",
      "epoch 195: train loss: 0.4770847411579206, test loss: 0.47663874401934536\n",
      "epoch 196: train loss: 0.4762077172003367, test loss: 0.47597906955412056\n",
      "epoch 197: train loss: 0.4753354488152516, test loss: 0.4753232076184877\n",
      "epoch 198: train loss: 0.47446789637098336, test loss: 0.47467112592578853\n",
      "epoch 199: train loss: 0.4736050206698652, test loss: 0.4740227925087679\n",
      "epoch 200: train loss: 0.47274678294261574, test loss: 0.473378175716594\n",
      "epoch 201: train loss: 0.4718931448427891, test loss: 0.47273724421189556\n",
      "epoch 202: train loss: 0.4710440684413023, test loss: 0.4720999669678146\n",
      "epoch 203: train loss: 0.4701995162210405, test loss: 0.4714663132650768\n",
      "epoch 204: train loss: 0.469359451071537, test loss: 0.47083625268907753\n",
      "epoch 205: train loss: 0.46852383628372757, test loss: 0.4702097551269858\n",
      "epoch 206: train loss: 0.46769263554477924, test loss: 0.4695867907648642\n",
      "epoch 207: train loss: 0.4668658129329906, test loss: 0.46896733008480623\n",
      "epoch 208: train loss: 0.4660433329127631, test loss: 0.4683513438620913\n",
      "epoch 209: train loss: 0.4652251603296434, test loss: 0.4677388031623565\n",
      "epoch 210: train loss: 0.4644112604054342, test loss: 0.4671296793387855\n",
      "epoch 211: train loss: 0.46360159873337353, test loss: 0.46652394402931646\n",
      "epoch 212: train loss: 0.462796141273381, test loss: 0.46592156915386573\n",
      "epoch 213: train loss: 0.46199485434737086, test loss: 0.465322526911571\n",
      "epoch 214: train loss: 0.461197704634629, test loss: 0.4647267897780509\n",
      "epoch 215: train loss: 0.46040465916725554, test loss: 0.4641343305026842\n",
      "epoch 216: train loss: 0.4596156853256705, test loss: 0.4635451221059059\n",
      "epoch 217: train loss: 0.45883075083418134, test loss: 0.46295913787652204\n",
      "epoch 218: train loss: 0.4580498237566127, test loss: 0.46237635136904276\n",
      "epoch 219: train loss: 0.4572728724919965, test loss: 0.461796736401034\n",
      "epoch 220: train loss: 0.4564998657703218, test loss: 0.46122026705048824\n",
      "epoch 221: train loss: 0.4557307726483444, test loss: 0.46064691765321214\n",
      "epoch 222: train loss: 0.4549655625054534, test loss: 0.46007666280023474\n",
      "epoch 223: train loss: 0.4542042050395959, test loss: 0.459509477335234\n",
      "epoch 224: train loss: 0.4534466702632581, test loss: 0.4589453363519816\n",
      "epoch 225: train loss: 0.4526929284995018, test loss: 0.4583842151918074\n",
      "epoch 226: train loss: 0.4519429503780563, test loss: 0.45782608944108283\n",
      "epoch 227: train loss: 0.451196706831464, test loss: 0.4572709349287231\n",
      "epoch 228: train loss: 0.4504541690912798, test loss: 0.456718727723709\n",
      "epoch 229: train loss: 0.4497153086843228, test loss: 0.45616944413262717\n",
      "epoch 230: train loss: 0.4489800974289801, test loss: 0.4556230606972312\n",
      "epoch 231: train loss: 0.44824850743156175, test loss: 0.4550795541920204\n",
      "epoch 232: train loss: 0.4475205110827065, test loss: 0.4545389016218385\n",
      "epoch 233: train loss: 0.44679608105383667, test loss: 0.45400108021949265\n",
      "epoch 234: train loss: 0.4460751902936626, test loss: 0.45346606744339046\n",
      "epoch 235: train loss: 0.445357812024735, test loss: 0.4529338409751975\n",
      "epoch 236: train loss: 0.44464391974004613, test loss: 0.452404378717514\n",
      "epoch 237: train loss: 0.4439334871996762, test loss: 0.45187765879157105\n",
      "epoch 238: train loss: 0.44322648842748813, test loss: 0.45135365953494655\n",
      "epoch 239: train loss: 0.44252289770786635, test loss: 0.4508323594993001\n",
      "epoch 240: train loss: 0.44182268958250215, test loss: 0.4503137374481287\n",
      "epoch 241: train loss: 0.44112583884722234, test loss: 0.4497977723545397\n",
      "epoch 242: train loss: 0.4404323205488624, test loss: 0.4492844433990467\n",
      "epoch 243: train loss: 0.439742109982183, test loss: 0.44877372996738146\n",
      "epoch 244: train loss: 0.43905518268682814, test loss: 0.4482656116483274\n",
      "epoch 245: train loss: 0.43837151444432704, test loss: 0.44776006823157255\n",
      "epoch 246: train loss: 0.4376910812751359, test loss: 0.4472570797055806\n",
      "epoch 247: train loss: 0.4370138594357213, test loss: 0.4467566262554833\n",
      "epoch 248: train loss: 0.4363398254156835, test loss: 0.4462586882609901\n",
      "epoch 249: train loss: 0.4356689559349205, test loss: 0.44576324629431935\n",
      "epoch 250: train loss: 0.4350012279408294, test loss: 0.4452702811181473\n",
      "epoch 251: train loss: 0.4343366186055485, test loss: 0.4447797736835766\n",
      "epoch 252: train loss: 0.43367510532323583, test loss: 0.4442917051281245\n",
      "epoch 253: train loss: 0.43301666570738634, test loss: 0.44380605677373014\n",
      "epoch 254: train loss: 0.432361277588185, test loss: 0.4433228101247802\n",
      "epoch 255: train loss: 0.4317089190098977, test loss: 0.44284194686615463\n",
      "epoch 256: train loss: 0.43105956822829783, test loss: 0.44236344886129075\n",
      "epoch 257: train loss: 0.4304132037081262, test loss: 0.4418872981502659\n",
      "epoch 258: train loss: 0.42976980412059, test loss: 0.44141347694789984\n",
      "epoch 259: train loss: 0.4291293483408925, test loss: 0.4409419676418754\n",
      "epoch 260: train loss: 0.42849181544579873, test loss: 0.44047275279087733\n",
      "epoch 261: train loss: 0.4278571847112339, test loss: 0.44000581512275083\n",
      "epoch 262: train loss: 0.42722543560991555, test loss: 0.43954113753267726\n",
      "epoch 263: train loss: 0.4265965478090172, test loss: 0.4390787030813695\n",
      "epoch 264: train loss: 0.42597050116786533, test loss: 0.4386184949932845\n",
      "epoch 265: train loss: 0.42534727573566705, test loss: 0.438160496654855\n",
      "epoch 266: train loss: 0.4247268517492692, test loss: 0.4377046916127385\n",
      "epoch 267: train loss: 0.4241092096309487, test loss: 0.4372510635720851\n",
      "epoch 268: train loss: 0.42349432998623243, test loss: 0.4367995963948223\n",
      "epoch 269: train loss: 0.42288219360174767, test loss: 0.4363502740979584\n",
      "epoch 270: train loss: 0.4222727814431012, test loss: 0.4359030808519033\n",
      "epoch 271: train loss: 0.4216660746527887, test loss: 0.43545800097880644\n",
      "epoch 272: train loss: 0.4210620545481312, test loss: 0.4350150189509131\n",
      "epoch 273: train loss: 0.42046070261924057, test loss: 0.4345741193889371\n",
      "epoch 274: train loss: 0.41986200052701333, test loss: 0.43413528706045135\n",
      "epoch 275: train loss: 0.41926593010115043, test loss: 0.4336985068782955\n",
      "epoch 276: train loss: 0.418672473338205, test loss: 0.43326376389900007\n",
      "epoch 277: train loss: 0.4180816123996573, test loss: 0.43283104332122774\n",
      "epoch 278: train loss: 0.41749332961001373, test loss: 0.4324003304842319\n",
      "epoch 279: train loss: 0.4169076074549342, test loss: 0.4319716108663309\n",
      "epoch 280: train loss: 0.4163244285793835, test loss: 0.43154487008339926\n",
      "epoch 281: train loss: 0.4157437757858075, test loss: 0.43112009388737527\n",
      "epoch 282: train loss: 0.41516563203233553, test loss: 0.430697268164785\n",
      "epoch 283: train loss: 0.4145899804310056, test loss: 0.43027637893528164\n",
      "epoch 284: train loss: 0.4140168042460145, test loss: 0.4298574123502021\n",
      "epoch 285: train loss: 0.41344608689199147, test loss: 0.4294403546911377\n",
      "epoch 286: train loss: 0.412877811932295, test loss: 0.42902519236852266\n",
      "epoch 287: train loss: 0.41231196307733226, test loss: 0.4286119119202364\n",
      "epoch 288: train loss: 0.4117485241829026, test loss: 0.4282005000102223\n",
      "epoch 289: train loss: 0.41118747924856097, test loss: 0.42779094342712126\n",
      "epoch 290: train loss: 0.41062881241600585, test loss: 0.4273832290829207\n",
      "epoch 291: train loss: 0.4100725079674874, test loss: 0.4269773440116185\n",
      "epoch 292: train loss: 0.4095185503242369, test loss: 0.4265732753679022\n",
      "epoch 293: train loss: 0.40896692404491797, test loss: 0.42617101042584227\n",
      "epoch 294: train loss: 0.40841761382409786, test loss: 0.42577053657760033\n",
      "epoch 295: train loss: 0.40787060449073964, test loss: 0.4253718413321528\n",
      "epoch 296: train loss: 0.4073258810067143, test loss: 0.4249749123140268\n",
      "epoch 297: train loss: 0.40678342846533255, test loss: 0.4245797372620524\n",
      "epoch 298: train loss: 0.4062432320898962, test loss: 0.4241863040281284\n",
      "epoch 299: train loss: 0.4057052772322693, test loss: 0.4237946005760007\n",
      "epoch 300: train loss: 0.4051695493714672, test loss: 0.42340461498005666\n",
      "epoch 301: train loss: 0.40463603411226545, test loss: 0.4230163354241316\n",
      "epoch 302: train loss: 0.40410471718382623, test loss: 0.42262975020032917\n",
      "epoch 303: train loss: 0.40357558443834324, test loss: 0.42224484770785575\n",
      "epoch 304: train loss: 0.4030486218497048, test loss: 0.4218616164518673\n",
      "epoch 305: train loss: 0.4025238155121741, test loss: 0.42148004504233016\n",
      "epoch 306: train loss: 0.40200115163908656, test loss: 0.42110012219289383\n",
      "epoch 307: train loss: 0.40148061656156514, test loss: 0.4207218367197779\n",
      "epoch 308: train loss: 0.4009621967272519, test loss: 0.420345177540671\n",
      "epoch 309: train loss: 0.40044587869905596, test loss: 0.41997013367364205\n",
      "epoch 310: train loss: 0.3999316491539186, test loss: 0.41959669423606455\n",
      "epoch 311: train loss: 0.3994194948815934, test loss: 0.4192248484435534\n",
      "epoch 312: train loss: 0.3989094027834432, test loss: 0.41885458560891364\n",
      "epoch 313: train loss: 0.3984013598712525, test loss: 0.4184858951411006\n",
      "epoch 314: train loss: 0.3978953532660543, test loss: 0.4181187665441934\n",
      "epoch 315: train loss: 0.39739137019697396, test loss: 0.41775318941637934\n",
      "epoch 316: train loss: 0.3968893980000858, test loss: 0.4173891534489502\n",
      "epoch 317: train loss: 0.3963894241172866, test loss: 0.41702664842531\n",
      "epoch 318: train loss: 0.395891436095182, test loss: 0.41666566421999507\n",
      "epoch 319: train loss: 0.39539542158398805, test loss: 0.41630619079770353\n",
      "epoch 320: train loss: 0.3949013683364463, test loss: 0.4159482182123385\n",
      "epoch 321: train loss: 0.39440926420675326, test loss: 0.4155917366060603\n",
      "epoch 322: train loss: 0.39391909714950374, test loss: 0.41523673620835055\n",
      "epoch 323: train loss: 0.3934308552186465, test loss: 0.41488320733508677\n",
      "epoch 324: train loss: 0.39294452656645473, test loss: 0.41453114038762856\n",
      "epoch 325: train loss: 0.39246009944250815, test loss: 0.41418052585191295\n",
      "epoch 326: train loss: 0.3919775621926891, test loss: 0.41383135429756135\n",
      "epoch 327: train loss: 0.39149690325819103, test loss: 0.41348361637699615\n",
      "epoch 328: train loss: 0.39101811117453905, test loss: 0.4131373028245685\n",
      "epoch 329: train loss: 0.3905411745706234, test loss: 0.4127924044556947\n",
      "epoch 330: train loss: 0.39006608216774497, test loss: 0.4124489121660042\n",
      "epoch 331: train loss: 0.38959282277867224, test loss: 0.41210681693049644\n",
      "epoch 332: train loss: 0.38912138530671087, test loss: 0.4117661098027078\n",
      "epoch 333: train loss: 0.3886517587447843, test loss: 0.41142678191388854\n",
      "epoch 334: train loss: 0.38818393217452607, test loss: 0.4110888244721883\n",
      "epoch 335: train loss: 0.38771789476538315, test loss: 0.4107522287618525\n",
      "epoch 336: train loss: 0.38725363577373106, test loss: 0.4104169861424265\n",
      "epoch 337: train loss: 0.3867911445419992, test loss: 0.4100830880479702\n",
      "epoch 338: train loss: 0.38633041049780686, test loss: 0.40975052598628137\n",
      "epoch 339: train loss: 0.38587142315311146, test loss: 0.4094192915381275\n",
      "epoch 340: train loss: 0.38541417210336504, test loss: 0.4090893763564874\n",
      "epoch 341: train loss: 0.3849586470266824, test loss: 0.40876077216580087\n",
      "epoch 342: train loss: 0.38450483768301985, test loss: 0.4084334707612275\n",
      "epoch 343: train loss: 0.3840527339133624, test loss: 0.40810746400791365\n",
      "epoch 344: train loss: 0.38360232563892277, test loss: 0.4077827438402688\n",
      "epoch 345: train loss: 0.38315360286034816, test loss: 0.4074593022612488\n",
      "epoch 346: train loss: 0.3827065556569387, test loss: 0.4071371313416489\n",
      "epoch 347: train loss: 0.3822611741858734, test loss: 0.40681622321940414\n",
      "epoch 348: train loss: 0.3818174486814474, test loss: 0.40649657009889817\n",
      "epoch 349: train loss: 0.3813753694543163, test loss: 0.40617816425027925\n",
      "epoch 350: train loss: 0.38093492689075154, test loss: 0.40586099800878583\n",
      "epoch 351: train loss: 0.38049611145190343, test loss: 0.4055450637740783\n",
      "epoch 352: train loss: 0.38005891367307343, test loss: 0.4052303540095787\n",
      "epoch 353: train loss: 0.37962332416299516, test loss: 0.40491686124181947\n",
      "epoch 354: train loss: 0.37918933360312423, test loss: 0.40460457805979755\n",
      "epoch 355: train loss: 0.3787569327469357, test loss: 0.4042934971143377\n",
      "epoch 356: train loss: 0.3783261124192306, test loss: 0.4039836111174624\n",
      "epoch 357: train loss: 0.3778968635154504, test loss: 0.40367491284176865\n",
      "epoch 358: train loss: 0.3774691770009996, test loss: 0.4033673951198128\n",
      "epoch 359: train loss: 0.3770430439105764, test loss: 0.4030610508435018\n",
      "epoch 360: train loss: 0.3766184553475109, test loss: 0.4027558729634921\n",
      "epoch 361: train loss: 0.37619540248311156, test loss: 0.40245185448859416\n",
      "epoch 362: train loss: 0.3757738765560185, test loss: 0.4021489884851856\n",
      "epoch 363: train loss: 0.3753538688715656, test loss: 0.40184726807662957\n",
      "epoch 364: train loss: 0.37493537080114825, test loss: 0.4015466864427003\n",
      "epoch 365: train loss: 0.37451837378160063, test loss: 0.401247236819016\n",
      "epoch 366: train loss: 0.37410286931457826, test loss: 0.40094891249647646\n",
      "epoch 367: train loss: 0.3736888489659485, test loss: 0.4006517068207088\n",
      "epoch 368: train loss: 0.37327630436518816, test loss: 0.4003556131915191\n",
      "epoch 369: train loss: 0.3728652272047882, test loss: 0.4000606250623495\n",
      "epoch 370: train loss: 0.3724556092396639, test loss: 0.39976673593974277\n",
      "epoch 371: train loss: 0.3720474422865743, test loss: 0.3994739393828114\n",
      "epoch 372: train loss: 0.37164071822354555, test loss: 0.39918222900271433\n",
      "epoch 373: train loss: 0.3712354289893027, test loss: 0.398891598462139\n",
      "epoch 374: train loss: 0.37083156658270683, test loss: 0.39860204147478895\n",
      "epoch 375: train loss: 0.37042912306219977, test loss: 0.3983135518048773\n",
      "epoch 376: train loss: 0.3700280905452534, test loss: 0.39802612326662673\n",
      "epoch 377: train loss: 0.3696284612078269, test loss: 0.3977397497237743\n",
      "epoch 378: train loss: 0.36923022728382876, test loss: 0.39745442508908213\n",
      "epoch 379: train loss: 0.3688333810645858, test loss: 0.397170143323854\n",
      "epoch 380: train loss: 0.3684379148983174, test loss: 0.39688689843745645\n",
      "epoch 381: train loss: 0.368043821189616, test loss: 0.39660468448684677\n",
      "epoch 382: train loss: 0.3676510923989333, test loss: 0.3963234955761048\n",
      "epoch 383: train loss: 0.36725972104207216, test loss: 0.396043325855971\n",
      "epoch 384: train loss: 0.36686969968968397, test loss: 0.39576416952338916\n",
      "epoch 385: train loss: 0.3664810209667722, test loss: 0.39548602082105505\n",
      "epoch 386: train loss: 0.3660936775522005, test loss: 0.39520887403696897\n",
      "epoch 387: train loss: 0.3657076621782066, test loss: 0.39493272350399394\n",
      "epoch 388: train loss: 0.36532296762992206, test loss: 0.3946575635994197\n",
      "epoch 389: train loss: 0.3649395867448967, test loss: 0.3943833887445297\n",
      "epoch 390: train loss: 0.3645575124126282, test loss: 0.39411019340417497\n",
      "epoch 391: train loss: 0.36417673757409685, test loss: 0.39383797208635074\n",
      "epoch 392: train loss: 0.3637972552213062, test loss: 0.39356671934177967\n",
      "epoch 393: train loss: 0.36341905839682725, test loss: 0.3932964297634986\n",
      "epoch 394: train loss: 0.3630421401933485, test loss: 0.3930270979864503\n",
      "epoch 395: train loss: 0.3626664937532312, test loss: 0.39275871868707946\n",
      "epoch 396: train loss: 0.36229211226806796, test loss: 0.3924912865829334\n",
      "epoch 397: train loss: 0.3619189889782479, test loss: 0.39222479643226776\n",
      "epoch 398: train loss: 0.36154711717252497, test loss: 0.3919592430336549\n",
      "epoch 399: train loss: 0.3611764901875918, test loss: 0.39169462122559895\n",
      "epoch 400: train loss: 0.3608071014076575, test loss: 0.3914309258861532\n",
      "epoch 401: train loss: 0.3604389442640307, test loss: 0.39116815193254256\n",
      "epoch 402: train loss: 0.36007201223470614, test loss: 0.3909062943207902\n",
      "epoch 403: train loss: 0.35970629884395644, test loss: 0.3906453480453483\n",
      "epoch 404: train loss: 0.35934179766192725, test loss: 0.3903853081387325\n",
      "epoch 405: train loss: 0.3589785023042378, test loss: 0.39012616967116065\n",
      "epoch 406: train loss: 0.35861640643158493, test loss: 0.38986792775019585\n",
      "epoch 407: train loss: 0.35825550374935117, test loss: 0.3896105775203927\n",
      "epoch 408: train loss: 0.35789578800721755, test loss: 0.38935411416294813\n",
      "epoch 409: train loss: 0.35753725299878, test loss: 0.38909853289535545\n",
      "epoch 410: train loss: 0.35717989256116955, test loss: 0.3888438289710626\n",
      "epoch 411: train loss: 0.3568237005746774, test loss: 0.3885899976791346\n",
      "epoch 412: train loss: 0.3564686709623824, test loss: 0.3883370343439179\n",
      "epoch 413: train loss: 0.35611479768978427, test loss: 0.38808493432471053\n",
      "epoch 414: train loss: 0.3557620747644388, test loss: 0.38783369301543497\n",
      "epoch 415: train loss: 0.3554104962355978, test loss: 0.3875833058443138\n",
      "epoch 416: train loss: 0.35506005619385284, test loss: 0.38733376827355026\n",
      "epoch 417: train loss: 0.35471074877078224, test loss: 0.38708507579901136\n",
      "epoch 418: train loss: 0.35436256813860145, test loss: 0.38683722394991493\n",
      "epoch 419: train loss: 0.35401550850981806, test loss: 0.38659020828851937\n",
      "epoch 420: train loss: 0.353669564136889, test loss: 0.38634402440981747\n",
      "epoch 421: train loss: 0.35332472931188214, test loss: 0.3860986679412335\n",
      "epoch 422: train loss: 0.35298099836614116, test loss: 0.38585413454232304\n",
      "epoch 423: train loss: 0.3526383656699534, test loss: 0.38561041990447636\n",
      "epoch 424: train loss: 0.3522968256322217, test loss: 0.38536751975062516\n",
      "epoch 425: train loss: 0.351956372700139, test loss: 0.38512542983495224\n",
      "epoch 426: train loss: 0.35161700135886675, test loss: 0.384884145942604\n",
      "epoch 427: train loss: 0.3512787061312157, test loss: 0.384643663889407\n",
      "epoch 428: train loss: 0.350941481577331, test loss: 0.3844039795215866\n",
      "epoch 429: train loss: 0.3506053222943798, test loss: 0.3841650887154887\n",
      "epoch 430: train loss: 0.35027022291624177, test loss: 0.3839269873773047\n",
      "epoch 431: train loss: 0.34993617811320304, test loss: 0.3836896714428001\n",
      "epoch 432: train loss: 0.34960318259165374, test loss: 0.3834531368770442\n",
      "epoch 433: train loss: 0.3492712310937871, test loss: 0.3832173796741447\n",
      "epoch 434: train loss: 0.34894031839730305, test loss: 0.3829823958569834\n",
      "epoch 435: train loss: 0.3486104393151134, test loss: 0.3827481814769562\n",
      "epoch 436: train loss: 0.34828158869505155, test loss: 0.3825147326137144\n",
      "epoch 437: train loss: 0.3479537614195831, test loss: 0.38228204537491\n",
      "epoch 438: train loss: 0.34762695240552133, test loss: 0.38205011589594307\n",
      "epoch 439: train loss: 0.3473011566037432, test loss: 0.38181894033971164\n",
      "epoch 440: train loss: 0.34697636899891093, test loss: 0.38158851489636475\n",
      "epoch 441: train loss: 0.34665258460919346, test loss: 0.38135883578305774\n",
      "epoch 442: train loss: 0.3463297984859925, test loss: 0.38112989924371005\n",
      "epoch 443: train loss: 0.3460080057136706, test loss: 0.3809017015487662\n",
      "epoch 444: train loss: 0.34568720140928205, test loss: 0.38067423899495806\n",
      "epoch 445: train loss: 0.34536738072230605, test loss: 0.38044750790507104\n",
      "epoch 446: train loss: 0.34504853883438286, test loss: 0.3802215046277114\n",
      "epoch 447: train loss: 0.3447306709590521, test loss: 0.3799962255370774\n",
      "epoch 448: train loss: 0.3444137723414941, test loss: 0.3797716670327314\n",
      "epoch 449: train loss: 0.3440978382582731, test loss: 0.37954782553937527\n",
      "epoch 450: train loss: 0.34378286401708374, test loss: 0.3793246975066279\n",
      "epoch 451: train loss: 0.34346884495649876, test loss: 0.37910227940880475\n",
      "epoch 452: train loss: 0.3431557764457206, test loss: 0.3788805677447006\n",
      "epoch 453: train loss: 0.34284365388433385, test loss: 0.37865955903737286\n",
      "epoch 454: train loss: 0.34253247270206183, test loss: 0.3784392498339294\n",
      "epoch 455: train loss: 0.34222222835852334, test loss: 0.37821963670531594\n",
      "epoch 456: train loss: 0.34191291634299353, test loss: 0.3780007162461084\n",
      "epoch 457: train loss: 0.34160453217416636, test loss: 0.3777824850743052\n",
      "epoch 458: train loss: 0.34129707139991905, test loss: 0.3775649398311224\n",
      "epoch 459: train loss: 0.3409905295970795, test loss: 0.37734807718079166\n",
      "epoch 460: train loss: 0.34068490237119425, test loss: 0.37713189381035966\n",
      "epoch 461: train loss: 0.3403801853563015, test loss: 0.37691638642948894\n",
      "epoch 462: train loss: 0.34007637421470316, test loss: 0.37670155177026204\n",
      "epoch 463: train loss: 0.33977346463674096, test loss: 0.37648738658698705\n",
      "epoch 464: train loss: 0.3394714523405743, test loss: 0.37627388765600484\n",
      "epoch 465: train loss: 0.3391703330719595, test loss: 0.37606105177549876\n",
      "epoch 466: train loss: 0.3388701026040323, test loss: 0.3758488757653062\n",
      "epoch 467: train loss: 0.338570756737091, test loss: 0.3756373564667316\n",
      "epoch 468: train loss: 0.3382722912983828, test loss: 0.37542649074236223\n",
      "epoch 469: train loss: 0.33797470214189124, test loss: 0.3752162754758851\n",
      "epoch 470: train loss: 0.3376779851481265, test loss: 0.37500670757190574\n",
      "epoch 471: train loss: 0.33738213622391683, test loss: 0.3747977839557693\n",
      "epoch 472: train loss: 0.337087151302202, test loss: 0.3745895015733832\n",
      "epoch 473: train loss: 0.3367930263418294, test loss: 0.37438185739104163\n",
      "epoch 474: train loss: 0.33649975732735127, test loss: 0.3741748483952514\n",
      "epoch 475: train loss: 0.3362073402688239, test loss: 0.3739684715925604\n",
      "epoch 476: train loss: 0.33591577120160865, test loss: 0.37376272400938704\n",
      "epoch 477: train loss: 0.33562504618617567, test loss: 0.3735576026918517\n",
      "epoch 478: train loss: 0.3353351613079076, test loss: 0.37335310470560956\n",
      "epoch 479: train loss: 0.3350461126769072, test loss: 0.37314922713568593\n",
      "epoch 480: train loss: 0.33475789642780457, test loss: 0.37294596708631234\n",
      "epoch 481: train loss: 0.33447050871956846, test loss: 0.3727433216807647\n",
      "epoch 482: train loss: 0.334183945735317, test loss: 0.3725412880612027\n",
      "epoch 483: train loss: 0.3338982036821318, test loss: 0.37233986338851155\n",
      "epoch 484: train loss: 0.33361327879087294, test loss: 0.37213904484214466\n",
      "epoch 485: train loss: 0.33332916731599566, test loss: 0.3719388296199678\n",
      "epoch 486: train loss: 0.33304586553536936, test loss: 0.37173921493810547\n",
      "epoch 487: train loss: 0.33276336975009735, test loss: 0.3715401980307882\n",
      "epoch 488: train loss: 0.3324816762843386, test loss: 0.3713417761502015\n",
      "epoch 489: train loss: 0.33220078148513155, test loss: 0.37114394656633637\n",
      "epoch 490: train loss: 0.3319206817222184, test loss: 0.3709467065668417\n",
      "epoch 491: train loss: 0.33164137338787214, test loss: 0.370750053456877\n",
      "epoch 492: train loss: 0.33136285289672457, test loss: 0.37055398455896776\n",
      "epoch 493: train loss: 0.3310851166855957, test loss: 0.3703584972128617\n",
      "epoch 494: train loss: 0.330808161213325, test loss: 0.3701635887753863\n",
      "epoch 495: train loss: 0.3305319829606042, test loss: 0.36996925662030816\n",
      "epoch 496: train loss: 0.3302565784298111, test loss: 0.3697754981381934\n",
      "epoch 497: train loss: 0.3299819441448453, test loss: 0.36958231073626935\n",
      "epoch 498: train loss: 0.32970807665096513, test loss: 0.3693896918382881\n",
      "epoch 499: train loss: 0.3294349725146267, test loss: 0.36919763888439094\n",
      "epoch 500: train loss: 0.3291626283233231, test loss: 0.36900614933097386\n",
      "epoch 501: train loss: 0.3288910406854261, test loss: 0.36881522065055533\n",
      "epoch 502: train loss: 0.3286202062300287, test loss: 0.3686248503316443\n",
      "epoch 503: train loss: 0.3283501216067897, test loss: 0.36843503587861043\n",
      "epoch 504: train loss: 0.3280807834857786, test loss: 0.3682457748115542\n",
      "epoch 505: train loss: 0.3278121885573229, test loss: 0.3680570646661802\n",
      "epoch 506: train loss: 0.3275443335318559, test loss: 0.36786890299367003\n",
      "epoch 507: train loss: 0.32727721513976654, test loss: 0.3676812873605573\n",
      "epoch 508: train loss: 0.32701083013125015, test loss: 0.3674942153486031\n",
      "epoch 509: train loss: 0.32674517527616037, test loss: 0.3673076845546737\n",
      "epoch 510: train loss: 0.32648024736386305, test loss: 0.3671216925906184\n",
      "epoch 511: train loss: 0.3262160432030903, test loss: 0.3669362370831493\n",
      "epoch 512: train loss: 0.3259525596217969, test loss: 0.3667513156737217\n",
      "epoch 513: train loss: 0.3256897934670178, test loss: 0.366566926018416\n",
      "epoch 514: train loss: 0.3254277416047255, test loss: 0.3663830657878204\n",
      "epoch 515: train loss: 0.32516640091969085, test loss: 0.36619973266691525\n",
      "epoch 516: train loss: 0.32490576831534357, test loss: 0.36601692435495814\n",
      "epoch 517: train loss: 0.3246458407136339, test loss: 0.3658346385653696\n",
      "epoch 518: train loss: 0.32438661505489635, test loss: 0.36565287302562105\n",
      "epoch 519: train loss: 0.32412808829771433, test loss: 0.36547162547712275\n",
      "epoch 520: train loss: 0.32387025741878517, test loss: 0.3652908936751136\n",
      "epoch 521: train loss: 0.32361311941278714, test loss: 0.36511067538855124\n",
      "epoch 522: train loss: 0.32335667129224777, test loss: 0.3649309684000036\n",
      "epoch 523: train loss: 0.3231009100874123, test loss: 0.36475177050554186\n",
      "epoch 524: train loss: 0.32284583284611423, test loss: 0.36457307951463364\n",
      "epoch 525: train loss: 0.32259143663364626, test loss: 0.3643948932500375\n",
      "epoch 526: train loss: 0.3223377185326332, test loss: 0.3642172095476988\n",
      "epoch 527: train loss: 0.3220846756429049, test loss: 0.36404002625664594\n",
      "epoch 528: train loss: 0.32183230508137095, test loss: 0.36386334123888814\n",
      "epoch 529: train loss: 0.32158060398189653, test loss: 0.3636871523693138\n",
      "epoch 530: train loss: 0.3213295694951787, test loss: 0.36351145753558955\n",
      "epoch 531: train loss: 0.32107919878862434, test loss: 0.3633362546380613\n",
      "epoch 532: train loss: 0.3208294890462289, test loss: 0.3631615415896551\n",
      "epoch 533: train loss: 0.320580437468456, test loss: 0.36298731631577946\n",
      "epoch 534: train loss: 0.32033204127211834, test loss: 0.3628135767542283\n",
      "epoch 535: train loss: 0.3200842976902593, test loss: 0.36264032085508524\n",
      "epoch 536: train loss: 0.31983720397203635, test loss: 0.3624675465806284\n",
      "epoch 537: train loss: 0.31959075738260373, test loss: 0.36229525190523615\n",
      "epoch 538: train loss: 0.3193449552029984, test loss: 0.36212343481529374\n",
      "epoch 539: train loss: 0.3190997947300247, test loss: 0.36195209330910116\n",
      "epoch 540: train loss: 0.3188552732761417, test loss: 0.36178122539678126\n",
      "epoch 541: train loss: 0.31861138816935075, test loss: 0.3616108291001886\n",
      "epoch 542: train loss: 0.3183681367530835, test loss: 0.36144090245282057\n",
      "epoch 543: train loss: 0.31812551638609216, test loss: 0.3612714434997272\n",
      "epoch 544: train loss: 0.31788352444233947, test loss: 0.3611024502974236\n",
      "epoch 545: train loss: 0.3176421583108899, test loss: 0.3609339209138019\n",
      "epoch 546: train loss: 0.31740141539580224, test loss: 0.36076585342804535\n",
      "epoch 547: train loss: 0.3171612931160227, test loss: 0.36059824593054174\n",
      "epoch 548: train loss: 0.31692178890527845, test loss: 0.36043109652279864\n",
      "epoch 549: train loss: 0.3166829002119729, test loss: 0.36026440331735915\n",
      "epoch 550: train loss: 0.31644462449908173, test loss: 0.3600981644377178\n",
      "epoch 551: train loss: 0.31620695924404885, test loss: 0.35993237801823874\n",
      "epoch 552: train loss: 0.3159699019386844, test loss: 0.3597670422040729\n",
      "epoch 553: train loss: 0.31573345008906273, test loss: 0.3596021551510771\n",
      "epoch 554: train loss: 0.31549760121542186, test loss: 0.3594377150257332\n",
      "epoch 555: train loss: 0.3152623528520633, test loss: 0.3592737200050686\n",
      "epoch 556: train loss: 0.3150277025472528, test loss: 0.35911016827657705\n",
      "epoch 557: train loss: 0.3147936478631223, test loss: 0.35894705803814014\n",
      "epoch 558: train loss: 0.3145601863755718, test loss: 0.3587843874979497\n",
      "epoch 559: train loss: 0.31432731567417316, test loss: 0.3586221548744312\n",
      "epoch 560: train loss: 0.31409503336207384, test loss: 0.35846035839616663\n",
      "epoch 561: train loss: 0.31386333705590197, test loss: 0.3582989963018196\n",
      "epoch 562: train loss: 0.31363222438567134, test loss: 0.3581380668400604\n",
      "epoch 563: train loss: 0.31340169299468845, test loss: 0.3579775682694914\n",
      "epoch 564: train loss: 0.31317174053945945, test loss: 0.35781749885857367\n",
      "epoch 565: train loss: 0.3129423646895975, test loss: 0.3576578568855541\n",
      "epoch 566: train loss: 0.31271356312773224, test loss: 0.35749864063839304\n",
      "epoch 567: train loss: 0.3124853335494183, test loss: 0.35733984841469274\n",
      "epoch 568: train loss: 0.31225767366304635, test loss: 0.35718147852162635\n",
      "epoch 569: train loss: 0.312030581189753, test loss: 0.3570235292758676\n",
      "epoch 570: train loss: 0.3118040538633329, test loss: 0.35686599900352106\n",
      "epoch 571: train loss: 0.31157808943015086, test loss: 0.3567088860400527\n",
      "epoch 572: train loss: 0.31135268564905483, test loss: 0.3565521887302221\n",
      "epoch 573: train loss: 0.3111278402912897, test loss: 0.35639590542801386\n",
      "epoch 574: train loss: 0.31090355114041146, test loss: 0.35624003449657077\n",
      "epoch 575: train loss: 0.3106798159922024, test loss: 0.3560845743081269\n",
      "epoch 576: train loss: 0.31045663265458684, test loss: 0.3559295232439414\n",
      "epoch 577: train loss: 0.31023399894754744, test loss: 0.3557748796942334\n",
      "epoch 578: train loss: 0.3100119127030426, test loss: 0.35562064205811644\n",
      "epoch 579: train loss: 0.30979037176492386, test loss: 0.3554668087435345\n",
      "epoch 580: train loss: 0.30956937398885437, test loss: 0.35531337816719827\n",
      "epoch 581: train loss: 0.30934891724222835, test loss: 0.3551603487545215\n",
      "epoch 582: train loss: 0.3091289994040903, test loss: 0.35500771893955874\n",
      "epoch 583: train loss: 0.3089096183650555, test loss: 0.35485548716494253\n",
      "epoch 584: train loss: 0.3086907720272313, test loss: 0.35470365188182285\n",
      "epoch 585: train loss: 0.3084724583041382, test loss: 0.3545522115498047\n",
      "epoch 586: train loss: 0.3082546751206328, test loss: 0.354401164636889\n",
      "epoch 587: train loss: 0.30803742041282983, test loss: 0.35425050961941096\n",
      "epoch 588: train loss: 0.3078206921280263, test loss: 0.3541002449819824\n",
      "epoch 589: train loss: 0.3076044882246253, test loss: 0.3539503692174311\n",
      "epoch 590: train loss: 0.3073888066720606, test loss: 0.35380088082674327\n",
      "epoch 591: train loss: 0.30717364545072207, test loss: 0.3536517783190054\n",
      "epoch 592: train loss: 0.3069590025518816, test loss: 0.35350306021134675\n",
      "epoch 593: train loss: 0.30674487597761935, test loss: 0.3533547250288822\n",
      "epoch 594: train loss: 0.30653126374075085, test loss: 0.35320677130465633\n",
      "epoch 595: train loss: 0.30631816386475474, test loss: 0.3530591975795865\n",
      "epoch 596: train loss: 0.3061055743837008, test loss: 0.3529120024024084\n",
      "epoch 597: train loss: 0.30589349334217875, test loss: 0.35276518432962006\n",
      "epoch 598: train loss: 0.30568191879522744, test loss: 0.3526187419254281\n",
      "epoch 599: train loss: 0.30547084880826497, test loss: 0.3524726737616928\n",
      "epoch 600: train loss: 0.3052602814570187, test loss: 0.35232697841787525\n",
      "epoch 601: train loss: 0.3050502148274566, test loss: 0.3521816544809838\n",
      "epoch 602: train loss: 0.30484064701571856, test loss: 0.3520367005455212\n",
      "epoch 603: train loss: 0.3046315761280481, test loss: 0.35189211521343267\n",
      "epoch 604: train loss: 0.3044230002807256, test loss: 0.35174789709405346\n",
      "epoch 605: train loss: 0.3042149176000008, test loss: 0.35160404480405844\n",
      "epoch 606: train loss: 0.3040073262220267, test loss: 0.35146055696741013\n",
      "epoch 607: train loss: 0.3038002242927935, test loss: 0.351317432215309\n",
      "epoch 608: train loss: 0.3035936099680633, test loss: 0.3511746691861425\n",
      "epoch 609: train loss: 0.30338748141330535, test loss: 0.35103226652543623\n",
      "epoch 610: train loss: 0.3031818368036315, test loss: 0.3508902228858044\n",
      "epoch 611: train loss: 0.3029766743237323, test loss: 0.35074853692690056\n",
      "epoch 612: train loss: 0.30277199216781403, test loss: 0.3506072073153699\n",
      "epoch 613: train loss: 0.30256778853953525, test loss: 0.3504662327248005\n",
      "epoch 614: train loss: 0.3023640616519449, test loss: 0.35032561183567623\n",
      "epoch 615: train loss: 0.30216080972742, test loss: 0.3501853433353294\n",
      "epoch 616: train loss: 0.30195803099760454, test loss: 0.3500454259178939\n",
      "epoch 617: train loss: 0.30175572370334836, test loss: 0.3499058582842586\n",
      "epoch 618: train loss: 0.30155388609464656, test loss: 0.34976663914202155\n",
      "epoch 619: train loss: 0.30135251643057953, test loss: 0.34962776720544425\n",
      "epoch 620: train loss: 0.3011516129792538, test loss: 0.3494892411954062\n",
      "epoch 621: train loss: 0.30095117401774213, test loss: 0.3493510598393604\n",
      "epoch 622: train loss: 0.3007511978320255, test loss: 0.3492132218712882\n",
      "epoch 623: train loss: 0.3005516827169345, test loss: 0.34907572603165593\n",
      "epoch 624: train loss: 0.300352626976092, test loss: 0.3489385710673701\n",
      "epoch 625: train loss: 0.30015402892185533, test loss: 0.34880175573173494\n",
      "epoch 626: train loss: 0.29995588687525987, test loss: 0.3486652787844088\n",
      "epoch 627: train loss: 0.2997581991659621, test loss: 0.3485291389913613\n",
      "epoch 628: train loss: 0.299560964132184, test loss: 0.34839333512483167\n",
      "epoch 629: train loss: 0.29936418012065724, test loss: 0.3482578659632857\n",
      "epoch 630: train loss: 0.29916784548656805, test loss: 0.348122730291375\n",
      "epoch 631: train loss: 0.29897195859350256, test loss: 0.34798792689989505\n",
      "epoch 632: train loss: 0.29877651781339215, test loss: 0.3478534545857447\n",
      "epoch 633: train loss: 0.29858152152645995, test loss: 0.34771931215188495\n",
      "epoch 634: train loss: 0.298386968121167, test loss: 0.34758549840729913\n",
      "epoch 635: train loss: 0.2981928559941593, test loss: 0.34745201216695243\n",
      "epoch 636: train loss: 0.2979991835502149, test loss: 0.34731885225175285\n",
      "epoch 637: train loss: 0.29780594920219194, test loss: 0.3471860174885113\n",
      "epoch 638: train loss: 0.2976131513709763, test loss: 0.34705350670990254\n",
      "epoch 639: train loss: 0.2974207884854303, test loss: 0.34692131875442733\n",
      "epoch 640: train loss: 0.29722885898234175, test loss: 0.3467894524663727\n",
      "epoch 641: train loss: 0.2970373613063726, test loss: 0.34665790669577506\n",
      "epoch 642: train loss: 0.2968462939100093, test loss: 0.34652668029838146\n",
      "epoch 643: train loss: 0.29665565525351206, test loss: 0.34639577213561323\n",
      "epoch 644: train loss: 0.29646544380486595, test loss: 0.34626518107452753\n",
      "epoch 645: train loss: 0.29627565803973105, test loss: 0.34613490598778157\n",
      "epoch 646: train loss: 0.29608629644139384, test loss: 0.34600494575359536\n",
      "epoch 647: train loss: 0.2958973575007187, test loss: 0.34587529925571575\n",
      "epoch 648: train loss: 0.2957088397160997, test loss: 0.34574596538338026\n",
      "epoch 649: train loss: 0.2955207415934126, test loss: 0.34561694303128176\n",
      "epoch 650: train loss: 0.29533306164596795, test loss: 0.3454882310995323\n",
      "epoch 651: train loss: 0.29514579839446325, test loss: 0.34535982849362895\n",
      "epoch 652: train loss: 0.294958950366937, test loss: 0.3452317341244182\n",
      "epoch 653: train loss: 0.2947725160987215, test loss: 0.3451039469080617\n",
      "epoch 654: train loss: 0.29458649413239757, test loss: 0.3449764657660018\n",
      "epoch 655: train loss: 0.29440088301774836, test loss: 0.344849289624928\n",
      "epoch 656: train loss: 0.294215681311714, test loss: 0.3447224174167426\n",
      "epoch 657: train loss: 0.29403088757834683, test loss: 0.34459584807852744\n",
      "epoch 658: train loss: 0.2938465003887664, test loss: 0.34446958055251076\n",
      "epoch 659: train loss: 0.2936625183211155, test loss: 0.34434361378603434\n",
      "epoch 660: train loss: 0.29347893996051555, test loss: 0.3442179467315205\n",
      "epoch 661: train loss: 0.29329576389902345, test loss: 0.34409257834643986\n",
      "epoch 662: train loss: 0.2931129887355877, test loss: 0.3439675075932793\n",
      "epoch 663: train loss: 0.2929306130760058, test loss: 0.34384273343950966\n",
      "epoch 664: train loss: 0.292748635532881, test loss: 0.3437182548575544\n",
      "epoch 665: train loss: 0.2925670547255803, test loss: 0.3435940708247582\n",
      "epoch 666: train loss: 0.2923858692801919, test loss: 0.34347018032335525\n",
      "epoch 667: train loss: 0.2922050778294839, test loss: 0.343346582340439\n",
      "epoch 668: train loss: 0.2920246790128621, test loss: 0.3432232758679314\n",
      "epoch 669: train loss: 0.29184467147632953, test loss: 0.3431002599025515\n",
      "epoch 670: train loss: 0.2916650538724447, test loss: 0.3429775334457868\n",
      "epoch 671: train loss: 0.29148582486028185, test loss: 0.34285509550386195\n",
      "epoch 672: train loss: 0.2913069831053901, test loss: 0.3427329450877095\n",
      "epoch 673: train loss: 0.2911285272797534, test loss: 0.3426110812129408\n",
      "epoch 674: train loss: 0.29095045606175113, test loss: 0.3424895028998158\n",
      "epoch 675: train loss: 0.2907727681361184, test loss: 0.34236820917321475\n",
      "epoch 676: train loss: 0.29059546219390686, test loss: 0.34224719906260914\n",
      "epoch 677: train loss: 0.2904185369324461, test loss: 0.34212647160203297\n",
      "epoch 678: train loss: 0.2902419910553048, test loss: 0.34200602583005457\n",
      "epoch 679: train loss: 0.2900658232722524, test loss: 0.3418858607897482\n",
      "epoch 680: train loss: 0.28989003229922133, test loss: 0.34176597552866644\n",
      "epoch 681: train loss: 0.28971461685826905, test loss: 0.34164636909881224\n",
      "epoch 682: train loss: 0.2895395756775404, test loss: 0.34152704055661104\n",
      "epoch 683: train loss: 0.2893649074912308, test loss: 0.3414079889628844\n",
      "epoch 684: train loss: 0.28919061103954913, test loss: 0.3412892133828219\n",
      "epoch 685: train loss: 0.2890166850686808, test loss: 0.34117071288595485\n",
      "epoch 686: train loss: 0.2888431283307515, test loss: 0.3410524865461295\n",
      "epoch 687: train loss: 0.2886699395837914, test loss: 0.34093453344148034\n",
      "epoch 688: train loss: 0.28849711759169855, test loss: 0.34081685265440415\n",
      "epoch 689: train loss: 0.2883246611242039, test loss: 0.3406994432715337\n",
      "epoch 690: train loss: 0.2881525689568355, test loss: 0.34058230438371173\n",
      "epoch 691: train loss: 0.28798083987088346, test loss: 0.340465435085966\n",
      "epoch 692: train loss: 0.2878094726533654, test loss: 0.34034883447748254\n",
      "epoch 693: train loss: 0.2876384660969912, test loss: 0.3402325016615817\n",
      "epoch 694: train loss: 0.2874678190001291, test loss: 0.34011643574569206\n",
      "epoch 695: train loss: 0.28729753016677145, test loss: 0.34000063584132595\n",
      "epoch 696: train loss: 0.2871275984065008, test loss: 0.33988510106405484\n",
      "epoch 697: train loss: 0.2869580225344563, test loss: 0.3397698305334844\n",
      "epoch 698: train loss: 0.2867888013713001, test loss: 0.33965482337323044\n",
      "epoch 699: train loss: 0.2866199337431843, test loss: 0.3395400787108948\n",
      "epoch 700: train loss: 0.2864514184817182, test loss: 0.33942559567804104\n",
      "epoch 701: train loss: 0.28628325442393526, test loss: 0.339311373410171\n",
      "epoch 702: train loss: 0.28611544041226095, test loss: 0.3391974110467008\n",
      "epoch 703: train loss: 0.2859479752944802, test loss: 0.33908370773093777\n",
      "epoch 704: train loss: 0.28578085792370544, test loss: 0.3389702626100571\n",
      "epoch 705: train loss: 0.2856140871583453, test loss: 0.338857074835078\n",
      "epoch 706: train loss: 0.2854476618620722, test loss: 0.3387441435608423\n",
      "epoch 707: train loss: 0.28528158090379174, test loss: 0.3386314679459901\n",
      "epoch 708: train loss: 0.2851158431576113, test loss: 0.3385190471529383\n",
      "epoch 709: train loss: 0.28495044750280907, test loss: 0.33840688034785743\n",
      "epoch 710: train loss: 0.2847853928238034, test loss: 0.33829496670064985\n",
      "epoch 711: train loss: 0.2846206780101226, test loss: 0.3381833053849276\n",
      "epoch 712: train loss: 0.2844563019563741, test loss: 0.33807189557799033\n",
      "epoch 713: train loss: 0.2842922635622153, test loss: 0.3379607364608036\n",
      "epoch 714: train loss: 0.28412856173232276, test loss: 0.3378498272179772\n",
      "epoch 715: train loss: 0.2839651953763632, test loss: 0.33773916703774404\n",
      "epoch 716: train loss: 0.28380216340896414, test loss: 0.33762875511193824\n",
      "epoch 717: train loss: 0.28363946474968416, test loss: 0.33751859063597484\n",
      "epoch 718: train loss: 0.2834770983229844, test loss: 0.337408672808828\n",
      "epoch 719: train loss: 0.28331506305819965, test loss: 0.3372990008330106\n",
      "epoch 720: train loss: 0.2831533578895095, test loss: 0.3371895739145538\n",
      "epoch 721: train loss: 0.28299198175591034, test loss: 0.3370803912629857\n",
      "epoch 722: train loss: 0.2828309336011868, test loss: 0.3369714520913121\n",
      "epoch 723: train loss: 0.282670212373884, test loss: 0.33686275561599516\n",
      "epoch 724: train loss: 0.2825098170272795, test loss: 0.33675430105693427\n",
      "epoch 725: train loss: 0.282349746519356, test loss: 0.33664608763744575\n",
      "epoch 726: train loss: 0.28218999981277365, test loss: 0.3365381145842427\n",
      "epoch 727: train loss: 0.28203057587484287, test loss: 0.3364303811274159\n",
      "epoch 728: train loss: 0.2818714736774973, test loss: 0.3363228865004143\n",
      "epoch 729: train loss: 0.2817126921972671, test loss: 0.3362156299400252\n",
      "epoch 730: train loss: 0.2815542304152521, test loss: 0.33610861068635556\n",
      "epoch 731: train loss: 0.2813960873170953, test loss: 0.33600182798281253\n",
      "epoch 732: train loss: 0.2812382618929567, test loss: 0.335895281076085\n",
      "epoch 733: train loss: 0.28108075313748704, test loss: 0.3357889692161241\n",
      "epoch 734: train loss: 0.280923560049802, test loss: 0.33568289165612525\n",
      "epoch 735: train loss: 0.2807666816334563, test loss: 0.33557704765250923\n",
      "epoch 736: train loss: 0.2806101168964182, test loss: 0.33547143646490385\n",
      "epoch 737: train loss: 0.2804538648510439, test loss: 0.3353660573561257\n",
      "epoch 738: train loss: 0.28029792451405255, test loss: 0.33526090959216204\n",
      "epoch 739: train loss: 0.2801422949065012, test loss: 0.33515599244215294\n",
      "epoch 740: train loss: 0.27998697505375963, test loss: 0.335051305178373\n",
      "epoch 741: train loss: 0.2798319639854859, test loss: 0.334946847076214\n",
      "epoch 742: train loss: 0.2796772607356015, test loss: 0.3348426174141672\n",
      "epoch 743: train loss: 0.27952286434226736, test loss: 0.33473861547380585\n",
      "epoch 744: train loss: 0.2793687738478593, test loss: 0.33463484053976755\n",
      "epoch 745: train loss: 0.2792149882989441, test loss: 0.33453129189973724\n",
      "epoch 746: train loss: 0.27906150674625574, test loss: 0.33442796884443027\n",
      "epoch 747: train loss: 0.27890832824467116, test loss: 0.33432487066757505\n",
      "epoch 748: train loss: 0.2787554518531875, test loss: 0.3342219966658962\n",
      "epoch 749: train loss: 0.2786028766348981, test loss: 0.33411934613909805\n",
      "epoch 750: train loss: 0.27845060165696917, test loss: 0.33401691838984787\n",
      "epoch 751: train loss: 0.27829862599061733, test loss: 0.33391471272375917\n",
      "epoch 752: train loss: 0.2781469487110861, test loss: 0.33381272844937543\n",
      "epoch 753: train loss: 0.27799556889762334, test loss: 0.33371096487815377\n",
      "epoch 754: train loss: 0.27784448563345854, test loss: 0.33360942132444926\n",
      "epoch 755: train loss: 0.27769369800578053, test loss: 0.33350809710549767\n",
      "epoch 756: train loss: 0.2775432051057149, test loss: 0.33340699154140074\n",
      "epoch 757: train loss: 0.277393006028302, test loss: 0.3333061039551099\n",
      "epoch 758: train loss: 0.27724309987247503, test loss: 0.33320543367241\n",
      "epoch 759: train loss: 0.277093485741038, test loss: 0.3331049800219046\n",
      "epoch 760: train loss: 0.2769441627406437, test loss: 0.3330047423349998\n",
      "epoch 761: train loss: 0.276795129981773, test loss: 0.33290471994588927\n",
      "epoch 762: train loss: 0.2766463865787126, test loss: 0.33280491219153857\n",
      "epoch 763: train loss: 0.27649793164953396, test loss: 0.3327053184116701\n",
      "epoch 764: train loss: 0.27634976431607233, test loss: 0.3326059379487484\n",
      "epoch 765: train loss: 0.2762018837039056, test loss: 0.33250677014796415\n",
      "epoch 766: train loss: 0.27605428894233347, test loss: 0.33240781435722055\n",
      "epoch 767: train loss: 0.27590697916435686, test loss: 0.3323090699271177\n",
      "epoch 768: train loss: 0.2757599535066569, test loss: 0.33221053621093816\n",
      "epoch 769: train loss: 0.2756132111095751, test loss: 0.33211221256463247\n",
      "epoch 770: train loss: 0.2754667511170925, test loss: 0.33201409834680445\n",
      "epoch 771: train loss: 0.27532057267680976, test loss: 0.3319161929186973\n",
      "epoch 772: train loss: 0.27517467493992726, test loss: 0.33181849564417865\n",
      "epoch 773: train loss: 0.2750290570612247, test loss: 0.33172100588972675\n",
      "epoch 774: train loss: 0.2748837181990418, test loss: 0.3316237230244167\n",
      "epoch 775: train loss: 0.27473865751525844, test loss: 0.33152664641990576\n",
      "epoch 776: train loss: 0.2745938741752753, test loss: 0.33142977545042\n",
      "epoch 777: train loss: 0.2744493673479942, test loss: 0.3313331094927405\n",
      "epoch 778: train loss: 0.27430513620579916, test loss: 0.33123664792618923\n",
      "epoch 779: train loss: 0.27416117992453704, test loss: 0.3311403901326158\n",
      "epoch 780: train loss: 0.27401749768349865, test loss: 0.331044335496384\n",
      "epoch 781: train loss: 0.2738740886653998, test loss: 0.3309484834043581\n",
      "epoch 782: train loss: 0.27373095205636266, test loss: 0.33085283324588954\n",
      "epoch 783: train loss: 0.2735880870458972, test loss: 0.3307573844128042\n",
      "epoch 784: train loss: 0.27344549282688224, test loss: 0.33066213629938845\n",
      "epoch 785: train loss: 0.27330316859554765, test loss: 0.3305670883023767\n",
      "epoch 786: train loss: 0.27316111355145567, test loss: 0.3304722398209379\n",
      "epoch 787: train loss: 0.27301932689748304, test loss: 0.33037759025666363\n",
      "epoch 788: train loss: 0.27287780783980276, test loss: 0.33028313901355394\n",
      "epoch 789: train loss: 0.2727365555878663, test loss: 0.3301888854980058\n",
      "epoch 790: train loss: 0.2725955693543859, test loss: 0.3300948291187997\n",
      "epoch 791: train loss: 0.27245484835531664, test loss: 0.33000096928708755\n",
      "epoch 792: train loss: 0.27231439180983913, test loss: 0.32990730541638014\n",
      "epoch 793: train loss: 0.27217419894034206, test loss: 0.3298138369225346\n",
      "epoch 794: train loss: 0.2720342689724046, test loss: 0.3297205632237422\n",
      "epoch 795: train loss: 0.2718946011347796, test loss: 0.32962748374051615\n",
      "epoch 796: train loss: 0.27175519465937603, test loss: 0.3295345978956795\n",
      "epoch 797: train loss: 0.2716160487812425, test loss: 0.329441905114353\n",
      "epoch 798: train loss: 0.27147716273854994, test loss: 0.32934940482394326\n",
      "epoch 799: train loss: 0.27133853577257505, test loss: 0.32925709645413065\n",
      "epoch 800: train loss: 0.2712001671276837, test loss: 0.3291649794368577\n",
      "epoch 801: train loss: 0.27106205605131406, test loss: 0.32907305320631713\n",
      "epoch 802: train loss: 0.27092420179396054, test loss: 0.3289813171989406\n",
      "epoch 803: train loss: 0.2707866036091573, test loss: 0.32888977085338666\n",
      "epoch 804: train loss: 0.27064926075346196, test loss: 0.3287984136105294\n",
      "epoch 805: train loss: 0.27051217248643944, test loss: 0.3287072449134474\n",
      "epoch 806: train loss: 0.2703753380706461, test loss: 0.3286162642074116\n",
      "epoch 807: train loss: 0.27023875677161385, test loss: 0.32852547093987483\n",
      "epoch 808: train loss: 0.27010242785783395, test loss: 0.32843486456045995\n",
      "epoch 809: train loss: 0.2699663506007419, test loss: 0.3283444445209493\n",
      "epoch 810: train loss: 0.2698305242747012, test loss: 0.32825421027527313\n",
      "epoch 811: train loss: 0.26969494815698825, test loss: 0.3281641612794989\n",
      "epoch 812: train loss: 0.26955962152777696, test loss: 0.3280742969918203\n",
      "epoch 813: train loss: 0.26942454367012303, test loss: 0.32798461687254643\n",
      "epoch 814: train loss: 0.26928971386994927, test loss: 0.327895120384091\n",
      "epoch 815: train loss: 0.2691551314160299, test loss: 0.3278058069909615\n",
      "epoch 816: train loss: 0.2690207955999764, test loss: 0.32771667615974875\n",
      "epoch 817: train loss: 0.26888670571622136, test loss: 0.3276277273591163\n",
      "epoch 818: train loss: 0.26875286106200486, test loss: 0.32753896005979\n",
      "epoch 819: train loss: 0.26861926093735883, test loss: 0.32745037373454716\n",
      "epoch 820: train loss: 0.2684859046450931, test loss: 0.3273619678582068\n",
      "epoch 821: train loss: 0.26835279149078034, test loss: 0.32727374190761904\n",
      "epoch 822: train loss: 0.2682199207827418, test loss: 0.3271856953616547\n",
      "epoch 823: train loss: 0.2680872918320331, test loss: 0.32709782770119533\n",
      "epoch 824: train loss: 0.2679549039524296, test loss: 0.32701013840912324\n",
      "epoch 825: train loss: 0.26782275646041265, test loss: 0.32692262697031127\n",
      "epoch 826: train loss: 0.2676908486751552, test loss: 0.3268352928716127\n",
      "epoch 827: train loss: 0.26755917991850786, test loss: 0.32674813560185184\n",
      "epoch 828: train loss: 0.267427749514985, test loss: 0.3266611546518137\n",
      "epoch 829: train loss: 0.267296556791751, test loss: 0.3265743495142342\n",
      "epoch 830: train loss: 0.26716560107860654, test loss: 0.3264877196837908\n",
      "epoch 831: train loss: 0.2670348817079746, test loss: 0.3264012646570928\n",
      "epoch 832: train loss: 0.2669043980148875, test loss: 0.3263149839326713\n",
      "epoch 833: train loss: 0.2667741493369729, test loss: 0.3262288770109702\n",
      "epoch 834: train loss: 0.2666441350144406, test loss: 0.3261429433943364\n",
      "epoch 835: train loss: 0.2665143543900694, test loss: 0.32605718258701055\n",
      "epoch 836: train loss: 0.2663848068091934, test loss: 0.32597159409511756\n",
      "epoch 837: train loss: 0.26625549161968953, test loss: 0.3258861774266574\n",
      "epoch 838: train loss: 0.266126408171964, test loss: 0.3258009320914958\n",
      "epoch 839: train loss: 0.26599755581893936, test loss: 0.32571585760135535\n",
      "epoch 840: train loss: 0.2658689339160418, test loss: 0.32563095346980575\n",
      "epoch 841: train loss: 0.2657405418211884, test loss: 0.3255462192122554\n",
      "epoch 842: train loss: 0.26561237889477396, test loss: 0.325461654345942\n",
      "epoch 843: train loss: 0.26548444449965886, test loss: 0.32537725838992393\n",
      "epoch 844: train loss: 0.2653567380011561, test loss: 0.32529303086507083\n",
      "epoch 845: train loss: 0.26522925876701914, test loss: 0.32520897129405524\n",
      "epoch 846: train loss: 0.2651020061674291, test loss: 0.3251250792013438\n",
      "epoch 847: train loss: 0.26497497957498295, test loss: 0.325041354113188\n",
      "epoch 848: train loss: 0.2648481783646806, test loss: 0.3249577955576162\n",
      "epoch 849: train loss: 0.2647216019139132, test loss: 0.32487440306442467\n",
      "epoch 850: train loss: 0.26459524960245095, test loss: 0.3247911761651689\n",
      "epoch 851: train loss: 0.26446912081243085, test loss: 0.3247081143931551\n",
      "epoch 852: train loss: 0.26434321492834484, test loss: 0.32462521728343224\n",
      "epoch 853: train loss: 0.2642175313370283, test loss: 0.32454248437278294\n",
      "epoch 854: train loss: 0.26409206942764746, test loss: 0.3244599151997155\n",
      "epoch 855: train loss: 0.26396682859168835, test loss: 0.3243775093044555\n",
      "epoch 856: train loss: 0.2638418082229449, test loss: 0.32429526622893784\n",
      "epoch 857: train loss: 0.2637170077175072, test loss: 0.3242131855167977\n",
      "epoch 858: train loss: 0.2635924264737502, test loss: 0.32413126671336345\n",
      "epoch 859: train loss: 0.2634680638923221, test loss: 0.32404950936564775\n",
      "epoch 860: train loss: 0.2633439193761331, test loss: 0.3239679130223399\n",
      "epoch 861: train loss: 0.263219992330344, test loss: 0.32388647723379776\n",
      "epoch 862: train loss: 0.263096282162355, test loss: 0.3238052015520397\n",
      "epoch 863: train loss: 0.2629727882817945, test loss: 0.3237240855307364\n",
      "epoch 864: train loss: 0.26284951010050805, test loss: 0.323643128725204\n",
      "epoch 865: train loss: 0.2627264470325472, test loss: 0.3235623306923951\n",
      "epoch 866: train loss: 0.2626035984941587, test loss: 0.32348169099089147\n",
      "epoch 867: train loss: 0.26248096390377357, test loss: 0.32340120918089676\n",
      "epoch 868: train loss: 0.26235854268199593, test loss: 0.32332088482422805\n",
      "epoch 869: train loss: 0.262236334251593, test loss: 0.3232407174843089\n",
      "epoch 870: train loss: 0.26211433803748335, test loss: 0.32316070672616126\n",
      "epoch 871: train loss: 0.26199255346672734, test loss: 0.3230808521163982\n",
      "epoch 872: train loss: 0.26187097996851577, test loss: 0.3230011532232166\n",
      "epoch 873: train loss: 0.26174961697415966, test loss: 0.3229216096163888\n",
      "epoch 874: train loss: 0.26162846391707983, test loss: 0.322842220867257\n",
      "epoch 875: train loss: 0.26150752023279655, test loss: 0.32276298654872365\n",
      "epoch 876: train loss: 0.2613867853589192, test loss: 0.32268390623524595\n",
      "epoch 877: train loss: 0.26126625873513576, test loss: 0.3226049795028277\n",
      "epoch 878: train loss: 0.261145939803203, test loss: 0.3225262059290122\n",
      "epoch 879: train loss: 0.2610258280069363, test loss: 0.3224475850928755\n",
      "epoch 880: train loss: 0.2609059227921993, test loss: 0.32236911657501843\n",
      "epoch 881: train loss: 0.26078622360689413, test loss: 0.3222907999575603\n",
      "epoch 882: train loss: 0.2606667299009514, test loss: 0.3222126348241314\n",
      "epoch 883: train loss: 0.2605474411263205, test loss: 0.32213462075986626\n",
      "epoch 884: train loss: 0.26042835673695935, test loss: 0.3220567573513964\n",
      "epoch 885: train loss: 0.260309476188825, test loss: 0.3219790441868436\n",
      "epoch 886: train loss: 0.26019079893986385, test loss: 0.321901480855813\n",
      "epoch 887: train loss: 0.26007232445000183, test loss: 0.321824066949386\n",
      "epoch 888: train loss: 0.2599540521811352, test loss: 0.321746802060114\n",
      "epoch 889: train loss: 0.2598359815971204, test loss: 0.321669685782011\n",
      "epoch 890: train loss: 0.2597181121637653, test loss: 0.3215927177105473\n",
      "epoch 891: train loss: 0.2596004433488192, test loss: 0.3215158974426427\n",
      "epoch 892: train loss: 0.25948297462196374, test loss: 0.3214392245766598\n",
      "epoch 893: train loss: 0.25936570545480353, test loss: 0.32136269871239725\n",
      "epoch 894: train loss: 0.25924863532085685, test loss: 0.3212863194510836\n",
      "epoch 895: train loss: 0.25913176369554675, test loss: 0.3212100863953704\n",
      "epoch 896: train loss: 0.25901509005619144, test loss: 0.32113399914932583\n",
      "epoch 897: train loss: 0.25889861388199553, test loss: 0.32105805731842835\n",
      "epoch 898: train loss: 0.258782334654041, test loss: 0.32098226050955986\n",
      "epoch 899: train loss: 0.258666251855278, test loss: 0.3209066083309999\n",
      "epoch 900: train loss: 0.258550364970516, test loss: 0.320831100392419\n",
      "epoch 901: train loss: 0.2584346734864152, test loss: 0.3207557363048722\n",
      "epoch 902: train loss: 0.25831917689147743, test loss: 0.32068051568079325\n",
      "epoch 903: train loss: 0.2582038746760374, test loss: 0.3206054381339879\n",
      "epoch 904: train loss: 0.25808876633225397, test loss: 0.32053050327962795\n",
      "epoch 905: train loss: 0.25797385135410184, test loss: 0.3204557107342451\n",
      "epoch 906: train loss: 0.2578591292373623, test loss: 0.3203810601157248\n",
      "epoch 907: train loss: 0.2577445994796153, test loss: 0.3203065510432998\n",
      "epoch 908: train loss: 0.2576302615802305, test loss: 0.32023218313754476\n",
      "epoch 909: train loss: 0.25751611504035915, test loss: 0.3201579560203698\n",
      "epoch 910: train loss: 0.25740215936292526, test loss: 0.32008386931501454\n",
      "epoch 911: train loss: 0.2572883940526179, test loss: 0.32000992264604206\n",
      "epoch 912: train loss: 0.25717481861588204, test loss: 0.31993611563933333\n",
      "epoch 913: train loss: 0.2570614325609111, test loss: 0.31986244792208096\n",
      "epoch 914: train loss: 0.256948235397638, test loss: 0.3197889191227835\n",
      "epoch 915: train loss: 0.256835226637728, test loss: 0.3197155288712396\n",
      "epoch 916: train loss: 0.2567224057945692, test loss: 0.3196422767985424\n",
      "epoch 917: train loss: 0.2566097723832657, test loss: 0.31956916253707335\n",
      "epoch 918: train loss: 0.2564973259206291, test loss: 0.3194961857204967\n",
      "epoch 919: train loss: 0.2563850659251703, test loss: 0.3194233459837543\n",
      "epoch 920: train loss: 0.25627299191709213, test loss: 0.319350642963059\n",
      "epoch 921: train loss: 0.2561611034182808, test loss: 0.3192780762958898\n",
      "epoch 922: train loss: 0.2560493999522987, test loss: 0.3192056456209857\n",
      "epoch 923: train loss: 0.25593788104437615, test loss: 0.31913335057834075\n",
      "epoch 924: train loss: 0.2558265462214039, test loss: 0.3190611908091979\n",
      "epoch 925: train loss: 0.2557153950119255, test loss: 0.318989165956044\n",
      "epoch 926: train loss: 0.2556044269461292, test loss: 0.3189172756626039\n",
      "epoch 927: train loss: 0.2554936415558409, test loss: 0.31884551957383533\n",
      "epoch 928: train loss: 0.2553830383745164, test loss: 0.31877389733592343\n",
      "epoch 929: train loss: 0.2552726169372335, test loss: 0.31870240859627513\n",
      "epoch 930: train loss: 0.25516237678068526, test loss: 0.3186310530035142\n",
      "epoch 931: train loss: 0.25505231744317164, test loss: 0.3185598302074755\n",
      "epoch 932: train loss: 0.2549424384645932, test loss: 0.3184887398592003\n",
      "epoch 933: train loss: 0.2548327393864427, test loss: 0.3184177816109301\n",
      "epoch 934: train loss: 0.2547232197517984, test loss: 0.3183469551161023\n",
      "epoch 935: train loss: 0.25461387910531674, test loss: 0.31827626002934445\n",
      "epoch 936: train loss: 0.25450471699322513, test loss: 0.3182056960064695\n",
      "epoch 937: train loss: 0.25439573296331436, test loss: 0.31813526270447\n",
      "epoch 938: train loss: 0.25428692656493207, test loss: 0.3180649597815139\n",
      "epoch 939: train loss: 0.2541782973489752, test loss: 0.3179947868969385\n",
      "epoch 940: train loss: 0.25406984486788325, test loss: 0.31792474371124607\n",
      "epoch 941: train loss: 0.25396156867563086, test loss: 0.3178548298860986\n",
      "epoch 942: train loss: 0.2538534683277214, test loss: 0.3177850450843127\n",
      "epoch 943: train loss: 0.25374554338117944, test loss: 0.3177153889698547\n",
      "epoch 944: train loss: 0.25363779339454434, test loss: 0.31764586120783583\n",
      "epoch 945: train loss: 0.25353021792786307, test loss: 0.317576461464507\n",
      "epoch 946: train loss: 0.2534228165426835, test loss: 0.31750718940725425\n",
      "epoch 947: train loss: 0.2533155888020477, test loss: 0.3174380447045937\n",
      "epoch 948: train loss: 0.25320853427048506, test loss: 0.3173690270261664\n",
      "epoch 949: train loss: 0.25310165251400574, test loss: 0.3173001360427343\n",
      "epoch 950: train loss: 0.25299494310009385, test loss: 0.3172313714261746\n",
      "epoch 951: train loss: 0.2528884055977009, test loss: 0.3171627328494756\n",
      "epoch 952: train loss: 0.25278203957723927, test loss: 0.31709421998673165\n",
      "epoch 953: train loss: 0.2526758446105756, test loss: 0.31702583251313843\n",
      "epoch 954: train loss: 0.2525698202710241, test loss: 0.31695757010498854\n",
      "epoch 955: train loss: 0.2524639661333405, test loss: 0.3168894324396664\n",
      "epoch 956: train loss: 0.2523582817737153, test loss: 0.3168214191956441\n",
      "epoch 957: train loss: 0.2522527667697672, test loss: 0.31675353005247636\n",
      "epoch 958: train loss: 0.2521474207005373, test loss: 0.3166857646907963\n",
      "epoch 959: train loss: 0.25204224314648227, test loss: 0.31661812279231044\n",
      "epoch 960: train loss: 0.2519372336894681, test loss: 0.3165506040397947\n",
      "epoch 961: train loss: 0.25183239191276413, test loss: 0.3164832081170894\n",
      "epoch 962: train loss: 0.25172771740103655, test loss: 0.31641593470909507\n",
      "epoch 963: train loss: 0.25162320974034236, test loss: 0.3163487835017678\n",
      "epoch 964: train loss: 0.25151886851812316, test loss: 0.31628175418211507\n",
      "epoch 965: train loss: 0.251414693323199, test loss: 0.316214846438191\n",
      "epoch 966: train loss: 0.2513106837457624, test loss: 0.31614805995909195\n",
      "epoch 967: train loss: 0.251206839377372, test loss: 0.3160813944349527\n",
      "epoch 968: train loss: 0.25110315981094705, test loss: 0.3160148495569412\n",
      "epoch 969: train loss: 0.2509996446407611, test loss: 0.3159484250172552\n",
      "epoch 970: train loss: 0.25089629346243586, test loss: 0.3158821205091171\n",
      "epoch 971: train loss: 0.2507931058729357, test loss: 0.3158159357267703\n",
      "epoch 972: train loss: 0.25069008147056165, test loss: 0.3157498703654746\n",
      "epoch 973: train loss: 0.2505872198549453, test loss: 0.315683924121502\n",
      "epoch 974: train loss: 0.2504845206270432, test loss: 0.31561809669213264\n",
      "epoch 975: train loss: 0.25038198338913115, test loss: 0.31555238777565037\n",
      "epoch 976: train loss: 0.25027960774479807, test loss: 0.315486797071339\n",
      "epoch 977: train loss: 0.2501773932989409, test loss: 0.3154213242794775\n",
      "epoch 978: train loss: 0.25007533965775813, test loss: 0.3153559691013365\n",
      "epoch 979: train loss: 0.2499734464287449, test loss: 0.3152907312391738\n",
      "epoch 980: train loss: 0.2498717132206868, test loss: 0.3152256103962305\n",
      "epoch 981: train loss: 0.24977013964365458, test loss: 0.3151606062767267\n",
      "epoch 982: train loss: 0.2496687253089985, test loss: 0.3150957185858578\n",
      "epoch 983: train loss: 0.24956746982934278, test loss: 0.3150309470297902\n",
      "epoch 984: train loss: 0.24946637281858003, test loss: 0.31496629131565734\n",
      "epoch 985: train loss: 0.24936543389186602, test loss: 0.3149017511515558\n",
      "epoch 986: train loss: 0.2492646526656139, test loss: 0.31483732624654137\n",
      "epoch 987: train loss: 0.24916402875748905, test loss: 0.314773016310625\n",
      "epoch 988: train loss: 0.24906356178640351, test loss: 0.3147088210547689\n",
      "epoch 989: train loss: 0.24896325137251082, test loss: 0.3146447401908827\n",
      "epoch 990: train loss: 0.2488630971372006, test loss: 0.31458077343181956\n",
      "epoch 991: train loss: 0.248763098703093, test loss: 0.3145169204913723\n",
      "epoch 992: train loss: 0.24866325569403394, test loss: 0.3144531810842696\n",
      "epoch 993: train loss: 0.24856356773508942, test loss: 0.3143895549261721\n",
      "epoch 994: train loss: 0.2484640344525407, test loss: 0.3143260417336686\n",
      "epoch 995: train loss: 0.24836465547387865, test loss: 0.3142626412242724\n",
      "epoch 996: train loss: 0.24826543042779897, test loss: 0.31419935311641745\n",
      "epoch 997: train loss: 0.248166358944197, test loss: 0.3141361771294545\n",
      "epoch 998: train loss: 0.24806744065416247, test loss: 0.3140731129836479\n",
      "epoch 999: train loss: 0.24796867518997462, test loss: 0.3140101604001709\n",
      "epoch 1000: train loss: 0.247870062185097, test loss: 0.3139473191011031\n",
      "epoch 1001: train loss: 0.24777160127417247, test loss: 0.31388458880942594\n",
      "epoch 1002: train loss: 0.2476732920930184, test loss: 0.31382196924901945\n",
      "epoch 1003: train loss: 0.24757513427862146, test loss: 0.31375946014465866\n",
      "epoch 1004: train loss: 0.2474771274691328, test loss: 0.3136970612220096\n",
      "epoch 1005: train loss: 0.24737927130386317, test loss: 0.3136347722076262\n",
      "epoch 1006: train loss: 0.24728156542327803, test loss: 0.31357259282894645\n",
      "epoch 1007: train loss: 0.24718400946899258, test loss: 0.3135105228142887\n",
      "epoch 1008: train loss: 0.24708660308376712, test loss: 0.3134485618928487\n",
      "epoch 1009: train loss: 0.24698934591150207, test loss: 0.31338670979469513\n",
      "epoch 1010: train loss: 0.24689223759723325, test loss: 0.3133249662507672\n",
      "epoch 1011: train loss: 0.2467952777871272, test loss: 0.3132633309928702\n",
      "epoch 1012: train loss: 0.24669846612847648, test loss: 0.31320180375367285\n",
      "epoch 1013: train loss: 0.24660180226969466, test loss: 0.313140384266703\n",
      "epoch 1014: train loss: 0.24650528586031215, test loss: 0.3130790722663449\n",
      "epoch 1015: train loss: 0.24640891655097102, test loss: 0.3130178674878356\n",
      "epoch 1016: train loss: 0.24631269399342082, test loss: 0.31295676966726144\n",
      "epoch 1017: train loss: 0.24621661784051363, test loss: 0.3128957785415544\n",
      "epoch 1018: train loss: 0.24612068774619977, test loss: 0.3128348938484895\n",
      "epoch 1019: train loss: 0.24602490336552307, test loss: 0.3127741153266807\n",
      "epoch 1020: train loss: 0.24592926435461632, test loss: 0.31271344271557794\n",
      "epoch 1021: train loss: 0.24583377037069695, test loss: 0.3126528757554638\n",
      "epoch 1022: train loss: 0.24573842107206237, test loss: 0.31259241418745004\n",
      "epoch 1023: train loss: 0.24564321611808548, test loss: 0.3125320577534746\n",
      "epoch 1024: train loss: 0.24554815516921058, test loss: 0.3124718061962979\n",
      "epoch 1025: train loss: 0.24545323788694845, test loss: 0.3124116592595001\n",
      "epoch 1026: train loss: 0.2453584639338724, test loss: 0.31235161668747735\n",
      "epoch 1027: train loss: 0.24526383297361368, test loss: 0.31229167822543913\n",
      "epoch 1028: train loss: 0.24516934467085716, test loss: 0.3122318436194045\n",
      "epoch 1029: train loss: 0.2450749986913371, test loss: 0.31217211261619926\n",
      "epoch 1030: train loss: 0.24498079470183282, test loss: 0.3121124849634528\n",
      "epoch 1031: train loss: 0.2448867323701643, test loss: 0.3120529604095945\n",
      "epoch 1032: train loss: 0.24479281136518824, test loss: 0.31199353870385127\n",
      "epoch 1033: train loss: 0.24469903135679338, test loss: 0.3119342195962438\n",
      "epoch 1034: train loss: 0.24460539201589682, test loss: 0.31187500283758385\n",
      "epoch 1035: train loss: 0.24451189301443932, test loss: 0.3118158881794709\n",
      "epoch 1036: train loss: 0.24441853402538155, test loss: 0.31175687537428953\n",
      "epoch 1037: train loss: 0.24432531472269964, test loss: 0.31169796417520546\n",
      "epoch 1038: train loss: 0.24423223478138148, test loss: 0.3116391543361635\n",
      "epoch 1039: train loss: 0.24413929387742203, test loss: 0.3115804456118838\n",
      "epoch 1040: train loss: 0.2440464916878198, test loss: 0.31152183775785935\n",
      "epoch 1041: train loss: 0.24395382789057243, test loss: 0.3114633305303524\n",
      "epoch 1042: train loss: 0.24386130216467286, test loss: 0.31140492368639205\n",
      "epoch 1043: train loss: 0.24376891419010518, test loss: 0.3113466169837709\n",
      "epoch 1044: train loss: 0.24367666364784088, test loss: 0.31128841018104225\n",
      "epoch 1045: train loss: 0.24358455021983447, test loss: 0.3112303030375169\n",
      "epoch 1046: train loss: 0.24349257358902, test loss: 0.3111722953132607\n",
      "epoch 1047: train loss: 0.24340073343930663, test loss: 0.31111438676909103\n",
      "epoch 1048: train loss: 0.24330902945557528, test loss: 0.3110565771665745\n",
      "epoch 1049: train loss: 0.2432174613236743, test loss: 0.31099886626802337\n",
      "epoch 1050: train loss: 0.24312602873041578, test loss: 0.31094125383649357\n",
      "epoch 1051: train loss: 0.24303473136357165, test loss: 0.3108837396357809\n",
      "epoch 1052: train loss: 0.24294356891186994, test loss: 0.31082632343041877\n",
      "epoch 1053: train loss: 0.24285254106499077, test loss: 0.31076900498567517\n",
      "epoch 1054: train loss: 0.24276164751356297, test loss: 0.3107117840675497\n",
      "epoch 1055: train loss: 0.2426708879491597, test loss: 0.31065466044277124\n",
      "epoch 1056: train loss: 0.24258026206429528, test loss: 0.3105976338787945\n",
      "epoch 1057: train loss: 0.24248976955242108, test loss: 0.31054070414379786\n",
      "epoch 1058: train loss: 0.24239941010792207, test loss: 0.31048387100668007\n",
      "epoch 1059: train loss: 0.2423091834261129, test loss: 0.31042713423705787\n",
      "epoch 1060: train loss: 0.2422190892032343, test loss: 0.310370493605263\n",
      "epoch 1061: train loss: 0.24212912713644955, test loss: 0.3103139488823396\n",
      "epoch 1062: train loss: 0.24203929692384063, test loss: 0.3102574998400414\n",
      "epoch 1063: train loss: 0.24194959826440482, test loss: 0.3102011462508293\n",
      "epoch 1064: train loss: 0.24186003085805094, test loss: 0.31014488788786815\n",
      "epoch 1065: train loss: 0.24177059440559576, test loss: 0.3100887245250245\n",
      "epoch 1066: train loss: 0.24168128860876056, test loss: 0.3100326559368638\n",
      "epoch 1067: train loss: 0.24159211317016754, test loss: 0.3099766818986475\n",
      "epoch 1068: train loss: 0.24150306779333625, test loss: 0.30992080218633106\n",
      "epoch 1069: train loss: 0.24141415218267998, test loss: 0.3098650165765605\n",
      "epoch 1070: train loss: 0.24132536604350258, test loss: 0.3098093248466705\n",
      "epoch 1071: train loss: 0.24123670908199465, test loss: 0.3097537267746811\n",
      "epoch 1072: train loss: 0.2411481810052303, test loss: 0.3096982221392958\n",
      "epoch 1073: train loss: 0.2410597815211636, test loss: 0.3096428107198985\n",
      "epoch 1074: train loss: 0.2409715103386252, test loss: 0.309587492296551\n",
      "epoch 1075: train loss: 0.24088336716731884, test loss: 0.30953226664999073\n",
      "epoch 1076: train loss: 0.24079535171781818, test loss: 0.30947713356162765\n",
      "epoch 1077: train loss: 0.24070746370156315, test loss: 0.30942209281354244\n",
      "epoch 1078: train loss: 0.24061970283085685, test loss: 0.30936714418848316\n",
      "epoch 1079: train loss: 0.24053206881886197, test loss: 0.30931228746986356\n",
      "epoch 1080: train loss: 0.24044456137959763, test loss: 0.30925752244175997\n",
      "epoch 1081: train loss: 0.2403571802279361, test loss: 0.3092028488889089\n",
      "epoch 1082: train loss: 0.24026992507959932, test loss: 0.309148266596705\n",
      "epoch 1083: train loss: 0.240182795651156, test loss: 0.30909377535119803\n",
      "epoch 1084: train loss: 0.2400957916600177, test loss: 0.30903937493909056\n",
      "epoch 1085: train loss: 0.2400089128244364, test loss: 0.30898506514773594\n",
      "epoch 1086: train loss: 0.23992215886350077, test loss: 0.30893084576513524\n",
      "epoch 1087: train loss: 0.23983552949713294, test loss: 0.3088767165799352\n",
      "epoch 1088: train loss: 0.23974902444608567, test loss: 0.3088226773814258\n",
      "epoch 1089: train loss: 0.23966264343193872, test loss: 0.30876872795953775\n",
      "epoch 1090: train loss: 0.23957638617709606, test loss: 0.3087148681048403\n",
      "epoch 1091: train loss: 0.23949025240478264, test loss: 0.3086610976085382\n",
      "epoch 1092: train loss: 0.23940424183904102, test loss: 0.3086074162624706\n",
      "epoch 1093: train loss: 0.23931835420472858, test loss: 0.30855382385910735\n",
      "epoch 1094: train loss: 0.2392325892275143, test loss: 0.30850032019154755\n",
      "epoch 1095: train loss: 0.2391469466338755, test loss: 0.30844690505351663\n",
      "epoch 1096: train loss: 0.2390614261510951, test loss: 0.3083935782393645\n",
      "epoch 1097: train loss: 0.23897602750725838, test loss: 0.30834033954406304\n",
      "epoch 1098: train loss: 0.23889075043124983, test loss: 0.3082871887632037\n",
      "epoch 1099: train loss: 0.23880559465275042, test loss: 0.3082341256929954\n",
      "epoch 1100: train loss: 0.23872055990223429, test loss: 0.3081811501302619\n",
      "epoch 1101: train loss: 0.23863564591096606, test loss: 0.30812826187244\n",
      "epoch 1102: train loss: 0.23855085241099744, test loss: 0.3080754607175771\n",
      "epoch 1103: train loss: 0.2384661791351648, test loss: 0.3080227464643286\n",
      "epoch 1104: train loss: 0.23838162581708566, test loss: 0.3079701189119561\n",
      "epoch 1105: train loss: 0.23829719219115628, test loss: 0.30791757786032503\n",
      "epoch 1106: train loss: 0.23821287799254828, test loss: 0.30786512310990244\n",
      "epoch 1107: train loss: 0.23812868295720602, test loss: 0.30781275446175466\n",
      "epoch 1108: train loss: 0.23804460682184356, test loss: 0.3077604717175452\n",
      "epoch 1109: train loss: 0.237960649323942, test loss: 0.30770827467953277\n",
      "epoch 1110: train loss: 0.23787681020174628, test loss: 0.3076561631505685\n",
      "epoch 1111: train loss: 0.2377930891942626, test loss: 0.3076041369340944\n",
      "epoch 1112: train loss: 0.23770948604125539, test loss: 0.3075521958341408\n",
      "epoch 1113: train loss: 0.23762600048324478, test loss: 0.3075003396553245\n",
      "epoch 1114: train loss: 0.23754263226150332, test loss: 0.30744856820284616\n",
      "epoch 1115: train loss: 0.23745938111805373, test loss: 0.30739688128248893\n",
      "epoch 1116: train loss: 0.2373762467956656, test loss: 0.3073452787006154\n",
      "epoch 1117: train loss: 0.23729322903785302, test loss: 0.3072937602641661\n",
      "epoch 1118: train loss: 0.2372103275888716, test loss: 0.3072423257806573\n",
      "epoch 1119: train loss: 0.23712754219371576, test loss: 0.3071909750581788\n",
      "epoch 1120: train loss: 0.23704487259811607, test loss: 0.3071397079053917\n",
      "epoch 1121: train loss: 0.2369623185485365, test loss: 0.3070885241315268\n",
      "epoch 1122: train loss: 0.23687987979217165, test loss: 0.3070374235463821\n",
      "epoch 1123: train loss: 0.23679755607694417, test loss: 0.30698640596032073\n",
      "epoch 1124: train loss: 0.23671534715150203, test loss: 0.3069354711842693\n",
      "epoch 1125: train loss: 0.23663325276521568, test loss: 0.30688461902971526\n",
      "epoch 1126: train loss: 0.23655127266817583, test loss: 0.3068338493087056\n",
      "epoch 1127: train loss: 0.2364694066111903, test loss: 0.30678316183384413\n",
      "epoch 1128: train loss: 0.23638765434578177, test loss: 0.30673255641828967\n",
      "epoch 1129: train loss: 0.236306015624185, test loss: 0.3066820328757546\n",
      "epoch 1130: train loss: 0.23622449019934413, test loss: 0.3066315910205018\n",
      "epoch 1131: train loss: 0.2361430778249104, test loss: 0.30658123066734366\n",
      "epoch 1132: train loss: 0.23606177825523925, test loss: 0.3065309516316395\n",
      "epoch 1133: train loss: 0.23598059124538792, test loss: 0.3064807537292938\n",
      "epoch 1134: train loss: 0.23589951655111285, test loss: 0.30643063677675425\n",
      "epoch 1135: train loss: 0.23581855392886716, test loss: 0.30638060059100963\n",
      "epoch 1136: train loss: 0.23573770313579806, test loss: 0.3063306449895883\n",
      "epoch 1137: train loss: 0.2356569639297444, test loss: 0.30628076979055563\n",
      "epoch 1138: train loss: 0.23557633606923403, test loss: 0.3062309748125125\n",
      "epoch 1139: train loss: 0.23549581931348165, test loss: 0.30618125987459327\n",
      "epoch 1140: train loss: 0.23541541342238573, test loss: 0.30613162479646383\n",
      "epoch 1141: train loss: 0.23533511815652677, test loss: 0.3060820693983199\n",
      "epoch 1142: train loss: 0.23525493327716418, test loss: 0.30603259350088463\n",
      "epoch 1143: train loss: 0.2351748585462342, test loss: 0.3059831969254073\n",
      "epoch 1144: train loss: 0.2350948937263474, test loss: 0.30593387949366113\n",
      "epoch 1145: train loss: 0.2350150385807861, test loss: 0.3058846410279413\n",
      "epoch 1146: train loss: 0.23493529287350223, test loss: 0.3058354813510634\n",
      "epoch 1147: train loss: 0.23485565636911462, test loss: 0.3057864002863614\n",
      "epoch 1148: train loss: 0.23477612883290694, test loss: 0.3057373976576857\n",
      "epoch 1149: train loss: 0.23469671003082493, test loss: 0.30568847328940146\n",
      "epoch 1150: train loss: 0.23461739972947435, test loss: 0.30563962700638675\n",
      "epoch 1151: train loss: 0.23453819769611847, test loss: 0.30559085863403046\n",
      "epoch 1152: train loss: 0.23445910369867579, test loss: 0.3055421679982311\n",
      "epoch 1153: train loss: 0.2343801175057176, test loss: 0.30549355492539404\n",
      "epoch 1154: train loss: 0.2343012388864657, test loss: 0.30544501924243084\n",
      "epoch 1155: train loss: 0.23422246761079016, test loss: 0.30539656077675637\n",
      "epoch 1156: train loss: 0.23414380344920688, test loss: 0.3053481793562878\n",
      "epoch 1157: train loss: 0.2340652461728755, test loss: 0.3052998748094425\n",
      "epoch 1158: train loss: 0.23398679555359678, test loss: 0.3052516469651361\n",
      "epoch 1159: train loss: 0.23390845136381064, test loss: 0.3052034956527814\n",
      "epoch 1160: train loss: 0.2338302133765937, test loss: 0.3051554207022856\n",
      "epoch 1161: train loss: 0.23375208136565728, test loss: 0.3051074219440494\n",
      "epoch 1162: train loss: 0.23367405510534475, test loss: 0.30505949920896486\n",
      "epoch 1163: train loss: 0.23359613437062954, test loss: 0.3050116523284138\n",
      "epoch 1164: train loss: 0.23351831893711314, test loss: 0.30496388113426603\n",
      "epoch 1165: train loss: 0.2334406085810223, test loss: 0.30491618545887755\n",
      "epoch 1166: train loss: 0.2333630030792075, test loss: 0.304868565135089\n",
      "epoch 1167: train loss: 0.2332855022091401, test loss: 0.30482101999622374\n",
      "epoch 1168: train loss: 0.23320810574891077, test loss: 0.30477354987608646\n",
      "epoch 1169: train loss: 0.2331308134772268, test loss: 0.30472615460896113\n",
      "epoch 1170: train loss: 0.2330536251734103, test loss: 0.3046788340296096\n",
      "epoch 1171: train loss: 0.23297654061739587, test loss: 0.30463158797326967\n",
      "epoch 1172: train loss: 0.23289955958972847, test loss: 0.3045844162756538\n",
      "epoch 1173: train loss: 0.23282268187156127, test loss: 0.304537318772947\n",
      "epoch 1174: train loss: 0.23274590724465363, test loss: 0.3044902953018054\n",
      "epoch 1175: train loss: 0.23266923549136892, test loss: 0.30444334569935455\n",
      "epoch 1176: train loss: 0.23259266639467235, test loss: 0.30439646980318796\n",
      "epoch 1177: train loss: 0.23251619973812893, test loss: 0.30434966745136527\n",
      "epoch 1178: train loss: 0.2324398353059014, test loss: 0.30430293848241047\n",
      "epoch 1179: train loss: 0.23236357288274817, test loss: 0.3042562827353105\n",
      "epoch 1180: train loss: 0.2322874122540211, test loss: 0.30420970004951403\n",
      "epoch 1181: train loss: 0.2322113532056637, test loss: 0.3041631902649287\n",
      "epoch 1182: train loss: 0.2321353955242087, test loss: 0.3041167532219209\n",
      "epoch 1183: train loss: 0.2320595389967766, test loss: 0.304070388761313\n",
      "epoch 1184: train loss: 0.23198378341107287, test loss: 0.3040240967243825\n",
      "epoch 1185: train loss: 0.23190812855538667, test loss: 0.3039778769528603\n",
      "epoch 1186: train loss: 0.23183257421858827, test loss: 0.3039317292889289\n",
      "epoch 1187: train loss: 0.2317571201901273, test loss: 0.303885653575221\n",
      "epoch 1188: train loss: 0.2316817662600309, test loss: 0.30383964965481786\n",
      "epoch 1189: train loss: 0.23160651221890127, test loss: 0.30379371737124794\n",
      "epoch 1190: train loss: 0.23153135785791423, test loss: 0.3037478565684851\n",
      "epoch 1191: train loss: 0.2314563029688169, test loss: 0.30370206709094705\n",
      "epoch 1192: train loss: 0.23138134734392574, test loss: 0.3036563487834939\n",
      "epoch 1193: train loss: 0.23130649077612475, test loss: 0.30361070149142705\n",
      "epoch 1194: train loss: 0.23123173305886355, test loss: 0.3035651250604868\n",
      "epoch 1195: train loss: 0.23115707398615531, test loss: 0.3035196193368514\n",
      "epoch 1196: train loss: 0.2310825133525747, test loss: 0.30347418416713556\n",
      "epoch 1197: train loss: 0.23100805095325644, test loss: 0.30342881939838867\n",
      "epoch 1198: train loss: 0.2309336865838928, test loss: 0.3033835248780935\n",
      "epoch 1199: train loss: 0.2308594200407321, test loss: 0.3033383004541647\n",
      "epoch 1200: train loss: 0.23078525112057682, test loss: 0.30329314597494705\n",
      "epoch 1201: train loss: 0.23071117962078136, test loss: 0.3032480612892144\n",
      "epoch 1202: train loss: 0.23063720533925053, test loss: 0.3032030462461679\n",
      "epoch 1203: train loss: 0.23056332807443752, test loss: 0.3031581006954346\n",
      "epoch 1204: train loss: 0.23048954762534207, test loss: 0.30311322448706585\n",
      "epoch 1205: train loss: 0.23041586379150858, test loss: 0.3030684174715363\n",
      "epoch 1206: train loss: 0.23034227637302437, test loss: 0.3030236794997418\n",
      "epoch 1207: train loss: 0.2302687851705176, test loss: 0.30297901042299846\n",
      "epoch 1208: train loss: 0.2301953899851557, test loss: 0.30293441009304106\n",
      "epoch 1209: train loss: 0.23012209061864358, test loss: 0.3028898783620216\n",
      "epoch 1210: train loss: 0.23004888687322145, test loss: 0.30284541508250756\n",
      "epoch 1211: train loss: 0.22997577855166335, test loss: 0.3028010201074811\n",
      "epoch 1212: train loss: 0.22990276545727528, test loss: 0.3027566932903373\n",
      "epoch 1213: train loss: 0.22982984739389345, test loss: 0.30271243448488255\n",
      "epoch 1214: train loss: 0.2297570241658822, test loss: 0.3026682435453335\n",
      "epoch 1215: train loss: 0.22968429557813258, test loss: 0.3026241203263156\n",
      "epoch 1216: train loss: 0.22961166143606065, test loss: 0.30258006468286136\n",
      "epoch 1217: train loss: 0.22953912154560513, test loss: 0.3025360764704096\n",
      "epoch 1218: train loss: 0.2294666757132263, test loss: 0.3024921555448033\n",
      "epoch 1219: train loss: 0.22939432374590396, test loss: 0.3024483017622888\n",
      "epoch 1220: train loss: 0.22932206545113565, test loss: 0.30240451497951437\n",
      "epoch 1221: train loss: 0.22924990063693515, test loss: 0.3023607950535285\n",
      "epoch 1222: train loss: 0.22917782911183032, test loss: 0.30231714184177866\n",
      "epoch 1223: train loss: 0.22910585068486194, test loss: 0.30227355520211036\n",
      "epoch 1224: train loss: 0.22903396516558167, test loss: 0.30223003499276524\n",
      "epoch 1225: train loss: 0.22896217236405023, test loss: 0.30218658107238006\n",
      "epoch 1226: train loss: 0.22889047209083613, test loss: 0.3021431932999851\n",
      "epoch 1227: train loss: 0.22881886415701358, test loss: 0.30209987153500323\n",
      "epoch 1228: train loss: 0.2287473483741609, test loss: 0.30205661563724817\n",
      "epoch 1229: train loss: 0.22867592455435914, test loss: 0.3020134254669233\n",
      "epoch 1230: train loss: 0.22860459251018997, test loss: 0.3019703008846204\n",
      "epoch 1231: train loss: 0.22853335205473427, test loss: 0.30192724175131835\n",
      "epoch 1232: train loss: 0.2284622030015705, test loss: 0.30188424792838187\n",
      "epoch 1233: train loss: 0.22839114516477302, test loss: 0.30184131927755986\n",
      "epoch 1234: train loss: 0.22832017835891033, test loss: 0.3017984556609845\n",
      "epoch 1235: train loss: 0.22824930239904362, test loss: 0.30175565694117\n",
      "epoch 1236: train loss: 0.22817851710072504, test loss: 0.30171292298101077\n",
      "epoch 1237: train loss: 0.22810782227999607, test loss: 0.3016702536437809\n",
      "epoch 1238: train loss: 0.22803721775338598, test loss: 0.30162764879313214\n",
      "epoch 1239: train loss: 0.22796670333791016, test loss: 0.30158510829309315\n",
      "epoch 1240: train loss: 0.2278962788510686, test loss: 0.3015426320080681\n",
      "epoch 1241: train loss: 0.22782594411084425, test loss: 0.30150021980283515\n",
      "epoch 1242: train loss: 0.22775569893570133, test loss: 0.30145787154254566\n",
      "epoch 1243: train loss: 0.22768554314458392, test loss: 0.3014155870927225\n",
      "epoch 1244: train loss: 0.2276154765569144, test loss: 0.301373366319259\n",
      "epoch 1245: train loss: 0.22754549899259174, test loss: 0.3013312090884178\n",
      "epoch 1246: train loss: 0.22747561027198998, test loss: 0.3012891152668294\n",
      "epoch 1247: train loss: 0.2274058102159568, test loss: 0.3012470847214911\n",
      "epoch 1248: train loss: 0.2273360986458118, test loss: 0.3012051173197657\n",
      "epoch 1249: train loss: 0.2272664753833451, test loss: 0.3011632129293801\n",
      "epoch 1250: train loss: 0.22719694025081572, test loss: 0.3011213714184245\n",
      "epoch 1251: train loss: 0.22712749307095018, test loss: 0.3010795926553509\n",
      "epoch 1252: train loss: 0.22705813366694066, test loss: 0.3010378765089717\n",
      "epoch 1253: train loss: 0.22698886186244396, test loss: 0.30099622284845906\n",
      "epoch 1254: train loss: 0.22691967748157965, test loss: 0.3009546315433433\n",
      "epoch 1255: train loss: 0.2268505803489286, test loss: 0.3009131024635115\n",
      "epoch 1256: train loss: 0.22678157028953166, test loss: 0.3008716354792069\n",
      "epoch 1257: train loss: 0.22671264712888792, test loss: 0.3008302304610273\n",
      "epoch 1258: train loss: 0.22664381069295356, test loss: 0.300788887279924\n",
      "epoch 1259: train loss: 0.22657506080813994, test loss: 0.3007476058072005\n",
      "epoch 1260: train loss: 0.22650639730131256, test loss: 0.3007063859145116\n",
      "epoch 1261: train loss: 0.22643781999978935, test loss: 0.3006652274738618\n",
      "epoch 1262: train loss: 0.22636932873133908, test loss: 0.3006241303576047\n",
      "epoch 1263: train loss: 0.22630092332418036, test loss: 0.3005830944384414\n",
      "epoch 1264: train loss: 0.2262326036069798, test loss: 0.30054211958941957\n",
      "epoch 1265: train loss: 0.22616436940885062, test loss: 0.300501205683932\n",
      "epoch 1266: train loss: 0.2260962205593514, test loss: 0.30046035259571596\n",
      "epoch 1267: train loss: 0.22602815688848443, test loss: 0.3004195601988516\n",
      "epoch 1268: train loss: 0.22596017822669445, test loss: 0.300378828367761\n",
      "epoch 1269: train loss: 0.22589228440486714, test loss: 0.300338156977207\n",
      "epoch 1270: train loss: 0.22582447525432778, test loss: 0.3002975459022923\n",
      "epoch 1271: train loss: 0.2257567506068397, test loss: 0.30025699501845793\n",
      "epoch 1272: train loss: 0.22568911029460306, test loss: 0.3002165042014823\n",
      "epoch 1273: train loss: 0.2256215541502533, test loss: 0.3001760733274805\n",
      "epoch 1274: train loss: 0.22555408200685995, test loss: 0.30013570227290226\n",
      "epoch 1275: train loss: 0.22548669369792493, test loss: 0.3000953909145317\n",
      "epoch 1276: train loss: 0.22541938905738154, test loss: 0.30005513912948606\n",
      "epoch 1277: train loss: 0.22535216791959264, test loss: 0.3000149467952142\n",
      "epoch 1278: train loss: 0.22528503011934986, test loss: 0.29997481378949586\n",
      "epoch 1279: train loss: 0.2252179754918717, test loss: 0.2999347399904406\n",
      "epoch 1280: train loss: 0.22515100387280249, test loss: 0.2998947252764865\n",
      "epoch 1281: train loss: 0.22508411509821094, test loss: 0.29985476952639906\n",
      "epoch 1282: train loss: 0.22501730900458866, test loss: 0.2998148726192706\n",
      "epoch 1283: train loss: 0.22495058542884913, test loss: 0.2997750344345185\n",
      "epoch 1284: train loss: 0.22488394420832605, test loss: 0.29973525485188446\n",
      "epoch 1285: train loss: 0.2248173851807722, test loss: 0.29969553375143376\n",
      "epoch 1286: train loss: 0.22475090818435808, test loss: 0.2996558710135535\n",
      "epoch 1287: train loss: 0.22468451305767062, test loss: 0.29961626651895235\n",
      "epoch 1288: train loss: 0.22461819963971155, test loss: 0.29957672014865866\n",
      "epoch 1289: train loss: 0.22455196776989667, test loss: 0.29953723178402\n",
      "epoch 1290: train loss: 0.22448581728805406, test loss: 0.2994978013067019\n",
      "epoch 1291: train loss: 0.2244197480344229, test loss: 0.299458428598687\n",
      "epoch 1292: train loss: 0.22435375984965247, test loss: 0.29941911354227363\n",
      "epoch 1293: train loss: 0.2242878525748003, test loss: 0.2993798560200752\n",
      "epoch 1294: train loss: 0.22422202605133143, test loss: 0.29934065591501896\n",
      "epoch 1295: train loss: 0.2241562801211168, test loss: 0.2993015131103449\n",
      "epoch 1296: train loss: 0.224090614626432, test loss: 0.2992624274896049\n",
      "epoch 1297: train loss: 0.2240250294099563, test loss: 0.2992233989366617\n",
      "epoch 1298: train loss: 0.22395952431477098, test loss: 0.2991844273356876\n",
      "epoch 1299: train loss: 0.22389409918435824, test loss: 0.29914551257116395\n",
      "epoch 1300: train loss: 0.22382875386260007, test loss: 0.2991066545278798\n",
      "epoch 1301: train loss: 0.2237634881937768, test loss: 0.29906785309093104\n",
      "epoch 1302: train loss: 0.22369830202256602, test loss: 0.299029108145719\n",
      "epoch 1303: train loss: 0.22363319519404107, test loss: 0.29899041957795025\n",
      "epoch 1304: train loss: 0.22356816755367034, test loss: 0.29895178727363486\n",
      "epoch 1305: train loss: 0.22350321894731537, test loss: 0.29891321111908586\n",
      "epoch 1306: train loss: 0.22343834922123007, test loss: 0.298874691000918\n",
      "epoch 1307: train loss: 0.22337355822205937, test loss: 0.29883622680604693\n",
      "epoch 1308: train loss: 0.2233088457968381, test loss: 0.29879781842168823\n",
      "epoch 1309: train loss: 0.22324421179298956, test loss: 0.2987594657353563\n",
      "epoch 1310: train loss: 0.2231796560583245, test loss: 0.2987211686348634\n",
      "epoch 1311: train loss: 0.22311517844103984, test loss: 0.2986829270083191\n",
      "epoch 1312: train loss: 0.22305077878971752, test loss: 0.29864474074412845\n",
      "epoch 1313: train loss: 0.22298645695332325, test loss: 0.29860660973099196\n",
      "epoch 1314: train loss: 0.22292221278120544, test loss: 0.2985685338579041\n",
      "epoch 1315: train loss: 0.22285804612309376, test loss: 0.2985305130141526\n",
      "epoch 1316: train loss: 0.22279395682909825, test loss: 0.2984925470893172\n",
      "epoch 1317: train loss: 0.222729944749708, test loss: 0.2984546359732691\n",
      "epoch 1318: train loss: 0.22266600973579007, test loss: 0.29841677955616963\n",
      "epoch 1319: train loss: 0.22260215163858807, test loss: 0.2983789777284697\n",
      "epoch 1320: train loss: 0.2225383703097213, test loss: 0.29834123038090854\n",
      "epoch 1321: train loss: 0.22247466560118354, test loss: 0.298303537404513\n",
      "epoch 1322: train loss: 0.2224110373653417, test loss: 0.29826589869059633\n",
      "epoch 1323: train loss: 0.22234748545493496, test loss: 0.29822831413075773\n",
      "epoch 1324: train loss: 0.22228400972307327, test loss: 0.29819078361688106\n",
      "epoch 1325: train loss: 0.22222061002323662, test loss: 0.2981533070411339\n",
      "epoch 1326: train loss: 0.2221572862092735, test loss: 0.298115884295967\n",
      "epoch 1327: train loss: 0.2220940381354002, test loss: 0.298078515274113\n",
      "epoch 1328: train loss: 0.22203086565619917, test loss: 0.29804119986858557\n",
      "epoch 1329: train loss: 0.22196776862661838, test loss: 0.2980039379726789\n",
      "epoch 1330: train loss: 0.22190474690196993, test loss: 0.2979667294799661\n",
      "epoch 1331: train loss: 0.22184180033792902, test loss: 0.297929574284299\n",
      "epoch 1332: train loss: 0.2217789287905328, test loss: 0.297892472279807\n",
      "epoch 1333: train loss: 0.22171613211617922, test loss: 0.2978554233608958\n",
      "epoch 1334: train loss: 0.22165341017162618, test loss: 0.2978184274222474\n",
      "epoch 1335: train loss: 0.22159076281399007, test loss: 0.29778148435881824\n",
      "epoch 1336: train loss: 0.22152818990074494, test loss: 0.2977445940658389\n",
      "epoch 1337: train loss: 0.2214656912897213, test loss: 0.2977077564388132\n",
      "epoch 1338: train loss: 0.22140326683910508, test loss: 0.29767097137351706\n",
      "epoch 1339: train loss: 0.22134091640743647, test loss: 0.2976342387659979\n",
      "epoch 1340: train loss: 0.221278639853609, test loss: 0.2975975585125737\n",
      "epoch 1341: train loss: 0.22121643703686827, test loss: 0.2975609305098319\n",
      "epoch 1342: train loss: 0.22115430781681106, test loss: 0.297524354654629\n",
      "epoch 1343: train loss: 0.22109225205338404, test loss: 0.29748783084408936\n",
      "epoch 1344: train loss: 0.2210302696068831, test loss: 0.29745135897560426\n",
      "epoch 1345: train loss: 0.22096836033795186, test loss: 0.2974149389468316\n",
      "epoch 1346: train loss: 0.22090652410758072, test loss: 0.29737857065569423\n",
      "epoch 1347: train loss: 0.22084476077710608, test loss: 0.29734225400038006\n",
      "epoch 1348: train loss: 0.22078307020820911, test loss: 0.2973059888793403\n",
      "epoch 1349: train loss: 0.22072145226291454, test loss: 0.29726977519128917\n",
      "epoch 1350: train loss: 0.2206599068035899, test loss: 0.2972336128352031\n",
      "epoch 1351: train loss: 0.2205984336929444, test loss: 0.2971975017103196\n",
      "epoch 1352: train loss: 0.22053703279402775, test loss: 0.29716144171613657\n",
      "epoch 1353: train loss: 0.2204757039702293, test loss: 0.2971254327524114\n",
      "epoch 1354: train loss: 0.22041444708527713, test loss: 0.2970894747191605\n",
      "epoch 1355: train loss: 0.2203532620032367, test loss: 0.29705356751665785\n",
      "epoch 1356: train loss: 0.22029214858850998, test loss: 0.29701771104543484\n",
      "epoch 1357: train loss: 0.22023110670583454, test loss: 0.296981905206279\n",
      "epoch 1358: train loss: 0.22017013622028256, test loss: 0.2969461499002333\n",
      "epoch 1359: train loss: 0.22010923699725957, test loss: 0.2969104450285957\n",
      "epoch 1360: train loss: 0.22004840890250374, test loss: 0.29687479049291754\n",
      "epoch 1361: train loss: 0.21998765180208468, test loss: 0.29683918619500377\n",
      "epoch 1362: train loss: 0.21992696556240254, test loss: 0.29680363203691124\n",
      "epoch 1363: train loss: 0.2198663500501871, test loss: 0.2967681279209484\n",
      "epoch 1364: train loss: 0.21980580513249645, test loss: 0.2967326737496745\n",
      "epoch 1365: train loss: 0.21974533067671653, test loss: 0.2966972694258985\n",
      "epoch 1366: train loss: 0.2196849265505597, test loss: 0.29666191485267873\n",
      "epoch 1367: train loss: 0.21962459262206402, test loss: 0.2966266099333216\n",
      "epoch 1368: train loss: 0.2195643287595921, test loss: 0.29659135457138114\n",
      "epoch 1369: train loss: 0.21950413483183034, test loss: 0.29655614867065827\n",
      "epoch 1370: train loss: 0.2194440107077877, test loss: 0.2965209921351999\n",
      "epoch 1371: train loss: 0.21938395625679497, test loss: 0.29648588486929794\n",
      "epoch 1372: train loss: 0.2193239713485038, test loss: 0.29645082677748913\n",
      "epoch 1373: train loss: 0.21926405585288555, test loss: 0.29641581776455356\n",
      "epoch 1374: train loss: 0.21920420964023057, test loss: 0.2963808577355144\n",
      "epoch 1375: train loss: 0.21914443258114708, test loss: 0.2963459465956371\n",
      "epoch 1376: train loss: 0.21908472454656028, test loss: 0.2963110842504284\n",
      "epoch 1377: train loss: 0.2190250854077116, test loss: 0.2962762706056356\n",
      "epoch 1378: train loss: 0.2189655150361572, test loss: 0.2962415055672461\n",
      "epoch 1379: train loss: 0.21890601330376783, test loss: 0.29620678904148634\n",
      "epoch 1380: train loss: 0.2188465800827273, test loss: 0.29617212093482126\n",
      "epoch 1381: train loss: 0.21878721524553182, test loss: 0.2961375011539532\n",
      "epoch 1382: train loss: 0.21872791866498886, test loss: 0.29610292960582174\n",
      "epoch 1383: train loss: 0.21866869021421656, test loss: 0.29606840619760255\n",
      "epoch 1384: train loss: 0.2186095297666425, test loss: 0.2960339308367067\n",
      "epoch 1385: train loss: 0.2185504371960029, test loss: 0.29599950343078\n",
      "epoch 1386: train loss: 0.21849141237634181, test loss: 0.29596512388770213\n",
      "epoch 1387: train loss: 0.2184324551820101, test loss: 0.2959307921155862\n",
      "epoch 1388: train loss: 0.21837356548766443, test loss: 0.2958965080227777\n",
      "epoch 1389: train loss: 0.21831474316826668, test loss: 0.2958622715178541\n",
      "epoch 1390: train loss: 0.21825598809908267, test loss: 0.29582808250962367\n",
      "epoch 1391: train loss: 0.2181973001556817, test loss: 0.2957939409071254\n",
      "epoch 1392: train loss: 0.2181386792139351, test loss: 0.29575984661962756\n",
      "epoch 1393: train loss: 0.21808012515001593, test loss: 0.2957257995566277\n",
      "epoch 1394: train loss: 0.21802163784039763, test loss: 0.29569179962785125\n",
      "epoch 1395: train loss: 0.21796321716185346, test loss: 0.2956578467432515\n",
      "epoch 1396: train loss: 0.21790486299145548, test loss: 0.29562394081300825\n",
      "epoch 1397: train loss: 0.2178465752065736, test loss: 0.2955900817475277\n",
      "epoch 1398: train loss: 0.2177883536848748, test loss: 0.2955562694574412\n",
      "epoch 1399: train loss: 0.21773019830432233, test loss: 0.29552250385360496\n",
      "epoch 1400: train loss: 0.21767210894317468, test loss: 0.2954887848470992\n",
      "epoch 1401: train loss: 0.21761408547998484, test loss: 0.29545511234922733\n",
      "epoch 1402: train loss: 0.2175561277935995, test loss: 0.29542148627151554\n",
      "epoch 1403: train loss: 0.21749823576315783, test loss: 0.295387906525712\n",
      "epoch 1404: train loss: 0.21744040926809116, test loss: 0.29535437302378603\n",
      "epoch 1405: train loss: 0.21738264818812175, test loss: 0.2953208856779275\n",
      "epoch 1406: train loss: 0.21732495240326202, test loss: 0.29528744440054633\n",
      "epoch 1407: train loss: 0.21726732179381375, test loss: 0.29525404910427144\n",
      "epoch 1408: train loss: 0.21720975624036729, test loss: 0.29522069970195053\n",
      "epoch 1409: train loss: 0.21715225562380056, test loss: 0.295187396106649\n",
      "epoch 1410: train loss: 0.21709481982527837, test loss: 0.29515413823164954\n",
      "epoch 1411: train loss: 0.21703744872625147, test loss: 0.29512092599045137\n",
      "epoch 1412: train loss: 0.21698014220845582, test loss: 0.29508775929676956\n",
      "epoch 1413: train loss: 0.2169229001539116, test loss: 0.29505463806453425\n",
      "epoch 1414: train loss: 0.21686572244492275, test loss: 0.29502156220789044\n",
      "epoch 1415: train loss: 0.21680860896407567, test loss: 0.29498853164119665\n",
      "epoch 1416: train loss: 0.2167515595942386, test loss: 0.2949555462790248\n",
      "epoch 1417: train loss: 0.21669457421856106, test loss: 0.29492260603615955\n",
      "epoch 1418: train loss: 0.21663765272047258, test loss: 0.29488971082759713\n",
      "epoch 1419: train loss: 0.2165807949836822, test loss: 0.29485686056854526\n",
      "epoch 1420: train loss: 0.2165240008921776, test loss: 0.29482405517442223\n",
      "epoch 1421: train loss: 0.21646727033022428, test loss: 0.29479129456085645\n",
      "epoch 1422: train loss: 0.21641060318236469, test loss: 0.29475857864368527\n",
      "epoch 1423: train loss: 0.2163539993334176, test loss: 0.29472590733895526\n",
      "epoch 1424: train loss: 0.21629745866847702, test loss: 0.2946932805629206\n",
      "epoch 1425: train loss: 0.21624098107291176, test loss: 0.29466069823204316\n",
      "epoch 1426: train loss: 0.21618456643236428, test loss: 0.2946281602629915\n",
      "epoch 1427: train loss: 0.21612821463275025, test loss: 0.29459566657264036\n",
      "epoch 1428: train loss: 0.21607192556025756, test loss: 0.29456321707807\n",
      "epoch 1429: train loss: 0.21601569910134544, test loss: 0.2945308116965655\n",
      "epoch 1430: train loss: 0.21595953514274396, test loss: 0.2944984503456164\n",
      "epoch 1431: train loss: 0.215903433571453, test loss: 0.29446613294291574\n",
      "epoch 1432: train loss: 0.2158473942747417, test loss: 0.29443385940635963\n",
      "epoch 1433: train loss: 0.21579141714014738, test loss: 0.2944016296540466\n",
      "epoch 1434: train loss: 0.21573550205547512, test loss: 0.29436944360427697\n",
      "epoch 1435: train loss: 0.2156796489087968, test loss: 0.29433730117555235\n",
      "epoch 1436: train loss: 0.21562385758845026, test loss: 0.2943052022865747\n",
      "epoch 1437: train loss: 0.21556812798303873, test loss: 0.29427314685624617\n",
      "epoch 1438: train loss: 0.21551245998142995, test loss: 0.29424113480366815\n",
      "epoch 1439: train loss: 0.21545685347275548, test loss: 0.2942091660481408\n",
      "epoch 1440: train loss: 0.21540130834640986, test loss: 0.29417724050916244\n",
      "epoch 1441: train loss: 0.21534582449204998, test loss: 0.2941453581064289\n",
      "epoch 1442: train loss: 0.2152904017995941, test loss: 0.294113518759833\n",
      "epoch 1443: train loss: 0.21523504015922149, test loss: 0.294081722389464\n",
      "epoch 1444: train loss: 0.21517973946137126, test loss: 0.2940499689156067\n",
      "epoch 1445: train loss: 0.21512449959674193, test loss: 0.29401825825874117\n",
      "epoch 1446: train loss: 0.21506932045629049, test loss: 0.2939865903395422\n",
      "epoch 1447: train loss: 0.21501420193123194, test loss: 0.29395496507887836\n",
      "epoch 1448: train loss: 0.21495914391303805, test loss: 0.29392338239781174\n",
      "epoch 1449: train loss: 0.21490414629343724, test loss: 0.2938918422175972\n",
      "epoch 1450: train loss: 0.21484920896441337, test loss: 0.2938603444596818\n",
      "epoch 1451: train loss: 0.2147943318182054, test loss: 0.29382888904570426\n",
      "epoch 1452: train loss: 0.21473951474730613, test loss: 0.29379747589749455\n",
      "epoch 1453: train loss: 0.21468475764446218, test loss: 0.29376610493707284\n",
      "epoch 1454: train loss: 0.21463006040267263, test loss: 0.29373477608664955\n",
      "epoch 1455: train loss: 0.21457542291518877, test loss: 0.29370348926862416\n",
      "epoch 1456: train loss: 0.2145208450755131, test loss: 0.2936722444055852\n",
      "epoch 1457: train loss: 0.2144663267773987, test loss: 0.2936410414203091\n",
      "epoch 1458: train loss: 0.21441186791484854, test loss: 0.29360988023576035\n",
      "epoch 1459: train loss: 0.21435746838211472, test loss: 0.29357876077509004\n",
      "epoch 1460: train loss: 0.21430312807369795, test loss: 0.29354768296163625\n",
      "epoch 1461: train loss: 0.21424884688434662, test loss: 0.29351664671892275\n",
      "epoch 1462: train loss: 0.21419462470905612, test loss: 0.2934856519706587\n",
      "epoch 1463: train loss: 0.2141404614430683, test loss: 0.29345469864073837\n",
      "epoch 1464: train loss: 0.2140863569818706, test loss: 0.2934237866532399\n",
      "epoch 1465: train loss: 0.2140323112211955, test loss: 0.2933929159324256\n",
      "epoch 1466: train loss: 0.21397832405701983, test loss: 0.29336208640274075\n",
      "epoch 1467: train loss: 0.21392439538556388, test loss: 0.2933312979888133\n",
      "epoch 1468: train loss: 0.2138705251032909, test loss: 0.2933005506154533\n",
      "epoch 1469: train loss: 0.21381671310690642, test loss: 0.2932698442076523\n",
      "epoch 1470: train loss: 0.21376295929335748, test loss: 0.293239178690583\n",
      "epoch 1471: train loss: 0.21370926355983194, test loss: 0.2932085539895985\n",
      "epoch 1472: train loss: 0.21365562580375794, test loss: 0.2931779700302318\n",
      "epoch 1473: train loss: 0.21360204592280305, test loss: 0.2931474267381952\n",
      "epoch 1474: train loss: 0.21354852381487377, test loss: 0.29311692403938017\n",
      "epoch 1475: train loss: 0.21349505937811464, test loss: 0.2930864618598561\n",
      "epoch 1476: train loss: 0.21344165251090783, test loss: 0.29305604012587044\n",
      "epoch 1477: train loss: 0.21338830311187237, test loss: 0.29302565876384784\n",
      "epoch 1478: train loss: 0.21333501107986322, test loss: 0.29299531770038967\n",
      "epoch 1479: train loss: 0.21328177631397122, test loss: 0.29296501686227344\n",
      "epoch 1480: train loss: 0.21322859871352173, test loss: 0.2929347561764525\n",
      "epoch 1481: train loss: 0.21317547817807445, test loss: 0.29290453557005525\n",
      "epoch 1482: train loss: 0.21312241460742265, test loss: 0.2928743549703847\n",
      "epoch 1483: train loss: 0.21306940790159237, test loss: 0.29284421430491814\n",
      "epoch 1484: train loss: 0.213016457960842, test loss: 0.2928141135013062\n",
      "epoch 1485: train loss: 0.21296356468566152, test loss: 0.2927840524873729\n",
      "epoch 1486: train loss: 0.21291072797677174, test loss: 0.2927540311911146\n",
      "epoch 1487: train loss: 0.2128579477351238, test loss: 0.29272404954069975\n",
      "epoch 1488: train loss: 0.21280522386189854, test loss: 0.2926941074644686\n",
      "epoch 1489: train loss: 0.21275255625850575, test loss: 0.2926642048909321\n",
      "epoch 1490: train loss: 0.21269994482658375, test loss: 0.29263434174877195\n",
      "epoch 1491: train loss: 0.21264738946799835, test loss: 0.29260451796683973\n",
      "epoch 1492: train loss: 0.21259489008484259, test loss: 0.2925747334741569\n",
      "epoch 1493: train loss: 0.21254244657943602, test loss: 0.2925449881999134\n",
      "epoch 1494: train loss: 0.21249005885432404, test loss: 0.2925152820734682\n",
      "epoch 1495: train loss: 0.2124377268122771, test loss: 0.29248561502434794\n",
      "epoch 1496: train loss: 0.21238545035629042, test loss: 0.2924559869822471\n",
      "epoch 1497: train loss: 0.2123332293895831, test loss: 0.292426397877027\n",
      "epoch 1498: train loss: 0.21228106381559758, test loss: 0.29239684763871554\n",
      "epoch 1499: train loss: 0.21222895353799906, test loss: 0.29236733619750666\n",
      "epoch 1500: train loss: 0.21217689846067486, test loss: 0.29233786348375995\n",
      "epoch 1501: train loss: 0.2121248984877337, test loss: 0.292308429428\n",
      "epoch 1502: train loss: 0.2120729535235052, test loss: 0.2922790339609161\n",
      "epoch 1503: train loss: 0.21202106347253932, test loss: 0.29224967701336146\n",
      "epoch 1504: train loss: 0.2119692282396056, test loss: 0.29222035851635314\n",
      "epoch 1505: train loss: 0.21191744772969265, test loss: 0.2921910784010712\n",
      "epoch 1506: train loss: 0.2118657218480076, test loss: 0.29216183659885847\n",
      "epoch 1507: train loss: 0.21181405049997518, test loss: 0.2921326330412197\n",
      "epoch 1508: train loss: 0.2117624335912376, test loss: 0.2921034676598218\n",
      "epoch 1509: train loss: 0.21171087102765362, test loss: 0.29207434038649255\n",
      "epoch 1510: train loss: 0.211659362715298, test loss: 0.29204525115322055\n",
      "epoch 1511: train loss: 0.21160790856046102, test loss: 0.2920161998921549\n",
      "epoch 1512: train loss: 0.21155650846964769, test loss: 0.29198718653560435\n",
      "epoch 1513: train loss: 0.2115051623495774, test loss: 0.2919582110160369\n",
      "epoch 1514: train loss: 0.21145387010718308, test loss: 0.2919292732660798\n",
      "epoch 1515: train loss: 0.2114026316496109, test loss: 0.2919003732185183\n",
      "epoch 1516: train loss: 0.21135144688421936, test loss: 0.291871510806296\n",
      "epoch 1517: train loss: 0.21130031571857888, test loss: 0.2918426859625137\n",
      "epoch 1518: train loss: 0.21124923806047127, test loss: 0.29181389862042934\n",
      "epoch 1519: train loss: 0.21119821381788917, test loss: 0.2917851487134575\n",
      "epoch 1520: train loss: 0.21114724289903514, test loss: 0.29175643617516883\n",
      "epoch 1521: train loss: 0.21109632521232144, test loss: 0.29172776093928965\n",
      "epoch 1522: train loss: 0.21104546066636942, test loss: 0.2916991229397015\n",
      "epoch 1523: train loss: 0.21099464917000885, test loss: 0.29167052211044064\n",
      "epoch 1524: train loss: 0.21094389063227711, test loss: 0.2916419583856978\n",
      "epoch 1525: train loss: 0.21089318496241932, test loss: 0.29161343169981735\n",
      "epoch 1526: train loss: 0.21084253206988696, test loss: 0.2915849419872972\n",
      "epoch 1527: train loss: 0.2107919318643377, test loss: 0.29155648918278826\n",
      "epoch 1528: train loss: 0.2107413842556351, test loss: 0.2915280732210938\n",
      "epoch 1529: train loss: 0.2106908891538474, test loss: 0.29149969403716935\n",
      "epoch 1530: train loss: 0.21064044646924757, test loss: 0.29147135156612203\n",
      "epoch 1531: train loss: 0.2105900561123123, test loss: 0.29144304574321006\n",
      "epoch 1532: train loss: 0.21053971799372173, test loss: 0.2914147765038425\n",
      "epoch 1533: train loss: 0.21048943202435885, test loss: 0.29138654378357887\n",
      "epoch 1534: train loss: 0.21043919811530884, test loss: 0.2913583475181282\n",
      "epoch 1535: train loss: 0.21038901617785855, test loss: 0.2913301876433494\n",
      "epoch 1536: train loss: 0.210338886123496, test loss: 0.29130206409525017\n",
      "epoch 1537: train loss: 0.21028880786390983, test loss: 0.2912739768099867\n",
      "epoch 1538: train loss: 0.2102387813109887, test loss: 0.2912459257238636\n",
      "epoch 1539: train loss: 0.2101888063768208, test loss: 0.291217910773333\n",
      "epoch 1540: train loss: 0.21013888297369324, test loss: 0.2911899318949944\n",
      "epoch 1541: train loss: 0.2100890110140916, test loss: 0.2911619890255943\n",
      "epoch 1542: train loss: 0.21003919041069935, test loss: 0.2911340821020254\n",
      "epoch 1543: train loss: 0.20998942107639723, test loss: 0.29110621106132667\n",
      "epoch 1544: train loss: 0.20993970292426284, test loss: 0.2910783758406825\n",
      "epoch 1545: train loss: 0.20989003586757007, test loss: 0.29105057637742265\n",
      "epoch 1546: train loss: 0.2098404198197885, test loss: 0.2910228126090214\n",
      "epoch 1547: train loss: 0.20979085469458295, test loss: 0.29099508447309774\n",
      "epoch 1548: train loss: 0.20974134040581283, test loss: 0.2909673919074142\n",
      "epoch 1549: train loss: 0.20969187686753193, test loss: 0.2909397348498771\n",
      "epoch 1550: train loss: 0.2096424639939874, test loss: 0.29091211323853594\n",
      "epoch 1551: train loss: 0.20959310169961967, test loss: 0.2908845270115825\n",
      "epoch 1552: train loss: 0.20954378989906153, test loss: 0.29085697610735134\n",
      "epoch 1553: train loss: 0.20949452850713823, test loss: 0.29082946046431857\n",
      "epoch 1554: train loss: 0.20944531743886607, test loss: 0.2908019800211019\n",
      "epoch 1555: train loss: 0.20939615660945268, test loss: 0.2907745347164602\n",
      "epoch 1556: train loss: 0.20934704593429607, test loss: 0.29074712448929274\n",
      "epoch 1557: train loss: 0.2092979853289844, test loss: 0.29071974927863936\n",
      "epoch 1558: train loss: 0.20924897470929502, test loss: 0.29069240902367954\n",
      "epoch 1559: train loss: 0.20920001399119442, test loss: 0.29066510366373216\n",
      "epoch 1560: train loss: 0.20915110309083765, test loss: 0.2906378331382555\n",
      "epoch 1561: train loss: 0.20910224192456742, test loss: 0.290610597386846\n",
      "epoch 1562: train loss: 0.2090534304089141, test loss: 0.2905833963492389\n",
      "epoch 1563: train loss: 0.20900466846059487, test loss: 0.2905562299653069\n",
      "epoch 1564: train loss: 0.2089559559965135, test loss: 0.29052909817506034\n",
      "epoch 1565: train loss: 0.20890729293375948, test loss: 0.2905020009186467\n",
      "epoch 1566: train loss: 0.20885867918960777, test loss: 0.29047493813634995\n",
      "epoch 1567: train loss: 0.2088101146815184, test loss: 0.29044790976859053\n",
      "epoch 1568: train loss: 0.20876159932713567, test loss: 0.29042091575592477\n",
      "epoch 1569: train loss: 0.20871313304428787, test loss: 0.29039395603904444\n",
      "epoch 1570: train loss: 0.20866471575098686, test loss: 0.2903670305587766\n",
      "epoch 1571: train loss: 0.2086163473654271, test loss: 0.2903401392560829\n",
      "epoch 1572: train loss: 0.20856802780598593, test loss: 0.2903132820720593\n",
      "epoch 1573: train loss: 0.20851975699122227, test loss: 0.29028645894793603\n",
      "epoch 1574: train loss: 0.20847153483987682, test loss: 0.29025966982507684\n",
      "epoch 1575: train loss: 0.20842336127087113, test loss: 0.29023291464497847\n",
      "epoch 1576: train loss: 0.20837523620330714, test loss: 0.2902061933492708\n",
      "epoch 1577: train loss: 0.20832715955646713, test loss: 0.2901795058797159\n",
      "epoch 1578: train loss: 0.20827913124981254, test loss: 0.2901528521782083\n",
      "epoch 1579: train loss: 0.20823115120298422, test loss: 0.29012623218677386\n",
      "epoch 1580: train loss: 0.2081832193358013, test loss: 0.2900996458475701\n",
      "epoch 1581: train loss: 0.2081353355682613, test loss: 0.2900730931028852\n",
      "epoch 1582: train loss: 0.20808749982053903, test loss: 0.2900465738951383\n",
      "epoch 1583: train loss: 0.20803971201298677, test loss: 0.29002008816687863\n",
      "epoch 1584: train loss: 0.2079919720661333, test loss: 0.28999363586078514\n",
      "epoch 1585: train loss: 0.2079442799006836, test loss: 0.2899672169196665\n",
      "epoch 1586: train loss: 0.20789663543751855, test loss: 0.2899408312864603\n",
      "epoch 1587: train loss: 0.207849038597694, test loss: 0.2899144789042331\n",
      "epoch 1588: train loss: 0.2078014893024409, test loss: 0.28988815971617976\n",
      "epoch 1589: train loss: 0.20775398747316445, test loss: 0.28986187366562316\n",
      "epoch 1590: train loss: 0.20770653303144354, test loss: 0.289835620696014\n",
      "epoch 1591: train loss: 0.2076591258990307, test loss: 0.28980940075093\n",
      "epoch 1592: train loss: 0.20761176599785125, test loss: 0.2897832137740762\n",
      "epoch 1593: train loss: 0.2075644532500031, test loss: 0.289757059709284\n",
      "epoch 1594: train loss: 0.20751718757775608, test loss: 0.28973093850051124\n",
      "epoch 1595: train loss: 0.20746996890355174, test loss: 0.28970485009184144\n",
      "epoch 1596: train loss: 0.2074227971500025, test loss: 0.2896787944274838\n",
      "epoch 1597: train loss: 0.2073756722398917, test loss: 0.2896527714517727\n",
      "epoch 1598: train loss: 0.20732859409617269, test loss: 0.2896267811091673\n",
      "epoch 1599: train loss: 0.20728156264196865, test loss: 0.2896008233442513\n",
      "epoch 1600: train loss: 0.20723457780057203, test loss: 0.2895748981017324\n",
      "epoch 1601: train loss: 0.20718763949544422, test loss: 0.2895490053264423\n",
      "epoch 1602: train loss: 0.2071407476502149, test loss: 0.289523144963336\n",
      "epoch 1603: train loss: 0.20709390218868173, test loss: 0.2894973169574916\n",
      "epoch 1604: train loss: 0.20704710303480997, test loss: 0.28947152125410996\n",
      "epoch 1605: train loss: 0.2070003501127318, test loss: 0.28944575779851434\n",
      "epoch 1606: train loss: 0.2069536433467462, test loss: 0.2894200265361501\n",
      "epoch 1607: train loss: 0.2069069826613182, test loss: 0.2893943274125843\n",
      "epoch 1608: train loss: 0.20686036798107865, test loss: 0.28936866037350534\n",
      "epoch 1609: train loss: 0.20681379923082358, test loss: 0.2893430253647227\n",
      "epoch 1610: train loss: 0.2067672763355141, test loss: 0.2893174223321665\n",
      "epoch 1611: train loss: 0.20672079922027553, test loss: 0.2892918512218874\n",
      "epoch 1612: train loss: 0.20667436781039747, test loss: 0.2892663119800559\n",
      "epoch 1613: train loss: 0.20662798203133273, test loss: 0.2892408045529623\n",
      "epoch 1614: train loss: 0.20658164180869762, test loss: 0.2892153288870162\n",
      "epoch 1615: train loss: 0.20653534706827084, test loss: 0.2891898849287462\n",
      "epoch 1616: train loss: 0.20648909773599355, test loss: 0.2891644726247997\n",
      "epoch 1617: train loss: 0.2064428937379688, test loss: 0.2891390919219425\n",
      "epoch 1618: train loss: 0.20639673500046093, test loss: 0.28911374276705815\n",
      "epoch 1619: train loss: 0.20635062144989524, test loss: 0.2890884251071483\n",
      "epoch 1620: train loss: 0.20630455301285783, test loss: 0.2890631388893317\n",
      "epoch 1621: train loss: 0.2062585296160947, test loss: 0.28903788406084424\n",
      "epoch 1622: train loss: 0.20621255118651177, test loss: 0.2890126605690386\n",
      "epoch 1623: train loss: 0.20616661765117417, test loss: 0.28898746836138384\n",
      "epoch 1624: train loss: 0.2061207289373059, test loss: 0.288962307385465\n",
      "epoch 1625: train loss: 0.20607488497228957, test loss: 0.2889371775889831\n",
      "epoch 1626: train loss: 0.20602908568366565, test loss: 0.2889120789197543\n",
      "epoch 1627: train loss: 0.20598333099913238, test loss: 0.28888701132571\n",
      "epoch 1628: train loss: 0.20593762084654524, test loss: 0.28886197475489667\n",
      "epoch 1629: train loss: 0.20589195515391648, test loss: 0.2888369691554749\n",
      "epoch 1630: train loss: 0.20584633384941475, test loss: 0.2888119944757196\n",
      "epoch 1631: train loss: 0.2058007568613647, test loss: 0.28878705066401955\n",
      "epoch 1632: train loss: 0.20575522411824665, test loss: 0.28876213766887704\n",
      "epoch 1633: train loss: 0.20570973554869593, test loss: 0.28873725543890755\n",
      "epoch 1634: train loss: 0.20566429108150278, test loss: 0.28871240392283953\n",
      "epoch 1635: train loss: 0.20561889064561178, test loss: 0.2886875830695141\n",
      "epoch 1636: train loss: 0.20557353417012145, test loss: 0.2886627928278843\n",
      "epoch 1637: train loss: 0.20552822158428385, test loss: 0.28863803314701575\n",
      "epoch 1638: train loss: 0.20548295281750428, test loss: 0.28861330397608526\n",
      "epoch 1639: train loss: 0.20543772779934075, test loss: 0.2885886052643811\n",
      "epoch 1640: train loss: 0.20539254645950358, test loss: 0.28856393696130284\n",
      "epoch 1641: train loss: 0.20534740872785512, test loss: 0.28853929901636044\n",
      "epoch 1642: train loss: 0.20530231453440934, test loss: 0.2885146913791746\n",
      "epoch 1643: train loss: 0.2052572638093312, test loss: 0.28849011399947605\n",
      "epoch 1644: train loss: 0.20521225648293664, test loss: 0.2884655668271053\n",
      "epoch 1645: train loss: 0.20516729248569185, test loss: 0.28844104981201246\n",
      "epoch 1646: train loss: 0.20512237174821304, test loss: 0.28841656290425705\n",
      "epoch 1647: train loss: 0.20507749420126611, test loss: 0.28839210605400717\n",
      "epoch 1648: train loss: 0.20503265977576596, test loss: 0.2883676792115399\n",
      "epoch 1649: train loss: 0.2049878684027766, test loss: 0.2883432823272404\n",
      "epoch 1650: train loss: 0.2049431200135102, test loss: 0.2883189153516021\n",
      "epoch 1651: train loss: 0.20489841453932725, test loss: 0.288294578235226\n",
      "epoch 1652: train loss: 0.2048537519117356, test loss: 0.2882702709288207\n",
      "epoch 1653: train loss: 0.20480913206239063, test loss: 0.28824599338320184\n",
      "epoch 1654: train loss: 0.20476455492309448, test loss: 0.288221745549292\n",
      "epoch 1655: train loss: 0.20472002042579596, test loss: 0.2881975273781203\n",
      "epoch 1656: train loss: 0.2046755285025898, test loss: 0.28817333882082236\n",
      "epoch 1657: train loss: 0.20463107908571665, test loss: 0.28814917982863936\n",
      "epoch 1658: train loss: 0.20458667210756246, test loss: 0.2881250503529186\n",
      "epoch 1659: train loss: 0.2045423075006583, test loss: 0.2881009503451124\n",
      "epoch 1660: train loss: 0.20449798519767973, test loss: 0.2880768797567788\n",
      "epoch 1661: train loss: 0.2044537051314466, test loss: 0.2880528385395802\n",
      "epoch 1662: train loss: 0.20440946723492265, test loss: 0.2880288266452835\n",
      "epoch 1663: train loss: 0.2043652714412152, test loss: 0.28800484402576026\n",
      "epoch 1664: train loss: 0.20432111768357453, test loss: 0.2879808906329858\n",
      "epoch 1665: train loss: 0.20427700589539385, test loss: 0.2879569664190392\n",
      "epoch 1666: train loss: 0.20423293601020873, test loss: 0.2879330713361029\n",
      "epoch 1667: train loss: 0.2041889079616967, test loss: 0.2879092053364627\n",
      "epoch 1668: train loss: 0.204144921683677, test loss: 0.28788536837250694\n",
      "epoch 1669: train loss: 0.20410097711011022, test loss: 0.2878615603967268\n",
      "epoch 1670: train loss: 0.20405707417509766, test loss: 0.28783778136171567\n",
      "epoch 1671: train loss: 0.20401321281288154, test loss: 0.28781403122016896\n",
      "epoch 1672: train loss: 0.203969392957844, test loss: 0.287790309924884\n",
      "epoch 1673: train loss: 0.20392561454450708, test loss: 0.2877666174287594\n",
      "epoch 1674: train loss: 0.20388187750753237, test loss: 0.28774295368479497\n",
      "epoch 1675: train loss: 0.2038381817817205, test loss: 0.28771931864609157\n",
      "epoch 1676: train loss: 0.20379452730201097, test loss: 0.2876957122658507\n",
      "epoch 1677: train loss: 0.2037509140034816, test loss: 0.2876721344973743\n",
      "epoch 1678: train loss: 0.2037073418213483, test loss: 0.2876485852940641\n",
      "epoch 1679: train loss: 0.20366381069096462, test loss: 0.2876250646094222\n",
      "epoch 1680: train loss: 0.20362032054782156, test loss: 0.28760157239705\n",
      "epoch 1681: train loss: 0.2035768713275469, test loss: 0.287578108610648\n",
      "epoch 1682: train loss: 0.20353346296590538, test loss: 0.287554673204016\n",
      "epoch 1683: train loss: 0.2034900953987977, test loss: 0.2875312661310527\n",
      "epoch 1684: train loss: 0.20344676856226065, test loss: 0.28750788734575494\n",
      "epoch 1685: train loss: 0.20340348239246667, test loss: 0.28748453680221814\n",
      "epoch 1686: train loss: 0.20336023682572332, test loss: 0.2874612144546355\n",
      "epoch 1687: train loss: 0.20331703179847302, test loss: 0.287437920257298\n",
      "epoch 1688: train loss: 0.2032738672472929, test loss: 0.287414654164594\n",
      "epoch 1689: train loss: 0.2032307431088941, test loss: 0.2873914161310092\n",
      "epoch 1690: train loss: 0.2031876593201218, test loss: 0.2873682061111262\n",
      "epoch 1691: train loss: 0.20314461581795457, test loss: 0.287345024059624\n",
      "epoch 1692: train loss: 0.20310161253950415, test loss: 0.28732186993127834\n",
      "epoch 1693: train loss: 0.20305864942201515, test loss: 0.287298743680961\n",
      "epoch 1694: train loss: 0.20301572640286472, test loss: 0.2872756452636397\n",
      "epoch 1695: train loss: 0.20297284341956207, test loss: 0.28725257463437764\n",
      "epoch 1696: train loss: 0.2029300004097483, test loss: 0.2872295317483336\n",
      "epoch 1697: train loss: 0.20288719731119584, test loss: 0.28720651656076135\n",
      "epoch 1698: train loss: 0.2028444340618085, test loss: 0.28718352902700967\n",
      "epoch 1699: train loss: 0.2028017105996207, test loss: 0.2871605691025217\n",
      "epoch 1700: train loss: 0.20275902686279743, test loss: 0.2871376367428352\n",
      "epoch 1701: train loss: 0.20271638278963372, test loss: 0.28711473190358194\n",
      "epoch 1702: train loss: 0.2026737783185546, test loss: 0.28709185454048763\n",
      "epoch 1703: train loss: 0.20263121338811435, test loss: 0.2870690046093715\n",
      "epoch 1704: train loss: 0.20258868793699655, test loss: 0.2870461820661461\n",
      "epoch 1705: train loss: 0.20254620190401348, test loss: 0.2870233868668173\n",
      "epoch 1706: train loss: 0.20250375522810607, test loss: 0.2870006189674835\n",
      "epoch 1707: train loss: 0.20246134784834333, test loss: 0.28697787832433624\n",
      "epoch 1708: train loss: 0.20241897970392198, test loss: 0.28695516489365885\n",
      "epoch 1709: train loss: 0.20237665073416647, test loss: 0.28693247863182714\n",
      "epoch 1710: train loss: 0.20233436087852827, test loss: 0.28690981949530875\n",
      "epoch 1711: train loss: 0.20229211007658576, test loss: 0.2868871874406629\n",
      "epoch 1712: train loss: 0.20224989826804404, test loss: 0.28686458242454016\n",
      "epoch 1713: train loss: 0.20220772539273407, test loss: 0.28684200440368235\n",
      "epoch 1714: train loss: 0.20216559139061302, test loss: 0.2868194533349223\n",
      "epoch 1715: train loss: 0.2021234962017635, test loss: 0.2867969291751833\n",
      "epoch 1716: train loss: 0.20208143976639337, test loss: 0.2867744318814791\n",
      "epoch 1717: train loss: 0.20203942202483552, test loss: 0.2867519614109137\n",
      "epoch 1718: train loss: 0.2019974429175474, test loss: 0.2867295177206811\n",
      "epoch 1719: train loss: 0.2019555023851108, test loss: 0.286707100768065\n",
      "epoch 1720: train loss: 0.20191360036823144, test loss: 0.28668471051043837\n",
      "epoch 1721: train loss: 0.20187173680773882, test loss: 0.28666234690526365\n",
      "epoch 1722: train loss: 0.20182991164458572, test loss: 0.2866400099100923\n",
      "epoch 1723: train loss: 0.20178812481984804, test loss: 0.2866176994825643\n",
      "epoch 1724: train loss: 0.20174637627472441, test loss: 0.2865954155804086\n",
      "epoch 1725: train loss: 0.20170466595053582, test loss: 0.2865731581614419\n",
      "epoch 1726: train loss: 0.2016629937887256, test loss: 0.2865509271835694\n",
      "epoch 1727: train loss: 0.20162135973085857, test loss: 0.2865287226047839\n",
      "epoch 1728: train loss: 0.20157976371862132, test loss: 0.2865065443831659\n",
      "epoch 1729: train loss: 0.20153820569382155, test loss: 0.2864843924768834\n",
      "epoch 1730: train loss: 0.20149668559838788, test loss: 0.2864622668441911\n",
      "epoch 1731: train loss: 0.2014552033743694, test loss: 0.2864401674434312\n",
      "epoch 1732: train loss: 0.20141375896393562, test loss: 0.28641809423303227\n",
      "epoch 1733: train loss: 0.20137235230937608, test loss: 0.28639604717150924\n",
      "epoch 1734: train loss: 0.20133098335309987, test loss: 0.28637402621746355\n",
      "epoch 1735: train loss: 0.2012896520376355, test loss: 0.2863520313295824\n",
      "epoch 1736: train loss: 0.2012483583056305, test loss: 0.286330062466639\n",
      "epoch 1737: train loss: 0.20120710209985135, test loss: 0.28630811958749186\n",
      "epoch 1738: train loss: 0.2011658833631828, test loss: 0.286286202651085\n",
      "epoch 1739: train loss: 0.20112470203862792, test loss: 0.28626431161644755\n",
      "epoch 1740: train loss: 0.2010835580693077, test loss: 0.28624244644269337\n",
      "epoch 1741: train loss: 0.2010424513984605, test loss: 0.28622060708902114\n",
      "epoch 1742: train loss: 0.20100138196944223, test loss: 0.2861987935147139\n",
      "epoch 1743: train loss: 0.2009603497257256, test loss: 0.28617700567913895\n",
      "epoch 1744: train loss: 0.20091935461090016, test loss: 0.2861552435417476\n",
      "epoch 1745: train loss: 0.20087839656867174, test loss: 0.2861335070620749\n",
      "epoch 1746: train loss: 0.20083747554286246, test loss: 0.2861117961997395\n",
      "epoch 1747: train loss: 0.20079659147741008, test loss: 0.2860901109144433\n",
      "epoch 1748: train loss: 0.20075574431636803, test loss: 0.2860684511659715\n",
      "epoch 1749: train loss: 0.20071493400390494, test loss: 0.28604681691419204\n",
      "epoch 1750: train loss: 0.2006741604843043, test loss: 0.28602520811905563\n",
      "epoch 1751: train loss: 0.2006334237019645, test loss: 0.2860036247405955\n",
      "epoch 1752: train loss: 0.20059272360139815, test loss: 0.2859820667389271\n",
      "epoch 1753: train loss: 0.2005520601272319, test loss: 0.28596053407424793\n",
      "epoch 1754: train loss: 0.20051143322420636, test loss: 0.28593902670683724\n",
      "epoch 1755: train loss: 0.2004708428371756, test loss: 0.2859175445970561\n",
      "epoch 1756: train loss: 0.20043028891110695, test loss: 0.2858960877053468\n",
      "epoch 1757: train loss: 0.20038977139108063, test loss: 0.28587465599223294\n",
      "epoch 1758: train loss: 0.2003492902222896, test loss: 0.2858532494183192\n",
      "epoch 1759: train loss: 0.20030884535003923, test loss: 0.2858318679442908\n",
      "epoch 1760: train loss: 0.20026843671974695, test loss: 0.28581051153091364\n",
      "epoch 1761: train loss: 0.20022806427694215, test loss: 0.28578918013903415\n",
      "epoch 1762: train loss: 0.20018772796726544, test loss: 0.2857678737295786\n",
      "epoch 1763: train loss: 0.20014742773646924, test loss: 0.2857465922635536\n",
      "epoch 1764: train loss: 0.20010716353041644, test loss: 0.285725335702045\n",
      "epoch 1765: train loss: 0.200066935295081, test loss: 0.28570410400621865\n",
      "epoch 1766: train loss: 0.20002674297654724, test loss: 0.28568289713731954\n",
      "epoch 1767: train loss: 0.1999865865210095, test loss: 0.28566171505667165\n",
      "epoch 1768: train loss: 0.19994646587477227, test loss: 0.2856405577256782\n",
      "epoch 1769: train loss: 0.1999063809842495, test loss: 0.2856194251058207\n",
      "epoch 1770: train loss: 0.19986633179596452, test loss: 0.2855983171586597\n",
      "epoch 1771: train loss: 0.1998263182565498, test loss: 0.28557723384583356\n",
      "epoch 1772: train loss: 0.19978634031274653, test loss: 0.28555617512905906\n",
      "epoch 1773: train loss: 0.1997463979114044, test loss: 0.2855351409701308\n",
      "epoch 1774: train loss: 0.19970649099948157, test loss: 0.28551413133092096\n",
      "epoch 1775: train loss: 0.19966661952404402, test loss: 0.28549314617337945\n",
      "epoch 1776: train loss: 0.1996267834322655, test loss: 0.28547218545953346\n",
      "epoch 1777: train loss: 0.1995869826714273, test loss: 0.2854512491514871\n",
      "epoch 1778: train loss: 0.1995472171889178, test loss: 0.2854303372114216\n",
      "epoch 1779: train loss: 0.1995074869322324, test loss: 0.2854094496015947\n",
      "epoch 1780: train loss: 0.199467791848973, test loss: 0.28538858628434094\n",
      "epoch 1781: train loss: 0.19942813188684824, test loss: 0.28536774722207087\n",
      "epoch 1782: train loss: 0.1993885069936725, test loss: 0.2853469323772713\n",
      "epoch 1783: train loss: 0.19934891711736633, test loss: 0.28532614171250514\n",
      "epoch 1784: train loss: 0.19930936220595574, test loss: 0.28530537519041077\n",
      "epoch 1785: train loss: 0.19926984220757213, test loss: 0.2852846327737022\n",
      "epoch 1786: train loss: 0.199230357070452, test loss: 0.2852639144251689\n",
      "epoch 1787: train loss: 0.19919090674293677, test loss: 0.2852432201076754\n",
      "epoch 1788: train loss: 0.19915149117347228, test loss: 0.28522254978416106\n",
      "epoch 1789: train loss: 0.1991121103106087, test loss: 0.28520190341764035\n",
      "epoch 1790: train loss: 0.19907276410300034, test loss: 0.28518128097120204\n",
      "epoch 1791: train loss: 0.1990334524994053, test loss: 0.28516068240800935\n",
      "epoch 1792: train loss: 0.19899417544868522, test loss: 0.2851401076912998\n",
      "epoch 1793: train loss: 0.19895493289980493, test loss: 0.2851195567843849\n",
      "epoch 1794: train loss: 0.19891572480183242, test loss: 0.2850990296506499\n",
      "epoch 1795: train loss: 0.19887655110393843, test loss: 0.2850785262535537\n",
      "epoch 1796: train loss: 0.19883741175539618, test loss: 0.2850580465566287\n",
      "epoch 1797: train loss: 0.19879830670558113, test loss: 0.2850375905234806\n",
      "epoch 1798: train loss: 0.19875923590397096, test loss: 0.2850171581177881\n",
      "epoch 1799: train loss: 0.19872019930014484, test loss: 0.28499674930330277\n",
      "epoch 1800: train loss: 0.19868119684378371, test loss: 0.28497636404384896\n",
      "epoch 1801: train loss: 0.1986422284846696, test loss: 0.2849560023033236\n",
      "epoch 1802: train loss: 0.1986032941726856, test loss: 0.28493566404569576\n",
      "epoch 1803: train loss: 0.1985643938578157, test loss: 0.2849153492350068\n",
      "epoch 1804: train loss: 0.19852552749014424, test loss: 0.28489505783537\n",
      "epoch 1805: train loss: 0.19848669501985597, test loss: 0.28487478981097064\n",
      "epoch 1806: train loss: 0.19844789639723562, test loss: 0.2848545451260652\n",
      "epoch 1807: train loss: 0.19840913157266762, test loss: 0.28483432374498185\n",
      "epoch 1808: train loss: 0.1983704004966361, test loss: 0.28481412563212005\n",
      "epoch 1809: train loss: 0.19833170311972442, test loss: 0.28479395075195024\n",
      "epoch 1810: train loss: 0.19829303939261492, test loss: 0.2847737990690137\n",
      "epoch 1811: train loss: 0.19825440926608887, test loss: 0.2847536705479225\n",
      "epoch 1812: train loss: 0.19821581269102606, test loss: 0.2847335651533592\n",
      "epoch 1813: train loss: 0.1981772496184046, test loss: 0.28471348285007664\n",
      "epoch 1814: train loss: 0.19813871999930066, test loss: 0.284693423602898\n",
      "epoch 1815: train loss: 0.19810022378488823, test loss: 0.2846733873767164\n",
      "epoch 1816: train loss: 0.1980617609264391, test loss: 0.28465337413649455\n",
      "epoch 1817: train loss: 0.19802333137532216, test loss: 0.2846333838472653\n",
      "epoch 1818: train loss: 0.1979849350830036, test loss: 0.2846134164741304\n",
      "epoch 1819: train loss: 0.1979465720010465, test loss: 0.28459347198226126\n",
      "epoch 1820: train loss: 0.19790824208111055, test loss: 0.28457355033689835\n",
      "epoch 1821: train loss: 0.19786994527495183, test loss: 0.284553651503351\n",
      "epoch 1822: train loss: 0.19783168153442263, test loss: 0.2845337754469973\n",
      "epoch 1823: train loss: 0.1977934508114712, test loss: 0.2845139221332842\n",
      "epoch 1824: train loss: 0.19775525305814157, test loss: 0.28449409152772676\n",
      "epoch 1825: train loss: 0.1977170882265731, test loss: 0.2844742835959084\n",
      "epoch 1826: train loss: 0.19767895626900053, test loss: 0.28445449830348063\n",
      "epoch 1827: train loss: 0.19764085713775356, test loss: 0.28443473561616295\n",
      "epoch 1828: train loss: 0.1976027907852567, test loss: 0.2844149954997425\n",
      "epoch 1829: train loss: 0.19756475716402902, test loss: 0.28439527792007413\n",
      "epoch 1830: train loss: 0.19752675622668384, test loss: 0.2843755828430799\n",
      "epoch 1831: train loss: 0.1974887879259286, test loss: 0.28435591023474927\n",
      "epoch 1832: train loss: 0.19745085221456476, test loss: 0.2843362600611387\n",
      "epoch 1833: train loss: 0.1974129490454872, test loss: 0.2843166322883716\n",
      "epoch 1834: train loss: 0.1973750783716843, test loss: 0.2842970268826381\n",
      "epoch 1835: train loss: 0.19733724014623763, test loss: 0.2842774438101948\n",
      "epoch 1836: train loss: 0.1972994343223218, test loss: 0.284257883037365\n",
      "epoch 1837: train loss: 0.19726166085320393, test loss: 0.2842383445305377\n",
      "epoch 1838: train loss: 0.19722391969224387, test loss: 0.28421882825616857\n",
      "epoch 1839: train loss: 0.19718621079289364, test loss: 0.28419933418077886\n",
      "epoch 1840: train loss: 0.19714853410869723, test loss: 0.2841798622709556\n",
      "epoch 1841: train loss: 0.1971108895932907, test loss: 0.2841604124933514\n",
      "epoch 1842: train loss: 0.19707327720040155, test loss: 0.2841409848146844\n",
      "epoch 1843: train loss: 0.19703569688384867, test loss: 0.2841215792017378\n",
      "epoch 1844: train loss: 0.1969981485975422, test loss: 0.2841021956213601\n",
      "epoch 1845: train loss: 0.1969606322954831, test loss: 0.2840828340404646\n",
      "epoch 1846: train loss: 0.19692314793176327, test loss: 0.2840634944260294\n",
      "epoch 1847: train loss: 0.1968856954605649, test loss: 0.28404417674509724\n",
      "epoch 1848: train loss: 0.19684827483616055, test loss: 0.2840248809647752\n",
      "epoch 1849: train loss: 0.19681088601291286, test loss: 0.2840056070522348\n",
      "epoch 1850: train loss: 0.19677352894527433, test loss: 0.28398635497471175\n",
      "epoch 1851: train loss: 0.1967362035877871, test loss: 0.28396712469950547\n",
      "epoch 1852: train loss: 0.1966989098950826, test loss: 0.2839479161939793\n",
      "epoch 1853: train loss: 0.19666164782188175, test loss: 0.28392872942556047\n",
      "epoch 1854: train loss: 0.19662441732299413, test loss: 0.2839095643617393\n",
      "epoch 1855: train loss: 0.19658721835331827, test loss: 0.28389042097006983\n",
      "epoch 1856: train loss: 0.1965500508678413, test loss: 0.283871299218169\n",
      "epoch 1857: train loss: 0.19651291482163852, test loss: 0.283852199073717\n",
      "epoch 1858: train loss: 0.19647581016987353, test loss: 0.28383312050445675\n",
      "epoch 1859: train loss: 0.19643873686779773, test loss: 0.28381406347819393\n",
      "epoch 1860: train loss: 0.19640169487075038, test loss: 0.28379502796279676\n",
      "epoch 1861: train loss: 0.19636468413415803, test loss: 0.28377601392619606\n",
      "epoch 1862: train loss: 0.19632770461353463, test loss: 0.28375702133638464\n",
      "epoch 1863: train loss: 0.19629075626448128, test loss: 0.28373805016141745\n",
      "epoch 1864: train loss: 0.1962538390426858, test loss: 0.28371910036941167\n",
      "epoch 1865: train loss: 0.19621695290392274, test loss: 0.28370017192854596\n",
      "epoch 1866: train loss: 0.19618009780405313, test loss: 0.28368126480706074\n",
      "epoch 1867: train loss: 0.19614327369902412, test loss: 0.28366237897325797\n",
      "epoch 1868: train loss: 0.19610648054486904, test loss: 0.283643514395501\n",
      "epoch 1869: train loss: 0.196069718297707, test loss: 0.28362467104221417\n",
      "epoch 1870: train loss: 0.19603298691374263, test loss: 0.28360584888188306\n",
      "epoch 1871: train loss: 0.19599628634926616, test loss: 0.2835870478830541\n",
      "epoch 1872: train loss: 0.19595961656065292, test loss: 0.28356826801433443\n",
      "epoch 1873: train loss: 0.19592297750436316, test loss: 0.2835495092443918\n",
      "epoch 1874: train loss: 0.19588636913694216, test loss: 0.2835307715419544\n",
      "epoch 1875: train loss: 0.1958497914150196, test loss: 0.28351205487581077\n",
      "epoch 1876: train loss: 0.19581324429530966, test loss: 0.28349335921480967\n",
      "epoch 1877: train loss: 0.19577672773461063, test loss: 0.2834746845278596\n",
      "epoch 1878: train loss: 0.1957402416898049, test loss: 0.2834560307839293\n",
      "epoch 1879: train loss: 0.19570378611785863, test loss: 0.28343739795204675\n",
      "epoch 1880: train loss: 0.19566736097582144, test loss: 0.2834187860013\n",
      "epoch 1881: train loss: 0.19563096622082657, test loss: 0.28340019490083634\n",
      "epoch 1882: train loss: 0.1955946018100903, test loss: 0.2833816246198621\n",
      "epoch 1883: train loss: 0.19555826770091186, test loss: 0.2833630751276431\n",
      "epoch 1884: train loss: 0.19552196385067344, test loss: 0.2833445463935039\n",
      "epoch 1885: train loss: 0.19548569021683973, test loss: 0.28332603838682807\n",
      "epoch 1886: train loss: 0.19544944675695777, test loss: 0.2833075510770579\n",
      "epoch 1887: train loss: 0.19541323342865688, test loss: 0.28328908443369394\n",
      "epoch 1888: train loss: 0.19537705018964846, test loss: 0.28327063842629563\n",
      "epoch 1889: train loss: 0.19534089699772558, test loss: 0.28325221302448034\n",
      "epoch 1890: train loss: 0.19530477381076294, test loss: 0.2832338081979237\n",
      "epoch 1891: train loss: 0.19526868058671681, test loss: 0.2832154239163593\n",
      "epoch 1892: train loss: 0.19523261728362457, test loss: 0.2831970601495787\n",
      "epoch 1893: train loss: 0.19519658385960462, test loss: 0.2831787168674309\n",
      "epoch 1894: train loss: 0.19516058027285627, test loss: 0.2831603940398228\n",
      "epoch 1895: train loss: 0.1951246064816594, test loss: 0.28314209163671866\n",
      "epoch 1896: train loss: 0.1950886624443745, test loss: 0.2831238096281398\n",
      "epoch 1897: train loss: 0.19505274811944215, test loss: 0.2831055479841651\n",
      "epoch 1898: train loss: 0.19501686346538313, test loss: 0.2830873066749301\n",
      "epoch 1899: train loss: 0.1949810084407981, test loss: 0.2830690856706273\n",
      "epoch 1900: train loss: 0.1949451830043673, test loss: 0.28305088494150615\n",
      "epoch 1901: train loss: 0.19490938711485073, test loss: 0.2830327044578725\n",
      "epoch 1902: train loss: 0.1948736207310873, test loss: 0.28301454419008876\n",
      "epoch 1903: train loss: 0.1948378838119955, test loss: 0.28299640410857363\n",
      "epoch 1904: train loss: 0.19480217631657248, test loss: 0.2829782841838021\n",
      "epoch 1905: train loss: 0.1947664982038942, test loss: 0.2829601843863052\n",
      "epoch 1906: train loss: 0.19473084943311533, test loss: 0.28294210468666975\n",
      "epoch 1907: train loss: 0.19469522996346866, test loss: 0.2829240450555386\n",
      "epoch 1908: train loss: 0.19465963975426553, test loss: 0.28290600546361\n",
      "epoch 1909: train loss: 0.19462407876489493, test loss: 0.282887985881638\n",
      "epoch 1910: train loss: 0.19458854695482392, test loss: 0.28286998628043186\n",
      "epoch 1911: train loss: 0.1945530442835971, test loss: 0.28285200663085613\n",
      "epoch 1912: train loss: 0.19451757071083658, test loss: 0.28283404690383057\n",
      "epoch 1913: train loss: 0.19448212619624172, test loss: 0.2828161070703299\n",
      "epoch 1914: train loss: 0.194446710699589, test loss: 0.28279818710138366\n",
      "epoch 1915: train loss: 0.19441132418073181, test loss: 0.2827802869680761\n",
      "epoch 1916: train loss: 0.19437596659960021, test loss: 0.2827624066415463\n",
      "epoch 1917: train loss: 0.194340637916201, test loss: 0.2827445460929874\n",
      "epoch 1918: train loss: 0.194305338090617, test loss: 0.2827267052936474\n",
      "epoch 1919: train loss: 0.19427006708300765, test loss: 0.28270888421482804\n",
      "epoch 1920: train loss: 0.19423482485360805, test loss: 0.28269108282788546\n",
      "epoch 1921: train loss: 0.19419961136272929, test loss: 0.28267330110422956\n",
      "epoch 1922: train loss: 0.19416442657075814, test loss: 0.2826555390153242\n",
      "epoch 1923: train loss: 0.1941292704381567, test loss: 0.28263779653268684\n",
      "epoch 1924: train loss: 0.1940941429254626, test loss: 0.28262007362788855\n",
      "epoch 1925: train loss: 0.19405904399328835, test loss: 0.2826023702725539\n",
      "epoch 1926: train loss: 0.19402397360232157, test loss: 0.28258468643836077\n",
      "epoch 1927: train loss: 0.1939889317133245, test loss: 0.28256702209703993\n",
      "epoch 1928: train loss: 0.1939539182871341, test loss: 0.2825493772203757\n",
      "epoch 1929: train loss: 0.19391893328466167, test loss: 0.28253175178020506\n",
      "epoch 1930: train loss: 0.19388397666689286, test loss: 0.28251414574841777\n",
      "epoch 1931: train loss: 0.19384904839488726, test loss: 0.2824965590969562\n",
      "epoch 1932: train loss: 0.1938141484297784, test loss: 0.2824789917978157\n",
      "epoch 1933: train loss: 0.19377927673277348, test loss: 0.2824614438230435\n",
      "epoch 1934: train loss: 0.19374443326515334, test loss: 0.2824439151447396\n",
      "epoch 1935: train loss: 0.19370961798827216, test loss: 0.28242640573505584\n",
      "epoch 1936: train loss: 0.19367483086355733, test loss: 0.28240891556619635\n",
      "epoch 1937: train loss: 0.19364007185250923, test loss: 0.2823914446104172\n",
      "epoch 1938: train loss: 0.19360534091670112, test loss: 0.282373992840026\n",
      "epoch 1939: train loss: 0.19357063801777888, test loss: 0.2823565602273823\n",
      "epoch 1940: train loss: 0.19353596311746113, test loss: 0.2823391467448973\n",
      "epoch 1941: train loss: 0.19350131617753857, test loss: 0.2823217523650333\n",
      "epoch 1942: train loss: 0.19346669715987425, test loss: 0.28230437706030437\n",
      "epoch 1943: train loss: 0.19343210602640315, test loss: 0.2822870208032754\n",
      "epoch 1944: train loss: 0.19339754273913212, test loss: 0.28226968356656257\n",
      "epoch 1945: train loss: 0.19336300726013975, test loss: 0.282252365322833\n",
      "epoch 1946: train loss: 0.193328499551576, test loss: 0.28223506604480453\n",
      "epoch 1947: train loss: 0.1932940195756623, test loss: 0.28221778570524597\n",
      "epoch 1948: train loss: 0.19325956729469115, test loss: 0.28220052427697645\n",
      "epoch 1949: train loss: 0.19322514267102617, test loss: 0.2821832817328659\n",
      "epoch 1950: train loss: 0.1931907456671017, test loss: 0.2821660580458342\n",
      "epoch 1951: train loss: 0.19315637624542284, test loss: 0.28214885318885186\n",
      "epoch 1952: train loss: 0.19312203436856515, test loss: 0.2821316671349394\n",
      "epoch 1953: train loss: 0.19308771999917465, test loss: 0.28211449985716736\n",
      "epoch 1954: train loss: 0.19305343309996736, test loss: 0.28209735132865604\n",
      "epoch 1955: train loss: 0.1930191736337295, test loss: 0.2820802215225756\n",
      "epoch 1956: train loss: 0.19298494156331705, test loss: 0.28206311041214605\n",
      "epoch 1957: train loss: 0.19295073685165565, test loss: 0.2820460179706367\n",
      "epoch 1958: train loss: 0.1929165594617406, test loss: 0.2820289441713663\n",
      "epoch 1959: train loss: 0.1928824093566365, test loss: 0.2820118889877031\n",
      "epoch 1960: train loss: 0.19284828649947713, test loss: 0.2819948523930643\n",
      "epoch 1961: train loss: 0.19281419085346535, test loss: 0.2819778343609165\n",
      "epoch 1962: train loss: 0.19278012238187292, test loss: 0.28196083486477486\n",
      "epoch 1963: train loss: 0.1927460810480403, test loss: 0.28194385387820375\n",
      "epoch 1964: train loss: 0.1927120668153766, test loss: 0.2819268913748161\n",
      "epoch 1965: train loss: 0.19267807964735922, test loss: 0.2819099473282735\n",
      "epoch 1966: train loss: 0.19264411950753388, test loss: 0.28189302171228614\n",
      "epoch 1967: train loss: 0.19261018635951443, test loss: 0.2818761145006124\n",
      "epoch 1968: train loss: 0.19257628016698258, test loss: 0.2818592256670592\n",
      "epoch 1969: train loss: 0.19254240089368785, test loss: 0.28184235518548134\n",
      "epoch 1970: train loss: 0.19250854850344745, test loss: 0.28182550302978204\n",
      "epoch 1971: train loss: 0.1924747229601458, test loss: 0.2818086691739121\n",
      "epoch 1972: train loss: 0.19244092422773507, test loss: 0.2817918535918704\n",
      "epoch 1973: train loss: 0.1924071522702342, test loss: 0.2817750562577036\n",
      "epoch 1974: train loss: 0.19237340705172923, test loss: 0.2817582771455057\n",
      "epoch 1975: train loss: 0.19233968853637315, test loss: 0.28174151622941834\n",
      "epoch 1976: train loss: 0.1923059966883855, test loss: 0.2817247734836307\n",
      "epoch 1977: train loss: 0.19227233147205247, test loss: 0.28170804888237905\n",
      "epoch 1978: train loss: 0.19223869285172654, test loss: 0.28169134239994686\n",
      "epoch 1979: train loss: 0.19220508079182655, test loss: 0.28167465401066477\n",
      "epoch 1980: train loss: 0.1921714952568372, test loss: 0.28165798368891026\n",
      "epoch 1981: train loss: 0.19213793621130926, test loss: 0.2816413314091077\n",
      "epoch 1982: train loss: 0.1921044036198593, test loss: 0.2816246971457283\n",
      "epoch 1983: train loss: 0.19207089744716946, test loss: 0.2816080808732897\n",
      "epoch 1984: train loss: 0.19203741765798732, test loss: 0.2815914825663562\n",
      "epoch 1985: train loss: 0.19200396421712576, test loss: 0.28157490219953857\n",
      "epoch 1986: train loss: 0.19197053708946293, test loss: 0.2815583397474937\n",
      "epoch 1987: train loss: 0.1919371362399419, test loss: 0.28154179518492484\n",
      "epoch 1988: train loss: 0.19190376163357073, test loss: 0.2815252684865811\n",
      "epoch 1989: train loss: 0.19187041323542198, test loss: 0.28150875962725813\n",
      "epoch 1990: train loss: 0.191837091010633, test loss: 0.2814922685817967\n",
      "epoch 1991: train loss: 0.1918037949244054, test loss: 0.28147579532508404\n",
      "epoch 1992: train loss: 0.19177052494200525, test loss: 0.2814593398320525\n",
      "epoch 1993: train loss: 0.1917372810287625, test loss: 0.2814429020776805\n",
      "epoch 1994: train loss: 0.19170406315007135, test loss: 0.28142648203699155\n",
      "epoch 1995: train loss: 0.19167087127138957, test loss: 0.2814100796850545\n",
      "epoch 1996: train loss: 0.1916377053582388, test loss: 0.28139369499698375\n",
      "epoch 1997: train loss: 0.19160456537620413, test loss: 0.28137732794793874\n",
      "epoch 1998: train loss: 0.1915714512909341, test loss: 0.28136097851312375\n",
      "epoch 1999: train loss: 0.19153836306814045, test loss: 0.28134464666778825\n",
      "epoch 2000: train loss: 0.19150530067359806, test loss: 0.2813283323872263\n",
      "epoch 2001: train loss: 0.1914722640731447, test loss: 0.28131203564677704\n",
      "epoch 2002: train loss: 0.19143925323268107, test loss: 0.281295756421824\n",
      "epoch 2003: train loss: 0.19140626811817044, test loss: 0.2812794946877952\n",
      "epoch 2004: train loss: 0.19137330869563862, test loss: 0.28126325042016315\n",
      "epoch 2005: train loss: 0.19134037493117387, test loss: 0.28124702359444487\n",
      "epoch 2006: train loss: 0.19130746679092664, test loss: 0.2812308141862012\n",
      "epoch 2007: train loss: 0.1912745842411095, test loss: 0.2812146221710376\n",
      "epoch 2008: train loss: 0.19124172724799693, test loss: 0.2811984475246032\n",
      "epoch 2009: train loss: 0.19120889577792533, test loss: 0.2811822902225911\n",
      "epoch 2010: train loss: 0.19117608979729261, test loss: 0.2811661502407383\n",
      "epoch 2011: train loss: 0.19114330927255835, test loss: 0.2811500275548256\n",
      "epoch 2012: train loss: 0.1911105541702436, test loss: 0.2811339221406772\n",
      "epoch 2013: train loss: 0.19107782445693047, test loss: 0.28111783397416085\n",
      "epoch 2014: train loss: 0.19104512009926225, test loss: 0.281101763031188\n",
      "epoch 2015: train loss: 0.19101244106394322, test loss: 0.2810857092877132\n",
      "epoch 2016: train loss: 0.19097978731773857, test loss: 0.2810696727197342\n",
      "epoch 2017: train loss: 0.190947158827474, test loss: 0.281053653303292\n",
      "epoch 2018: train loss: 0.19091455556003606, test loss: 0.28103765101447054\n",
      "epoch 2019: train loss: 0.1908819774823714, test loss: 0.2810216658293967\n",
      "epoch 2020: train loss: 0.1908494245614872, test loss: 0.2810056977242403\n",
      "epoch 2021: train loss: 0.19081689676445066, test loss: 0.2809897466752137\n",
      "epoch 2022: train loss: 0.19078439405838904, test loss: 0.28097381265857213\n",
      "epoch 2023: train loss: 0.19075191641048947, test loss: 0.28095789565061324\n",
      "epoch 2024: train loss: 0.19071946378799876, test loss: 0.28094199562767697\n",
      "epoch 2025: train loss: 0.19068703615822344, test loss: 0.2809261125661461\n",
      "epoch 2026: train loss: 0.1906546334885294, test loss: 0.280910246442445\n",
      "epoch 2027: train loss: 0.19062225574634195, test loss: 0.28089439723304066\n",
      "epoch 2028: train loss: 0.19058990289914546, test loss: 0.2808785649144421\n",
      "epoch 2029: train loss: 0.1905575749144836, test loss: 0.2808627494632001\n",
      "epoch 2030: train loss: 0.19052527175995876, test loss: 0.28084695085590755\n",
      "epoch 2031: train loss: 0.19049299340323217, test loss: 0.28083116906919897\n",
      "epoch 2032: train loss: 0.19046073981202377, test loss: 0.28081540407975053\n",
      "epoch 2033: train loss: 0.19042851095411206, test loss: 0.28079965586428013\n",
      "epoch 2034: train loss: 0.19039630679733385, test loss: 0.2807839243995471\n",
      "epoch 2035: train loss: 0.19036412730958424, test loss: 0.2807682096623521\n",
      "epoch 2036: train loss: 0.19033197245881653, test loss: 0.2807525116295372\n",
      "epoch 2037: train loss: 0.1902998422130419, test loss: 0.28073683027798574\n",
      "epoch 2038: train loss: 0.19026773654032958, test loss: 0.28072116558462207\n",
      "epoch 2039: train loss: 0.19023565540880646, test loss: 0.2807055175264115\n",
      "epoch 2040: train loss: 0.19020359878665694, test loss: 0.2806898860803605\n",
      "epoch 2041: train loss: 0.190171566642123, test loss: 0.28067427122351624\n",
      "epoch 2042: train loss: 0.19013955894350396, test loss: 0.28065867293296654\n",
      "epoch 2043: train loss: 0.1901075756591564, test loss: 0.28064309118584024\n",
      "epoch 2044: train loss: 0.19007561675749393, test loss: 0.2806275259593064\n",
      "epoch 2045: train loss: 0.19004368220698717, test loss: 0.28061197723057474\n",
      "epoch 2046: train loss: 0.19001177197616356, test loss: 0.28059644497689534\n",
      "epoch 2047: train loss: 0.18997988603360721, test loss: 0.2805809291755584\n",
      "epoch 2048: train loss: 0.18994802434795888, test loss: 0.2805654298038947\n",
      "epoch 2049: train loss: 0.18991618688791567, test loss: 0.2805499468392749\n",
      "epoch 2050: train loss: 0.18988437362223123, test loss: 0.28053448025910965\n",
      "epoch 2051: train loss: 0.18985258451971515, test loss: 0.28051903004084966\n",
      "epoch 2052: train loss: 0.1898208195492333, test loss: 0.2805035961619854\n",
      "epoch 2053: train loss: 0.18978907867970732, test loss: 0.28048817860004716\n",
      "epoch 2054: train loss: 0.18975736188011483, test loss: 0.28047277733260495\n",
      "epoch 2055: train loss: 0.18972566911948902, test loss: 0.2804573923372683\n",
      "epoch 2056: train loss: 0.1896940003669188, test loss: 0.28044202359168613\n",
      "epoch 2057: train loss: 0.18966235559154843, test loss: 0.280426671073547\n",
      "epoch 2058: train loss: 0.18963073476257736, test loss: 0.28041133476057867\n",
      "epoch 2059: train loss: 0.18959913784926052, test loss: 0.28039601463054803\n",
      "epoch 2060: train loss: 0.18956756482090775, test loss: 0.2803807106612613\n",
      "epoch 2061: train loss: 0.1895360156468838, test loss: 0.2803654228305637\n",
      "epoch 2062: train loss: 0.18950449029660837, test loss: 0.2803501511163393\n",
      "epoch 2063: train loss: 0.18947298873955581, test loss: 0.28033489549651125\n",
      "epoch 2064: train loss: 0.18944151094525508, test loss: 0.28031965594904146\n",
      "epoch 2065: train loss: 0.18941005688328957, test loss: 0.2803044324519303\n",
      "epoch 2066: train loss: 0.18937862652329693, test loss: 0.28028922498321707\n",
      "epoch 2067: train loss: 0.18934721983496922, test loss: 0.28027403352097957\n",
      "epoch 2068: train loss: 0.1893158367880524, test loss: 0.28025885804333384\n",
      "epoch 2069: train loss: 0.18928447735234655, test loss: 0.2802436985284345\n",
      "epoch 2070: train loss: 0.18925314149770556, test loss: 0.28022855495447446\n",
      "epoch 2071: train loss: 0.18922182919403702, test loss: 0.2802134272996847\n",
      "epoch 2072: train loss: 0.18919054041130223, test loss: 0.2801983155423343\n",
      "epoch 2073: train loss: 0.18915927511951586, test loss: 0.2801832196607306\n",
      "epoch 2074: train loss: 0.18912803328874603, test loss: 0.2801681396332187\n",
      "epoch 2075: train loss: 0.18909681488911403, test loss: 0.28015307543818146\n",
      "epoch 2076: train loss: 0.18906561989079448, test loss: 0.28013802705403973\n",
      "epoch 2077: train loss: 0.1890344482640149, test loss: 0.2801229944592521\n",
      "epoch 2078: train loss: 0.18900329997905563, test loss: 0.28010797763231454\n",
      "epoch 2079: train loss: 0.1889721750062499, test loss: 0.28009297655176063\n",
      "epoch 2080: train loss: 0.18894107331598367, test loss: 0.2800779911961615\n",
      "epoch 2081: train loss: 0.18890999487869528, test loss: 0.2800630215441256\n",
      "epoch 2082: train loss: 0.18887893966487565, test loss: 0.28004806757429856\n",
      "epoch 2083: train loss: 0.1888479076450679, test loss: 0.28003312926536333\n",
      "epoch 2084: train loss: 0.1888168987898675, test loss: 0.2800182065960399\n",
      "epoch 2085: train loss: 0.18878591306992182, test loss: 0.28000329954508535\n",
      "epoch 2086: train loss: 0.18875495045593035, test loss: 0.2799884080912938\n",
      "epoch 2087: train loss: 0.1887240109186444, test loss: 0.27997353221349597\n",
      "epoch 2088: train loss: 0.18869309442886692, test loss: 0.27995867189055956\n",
      "epoch 2089: train loss: 0.18866220095745262, test loss: 0.2799438271013891\n",
      "epoch 2090: train loss: 0.18863133047530767, test loss: 0.27992899782492553\n",
      "epoch 2091: train loss: 0.18860048295338963, test loss: 0.2799141840401464\n",
      "epoch 2092: train loss: 0.18856965836270725, test loss: 0.2798993857260658\n",
      "epoch 2093: train loss: 0.18853885667432066, test loss: 0.2798846028617342\n",
      "epoch 2094: train loss: 0.18850807785934087, test loss: 0.2798698354262383\n",
      "epoch 2095: train loss: 0.18847732188892988, test loss: 0.279855083398701\n",
      "epoch 2096: train loss: 0.18844658873430048, test loss: 0.2798403467582816\n",
      "epoch 2097: train loss: 0.18841587836671628, test loss: 0.2798256254841752\n",
      "epoch 2098: train loss: 0.18838519075749147, test loss: 0.2798109195556131\n",
      "epoch 2099: train loss: 0.18835452587799068, test loss: 0.2797962289518623\n",
      "epoch 2100: train loss: 0.18832388369962885, test loss: 0.27978155365222585\n",
      "epoch 2101: train loss: 0.18829326419387138, test loss: 0.27976689363604235\n",
      "epoch 2102: train loss: 0.1882626673322337, test loss: 0.2797522488826863\n",
      "epoch 2103: train loss: 0.18823209308628125, test loss: 0.27973761937156766\n",
      "epoch 2104: train loss: 0.1882015414276296, test loss: 0.27972300508213194\n",
      "epoch 2105: train loss: 0.18817101232794398, test loss: 0.2797084059938602\n",
      "epoch 2106: train loss: 0.18814050575893937, test loss: 0.2796938220862687\n",
      "epoch 2107: train loss: 0.18811002169238036, test loss: 0.27967925333890903\n",
      "epoch 2108: train loss: 0.1880795601000811, test loss: 0.27966469973136815\n",
      "epoch 2109: train loss: 0.18804912095390502, test loss: 0.279650161243268\n",
      "epoch 2110: train loss: 0.188018704225765, test loss: 0.2796356378542657\n",
      "epoch 2111: train loss: 0.18798830988762283, test loss: 0.2796211295440532\n",
      "epoch 2112: train loss: 0.18795793791148974, test loss: 0.2796066362923576\n",
      "epoch 2113: train loss: 0.18792758826942546, test loss: 0.27959215807894056\n",
      "epoch 2114: train loss: 0.18789726093353906, test loss: 0.27957769488359874\n",
      "epoch 2115: train loss: 0.18786695587598792, test loss: 0.2795632466861633\n",
      "epoch 2116: train loss: 0.18783667306897833, test loss: 0.2795488134665001\n",
      "epoch 2117: train loss: 0.18780641248476498, test loss: 0.2795343952045096\n",
      "epoch 2118: train loss: 0.18777617409565106, test loss: 0.2795199918801267\n",
      "epoch 2119: train loss: 0.1877459578739881, test loss: 0.27950560347332054\n",
      "epoch 2120: train loss: 0.18771576379217564, test loss: 0.27949122996409465\n",
      "epoch 2121: train loss: 0.18768559182266156, test loss: 0.27947687133248683\n",
      "epoch 2122: train loss: 0.1876554419379416, test loss: 0.2794625275585691\n",
      "epoch 2123: train loss: 0.1876253141105595, test loss: 0.2794481986224475\n",
      "epoch 2124: train loss: 0.1875952083131067, test loss: 0.279433884504262\n",
      "epoch 2125: train loss: 0.18756512451822238, test loss: 0.27941958518418664\n",
      "epoch 2126: train loss: 0.1875350626985932, test loss: 0.27940530064242936\n",
      "epoch 2127: train loss: 0.1875050228269535, test loss: 0.27939103085923167\n",
      "epoch 2128: train loss: 0.18747500487608476, test loss: 0.2793767758148691\n",
      "epoch 2129: train loss: 0.18744500881881584, test loss: 0.2793625354896507\n",
      "epoch 2130: train loss: 0.18741503462802286, test loss: 0.27934830986391895\n",
      "epoch 2131: train loss: 0.18738508227662884, test loss: 0.2793340989180501\n",
      "epoch 2132: train loss: 0.18735515173760375, test loss: 0.27931990263245376\n",
      "epoch 2133: train loss: 0.1873252429839647, test loss: 0.27930572098757267\n",
      "epoch 2134: train loss: 0.18729535598877511, test loss: 0.2792915539638832\n",
      "epoch 2135: train loss: 0.18726549072514548, test loss: 0.27927740154189473\n",
      "epoch 2136: train loss: 0.1872356471662326, test loss: 0.2792632637021499\n",
      "epoch 2137: train loss: 0.18720582528523977, test loss: 0.27924914042522425\n",
      "epoch 2138: train loss: 0.1871760250554167, test loss: 0.27923503169172653\n",
      "epoch 2139: train loss: 0.18714624645005926, test loss: 0.2792209374822983\n",
      "epoch 2140: train loss: 0.1871164894425096, test loss: 0.27920685777761417\n",
      "epoch 2141: train loss: 0.18708675400615576, test loss: 0.27919279255838125\n",
      "epoch 2142: train loss: 0.18705704011443178, test loss: 0.2791787418053396\n",
      "epoch 2143: train loss: 0.1870273477408176, test loss: 0.27916470549926176\n",
      "epoch 2144: train loss: 0.18699767685883892, test loss: 0.279150683620953\n",
      "epoch 2145: train loss: 0.186968027442067, test loss: 0.27913667615125115\n",
      "epoch 2146: train loss: 0.1869383994641187, test loss: 0.27912268307102633\n",
      "epoch 2147: train loss: 0.18690879289865633, test loss: 0.2791087043611812\n",
      "epoch 2148: train loss: 0.18687920771938749, test loss: 0.2790947400026504\n",
      "epoch 2149: train loss: 0.1868496439000652, test loss: 0.27908078997640123\n",
      "epoch 2150: train loss: 0.1868201014144875, test loss: 0.27906685426343286\n",
      "epoch 2151: train loss: 0.18679058023649744, test loss: 0.2790529328447768\n",
      "epoch 2152: train loss: 0.18676108033998318, test loss: 0.27903902570149636\n",
      "epoch 2153: train loss: 0.18673160169887765, test loss: 0.27902513281468694\n",
      "epoch 2154: train loss: 0.18670214428715856, test loss: 0.27901125416547584\n",
      "epoch 2155: train loss: 0.18667270807884825, test loss: 0.27899738973502214\n",
      "epoch 2156: train loss: 0.18664329304801378, test loss: 0.27898353950451665\n",
      "epoch 2157: train loss: 0.1866138991687665, test loss: 0.27896970345518207\n",
      "epoch 2158: train loss: 0.18658452641526221, test loss: 0.2789558815682724\n",
      "epoch 2159: train loss: 0.18655517476170108, test loss: 0.2789420738250735\n",
      "epoch 2160: train loss: 0.18652584418232734, test loss: 0.2789282802069026\n",
      "epoch 2161: train loss: 0.1864965346514294, test loss: 0.27891450069510826\n",
      "epoch 2162: train loss: 0.18646724614333954, test loss: 0.2789007352710706\n",
      "epoch 2163: train loss: 0.18643797863243422, test loss: 0.278886983916201\n",
      "epoch 2164: train loss: 0.18640873209313333, test loss: 0.2788732466119418\n",
      "epoch 2165: train loss: 0.18637950649990082, test loss: 0.27885952333976677\n",
      "epoch 2166: train loss: 0.186350301827244, test loss: 0.2788458140811808\n",
      "epoch 2167: train loss: 0.18632111804971385, test loss: 0.2788321188177196\n",
      "epoch 2168: train loss: 0.18629195514190475, test loss: 0.27881843753095\n",
      "epoch 2169: train loss: 0.18626281307845446, test loss: 0.27880477020246963\n",
      "epoch 2170: train loss: 0.18623369183404384, test loss: 0.27879111681390706\n",
      "epoch 2171: train loss: 0.18620459138339712, test loss: 0.27877747734692154\n",
      "epoch 2172: train loss: 0.1861755117012813, test loss: 0.27876385178320295\n",
      "epoch 2173: train loss: 0.18614645276250674, test loss: 0.27875024010447214\n",
      "epoch 2174: train loss: 0.18611741454192626, test loss: 0.27873664229248013\n",
      "epoch 2175: train loss: 0.18608839701443575, test loss: 0.2787230583290086\n",
      "epoch 2176: train loss: 0.1860594001549737, test loss: 0.2787094881958699\n",
      "epoch 2177: train loss: 0.1860304239385211, test loss: 0.27869593187490627\n",
      "epoch 2178: train loss: 0.18600146834010162, test loss: 0.2786823893479907\n",
      "epoch 2179: train loss: 0.18597253333478125, test loss: 0.2786688605970264\n",
      "epoch 2180: train loss: 0.1859436188976683, test loss: 0.27865534560394634\n",
      "epoch 2181: train loss: 0.18591472500391334, test loss: 0.2786418443507142\n",
      "epoch 2182: train loss: 0.18588585162870913, test loss: 0.27862835681932324\n",
      "epoch 2183: train loss: 0.18585699874729036, test loss: 0.2786148829917972\n",
      "epoch 2184: train loss: 0.1858281663349338, test loss: 0.27860142285018924\n",
      "epoch 2185: train loss: 0.18579935436695802, test loss: 0.2785879763765827\n",
      "epoch 2186: train loss: 0.18577056281872337, test loss: 0.2785745435530907\n",
      "epoch 2187: train loss: 0.18574179166563204, test loss: 0.2785611243618561\n",
      "epoch 2188: train loss: 0.18571304088312768, test loss: 0.27854771878505136\n",
      "epoch 2189: train loss: 0.18568431044669542, test loss: 0.2785343268048786\n",
      "epoch 2190: train loss: 0.18565560033186196, test loss: 0.27852094840356967\n",
      "epoch 2191: train loss: 0.1856269105141952, test loss: 0.2785075835633857\n",
      "epoch 2192: train loss: 0.18559824096930444, test loss: 0.2784942322666173\n",
      "epoch 2193: train loss: 0.18556959167283998, test loss: 0.27848089449558455\n",
      "epoch 2194: train loss: 0.1855409626004934, test loss: 0.2784675702326367\n",
      "epoch 2195: train loss: 0.1855123537279971, test loss: 0.2784542594601525\n",
      "epoch 2196: train loss: 0.18548376503112438, test loss: 0.2784409621605396\n",
      "epoch 2197: train loss: 0.18545519648568937, test loss: 0.27842767831623505\n",
      "epoch 2198: train loss: 0.18542664806754708, test loss: 0.27841440790970473\n",
      "epoch 2199: train loss: 0.18539811975259302, test loss: 0.27840115092344364\n",
      "epoch 2200: train loss: 0.1853696115167632, test loss: 0.2783879073399758\n",
      "epoch 2201: train loss: 0.1853411233360342, test loss: 0.27837467714185415\n",
      "epoch 2202: train loss: 0.185312655186423, test loss: 0.2783614603116602\n",
      "epoch 2203: train loss: 0.18528420704398676, test loss: 0.2783482568320044\n",
      "epoch 2204: train loss: 0.18525577888482295, test loss: 0.2783350666855261\n",
      "epoch 2205: train loss: 0.18522737068506914, test loss: 0.2783218898548929\n",
      "epoch 2206: train loss: 0.18519898242090296, test loss: 0.2783087263228014\n",
      "epoch 2207: train loss: 0.1851706140685419, test loss: 0.27829557607197636\n",
      "epoch 2208: train loss: 0.1851422656042434, test loss: 0.2782824390851713\n",
      "epoch 2209: train loss: 0.18511393700430476, test loss: 0.27826931534516797\n",
      "epoch 2210: train loss: 0.1850856282450627, test loss: 0.27825620483477664\n",
      "epoch 2211: train loss: 0.1850573393028939, test loss: 0.2782431075368357\n",
      "epoch 2212: train loss: 0.18502907015421444, test loss: 0.27823002343421194\n",
      "epoch 2213: train loss: 0.1850008207754797, test loss: 0.2782169525098002\n",
      "epoch 2214: train loss: 0.18497259114318468, test loss: 0.2782038947465236\n",
      "epoch 2215: train loss: 0.18494438123386342, test loss: 0.27819085012733313\n",
      "epoch 2216: train loss: 0.18491619102408932, test loss: 0.27817781863520785\n",
      "epoch 2217: train loss: 0.18488802049047495, test loss: 0.2781648002531549\n",
      "epoch 2218: train loss: 0.18485986960967168, test loss: 0.2781517949642091\n",
      "epoch 2219: train loss: 0.18483173835837, test loss: 0.2781388027514333\n",
      "epoch 2220: train loss: 0.18480362671329928, test loss: 0.278125823597918\n",
      "epoch 2221: train loss: 0.18477553465122756, test loss: 0.27811285748678144\n",
      "epoch 2222: train loss: 0.18474746214896168, test loss: 0.2780999044011695\n",
      "epoch 2223: train loss: 0.1847194091833471, test loss: 0.2780869643242558\n",
      "epoch 2224: train loss: 0.1846913757312678, test loss: 0.27807403723924135\n",
      "epoch 2225: train loss: 0.1846633617696462, test loss: 0.2780611231293546\n",
      "epoch 2226: train loss: 0.18463536727544314, test loss: 0.2780482219778516\n",
      "epoch 2227: train loss: 0.18460739222565767, test loss: 0.27803533376801587\n",
      "epoch 2228: train loss: 0.1845794365973272, test loss: 0.27802245848315793\n",
      "epoch 2229: train loss: 0.18455150036752715, test loss: 0.2780095961066158\n",
      "epoch 2230: train loss: 0.18452358351337111, test loss: 0.2779967466217545\n",
      "epoch 2231: train loss: 0.1844956860120105, test loss: 0.27798391001196654\n",
      "epoch 2232: train loss: 0.18446780784063477, test loss: 0.27797108626067124\n",
      "epoch 2233: train loss: 0.18443994897647115, test loss: 0.27795827535131507\n",
      "epoch 2234: train loss: 0.18441210939678462, test loss: 0.27794547726737145\n",
      "epoch 2235: train loss: 0.18438428907887772, test loss: 0.2779326919923408\n",
      "epoch 2236: train loss: 0.1843564880000907, test loss: 0.27791991950975026\n",
      "epoch 2237: train loss: 0.18432870613780125, test loss: 0.277907159803154\n",
      "epoch 2238: train loss: 0.18430094346942447, test loss: 0.2778944128561328\n",
      "epoch 2239: train loss: 0.18427319997241284, test loss: 0.27788167865229424\n",
      "epoch 2240: train loss: 0.18424547562425608, test loss: 0.2778689571752725\n",
      "epoch 2241: train loss: 0.1842177704024811, test loss: 0.2778562484087284\n",
      "epoch 2242: train loss: 0.18419008428465197, test loss: 0.2778435523363494\n",
      "epoch 2243: train loss: 0.18416241724836965, test loss: 0.2778308689418491\n",
      "epoch 2244: train loss: 0.18413476927127229, test loss: 0.277818198208968\n",
      "epoch 2245: train loss: 0.18410714033103465, test loss: 0.2778055401214727\n",
      "epoch 2246: train loss: 0.1840795304053684, test loss: 0.27779289466315626\n",
      "epoch 2247: train loss: 0.18405193947202206, test loss: 0.2777802618178379\n",
      "epoch 2248: train loss: 0.18402436750878062, test loss: 0.27776764156936307\n",
      "epoch 2249: train loss: 0.18399681449346578, test loss: 0.27775503390160367\n",
      "epoch 2250: train loss: 0.18396928040393554, test loss: 0.27774243879845734\n",
      "epoch 2251: train loss: 0.1839417652180845, test loss: 0.277729856243848\n",
      "epoch 2252: train loss: 0.18391426891384352, test loss: 0.2777172862217255\n",
      "epoch 2253: train loss: 0.18388679146917977, test loss: 0.27770472871606566\n",
      "epoch 2254: train loss: 0.18385933286209652, test loss: 0.2776921837108703\n",
      "epoch 2255: train loss: 0.18383189307063325, test loss: 0.27767965119016697\n",
      "epoch 2256: train loss: 0.18380447207286546, test loss: 0.27766713113800917\n",
      "epoch 2257: train loss: 0.1837770698469046, test loss: 0.2776546235384759\n",
      "epoch 2258: train loss: 0.18374968637089795, test loss: 0.2776421283756722\n",
      "epoch 2259: train loss: 0.18372232162302865, test loss: 0.2776296456337284\n",
      "epoch 2260: train loss: 0.18369497558151565, test loss: 0.2776171752968007\n",
      "epoch 2261: train loss: 0.18366764822461348, test loss: 0.2776047173490707\n",
      "epoch 2262: train loss: 0.18364033953061232, test loss: 0.27759227177474555\n",
      "epoch 2263: train loss: 0.18361304947783774, test loss: 0.27757983855805773\n",
      "epoch 2264: train loss: 0.18358577804465093, test loss: 0.2775674176832653\n",
      "epoch 2265: train loss: 0.1835585252094483, test loss: 0.27755500913465153\n",
      "epoch 2266: train loss: 0.18353129095066165, test loss: 0.2775426128965249\n",
      "epoch 2267: train loss: 0.183504075246758, test loss: 0.2775302289532193\n",
      "epoch 2268: train loss: 0.1834768780762394, test loss: 0.27751785728909384\n",
      "epoch 2269: train loss: 0.1834496994176432, test loss: 0.2775054978885324\n",
      "epoch 2270: train loss: 0.18342253924954155, test loss: 0.2774931507359443\n",
      "epoch 2271: train loss: 0.18339539755054154, test loss: 0.2774808158157638\n",
      "epoch 2272: train loss: 0.18336827429928534, test loss: 0.2774684931124501\n",
      "epoch 2273: train loss: 0.18334116947444964, test loss: 0.2774561826104873\n",
      "epoch 2274: train loss: 0.18331408305474597, test loss: 0.2774438842943845\n",
      "epoch 2275: train loss: 0.18328701501892056, test loss: 0.2774315981486755\n",
      "epoch 2276: train loss: 0.18325996534575403, test loss: 0.27741932415791903\n",
      "epoch 2277: train loss: 0.18323293401406168, test loss: 0.27740706230669837\n",
      "epoch 2278: train loss: 0.18320592100269317, test loss: 0.2773948125796216\n",
      "epoch 2279: train loss: 0.1831789262905325, test loss: 0.2773825749613214\n",
      "epoch 2280: train loss: 0.18315194985649805, test loss: 0.27737034943645505\n",
      "epoch 2281: train loss: 0.18312499167954222, test loss: 0.27735813598970427\n",
      "epoch 2282: train loss: 0.1830980517386517, test loss: 0.27734593460577545\n",
      "epoch 2283: train loss: 0.18307113001284733, test loss: 0.27733374526939936\n",
      "epoch 2284: train loss: 0.18304422648118376, test loss: 0.2773215679653308\n",
      "epoch 2285: train loss: 0.18301734112274975, test loss: 0.2773094026783495\n",
      "epoch 2286: train loss: 0.18299047391666778, test loss: 0.27729724939325917\n",
      "epoch 2287: train loss: 0.18296362484209425, test loss: 0.27728510809488766\n",
      "epoch 2288: train loss: 0.18293679387821918, test loss: 0.27727297876808715\n",
      "epoch 2289: train loss: 0.1829099810042663, test loss: 0.277260861397734\n",
      "epoch 2290: train loss: 0.1828831861994929, test loss: 0.27724875596872856\n",
      "epoch 2291: train loss: 0.18285640944318984, test loss: 0.27723666246599526\n",
      "epoch 2292: train loss: 0.1828296507146814, test loss: 0.27722458087448254\n",
      "epoch 2293: train loss: 0.1828029099933253, test loss: 0.2772125111791629\n",
      "epoch 2294: train loss: 0.18277618725851233, test loss: 0.27720045336503246\n",
      "epoch 2295: train loss: 0.18274948248966688, test loss: 0.27718840741711154\n",
      "epoch 2296: train loss: 0.18272279566624625, test loss: 0.277176373320444\n",
      "epoch 2297: train loss: 0.1826961267677409, test loss: 0.27716435106009757\n",
      "epoch 2298: train loss: 0.18266947577367446, test loss: 0.27715234062116356\n",
      "epoch 2299: train loss: 0.18264284266360334, test loss: 0.2771403419887573\n",
      "epoch 2300: train loss: 0.18261622741711697, test loss: 0.2771283551480174\n",
      "epoch 2301: train loss: 0.18258963001383768, test loss: 0.27711638008410616\n",
      "epoch 2302: train loss: 0.1825630504334204, test loss: 0.2771044167822094\n",
      "epoch 2303: train loss: 0.1825364886555528, test loss: 0.2770924652275364\n",
      "epoch 2304: train loss: 0.1825099446599553, test loss: 0.2770805254053198\n",
      "epoch 2305: train loss: 0.1824834184263808, test loss: 0.27706859730081584\n",
      "epoch 2306: train loss: 0.18245690993461472, test loss: 0.2770566808993039\n",
      "epoch 2307: train loss: 0.1824304191644749, test loss: 0.27704477618608686\n",
      "epoch 2308: train loss: 0.1824039460958115, test loss: 0.2770328831464905\n",
      "epoch 2309: train loss: 0.18237749070850717, test loss: 0.2770210017658641\n",
      "epoch 2310: train loss: 0.1823510529824766, test loss: 0.27700913202958\n",
      "epoch 2311: train loss: 0.1823246328976667, test loss: 0.2769972739230337\n",
      "epoch 2312: train loss: 0.1822982304340566, test loss: 0.27698542743164367\n",
      "epoch 2313: train loss: 0.1822718455716573, test loss: 0.2769735925408514\n",
      "epoch 2314: train loss: 0.1822454782905118, test loss: 0.2769617692361216\n",
      "epoch 2315: train loss: 0.1822191285706951, test loss: 0.2769499575029415\n",
      "epoch 2316: train loss: 0.1821927963923141, test loss: 0.2769381573268214\n",
      "epoch 2317: train loss: 0.1821664817355073, test loss: 0.2769263686932946\n",
      "epoch 2318: train loss: 0.182140184580445, test loss: 0.27691459158791704\n",
      "epoch 2319: train loss: 0.18211390490732918, test loss: 0.27690282599626725\n",
      "epoch 2320: train loss: 0.18208764269639327, test loss: 0.27689107190394674\n",
      "epoch 2321: train loss: 0.18206139792790246, test loss: 0.2768793292965796\n",
      "epoch 2322: train loss: 0.18203517058215327, test loss: 0.2768675981598124\n",
      "epoch 2323: train loss: 0.1820089606394734, test loss: 0.2768558784793144\n",
      "epoch 2324: train loss: 0.18198276808022232, test loss: 0.2768441702407774\n",
      "epoch 2325: train loss: 0.18195659288479044, test loss: 0.27683247342991557\n",
      "epoch 2326: train loss: 0.18193043503359946, test loss: 0.2768207880324656\n",
      "epoch 2327: train loss: 0.1819042945071022, test loss: 0.2768091140341866\n",
      "epoch 2328: train loss: 0.1818781712857826, test loss: 0.27679745142086\n",
      "epoch 2329: train loss: 0.18185206535015566, test loss: 0.2767858001782894\n",
      "epoch 2330: train loss: 0.1818259766807672, test loss: 0.2767741602923008\n",
      "epoch 2331: train loss: 0.18179990525819398, test loss: 0.2767625317487424\n",
      "epoch 2332: train loss: 0.18177385106304367, test loss: 0.27675091453348466\n",
      "epoch 2333: train loss: 0.18174781407595458, test loss: 0.2767393086324199\n",
      "epoch 2334: train loss: 0.18172179427759574, test loss: 0.27672771403146285\n",
      "epoch 2335: train loss: 0.18169579164866687, test loss: 0.2767161307165501\n",
      "epoch 2336: train loss: 0.18166980616989833, test loss: 0.27670455867364013\n",
      "epoch 2337: train loss: 0.1816438378220507, test loss: 0.27669299788871365\n",
      "epoch 2338: train loss: 0.18161788658591546, test loss: 0.27668144834777314\n",
      "epoch 2339: train loss: 0.18159195244231413, test loss: 0.27666991003684294\n",
      "epoch 2340: train loss: 0.18156603537209867, test loss: 0.2766583829419692\n",
      "epoch 2341: train loss: 0.18154013535615143, test loss: 0.2766468670492199\n",
      "epoch 2342: train loss: 0.18151425237538465, test loss: 0.2766353623446849\n",
      "epoch 2343: train loss: 0.18148838641074108, test loss: 0.2766238688144753\n",
      "epoch 2344: train loss: 0.18146253744319327, test loss: 0.27661238644472447\n",
      "epoch 2345: train loss: 0.18143670545374405, test loss: 0.27660091522158703\n",
      "epoch 2346: train loss: 0.18141089042342604, test loss: 0.27658945513123917\n",
      "epoch 2347: train loss: 0.18138509233330175, test loss: 0.27657800615987876\n",
      "epoch 2348: train loss: 0.1813593111644637, test loss: 0.27656656829372495\n",
      "epoch 2349: train loss: 0.18133354689803396, test loss: 0.27655514151901867\n",
      "epoch 2350: train loss: 0.18130779951516454, test loss: 0.2765437258220219\n",
      "epoch 2351: train loss: 0.18128206899703705, test loss: 0.2765323211890183\n",
      "epoch 2352: train loss: 0.18125635532486262, test loss: 0.27652092760631264\n",
      "epoch 2353: train loss: 0.181230658479882, test loss: 0.2765095450602309\n",
      "epoch 2354: train loss: 0.18120497844336547, test loss: 0.27649817353712064\n",
      "epoch 2355: train loss: 0.18117931519661262, test loss: 0.27648681302335026\n",
      "epoch 2356: train loss: 0.1811536687209525, test loss: 0.2764754635053095\n",
      "epoch 2357: train loss: 0.18112803899774346, test loss: 0.2764641249694093\n",
      "epoch 2358: train loss: 0.18110242600837304, test loss: 0.2764527974020814\n",
      "epoch 2359: train loss: 0.18107682973425804, test loss: 0.2764414807897789\n",
      "epoch 2360: train loss: 0.1810512501568443, test loss: 0.2764301751189756\n",
      "epoch 2361: train loss: 0.1810256872576069, test loss: 0.27641888037616646\n",
      "epoch 2362: train loss: 0.18100014101804984, test loss: 0.27640759654786734\n",
      "epoch 2363: train loss: 0.18097461141970603, test loss: 0.2763963236206148\n",
      "epoch 2364: train loss: 0.18094909844413729, test loss: 0.2763850615809664\n",
      "epoch 2365: train loss: 0.18092360207293431, test loss: 0.2763738104155005\n",
      "epoch 2366: train loss: 0.1808981222877167, test loss: 0.2763625701108161\n",
      "epoch 2367: train loss: 0.18087265907013259, test loss: 0.27635134065353295\n",
      "epoch 2368: train loss: 0.1808472124018589, test loss: 0.2763401220302915\n",
      "epoch 2369: train loss: 0.18082178226460105, test loss: 0.27632891422775296\n",
      "epoch 2370: train loss: 0.18079636864009316, test loss: 0.2763177172325988\n",
      "epoch 2371: train loss: 0.1807709715100978, test loss: 0.2763065310315314\n",
      "epoch 2372: train loss: 0.18074559085640587, test loss: 0.27629535561127344\n",
      "epoch 2373: train loss: 0.18072022666083684, test loss: 0.27628419095856815\n",
      "epoch 2374: train loss: 0.18069487890523842, test loss: 0.27627303706017914\n",
      "epoch 2375: train loss: 0.18066954757148654, test loss: 0.2762618939028904\n",
      "epoch 2376: train loss: 0.18064423264148544, test loss: 0.27625076147350647\n",
      "epoch 2377: train loss: 0.1806189340971674, test loss: 0.276239639758852\n",
      "epoch 2378: train loss: 0.18059365192049298, test loss: 0.27622852874577203\n",
      "epoch 2379: train loss: 0.18056838609345063, test loss: 0.2762174284211317\n",
      "epoch 2380: train loss: 0.18054313659805687, test loss: 0.27620633877181655\n",
      "epoch 2381: train loss: 0.1805179034163561, test loss: 0.27619525978473214\n",
      "epoch 2382: train loss: 0.1804926865304207, test loss: 0.2761841914468042\n",
      "epoch 2383: train loss: 0.18046748592235085, test loss: 0.2761731337449785\n",
      "epoch 2384: train loss: 0.18044230157427435, test loss: 0.27616208666622094\n",
      "epoch 2385: train loss: 0.18041713346834687, test loss: 0.2761510501975174\n",
      "epoch 2386: train loss: 0.18039198158675177, test loss: 0.27614002432587365\n",
      "epoch 2387: train loss: 0.18036684591169982, test loss: 0.27612900903831555\n",
      "epoch 2388: train loss: 0.1803417264254296, test loss: 0.27611800432188865\n",
      "epoch 2389: train loss: 0.18031662311020696, test loss: 0.2761070101636585\n",
      "epoch 2390: train loss: 0.18029153594832537, test loss: 0.2760960265507105\n",
      "epoch 2391: train loss: 0.18026646492210552, test loss: 0.27608505347014967\n",
      "epoch 2392: train loss: 0.18024141001389554, test loss: 0.2760740909091009\n",
      "epoch 2393: train loss: 0.1802163712060709, test loss: 0.27606313885470884\n",
      "epoch 2394: train loss: 0.18019134848103405, test loss: 0.27605219729413755\n",
      "epoch 2395: train loss: 0.18016634182121485, test loss: 0.276041266214571\n",
      "epoch 2396: train loss: 0.18014135120907027, test loss: 0.27603034560321255\n",
      "epoch 2397: train loss: 0.18011637662708416, test loss: 0.2760194354472853\n",
      "epoch 2398: train loss: 0.18009141805776757, test loss: 0.2760085357340317\n",
      "epoch 2399: train loss: 0.18006647548365837, test loss: 0.27599764645071384\n",
      "epoch 2400: train loss: 0.18004154888732146, test loss: 0.27598676758461305\n",
      "epoch 2401: train loss: 0.18001663825134848, test loss: 0.27597589912303017\n",
      "epoch 2402: train loss: 0.17999174355835795, test loss: 0.2759650410532855\n",
      "epoch 2403: train loss: 0.17996686479099502, test loss: 0.2759541933627186\n",
      "epoch 2404: train loss: 0.1799420019319317, test loss: 0.27594335603868825\n",
      "epoch 2405: train loss: 0.17991715496386654, test loss: 0.27593252906857263\n",
      "epoch 2406: train loss: 0.17989232386952458, test loss: 0.275921712439769\n",
      "epoch 2407: train loss: 0.17986750863165768, test loss: 0.2759109061396938\n",
      "epoch 2408: train loss: 0.1798427092330439, test loss: 0.275900110155783\n",
      "epoch 2409: train loss: 0.17981792565648785, test loss: 0.275889324475491\n",
      "epoch 2410: train loss: 0.17979315788482048, test loss: 0.27587854908629195\n",
      "epoch 2411: train loss: 0.17976840590089915, test loss: 0.27586778397567857\n",
      "epoch 2412: train loss: 0.17974366968760738, test loss: 0.2758570291311629\n",
      "epoch 2413: train loss: 0.17971894922785503, test loss: 0.27584628454027565\n",
      "epoch 2414: train loss: 0.17969424450457802, test loss: 0.27583555019056677\n",
      "epoch 2415: train loss: 0.1796695555007385, test loss: 0.2758248260696049\n",
      "epoch 2416: train loss: 0.17964488219932456, test loss: 0.2758141121649776\n",
      "epoch 2417: train loss: 0.17962022458335042, test loss: 0.2758034084642913\n",
      "epoch 2418: train loss: 0.1795955826358563, test loss: 0.2757927149551711\n",
      "epoch 2419: train loss: 0.17957095633990813, test loss: 0.2757820316252611\n",
      "epoch 2420: train loss: 0.17954634567859798, test loss: 0.2757713584622239\n",
      "epoch 2421: train loss: 0.1795217506350435, test loss: 0.27576069545374066\n",
      "epoch 2422: train loss: 0.17949717119238826, test loss: 0.2757500425875116\n",
      "epoch 2423: train loss: 0.17947260733380144, test loss: 0.2757393998512553\n",
      "epoch 2424: train loss: 0.17944805904247793, test loss: 0.2757287672327089\n",
      "epoch 2425: train loss: 0.17942352630163833, test loss: 0.2757181447196281\n",
      "epoch 2426: train loss: 0.1793990090945286, test loss: 0.2757075322997872\n",
      "epoch 2427: train loss: 0.17937450740442032, test loss: 0.27569692996097883\n",
      "epoch 2428: train loss: 0.1793500212146106, test loss: 0.2756863376910143\n",
      "epoch 2429: train loss: 0.1793255505084219, test loss: 0.275675755477723\n",
      "epoch 2430: train loss: 0.179301095269202, test loss: 0.275665183308953\n",
      "epoch 2431: train loss: 0.17927665548032404, test loss: 0.2756546211725706\n",
      "epoch 2432: train loss: 0.17925223112518648, test loss: 0.2756440690564603\n",
      "epoch 2433: train loss: 0.1792278221872129, test loss: 0.27563352694852483\n",
      "epoch 2434: train loss: 0.179203428649852, test loss: 0.27562299483668545\n",
      "epoch 2435: train loss: 0.1791790504965779, test loss: 0.27561247270888134\n",
      "epoch 2436: train loss: 0.17915468771088944, test loss: 0.27560196055307\n",
      "epoch 2437: train loss: 0.17913034027631056, test loss: 0.27559145835722687\n",
      "epoch 2438: train loss: 0.17910600817639036, test loss: 0.2755809661093458\n",
      "epoch 2439: train loss: 0.17908169139470267, test loss: 0.2755704837974384\n",
      "epoch 2440: train loss: 0.17905738991484632, test loss: 0.27556001140953446\n",
      "epoch 2441: train loss: 0.1790331037204448, test loss: 0.2755495489336818\n",
      "epoch 2442: train loss: 0.1790088327951466, test loss: 0.27553909635794605\n",
      "epoch 2443: train loss: 0.1789845771226247, test loss: 0.2755286536704109\n",
      "epoch 2444: train loss: 0.17896033668657696, test loss: 0.27551822085917793\n",
      "epoch 2445: train loss: 0.17893611147072583, test loss: 0.27550779791236646\n",
      "epoch 2446: train loss: 0.1789119014588183, test loss: 0.27549738481811376\n",
      "epoch 2447: train loss: 0.17888770663462586, test loss: 0.27548698156457496\n",
      "epoch 2448: train loss: 0.17886352698194458, test loss: 0.2754765881399227\n",
      "epoch 2449: train loss: 0.17883936248459498, test loss: 0.2754662045323476\n",
      "epoch 2450: train loss: 0.1788152131264219, test loss: 0.27545583073005786\n",
      "epoch 2451: train loss: 0.17879107889129453, test loss: 0.27544546672127934\n",
      "epoch 2452: train loss: 0.17876695976310653, test loss: 0.2754351124942556\n",
      "epoch 2453: train loss: 0.17874285572577556, test loss: 0.2754247680372478\n",
      "epoch 2454: train loss: 0.1787187667632437, test loss: 0.2754144333385345\n",
      "epoch 2455: train loss: 0.17869469285947706, test loss: 0.2754041083864119\n",
      "epoch 2456: train loss: 0.17867063399846594, test loss: 0.2753937931691938\n",
      "epoch 2457: train loss: 0.17864659016422457, test loss: 0.2753834876752113\n",
      "epoch 2458: train loss: 0.17862256134079155, test loss: 0.2753731918928131\n",
      "epoch 2459: train loss: 0.17859854751222895, test loss: 0.2753629058103652\n",
      "epoch 2460: train loss: 0.1785745486626232, test loss: 0.27535262941625094\n",
      "epoch 2461: train loss: 0.17855056477608444, test loss: 0.275342362698871\n",
      "epoch 2462: train loss: 0.17852659583674654, test loss: 0.27533210564664357\n",
      "epoch 2463: train loss: 0.17850264182876746, test loss: 0.2753218582480038\n",
      "epoch 2464: train loss: 0.17847870273632857, test loss: 0.2753116204914043\n",
      "epoch 2465: train loss: 0.1784547785436352, test loss: 0.27530139236531476\n",
      "epoch 2466: train loss: 0.1784308692349161, test loss: 0.27529117385822216\n",
      "epoch 2467: train loss: 0.17840697479442397, test loss: 0.2752809649586307\n",
      "epoch 2468: train loss: 0.17838309520643467, test loss: 0.27527076565506137\n",
      "epoch 2469: train loss: 0.1783592304552478, test loss: 0.2752605759360527\n",
      "epoch 2470: train loss: 0.1783353805251864, test loss: 0.2752503957901597\n",
      "epoch 2471: train loss: 0.17831154540059704, test loss: 0.2752402252059549\n",
      "epoch 2472: train loss: 0.17828772506584945, test loss: 0.2752300641720278\n",
      "epoch 2473: train loss: 0.1782639195053369, test loss: 0.27521991267698437\n",
      "epoch 2474: train loss: 0.17824012870347578, test loss: 0.2752097707094481\n",
      "epoch 2475: train loss: 0.1782163526447059, test loss: 0.27519963825805893\n",
      "epoch 2476: train loss: 0.17819259131349013, test loss: 0.27518951531147395\n",
      "epoch 2477: train loss: 0.17816884469431465, test loss: 0.275179401858367\n",
      "epoch 2478: train loss: 0.17814511277168857, test loss: 0.27516929788742855\n",
      "epoch 2479: train loss: 0.17812139553014425, test loss: 0.27515920338736616\n",
      "epoch 2480: train loss: 0.1780976929542369, test loss: 0.2751491183469039\n",
      "epoch 2481: train loss: 0.17807400502854492, test loss: 0.2751390427547826\n",
      "epoch 2482: train loss: 0.17805033173766943, test loss: 0.27512897659975977\n",
      "epoch 2483: train loss: 0.1780266730662346, test loss: 0.2751189198706096\n",
      "epoch 2484: train loss: 0.17800302899888737, test loss: 0.2751088725561229\n",
      "epoch 2485: train loss: 0.17797939952029754, test loss: 0.2750988346451069\n",
      "epoch 2486: train loss: 0.17795578461515768, test loss: 0.27508880612638587\n",
      "epoch 2487: train loss: 0.177932184268183, test loss: 0.27507878698879984\n",
      "epoch 2488: train loss: 0.17790859846411142, test loss: 0.27506877722120604\n",
      "epoch 2489: train loss: 0.17788502718770352, test loss: 0.27505877681247776\n",
      "epoch 2490: train loss: 0.17786147042374256, test loss: 0.275048785751505\n",
      "epoch 2491: train loss: 0.17783792815703406, test loss: 0.2750388040271939\n",
      "epoch 2492: train loss: 0.17781440037240642, test loss: 0.2750288316284671\n",
      "epoch 2493: train loss: 0.1777908870547102, test loss: 0.2750188685442636\n",
      "epoch 2494: train loss: 0.17776738818881854, test loss: 0.2750089147635388\n",
      "epoch 2495: train loss: 0.17774390375962681, test loss: 0.2749989702752641\n",
      "epoch 2496: train loss: 0.17772043375205293, test loss: 0.27498903506842753\n",
      "epoch 2497: train loss: 0.17769697815103694, test loss: 0.274979109132033\n",
      "epoch 2498: train loss: 0.17767353694154117, test loss: 0.27496919245510076\n",
      "epoch 2499: train loss: 0.1776501101085502, test loss: 0.27495928502666744\n",
      "epoch 2500: train loss: 0.17762669763707067, test loss: 0.2749493868357855\n",
      "epoch 2501: train loss: 0.17760329951213147, test loss: 0.2749394978715236\n",
      "epoch 2502: train loss: 0.1775799157187835, test loss: 0.2749296181229665\n",
      "epoch 2503: train loss: 0.17755654624209968, test loss: 0.27491974757921506\n",
      "epoch 2504: train loss: 0.17753319106717502, test loss: 0.27490988622938606\n",
      "epoch 2505: train loss: 0.1775098501791263, test loss: 0.2749000340626123\n",
      "epoch 2506: train loss: 0.17748652356309244, test loss: 0.2748901910680427\n",
      "epoch 2507: train loss: 0.17746321120423403, test loss: 0.27488035723484183\n",
      "epoch 2508: train loss: 0.1774399130877337, test loss: 0.27487053255219035\n",
      "epoch 2509: train loss: 0.17741662919879567, test loss: 0.2748607170092847\n",
      "epoch 2510: train loss: 0.17739335952264595, test loss: 0.27485091059533745\n",
      "epoch 2511: train loss: 0.1773701040445324, test loss: 0.2748411132995765\n",
      "epoch 2512: train loss: 0.17734686274972436, test loss: 0.274831325111246\n",
      "epoch 2513: train loss: 0.17732363562351292, test loss: 0.27482154601960557\n",
      "epoch 2514: train loss: 0.17730042265121063, test loss: 0.2748117760139307\n",
      "epoch 2515: train loss: 0.1772772238181517, test loss: 0.27480201508351243\n",
      "epoch 2516: train loss: 0.1772540391096918, test loss: 0.27479226321765776\n",
      "epoch 2517: train loss: 0.17723086851120812, test loss: 0.27478252040568907\n",
      "epoch 2518: train loss: 0.1772077120080991, test loss: 0.27477278663694443\n",
      "epoch 2519: train loss: 0.1771845695857847, test loss: 0.2747630619007776\n",
      "epoch 2520: train loss: 0.17716144122970628, test loss: 0.27475334618655767\n",
      "epoch 2521: train loss: 0.1771383269253263, test loss: 0.2747436394836696\n",
      "epoch 2522: train loss: 0.17711522665812865, test loss: 0.2747339417815136\n",
      "epoch 2523: train loss: 0.17709214041361837, test loss: 0.27472425306950543\n",
      "epoch 2524: train loss: 0.17706906817732165, test loss: 0.27471457333707633\n",
      "epoch 2525: train loss: 0.17704600993478586, test loss: 0.2747049025736729\n",
      "epoch 2526: train loss: 0.17702296567157955, test loss: 0.2746952407687572\n",
      "epoch 2527: train loss: 0.1769999353732922, test loss: 0.27468558791180664\n",
      "epoch 2528: train loss: 0.1769769190255343, test loss: 0.2746759439923139\n",
      "epoch 2529: train loss: 0.17695391661393745, test loss: 0.27466630899978706\n",
      "epoch 2530: train loss: 0.17693092812415412, test loss: 0.27465668292374945\n",
      "epoch 2531: train loss: 0.17690795354185768, test loss: 0.27464706575373965\n",
      "epoch 2532: train loss: 0.17688499285274226, test loss: 0.2746374574793115\n",
      "epoch 2533: train loss: 0.1768620460425231, test loss: 0.2746278580900341\n",
      "epoch 2534: train loss: 0.17683911309693592, test loss: 0.2746182675754914\n",
      "epoch 2535: train loss: 0.17681619400173734, test loss: 0.2746086859252829\n",
      "epoch 2536: train loss: 0.17679328874270467, test loss: 0.27459911312902313\n",
      "epoch 2537: train loss: 0.1767703973056359, test loss: 0.2745895491763415\n",
      "epoch 2538: train loss: 0.17674751967634955, test loss: 0.2745799940568827\n",
      "epoch 2539: train loss: 0.17672465584068486, test loss: 0.27457044776030637\n",
      "epoch 2540: train loss: 0.17670180578450156, test loss: 0.27456091027628715\n",
      "epoch 2541: train loss: 0.17667896949367992, test loss: 0.2745513815945147\n",
      "epoch 2542: train loss: 0.17665614695412063, test loss: 0.2745418617046937\n",
      "epoch 2543: train loss: 0.1766333381517449, test loss: 0.27453235059654363\n",
      "epoch 2544: train loss: 0.17661054307249427, test loss: 0.2745228482597989\n",
      "epoch 2545: train loss: 0.17658776170233065, test loss: 0.27451335468420895\n",
      "epoch 2546: train loss: 0.17656499402723633, test loss: 0.2745038698595377\n",
      "epoch 2547: train loss: 0.17654224003321375, test loss: 0.27449439377556445\n",
      "epoch 2548: train loss: 0.17651949970628583, test loss: 0.2744849264220828\n",
      "epoch 2549: train loss: 0.17649677303249545, test loss: 0.2744754677889014\n",
      "epoch 2550: train loss: 0.17647405999790577, test loss: 0.27446601786584357\n",
      "epoch 2551: train loss: 0.17645136058860023, test loss: 0.27445657664274725\n",
      "epoch 2552: train loss: 0.17642867479068208, test loss: 0.2744471441094652\n",
      "epoch 2553: train loss: 0.17640600259027472, test loss: 0.2744377202558648\n",
      "epoch 2554: train loss: 0.17638334397352182, test loss: 0.2744283050718282\n",
      "epoch 2555: train loss: 0.17636069892658665, test loss: 0.2744188985472519\n",
      "epoch 2556: train loss: 0.17633806743565272, test loss: 0.2744095006720471\n",
      "epoch 2557: train loss: 0.1763154494869233, test loss: 0.27440011143613974\n",
      "epoch 2558: train loss: 0.17629284506662163, test loss: 0.27439073082946996\n",
      "epoch 2559: train loss: 0.17627025416099065, test loss: 0.2743813588419928\n",
      "epoch 2560: train loss: 0.1762476767562933, test loss: 0.2743719954636774\n",
      "epoch 2561: train loss: 0.17622511283881206, test loss: 0.27436264068450766\n",
      "epoch 2562: train loss: 0.1762025623948493, test loss: 0.2743532944944818\n",
      "epoch 2563: train loss: 0.17618002541072697, test loss: 0.2743439568836123\n",
      "epoch 2564: train loss: 0.1761575018727868, test loss: 0.2743346278419263\n",
      "epoch 2565: train loss: 0.17613499176739, test loss: 0.2743253073594651\n",
      "epoch 2566: train loss: 0.17611249508091745, test loss: 0.2743159954262844\n",
      "epoch 2567: train loss: 0.17609001179976957, test loss: 0.2743066920324541\n",
      "epoch 2568: train loss: 0.17606754191036622, test loss: 0.2742973971680587\n",
      "epoch 2569: train loss: 0.17604508539914676, test loss: 0.2742881108231965\n",
      "epoch 2570: train loss: 0.17602264225257006, test loss: 0.2742788329879803\n",
      "epoch 2571: train loss: 0.1760002124571143, test loss: 0.2742695636525372\n",
      "epoch 2572: train loss: 0.17597779599927704, test loss: 0.27426030280700814\n",
      "epoch 2573: train loss: 0.1759553928655752, test loss: 0.2742510504415485\n",
      "epoch 2574: train loss: 0.17593300304254497, test loss: 0.2742418065463278\n",
      "epoch 2575: train loss: 0.17591062651674186, test loss: 0.27423257111152943\n",
      "epoch 2576: train loss: 0.17588826327474041, test loss: 0.274223344127351\n",
      "epoch 2577: train loss: 0.17586591330313456, test loss: 0.2742141255840042\n",
      "epoch 2578: train loss: 0.17584357658853733, test loss: 0.27420491547171477\n",
      "epoch 2579: train loss: 0.17582125311758084, test loss: 0.2741957137807223\n",
      "epoch 2580: train loss: 0.17579894287691622, test loss: 0.2741865205012805\n",
      "epoch 2581: train loss: 0.17577664585321384, test loss: 0.27417733562365704\n",
      "epoch 2582: train loss: 0.17575436203316286, test loss: 0.27416815913813347\n",
      "epoch 2583: train loss: 0.17573209140347154, test loss: 0.27415899103500513\n",
      "epoch 2584: train loss: 0.175709833950867, test loss: 0.2741498313045814\n",
      "epoch 2585: train loss: 0.17568758966209536, test loss: 0.27414067993718566\n",
      "epoch 2586: train loss: 0.17566535852392165, test loss: 0.27413153692315473\n",
      "epoch 2587: train loss: 0.1756431405231295, test loss: 0.2741224022528395\n",
      "epoch 2588: train loss: 0.17562093564652156, test loss: 0.27411327591660467\n",
      "epoch 2589: train loss: 0.17559874388091912, test loss: 0.27410415790482856\n",
      "epoch 2590: train loss: 0.1755765652131624, test loss: 0.2740950482079033\n",
      "epoch 2591: train loss: 0.17555439963011008, test loss: 0.27408594681623466\n",
      "epoch 2592: train loss: 0.17553224711863954, test loss: 0.27407685372024226\n",
      "epoch 2593: train loss: 0.17551010766564695, test loss: 0.27406776891035917\n",
      "epoch 2594: train loss: 0.1754879812580469, test loss: 0.27405869237703234\n",
      "epoch 2595: train loss: 0.17546586788277266, test loss: 0.2740496241107221\n",
      "epoch 2596: train loss: 0.17544376752677593, test loss: 0.2740405641019025\n",
      "epoch 2597: train loss: 0.175421680177027, test loss: 0.2740315123410611\n",
      "epoch 2598: train loss: 0.17539960582051453, test loss: 0.27402246881869907\n",
      "epoch 2599: train loss: 0.1753775444442457, test loss: 0.2740134335253311\n",
      "epoch 2600: train loss: 0.17535549603524594, test loss: 0.27400440645148527\n",
      "epoch 2601: train loss: 0.17533346058055913, test loss: 0.27399538758770325\n",
      "epoch 2602: train loss: 0.17531143806724755, test loss: 0.2739863769245401\n",
      "epoch 2603: train loss: 0.17528942848239157, test loss: 0.2739773744525643\n",
      "epoch 2604: train loss: 0.1752674318130901, test loss: 0.27396838016235775\n",
      "epoch 2605: train loss: 0.17524544804645992, test loss: 0.27395939404451586\n",
      "epoch 2606: train loss: 0.1752234771696363, test loss: 0.27395041608964704\n",
      "epoch 2607: train loss: 0.17520151916977247, test loss: 0.2739414462883734\n",
      "epoch 2608: train loss: 0.17517957403404, test loss: 0.2739324846313302\n",
      "epoch 2609: train loss: 0.17515764174962828, test loss: 0.27392353110916595\n",
      "epoch 2610: train loss: 0.175135722303745, test loss: 0.27391458571254257\n",
      "epoch 2611: train loss: 0.17511381568361573, test loss: 0.27390564843213494\n",
      "epoch 2612: train loss: 0.17509192187648406, test loss: 0.27389671925863157\n",
      "epoch 2613: train loss: 0.17507004086961153, test loss: 0.2738877981827338\n",
      "epoch 2614: train loss: 0.17504817265027772, test loss: 0.2738788851951563\n",
      "epoch 2615: train loss: 0.17502631720577994, test loss: 0.27386998028662674\n",
      "epoch 2616: train loss: 0.17500447452343348, test loss: 0.27386108344788623\n",
      "epoch 2617: train loss: 0.17498264459057136, test loss: 0.2738521946696887\n",
      "epoch 2618: train loss: 0.17496082739454452, test loss: 0.2738433139428012\n",
      "epoch 2619: train loss: 0.17493902292272154, test loss: 0.27383444125800394\n",
      "epoch 2620: train loss: 0.17491723116248883, test loss: 0.2738255766060901\n",
      "epoch 2621: train loss: 0.17489545210125043, test loss: 0.2738167199778658\n",
      "epoch 2622: train loss: 0.17487368572642814, test loss: 0.27380787136415025\n",
      "epoch 2623: train loss: 0.1748519320254613, test loss: 0.27379903075577566\n",
      "epoch 2624: train loss: 0.17483019098580685, test loss: 0.273790198143587\n",
      "epoch 2625: train loss: 0.17480846259493937, test loss: 0.2737813735184424\n",
      "epoch 2626: train loss: 0.17478674684035092, test loss: 0.27377255687121277\n",
      "epoch 2627: train loss: 0.17476504370955115, test loss: 0.2737637481927819\n",
      "epoch 2628: train loss: 0.17474335319006712, test loss: 0.2737549474740462\n",
      "epoch 2629: train loss: 0.17472167526944332, test loss: 0.2737461547059154\n",
      "epoch 2630: train loss: 0.1747000099352417, test loss: 0.27373736987931174\n",
      "epoch 2631: train loss: 0.17467835717504157, test loss: 0.27372859298517016\n",
      "epoch 2632: train loss: 0.17465671697643967, test loss: 0.27371982401443856\n",
      "epoch 2633: train loss: 0.17463508932704983, test loss: 0.2737110629580775\n",
      "epoch 2634: train loss: 0.1746134742145035, test loss: 0.2737023098070604\n",
      "epoch 2635: train loss: 0.1745918716264491, test loss: 0.27369356455237304\n",
      "epoch 2636: train loss: 0.1745702815505524, test loss: 0.2736848271850142\n",
      "epoch 2637: train loss: 0.1745487039744964, test loss: 0.27367609769599516\n",
      "epoch 2638: train loss: 0.17452713888598112, test loss: 0.27366737607634\n",
      "epoch 2639: train loss: 0.17450558627272394, test loss: 0.27365866231708513\n",
      "epoch 2640: train loss: 0.1744840461224591, test loss: 0.27364995640927975\n",
      "epoch 2641: train loss: 0.17446251842293806, test loss: 0.27364125834398567\n",
      "epoch 2642: train loss: 0.17444100316192923, test loss: 0.2736325681122771\n",
      "epoch 2643: train loss: 0.17441950032721817, test loss: 0.27362388570524077\n",
      "epoch 2644: train loss: 0.17439800990660728, test loss: 0.2736152111139761\n",
      "epoch 2645: train loss: 0.17437653188791596, test loss: 0.2736065443295947\n",
      "epoch 2646: train loss: 0.17435506625898053, test loss: 0.27359788534322105\n",
      "epoch 2647: train loss: 0.17433361300765418, test loss: 0.2735892341459916\n",
      "epoch 2648: train loss: 0.17431217212180697, test loss: 0.2735805907290556\n",
      "epoch 2649: train loss: 0.1742907435893258, test loss: 0.2735719550835745\n",
      "epoch 2650: train loss: 0.1742693273981144, test loss: 0.2735633272007222\n",
      "epoch 2651: train loss: 0.1742479235360931, test loss: 0.2735547070716848\n",
      "epoch 2652: train loss: 0.1742265319911992, test loss: 0.27354609468766095\n",
      "epoch 2653: train loss: 0.17420515275138654, test loss: 0.2735374900398615\n",
      "epoch 2654: train loss: 0.17418378580462568, test loss: 0.2735288931195095\n",
      "epoch 2655: train loss: 0.17416243113890384, test loss: 0.27352030391784066\n",
      "epoch 2656: train loss: 0.17414108874222492, test loss: 0.27351172242610233\n",
      "epoch 2657: train loss: 0.17411975860260923, test loss: 0.2735031486355546\n",
      "epoch 2658: train loss: 0.17409844070809377, test loss: 0.2734945825374695\n",
      "epoch 2659: train loss: 0.1740771350467321, test loss: 0.27348602412313133\n",
      "epoch 2660: train loss: 0.17405584160659424, test loss: 0.27347747338383654\n",
      "epoch 2661: train loss: 0.1740345603757666, test loss: 0.2734689303108937\n",
      "epoch 2662: train loss: 0.1740132913423521, test loss: 0.2734603948956236\n",
      "epoch 2663: train loss: 0.17399203449447012, test loss: 0.273451867129359\n",
      "epoch 2664: train loss: 0.17397078982025627, test loss: 0.27344334700344475\n",
      "epoch 2665: train loss: 0.17394955730786257, test loss: 0.273434834509238\n",
      "epoch 2666: train loss: 0.17392833694545756, test loss: 0.2734263296381076\n",
      "epoch 2667: train loss: 0.17390712872122582, test loss: 0.2734178323814346\n",
      "epoch 2668: train loss: 0.17388593262336824, test loss: 0.273409342730612\n",
      "epoch 2669: train loss: 0.173864748640102, test loss: 0.27340086067704483\n",
      "epoch 2670: train loss: 0.17384357675966056, test loss: 0.2733923862121501\n",
      "epoch 2671: train loss: 0.17382241697029335, test loss: 0.2733839193273566\n",
      "epoch 2672: train loss: 0.17380126926026615, test loss: 0.27337546001410523\n",
      "epoch 2673: train loss: 0.17378013361786074, test loss: 0.27336700826384863\n",
      "epoch 2674: train loss: 0.17375901003137506, test loss: 0.2733585640680514\n",
      "epoch 2675: train loss: 0.17373789848912305, test loss: 0.2733501274181901\n",
      "epoch 2676: train loss: 0.17371679897943473, test loss: 0.2733416983057529\n",
      "epoch 2677: train loss: 0.17369571149065607, test loss: 0.27333327672223984\n",
      "epoch 2678: train loss: 0.17367463601114913, test loss: 0.2733248626591629\n",
      "epoch 2679: train loss: 0.17365357252929184, test loss: 0.2733164561080458\n",
      "epoch 2680: train loss: 0.17363252103347793, test loss: 0.2733080570604239\n",
      "epoch 2681: train loss: 0.1736114815121173, test loss: 0.2732996655078443\n",
      "epoch 2682: train loss: 0.1735904539536355, test loss: 0.27329128144186593\n",
      "epoch 2683: train loss: 0.17356943834647395, test loss: 0.2732829048540594\n",
      "epoch 2684: train loss: 0.17354843467908995, test loss: 0.273274535736007\n",
      "epoch 2685: train loss: 0.17352744293995648, test loss: 0.27326617407930254\n",
      "epoch 2686: train loss: 0.17350646311756243, test loss: 0.2732578198755516\n",
      "epoch 2687: train loss: 0.1734854952004122, test loss: 0.2732494731163713\n",
      "epoch 2688: train loss: 0.17346453917702603, test loss: 0.27324113379339054\n",
      "epoch 2689: train loss: 0.17344359503593978, test loss: 0.27323280189824956\n",
      "epoch 2690: train loss: 0.17342266276570498, test loss: 0.27322447742260025\n",
      "epoch 2691: train loss: 0.1734017423548888, test loss: 0.2732161603581061\n",
      "epoch 2692: train loss: 0.17338083379207386, test loss: 0.27320785069644193\n",
      "epoch 2693: train loss: 0.1733599370658585, test loss: 0.2731995484292944\n",
      "epoch 2694: train loss: 0.17333905216485646, test loss: 0.2731912535483613\n",
      "epoch 2695: train loss: 0.17331817907769712, test loss: 0.2731829660453521\n",
      "epoch 2696: train loss: 0.17329731779302515, test loss: 0.2731746859119876\n",
      "epoch 2697: train loss: 0.17327646829950086, test loss: 0.2731664131400002\n",
      "epoch 2698: train loss: 0.17325563058579987, test loss: 0.27315814772113334\n",
      "epoch 2699: train loss: 0.17323480464061325, test loss: 0.27314988964714215\n",
      "epoch 2700: train loss: 0.17321399045264738, test loss: 0.2731416389097932\n",
      "epoch 2701: train loss: 0.17319318801062394, test loss: 0.273133395500864\n",
      "epoch 2702: train loss: 0.17317239730328005, test loss: 0.27312515941214394\n",
      "epoch 2703: train loss: 0.17315161831936807, test loss: 0.2731169306354331\n",
      "epoch 2704: train loss: 0.17313085104765563, test loss: 0.27310870916254343\n",
      "epoch 2705: train loss: 0.17311009547692546, test loss: 0.2731004949852977\n",
      "epoch 2706: train loss: 0.17308935159597563, test loss: 0.2730922880955302\n",
      "epoch 2707: train loss: 0.1730686193936194, test loss: 0.2730840884850865\n",
      "epoch 2708: train loss: 0.17304789885868513, test loss: 0.2730758961458229\n",
      "epoch 2709: train loss: 0.17302718998001632, test loss: 0.27306771106960753\n",
      "epoch 2710: train loss: 0.1730064927464715, test loss: 0.27305953324831933\n",
      "epoch 2711: train loss: 0.17298580714692438, test loss: 0.27305136267384855\n",
      "epoch 2712: train loss: 0.17296513317026366, test loss: 0.2730431993380964\n",
      "epoch 2713: train loss: 0.17294447080539307, test loss: 0.27303504323297534\n",
      "epoch 2714: train loss: 0.17292382004123133, test loss: 0.27302689435040894\n",
      "epoch 2715: train loss: 0.17290318086671216, test loss: 0.2730187526823318\n",
      "epoch 2716: train loss: 0.17288255327078417, test loss: 0.2730106182206896\n",
      "epoch 2717: train loss: 0.17286193724241083, test loss: 0.27300249095743906\n",
      "epoch 2718: train loss: 0.17284133277057065, test loss: 0.2729943708845479\n",
      "epoch 2719: train loss: 0.1728207398442569, test loss: 0.27298625799399495\n",
      "epoch 2720: train loss: 0.17280015845247776, test loss: 0.27297815227776995\n",
      "epoch 2721: train loss: 0.17277958858425602, test loss: 0.27297005372787353\n",
      "epoch 2722: train loss: 0.17275903022862957, test loss: 0.2729619623363175\n",
      "epoch 2723: train loss: 0.17273848337465075, test loss: 0.27295387809512434\n",
      "epoch 2724: train loss: 0.1727179480113869, test loss: 0.2729458009963277\n",
      "epoch 2725: train loss: 0.17269742412791989, test loss: 0.2729377310319719\n",
      "epoch 2726: train loss: 0.17267691171334626, test loss: 0.2729296681941122\n",
      "epoch 2727: train loss: 0.17265641075677737, test loss: 0.2729216124748149\n",
      "epoch 2728: train loss: 0.17263592124733904, test loss: 0.27291356386615695\n",
      "epoch 2729: train loss: 0.17261544317417177, test loss: 0.2729055223602262\n",
      "epoch 2730: train loss: 0.17259497652643066, test loss: 0.2728974879491213\n",
      "epoch 2731: train loss: 0.17257452129328532, test loss: 0.27288946062495156\n",
      "epoch 2732: train loss: 0.17255407746391985, test loss: 0.2728814403798373\n",
      "epoch 2733: train loss: 0.172533645027533, test loss: 0.2728734272059095\n",
      "epoch 2734: train loss: 0.1725132239733378, test loss: 0.27286542109530976\n",
      "epoch 2735: train loss: 0.17249281429056193, test loss: 0.27285742204019053\n",
      "epoch 2736: train loss: 0.17247241596844737, test loss: 0.27284943003271495\n",
      "epoch 2737: train loss: 0.17245202899625053, test loss: 0.2728414450650567\n",
      "epoch 2738: train loss: 0.1724316533632422, test loss: 0.2728334671294004\n",
      "epoch 2739: train loss: 0.17241128905870753, test loss: 0.27282549621794105\n",
      "epoch 2740: train loss: 0.172390936071946, test loss: 0.27281753232288436\n",
      "epoch 2741: train loss: 0.17237059439227143, test loss: 0.27280957543644674\n",
      "epoch 2742: train loss: 0.17235026400901177, test loss: 0.27280162555085513\n",
      "epoch 2743: train loss: 0.1723299449115094, test loss: 0.27279368265834697\n",
      "epoch 2744: train loss: 0.1723096370891209, test loss: 0.27278574675117034\n",
      "epoch 2745: train loss: 0.17228934053121692, test loss: 0.27277781782158395\n",
      "epoch 2746: train loss: 0.17226905522718242, test loss: 0.2727698958618568\n",
      "epoch 2747: train loss: 0.1722487811664164, test loss: 0.27276198086426867\n",
      "epoch 2748: train loss: 0.17222851833833225, test loss: 0.27275407282110964\n",
      "epoch 2749: train loss: 0.1722082667323571, test loss: 0.2727461717246803\n",
      "epoch 2750: train loss: 0.17218802633793243, test loss: 0.27273827756729163\n",
      "epoch 2751: train loss: 0.17216779714451363, test loss: 0.2727303903412653\n",
      "epoch 2752: train loss: 0.17214757914157025, test loss: 0.2727225100389331\n",
      "epoch 2753: train loss: 0.17212737231858577, test loss: 0.2727146366526374\n",
      "epoch 2754: train loss: 0.1721071766650576, test loss: 0.27270677017473083\n",
      "epoch 2755: train loss: 0.17208699217049722, test loss: 0.27269891059757656\n",
      "epoch 2756: train loss: 0.17206681882443, test loss: 0.27269105791354803\n",
      "epoch 2757: train loss: 0.17204665661639523, test loss: 0.2726832121150289\n",
      "epoch 2758: train loss: 0.1720265055359461, test loss: 0.27267537319441315\n",
      "epoch 2759: train loss: 0.17200636557264956, test loss: 0.2726675411441054\n",
      "epoch 2760: train loss: 0.1719862367160866, test loss: 0.27265971595652017\n",
      "epoch 2761: train loss: 0.1719661189558518, test loss: 0.27265189762408243\n",
      "epoch 2762: train loss: 0.1719460122815537, test loss: 0.2726440861392273\n",
      "epoch 2763: train loss: 0.17192591668281454, test loss: 0.27263628149440017\n",
      "epoch 2764: train loss: 0.17190583214927038, test loss: 0.2726284836820567\n",
      "epoch 2765: train loss: 0.17188575867057082, test loss: 0.2726206926946628\n",
      "epoch 2766: train loss: 0.1718656962363794, test loss: 0.2726129085246943\n",
      "epoch 2767: train loss: 0.1718456448363731, test loss: 0.27260513116463747\n",
      "epoch 2768: train loss: 0.17182560446024275, test loss: 0.2725973606069886\n",
      "epoch 2769: train loss: 0.17180557509769268, test loss: 0.27258959684425405\n",
      "epoch 2770: train loss: 0.1717855567384409, test loss: 0.2725818398689505\n",
      "epoch 2771: train loss: 0.1717655493722189, test loss: 0.27257408967360464\n",
      "epoch 2772: train loss: 0.17174555298877187, test loss: 0.27256634625075304\n",
      "epoch 2773: train loss: 0.17172556757785837, test loss: 0.27255860959294265\n",
      "epoch 2774: train loss: 0.17170559312925068, test loss: 0.27255087969273034\n",
      "epoch 2775: train loss: 0.17168562963273432, test loss: 0.2725431565426829\n",
      "epoch 2776: train loss: 0.17166567707810843, test loss: 0.2725354401353773\n",
      "epoch 2777: train loss: 0.1716457354551856, test loss: 0.27252773046340034\n",
      "epoch 2778: train loss: 0.17162580475379183, test loss: 0.27252002751934906\n",
      "epoch 2779: train loss: 0.17160588496376636, test loss: 0.2725123312958302\n",
      "epoch 2780: train loss: 0.17158597607496212, test loss: 0.27250464178546063\n",
      "epoch 2781: train loss: 0.17156607807724503, test loss: 0.27249695898086707\n",
      "epoch 2782: train loss: 0.17154619096049456, test loss: 0.27248928287468616\n",
      "epoch 2783: train loss: 0.17152631471460345, test loss: 0.27248161345956445\n",
      "epoch 2784: train loss: 0.17150644932947767, test loss: 0.2724739507281584\n",
      "epoch 2785: train loss: 0.17148659479503656, test loss: 0.2724662946731344\n",
      "epoch 2786: train loss: 0.1714667511012125, test loss: 0.2724586452871685\n",
      "epoch 2787: train loss: 0.17144691823795136, test loss: 0.2724510025629467\n",
      "epoch 2788: train loss: 0.17142709619521185, test loss: 0.2724433664931649\n",
      "epoch 2789: train loss: 0.17140728496296617, test loss: 0.27243573707052876\n",
      "epoch 2790: train loss: 0.1713874845311995, test loss: 0.2724281142877536\n",
      "epoch 2791: train loss: 0.17136769488991022, test loss: 0.27242049813756475\n",
      "epoch 2792: train loss: 0.17134791602910973, test loss: 0.27241288861269697\n",
      "epoch 2793: train loss: 0.17132814793882262, test loss: 0.27240528570589506\n",
      "epoch 2794: train loss: 0.17130839060908634, test loss: 0.27239768940991343\n",
      "epoch 2795: train loss: 0.17128864402995161, test loss: 0.27239009971751627\n",
      "epoch 2796: train loss: 0.17126890819148208, test loss: 0.2723825166214773\n",
      "epoch 2797: train loss: 0.1712491830837543, test loss: 0.27237494011458\n",
      "epoch 2798: train loss: 0.17122946869685787, test loss: 0.2723673701896177\n",
      "epoch 2799: train loss: 0.17120976502089533, test loss: 0.2723598068393931\n",
      "epoch 2800: train loss: 0.1711900720459822, test loss: 0.2723522500567186\n",
      "epoch 2801: train loss: 0.1711703897622467, test loss: 0.27234469983441634\n",
      "epoch 2802: train loss: 0.1711507181598302, test loss: 0.27233715616531795\n",
      "epoch 2803: train loss: 0.17113105722888677, test loss: 0.2723296190422646\n",
      "epoch 2804: train loss: 0.17111140695958332, test loss: 0.27232208845810724\n",
      "epoch 2805: train loss: 0.1710917673420996, test loss: 0.2723145644057061\n",
      "epoch 2806: train loss: 0.1710721383666282, test loss: 0.27230704687793117\n",
      "epoch 2807: train loss: 0.17105252002337445, test loss: 0.27229953586766187\n",
      "epoch 2808: train loss: 0.17103291230255638, test loss: 0.27229203136778707\n",
      "epoch 2809: train loss: 0.17101331519440474, test loss: 0.2722845333712052\n",
      "epoch 2810: train loss: 0.17099372868916313, test loss: 0.2722770418708242\n",
      "epoch 2811: train loss: 0.1709741527770877, test loss: 0.27226955685956133\n",
      "epoch 2812: train loss: 0.1709545874484473, test loss: 0.2722620783303436\n",
      "epoch 2813: train loss: 0.17093503269352348, test loss: 0.27225460627610704\n",
      "epoch 2814: train loss: 0.17091548850261026, test loss: 0.27224714068979744\n",
      "epoch 2815: train loss: 0.1708959548660144, test loss: 0.27223968156436973\n",
      "epoch 2816: train loss: 0.17087643177405526, test loss: 0.2722322288927884\n",
      "epoch 2817: train loss: 0.17085691921706464, test loss: 0.27222478266802724\n",
      "epoch 2818: train loss: 0.17083741718538692, test loss: 0.2722173428830695\n",
      "epoch 2819: train loss: 0.1708179256693791, test loss: 0.27220990953090757\n",
      "epoch 2820: train loss: 0.17079844465941044, test loss: 0.27220248260454327\n",
      "epoch 2821: train loss: 0.17077897414586296, test loss: 0.27219506209698774\n",
      "epoch 2822: train loss: 0.17075951411913087, test loss: 0.27218764800126144\n",
      "epoch 2823: train loss: 0.17074006456962107, test loss: 0.2721802403103941\n",
      "epoch 2824: train loss: 0.17072062548775263, test loss: 0.2721728390174246\n",
      "epoch 2825: train loss: 0.1707011968639571, test loss: 0.2721654441154013\n",
      "epoch 2826: train loss: 0.17068177868867845, test loss: 0.27215805559738143\n",
      "epoch 2827: train loss: 0.170662370952373, test loss: 0.2721506734564319\n",
      "epoch 2828: train loss: 0.17064297364550923, test loss: 0.2721432976856284\n",
      "epoch 2829: train loss: 0.17062358675856817, test loss: 0.27213592827805605\n",
      "epoch 2830: train loss: 0.17060421028204295, test loss: 0.27212856522680917\n",
      "epoch 2831: train loss: 0.1705848442064391, test loss: 0.2721212085249911\n",
      "epoch 2832: train loss: 0.17056548852227427, test loss: 0.2721138581657143\n",
      "epoch 2833: train loss: 0.17054614322007844, test loss: 0.27210651414210063\n",
      "epoch 2834: train loss: 0.17052680829039363, test loss: 0.2720991764472808\n",
      "epoch 2835: train loss: 0.17050748372377433, test loss: 0.27209184507439466\n",
      "epoch 2836: train loss: 0.1704881695107869, test loss: 0.2720845200165912\n",
      "epoch 2837: train loss: 0.17046886564201003, test loss: 0.2720772012670286\n",
      "epoch 2838: train loss: 0.17044957210803444, test loss: 0.27206988881887384\n",
      "epoch 2839: train loss: 0.170430288899463, test loss: 0.27206258266530314\n",
      "epoch 2840: train loss: 0.1704110160069105, test loss: 0.27205528279950164\n",
      "epoch 2841: train loss: 0.17039175342100413, test loss: 0.27204798921466355\n",
      "epoch 2842: train loss: 0.1703725011323829, test loss: 0.2720407019039921\n",
      "epoch 2843: train loss: 0.17035325913169772, test loss: 0.27203342086069954\n",
      "epoch 2844: train loss: 0.1703340274096118, test loss: 0.27202614607800685\n",
      "epoch 2845: train loss: 0.1703148059568001, test loss: 0.27201887754914433\n",
      "epoch 2846: train loss: 0.1702955947639496, test loss: 0.2720116152673509\n",
      "epoch 2847: train loss: 0.17027639382175927, test loss: 0.2720043592258747\n",
      "epoch 2848: train loss: 0.17025720312093995, test loss: 0.27199710941797245\n",
      "epoch 2849: train loss: 0.1702380226522145, test loss: 0.27198986583691004\n",
      "epoch 2850: train loss: 0.1702188524063174, test loss: 0.2719826284759623\n",
      "epoch 2851: train loss: 0.1701996923739952, test loss: 0.27197539732841247\n",
      "epoch 2852: train loss: 0.17018054254600634, test loss: 0.2719681723875533\n",
      "epoch 2853: train loss: 0.17016140291312096, test loss: 0.2719609536466858\n",
      "epoch 2854: train loss: 0.17014227346612096, test loss: 0.2719537410991202\n",
      "epoch 2855: train loss: 0.17012315419580012, test loss: 0.27194653473817526\n",
      "epoch 2856: train loss: 0.170104045092964, test loss: 0.2719393345571789\n",
      "epoch 2857: train loss: 0.17008494614842984, test loss: 0.2719321405494674\n",
      "epoch 2858: train loss: 0.17006585735302657, test loss: 0.2719249527083862\n",
      "epoch 2859: train loss: 0.17004677869759496, test loss: 0.2719177710272892\n",
      "epoch 2860: train loss: 0.17002771017298735, test loss: 0.2719105954995392\n",
      "epoch 2861: train loss: 0.1700086517700678, test loss: 0.27190342611850765\n",
      "epoch 2862: train loss: 0.16998960347971198, test loss: 0.27189626287757496\n",
      "epoch 2863: train loss: 0.16997056529280719, test loss: 0.27188910577012987\n",
      "epoch 2864: train loss: 0.16995153720025236, test loss: 0.27188195478957006\n",
      "epoch 2865: train loss: 0.16993251919295807, test loss: 0.27187480992930174\n",
      "epoch 2866: train loss: 0.1699135112618463, test loss: 0.27186767118273997\n",
      "epoch 2867: train loss: 0.16989451339785078, test loss: 0.2718605385433083\n",
      "epoch 2868: train loss: 0.16987552559191654, test loss: 0.27185341200443897\n",
      "epoch 2869: train loss: 0.16985654783500045, test loss: 0.2718462915595728\n",
      "epoch 2870: train loss: 0.1698375801180705, test loss: 0.27183917720215933\n",
      "epoch 2871: train loss: 0.16981862243210644, test loss: 0.2718320689256565\n",
      "epoch 2872: train loss: 0.1697996747680993, test loss: 0.271824966723531\n",
      "epoch 2873: train loss: 0.16978073711705163, test loss: 0.27181787058925805\n",
      "epoch 2874: train loss: 0.16976180946997743, test loss: 0.2718107805163214\n",
      "epoch 2875: train loss: 0.16974289181790206, test loss: 0.27180369649821334\n",
      "epoch 2876: train loss: 0.16972398415186213, test loss: 0.27179661852843473\n",
      "epoch 2877: train loss: 0.16970508646290586, test loss: 0.27178954660049487\n",
      "epoch 2878: train loss: 0.16968619874209265, test loss: 0.2717824807079115\n",
      "epoch 2879: train loss: 0.1696673209804932, test loss: 0.27177542084421114\n",
      "epoch 2880: train loss: 0.16964845316918964, test loss: 0.27176836700292833\n",
      "epoch 2881: train loss: 0.16962959529927532, test loss: 0.2717613191776065\n",
      "epoch 2882: train loss: 0.1696107473618548, test loss: 0.2717542773617973\n",
      "epoch 2883: train loss: 0.169591909348044, test loss: 0.2717472415490609\n",
      "epoch 2884: train loss: 0.16957308124896997, test loss: 0.27174021173296575\n",
      "epoch 2885: train loss: 0.16955426305577107, test loss: 0.2717331879070888\n",
      "epoch 2886: train loss: 0.16953545475959675, test loss: 0.2717261700650155\n",
      "epoch 2887: train loss: 0.16951665635160773, test loss: 0.27171915820033943\n",
      "epoch 2888: train loss: 0.1694978678229758, test loss: 0.27171215230666285\n",
      "epoch 2889: train loss: 0.16947908916488402, test loss: 0.271705152377596\n",
      "epoch 2890: train loss: 0.16946032036852635, test loss: 0.2716981584067579\n",
      "epoch 2891: train loss: 0.16944156142510813, test loss: 0.2716911703877754\n",
      "epoch 2892: train loss: 0.1694228123258455, test loss: 0.2716841883142841\n",
      "epoch 2893: train loss: 0.1694040730619659, test loss: 0.2716772121799277\n",
      "epoch 2894: train loss: 0.16938534362470764, test loss: 0.2716702419783581\n",
      "epoch 2895: train loss: 0.16936662400532027, test loss: 0.2716632777032357\n",
      "epoch 2896: train loss: 0.16934791419506412, test loss: 0.2716563193482289\n",
      "epoch 2897: train loss: 0.16932921418521063, test loss: 0.27164936690701474\n",
      "epoch 2898: train loss: 0.16931052396704224, test loss: 0.271642420373278\n",
      "epoch 2899: train loss: 0.16929184353185225, test loss: 0.27163547974071206\n",
      "epoch 2900: train loss: 0.16927317287094495, test loss: 0.2716285450030183\n",
      "epoch 2901: train loss: 0.16925451197563568, test loss: 0.2716216161539064\n",
      "epoch 2902: train loss: 0.16923586083725048, test loss: 0.2716146931870942\n",
      "epoch 2903: train loss: 0.16921721944712634, test loss: 0.2716077760963078\n",
      "epoch 2904: train loss: 0.16919858779661115, test loss: 0.27160086487528123\n",
      "epoch 2905: train loss: 0.1691799658770637, test loss: 0.27159395951775683\n",
      "epoch 2906: train loss: 0.16916135367985347, test loss: 0.27158706001748517\n",
      "epoch 2907: train loss: 0.16914275119636085, test loss: 0.2715801663682247\n",
      "epoch 2908: train loss: 0.16912415841797704, test loss: 0.2715732785637421\n",
      "epoch 2909: train loss: 0.16910557533610399, test loss: 0.2715663965978121\n",
      "epoch 2910: train loss: 0.16908700194215437, test loss: 0.2715595204642176\n",
      "epoch 2911: train loss: 0.16906843822755172, test loss: 0.27155265015674956\n",
      "epoch 2912: train loss: 0.16904988418373013, test loss: 0.2715457856692069\n",
      "epoch 2913: train loss: 0.16903133980213458, test loss: 0.27153892699539667\n",
      "epoch 2914: train loss: 0.1690128050742206, test loss: 0.2715320741291338\n",
      "epoch 2915: train loss: 0.16899427999145444, test loss: 0.2715252270642416\n",
      "epoch 2916: train loss: 0.16897576454531316, test loss: 0.27151838579455095\n",
      "epoch 2917: train loss: 0.16895725872728418, test loss: 0.27151155031390095\n",
      "epoch 2918: train loss: 0.16893876252886564, test loss: 0.2715047206161387\n",
      "epoch 2919: train loss: 0.16892027594156644, test loss: 0.27149789669511915\n",
      "epoch 2920: train loss: 0.16890179895690594, test loss: 0.27149107854470533\n",
      "epoch 2921: train loss: 0.16888333156641408, test loss: 0.2714842661587682\n",
      "epoch 2922: train loss: 0.1688648737616313, test loss: 0.2714774595311865\n",
      "epoch 2923: train loss: 0.16884642553410878, test loss: 0.27147065865584713\n",
      "epoch 2924: train loss: 0.16882798687540795, test loss: 0.27146386352664464\n",
      "epoch 2925: train loss: 0.16880955777710085, test loss: 0.27145707413748166\n",
      "epoch 2926: train loss: 0.16879113823077016, test loss: 0.2714502904822687\n",
      "epoch 2927: train loss: 0.1687727282280088, test loss: 0.271443512554924\n",
      "epoch 2928: train loss: 0.16875432776042026, test loss: 0.2714367403493738\n",
      "epoch 2929: train loss: 0.16873593681961843, test loss: 0.27142997385955214\n",
      "epoch 2930: train loss: 0.16871755539722766, test loss: 0.2714232130794008\n",
      "epoch 2931: train loss: 0.16869918348488264, test loss: 0.2714164580028695\n",
      "epoch 2932: train loss: 0.16868082107422847, test loss: 0.2714097086239159\n",
      "epoch 2933: train loss: 0.16866246815692063, test loss: 0.271402964936505\n",
      "epoch 2934: train loss: 0.16864412472462498, test loss: 0.27139622693461013\n",
      "epoch 2935: train loss: 0.1686257907690176, test loss: 0.27138949461221196\n",
      "epoch 2936: train loss: 0.1686074662817851, test loss: 0.27138276796329924\n",
      "epoch 2937: train loss: 0.1685891512546241, test loss: 0.2713760469818683\n",
      "epoch 2938: train loss: 0.16857084567924174, test loss: 0.2713693316619231\n",
      "epoch 2939: train loss: 0.16855254954735538, test loss: 0.2713626219974757\n",
      "epoch 2940: train loss: 0.1685342628506925, test loss: 0.27135591798254544\n",
      "epoch 2941: train loss: 0.16851598558099098, test loss: 0.27134921961115965\n",
      "epoch 2942: train loss: 0.16849771772999886, test loss: 0.2713425268773532\n",
      "epoch 2943: train loss: 0.16847945928947433, test loss: 0.27133583977516873\n",
      "epoch 2944: train loss: 0.16846121025118577, test loss: 0.27132915829865656\n",
      "epoch 2945: train loss: 0.1684429706069119, test loss: 0.27132248244187457\n",
      "epoch 2946: train loss: 0.1684247403484413, test loss: 0.2713158121988883\n",
      "epoch 2947: train loss: 0.16840651946757298, test loss: 0.2713091475637709\n",
      "epoch 2948: train loss: 0.16838830795611584, test loss: 0.2713024885306033\n",
      "epoch 2949: train loss: 0.16837010580588901, test loss: 0.2712958350934738\n",
      "epoch 2950: train loss: 0.1683519130087217, test loss: 0.2712891872464786\n",
      "epoch 2951: train loss: 0.16833372955645312, test loss: 0.27128254498372106\n",
      "epoch 2952: train loss: 0.1683155554409326, test loss: 0.27127590829931253\n",
      "epoch 2953: train loss: 0.16829739065401944, test loss: 0.2712692771873717\n",
      "epoch 2954: train loss: 0.16827923518758311, test loss: 0.2712626516420248\n",
      "epoch 2955: train loss: 0.1682610890335029, test loss: 0.27125603165740564\n",
      "epoch 2956: train loss: 0.16824295218366822, test loss: 0.2712494172276556\n",
      "epoch 2957: train loss: 0.16822482462997834, test loss: 0.27124280834692355\n",
      "epoch 2958: train loss: 0.16820670636434265, test loss: 0.2712362050093658\n",
      "epoch 2959: train loss: 0.1681885973786804, test loss: 0.2712296072091462\n",
      "epoch 2960: train loss: 0.16817049766492068, test loss: 0.27122301494043605\n",
      "epoch 2961: train loss: 0.1681524072150026, test loss: 0.27121642819741415\n",
      "epoch 2962: train loss: 0.16813432602087514, test loss: 0.2712098469742668\n",
      "epoch 2963: train loss: 0.16811625407449718, test loss: 0.2712032712651877\n",
      "epoch 2964: train loss: 0.16809819136783738, test loss: 0.2711967010643778\n",
      "epoch 2965: train loss: 0.16808013789287435, test loss: 0.2711901363660459\n",
      "epoch 2966: train loss: 0.1680620936415965, test loss: 0.2711835771644078\n",
      "epoch 2967: train loss: 0.168044058606002, test loss: 0.27117702345368677\n",
      "epoch 2968: train loss: 0.16802603277809883, test loss: 0.27117047522811366\n",
      "epoch 2969: train loss: 0.16800801614990488, test loss: 0.27116393248192666\n",
      "epoch 2970: train loss: 0.16799000871344766, test loss: 0.2711573952093712\n",
      "epoch 2971: train loss: 0.16797201046076446, test loss: 0.27115086340469996\n",
      "epoch 2972: train loss: 0.16795402138390242, test loss: 0.2711443370621733\n",
      "epoch 2973: train loss: 0.16793604147491828, test loss: 0.27113781617605864\n",
      "epoch 2974: train loss: 0.1679180707258785, test loss: 0.27113130074063085\n",
      "epoch 2975: train loss: 0.16790010912885933, test loss: 0.27112479075017204\n",
      "epoch 2976: train loss: 0.1678821566759465, test loss: 0.27111828619897166\n",
      "epoch 2977: train loss: 0.16786421335923563, test loss: 0.2711117870813264\n",
      "epoch 2978: train loss: 0.16784627917083192, test loss: 0.27110529339154027\n",
      "epoch 2979: train loss: 0.16782835410285002, test loss: 0.2710988051239245\n",
      "epoch 2980: train loss: 0.16781043814741442, test loss: 0.27109232227279767\n",
      "epoch 2981: train loss: 0.16779253129665914, test loss: 0.27108584483248543\n",
      "epoch 2982: train loss: 0.16777463354272773, test loss: 0.27107937279732086\n",
      "epoch 2983: train loss: 0.16775674487777337, test loss: 0.27107290616164403\n",
      "epoch 2984: train loss: 0.16773886529395873, test loss: 0.27106644491980253\n",
      "epoch 2985: train loss: 0.16772099478345617, test loss: 0.27105998906615086\n",
      "epoch 2986: train loss: 0.16770313333844733, test loss: 0.2710535385950508\n",
      "epoch 2987: train loss: 0.16768528095112356, test loss: 0.2710470935008714\n",
      "epoch 2988: train loss: 0.16766743761368566, test loss: 0.27104065377798875\n",
      "epoch 2989: train loss: 0.16764960331834383, test loss: 0.2710342194207861\n",
      "epoch 2990: train loss: 0.16763177805731783, test loss: 0.27102779042365394\n",
      "epoch 2991: train loss: 0.16761396182283683, test loss: 0.2710213667809898\n",
      "epoch 2992: train loss: 0.1675961546071394, test loss: 0.27101494848719837\n",
      "epoch 2993: train loss: 0.1675783564024735, test loss: 0.27100853553669146\n",
      "epoch 2994: train loss: 0.1675605672010967, test loss: 0.27100212792388795\n",
      "epoch 2995: train loss: 0.16754278699527567, test loss: 0.27099572564321384\n",
      "epoch 2996: train loss: 0.16752501577728665, test loss: 0.2709893286891022\n",
      "epoch 2997: train loss: 0.16750725353941523, test loss: 0.2709829370559931\n",
      "epoch 2998: train loss: 0.16748950027395623, test loss: 0.2709765507383338\n",
      "epoch 2999: train loss: 0.1674717559732139, test loss: 0.2709701697305785\n",
      "epoch 3000: train loss: 0.16745402062950177, test loss: 0.27096379402718845\n",
      "epoch 3001: train loss: 0.16743629423514267, test loss: 0.270957423622632\n",
      "epoch 3002: train loss: 0.16741857678246866, test loss: 0.2709510585113844\n",
      "epoch 3003: train loss: 0.16740086826382125, test loss: 0.2709446986879279\n",
      "epoch 3004: train loss: 0.16738316867155098, test loss: 0.27093834414675194\n",
      "epoch 3005: train loss: 0.1673654779980178, test loss: 0.2709319948823527\n",
      "epoch 3006: train loss: 0.16734779623559082, test loss: 0.2709256508892336\n",
      "epoch 3007: train loss: 0.16733012337664832, test loss: 0.2709193121619045\n",
      "epoch 3008: train loss: 0.16731245941357784, test loss: 0.27091297869488296\n",
      "epoch 3009: train loss: 0.16729480433877608, test loss: 0.2709066504826929\n",
      "epoch 3010: train loss: 0.16727715814464894, test loss: 0.27090032751986537\n",
      "epoch 3011: train loss: 0.16725952082361148, test loss: 0.27089400980093825\n",
      "epoch 3012: train loss: 0.16724189236808779, test loss: 0.27088769732045653\n",
      "epoch 3013: train loss: 0.16722427277051125, test loss: 0.27088139007297196\n",
      "epoch 3014: train loss: 0.1672066620233242, test loss: 0.2708750880530431\n",
      "epoch 3015: train loss: 0.16718906011897822, test loss: 0.2708687912552355\n",
      "epoch 3016: train loss: 0.16717146704993385, test loss: 0.27086249967412157\n",
      "epoch 3017: train loss: 0.1671538828086608, test loss: 0.27085621330428056\n",
      "epoch 3018: train loss: 0.16713630738763768, test loss: 0.2708499321402986\n",
      "epoch 3019: train loss: 0.16711874077935238, test loss: 0.2708436561767686\n",
      "epoch 3020: train loss: 0.1671011829763016, test loss: 0.2708373854082902\n",
      "epoch 3021: train loss: 0.16708363397099113, test loss: 0.2708311198294701\n",
      "epoch 3022: train loss: 0.16706609375593579, test loss: 0.2708248594349217\n",
      "epoch 3023: train loss: 0.16704856232365933, test loss: 0.270818604219265\n",
      "epoch 3024: train loss: 0.16703103966669453, test loss: 0.2708123541771271\n",
      "epoch 3025: train loss: 0.16701352577758308, test loss: 0.27080610930314175\n",
      "epoch 3026: train loss: 0.16699602064887556, test loss: 0.27079986959194935\n",
      "epoch 3027: train loss: 0.16697852427313165, test loss: 0.27079363503819714\n",
      "epoch 3028: train loss: 0.16696103664291984, test loss: 0.27078740563653914\n",
      "epoch 3029: train loss: 0.1669435577508174, test loss: 0.2707811813816361\n",
      "epoch 3030: train loss: 0.1669260875894106, test loss: 0.27077496226815545\n",
      "epoch 3031: train loss: 0.16690862615129473, test loss: 0.27076874829077135\n",
      "epoch 3032: train loss: 0.16689117342907367, test loss: 0.27076253944416456\n",
      "epoch 3033: train loss: 0.16687372941536033, test loss: 0.2707563357230228\n",
      "epoch 3034: train loss: 0.16685629410277636, test loss: 0.27075013712204027\n",
      "epoch 3035: train loss: 0.16683886748395224, test loss: 0.2707439436359178\n",
      "epoch 3036: train loss: 0.1668214495515273, test loss: 0.270737755259363\n",
      "epoch 3037: train loss: 0.16680404029814958, test loss: 0.27073157198709014\n",
      "epoch 3038: train loss: 0.166786639716476, test loss: 0.27072539381382\n",
      "epoch 3039: train loss: 0.1667692477991722, test loss: 0.2707192207342802\n",
      "epoch 3040: train loss: 0.16675186453891253, test loss: 0.2707130527432048\n",
      "epoch 3041: train loss: 0.16673448992838008, test loss: 0.27070688983533453\n",
      "epoch 3042: train loss: 0.1667171239602667, test loss: 0.2707007320054167\n",
      "epoch 3043: train loss: 0.16669976662727293, test loss: 0.2706945792482054\n",
      "epoch 3044: train loss: 0.166682417922108, test loss: 0.270688431558461\n",
      "epoch 3045: train loss: 0.16666507783748982, test loss: 0.2706822889309506\n",
      "epoch 3046: train loss: 0.16664774636614502, test loss: 0.2706761513604479\n",
      "epoch 3047: train loss: 0.16663042350080876, test loss: 0.27067001884173325\n",
      "epoch 3048: train loss: 0.16661310923422498, test loss: 0.27066389136959307\n",
      "epoch 3049: train loss: 0.16659580355914616, test loss: 0.27065776893882093\n",
      "epoch 3050: train loss: 0.16657850646833341, test loss: 0.27065165154421655\n",
      "epoch 3051: train loss: 0.16656121795455653, test loss: 0.2706455391805863\n",
      "epoch 3052: train loss: 0.1665439380105937, test loss: 0.27063943184274303\n",
      "epoch 3053: train loss: 0.16652666662923188, test loss: 0.27063332952550595\n",
      "epoch 3054: train loss: 0.1665094038032665, test loss: 0.27062723222370094\n",
      "epoch 3055: train loss: 0.16649214952550154, test loss: 0.27062113993216036\n",
      "epoch 3056: train loss: 0.16647490378874955, test loss: 0.2706150526457229\n",
      "epoch 3057: train loss: 0.16645766658583155, test loss: 0.2706089703592338\n",
      "epoch 3058: train loss: 0.16644043790957713, test loss: 0.27060289306754476\n",
      "epoch 3059: train loss: 0.16642321775282426, test loss: 0.27059682076551383\n",
      "epoch 3060: train loss: 0.1664060061084195, test loss: 0.2705907534480055\n",
      "epoch 3061: train loss: 0.16638880296921793, test loss: 0.2705846911098908\n",
      "epoch 3062: train loss: 0.16637160832808293, test loss: 0.270578633746047\n",
      "epoch 3063: train loss: 0.16635442217788637, test loss: 0.270572581351358\n",
      "epoch 3064: train loss: 0.16633724451150858, test loss: 0.27056653392071384\n",
      "epoch 3065: train loss: 0.16632007532183835, test loss: 0.27056049144901106\n",
      "epoch 3066: train loss: 0.16630291460177282, test loss: 0.27055445393115257\n",
      "epoch 3067: train loss: 0.1662857623442175, test loss: 0.2705484213620476\n",
      "epoch 3068: train loss: 0.1662686185420863, test loss: 0.2705423937366118\n",
      "epoch 3069: train loss: 0.1662514831883015, test loss: 0.27053637104976713\n",
      "epoch 3070: train loss: 0.16623435627579375, test loss: 0.27053035329644187\n",
      "epoch 3071: train loss: 0.16621723779750203, test loss: 0.27052434047157065\n",
      "epoch 3072: train loss: 0.16620012774637352, test loss: 0.2705183325700943\n",
      "epoch 3073: train loss: 0.166183026115364, test loss: 0.27051232958696014\n",
      "epoch 3074: train loss: 0.16616593289743725, test loss: 0.27050633151712167\n",
      "epoch 3075: train loss: 0.16614884808556554, test loss: 0.27050033835553877\n",
      "epoch 3076: train loss: 0.1661317716727293, test loss: 0.2704943500971774\n",
      "epoch 3077: train loss: 0.16611470365191727, test loss: 0.2704883667370101\n",
      "epoch 3078: train loss: 0.16609764401612645, test loss: 0.27048238827001525\n",
      "epoch 3079: train loss: 0.166080592758362, test loss: 0.27047641469117795\n",
      "epoch 3080: train loss: 0.1660635498716375, test loss: 0.27047044599548914\n",
      "epoch 3081: train loss: 0.1660465153489744, test loss: 0.2704644821779462\n",
      "epoch 3082: train loss: 0.16602948918340274, test loss: 0.2704585232335528\n",
      "epoch 3083: train loss: 0.16601247136796052, test loss: 0.27045256915731847\n",
      "epoch 3084: train loss: 0.1659954618956939, test loss: 0.27044661994425945\n",
      "epoch 3085: train loss: 0.16597846075965728, test loss: 0.2704406755893977\n",
      "epoch 3086: train loss: 0.1659614679529132, test loss: 0.27043473608776175\n",
      "epoch 3087: train loss: 0.1659444834685323, test loss: 0.270428801434386\n",
      "epoch 3088: train loss: 0.1659275072995934, test loss: 0.2704228716243112\n",
      "epoch 3089: train loss: 0.1659105394391834, test loss: 0.2704169466525842\n",
      "epoch 3090: train loss: 0.1658935798803973, test loss: 0.2704110265142579\n",
      "epoch 3091: train loss: 0.16587662861633817, test loss: 0.2704051112043917\n",
      "epoch 3092: train loss: 0.16585968564011716, test loss: 0.27039920071805057\n",
      "epoch 3093: train loss: 0.16584275094485348, test loss: 0.27039329505030607\n",
      "epoch 3094: train loss: 0.16582582452367445, test loss: 0.2703873941962357\n",
      "epoch 3095: train loss: 0.16580890636971538, test loss: 0.270381498150923\n",
      "epoch 3096: train loss: 0.16579199647611959, test loss: 0.2703756069094577\n",
      "epoch 3097: train loss: 0.16577509483603836, test loss: 0.27036972046693564\n",
      "epoch 3098: train loss: 0.1657582014426312, test loss: 0.27036383881845866\n",
      "epoch 3099: train loss: 0.16574131628906524, test loss: 0.27035796195913464\n",
      "epoch 3100: train loss: 0.165724439368516, test loss: 0.2703520898840776\n",
      "epoch 3101: train loss: 0.1657075706741667, test loss: 0.27034622258840757\n",
      "epoch 3102: train loss: 0.16569071019920847, test loss: 0.2703403600672507\n",
      "epoch 3103: train loss: 0.1656738579368406, test loss: 0.27033450231573897\n",
      "epoch 3104: train loss: 0.16565701388027018, test loss: 0.27032864932901063\n",
      "epoch 3105: train loss: 0.1656401780227122, test loss: 0.2703228011022098\n",
      "epoch 3106: train loss: 0.16562335035738962, test loss: 0.2703169576304867\n",
      "epoch 3107: train loss: 0.16560653087753316, test loss: 0.2703111189089973\n",
      "epoch 3108: train loss: 0.16558971957638158, test loss: 0.2703052849329039\n",
      "epoch 3109: train loss: 0.16557291644718145, test loss: 0.2702994556973745\n",
      "epoch 3110: train loss: 0.1655561214831872, test loss: 0.2702936311975833\n",
      "epoch 3111: train loss: 0.16553933467766108, test loss: 0.27028781142871033\n",
      "epoch 3112: train loss: 0.1655225560238732, test loss: 0.2702819963859416\n",
      "epoch 3113: train loss: 0.16550578551510145, test loss: 0.2702761860644689\n",
      "epoch 3114: train loss: 0.16548902314463157, test loss: 0.2702703804594903\n",
      "epoch 3115: train loss: 0.1654722689057571, test loss: 0.27026457956620953\n",
      "epoch 3116: train loss: 0.1654555227917793, test loss: 0.27025878337983644\n",
      "epoch 3117: train loss: 0.1654387847960073, test loss: 0.2702529918955864\n",
      "epoch 3118: train loss: 0.16542205491175793, test loss: 0.27024720510868117\n",
      "epoch 3119: train loss: 0.16540533313235578, test loss: 0.27024142301434806\n",
      "epoch 3120: train loss: 0.16538861945113323, test loss: 0.27023564560782043\n",
      "epoch 3121: train loss: 0.16537191386143021, test loss: 0.27022987288433753\n",
      "epoch 3122: train loss: 0.16535521635659467, test loss: 0.2702241048391443\n",
      "epoch 3123: train loss: 0.16533852692998197, test loss: 0.2702183414674917\n",
      "epoch 3124: train loss: 0.16532184557495527, test loss: 0.2702125827646365\n",
      "epoch 3125: train loss: 0.16530517228488548, test loss: 0.2702068287258413\n",
      "epoch 3126: train loss: 0.16528850705315107, test loss: 0.2702010793463745\n",
      "epoch 3127: train loss: 0.1652718498731383, test loss: 0.27019533462151035\n",
      "epoch 3128: train loss: 0.16525520073824088, test loss: 0.2701895945465289\n",
      "epoch 3129: train loss: 0.16523855964186027, test loss: 0.2701838591167162\n",
      "epoch 3130: train loss: 0.16522192657740564, test loss: 0.2701781283273637\n",
      "epoch 3131: train loss: 0.16520530153829363, test loss: 0.27017240217376903\n",
      "epoch 3132: train loss: 0.16518868451794844, test loss: 0.2701666806512353\n",
      "epoch 3133: train loss: 0.1651720755098021, test loss: 0.2701609637550716\n",
      "epoch 3134: train loss: 0.1651554745072939, test loss: 0.27015525148059266\n",
      "epoch 3135: train loss: 0.16513888150387096, test loss: 0.2701495438231191\n",
      "epoch 3136: train loss: 0.16512229649298782, test loss: 0.2701438407779772\n",
      "epoch 3137: train loss: 0.1651057194681066, test loss: 0.27013814234049893\n",
      "epoch 3138: train loss: 0.16508915042269692, test loss: 0.27013244850602214\n",
      "epoch 3139: train loss: 0.16507258935023597, test loss: 0.27012675926989016\n",
      "epoch 3140: train loss: 0.16505603624420837, test loss: 0.2701210746274523\n",
      "epoch 3141: train loss: 0.16503949109810634, test loss: 0.2701153945740634\n",
      "epoch 3142: train loss: 0.1650229539054295, test loss: 0.2701097191050841\n",
      "epoch 3143: train loss: 0.165006424659685, test loss: 0.2701040482158807\n",
      "epoch 3144: train loss: 0.1649899033543874, test loss: 0.27009838190182506\n",
      "epoch 3145: train loss: 0.16497338998305877, test loss: 0.270092720158295\n",
      "epoch 3146: train loss: 0.16495688453922863, test loss: 0.27008706298067364\n",
      "epoch 3147: train loss: 0.16494038701643382, test loss: 0.27008141036435007\n",
      "epoch 3148: train loss: 0.1649238974082187, test loss: 0.2700757623047188\n",
      "epoch 3149: train loss: 0.16490741570813505, test loss: 0.27007011879718024\n",
      "epoch 3150: train loss: 0.16489094190974193, test loss: 0.2700644798371401\n",
      "epoch 3151: train loss: 0.16487447600660596, test loss: 0.2700588454200099\n",
      "epoch 3152: train loss: 0.16485801799230093, test loss: 0.2700532155412069\n",
      "epoch 3153: train loss: 0.16484156786040816, test loss: 0.27004759019615376\n",
      "epoch 3154: train loss: 0.16482512560451631, test loss: 0.2700419693802788\n",
      "epoch 3155: train loss: 0.16480869121822128, test loss: 0.27003635308901586\n",
      "epoch 3156: train loss: 0.1647922646951263, test loss: 0.2700307413178045\n",
      "epoch 3157: train loss: 0.16477584602884213, test loss: 0.27002513406209\n",
      "epoch 3158: train loss: 0.16475943521298653, test loss: 0.2700195313173227\n",
      "epoch 3159: train loss: 0.1647430322411848, test loss: 0.27001393307895893\n",
      "epoch 3160: train loss: 0.1647266371070694, test loss: 0.27000833934246043\n",
      "epoch 3161: train loss: 0.16471024980428017, test loss: 0.27000275010329455\n",
      "epoch 3162: train loss: 0.1646938703264641, test loss: 0.26999716535693413\n",
      "epoch 3163: train loss: 0.16467749866727543, test loss: 0.26999158509885746\n",
      "epoch 3164: train loss: 0.16466113482037578, test loss: 0.26998600932454847\n",
      "epoch 3165: train loss: 0.16464477877943395, test loss: 0.2699804380294965\n",
      "epoch 3166: train loss: 0.16462843053812584, test loss: 0.2699748712091965\n",
      "epoch 3167: train loss: 0.1646120900901347, test loss: 0.2699693088591488\n",
      "epoch 3168: train loss: 0.16459575742915092, test loss: 0.26996375097485936\n",
      "epoch 3169: train loss: 0.1645794325488722, test loss: 0.2699581975518395\n",
      "epoch 3170: train loss: 0.16456311544300312, test loss: 0.26995264858560597\n",
      "epoch 3171: train loss: 0.1645468061052558, test loss: 0.2699471040716812\n",
      "epoch 3172: train loss: 0.16453050452934928, test loss: 0.2699415640055929\n",
      "epoch 3173: train loss: 0.16451421070900976, test loss: 0.26993602838287417\n",
      "epoch 3174: train loss: 0.1644979246379707, test loss: 0.26993049719906376\n",
      "epoch 3175: train loss: 0.16448164630997256, test loss: 0.26992497044970565\n",
      "epoch 3176: train loss: 0.16446537571876302, test loss: 0.26991944813034935\n",
      "epoch 3177: train loss: 0.16444911285809677, test loss: 0.26991393023654986\n",
      "epoch 3178: train loss: 0.16443285772173566, test loss: 0.26990841676386734\n",
      "epoch 3179: train loss: 0.16441661030344862, test loss: 0.26990290770786773\n",
      "epoch 3180: train loss: 0.16440037059701154, test loss: 0.2698974030641219\n",
      "epoch 3181: train loss: 0.1643841385962076, test loss: 0.2698919028282065\n",
      "epoch 3182: train loss: 0.1643679142948268, test loss: 0.2698864069957034\n",
      "epoch 3183: train loss: 0.16435169768666633, test loss: 0.2698809155621998\n",
      "epoch 3184: train loss: 0.1643354887655304, test loss: 0.2698754285232883\n",
      "epoch 3185: train loss: 0.1643192875252301, test loss: 0.26986994587456686\n",
      "epoch 3186: train loss: 0.16430309395958376, test loss: 0.26986446761163885\n",
      "epoch 3187: train loss: 0.16428690806241647, test loss: 0.2698589937301129\n",
      "epoch 3188: train loss: 0.16427072982756055, test loss: 0.269853524225603\n",
      "epoch 3189: train loss: 0.16425455924885504, test loss: 0.26984805909372833\n",
      "epoch 3190: train loss: 0.16423839632014625, test loss: 0.2698425983301137\n",
      "epoch 3191: train loss: 0.16422224103528713, test loss: 0.26983714193038894\n",
      "epoch 3192: train loss: 0.1642060933881379, test loss: 0.2698316898901894\n",
      "epoch 3193: train loss: 0.1641899533725654, test loss: 0.2698262422051554\n",
      "epoch 3194: train loss: 0.1641738209824436, test loss: 0.2698207988709329\n",
      "epoch 3195: train loss: 0.16415769621165338, test loss: 0.26981535988317296\n",
      "epoch 3196: train loss: 0.16414157905408253, test loss: 0.26980992523753194\n",
      "epoch 3197: train loss: 0.1641254695036256, test loss: 0.26980449492967146\n",
      "epoch 3198: train loss: 0.16410936755418418, test loss: 0.26979906895525846\n",
      "epoch 3199: train loss: 0.16409327319966663, test loss: 0.26979364730996497\n",
      "epoch 3200: train loss: 0.1640771864339884, test loss: 0.26978822998946844\n",
      "epoch 3201: train loss: 0.1640611072510714, test loss: 0.26978281698945145\n",
      "epoch 3202: train loss: 0.16404503564484474, test loss: 0.2697774083056019\n",
      "epoch 3203: train loss: 0.16402897160924426, test loss: 0.26977200393361267\n",
      "epoch 3204: train loss: 0.16401291513821264, test loss: 0.2697666038691823\n",
      "epoch 3205: train loss: 0.16399686622569923, test loss: 0.2697612081080139\n",
      "epoch 3206: train loss: 0.1639808248656604, test loss: 0.2697558166458164\n",
      "epoch 3207: train loss: 0.1639647910520592, test loss: 0.2697504294783036\n",
      "epoch 3208: train loss: 0.16394876477886555, test loss: 0.2697450466011944\n",
      "epoch 3209: train loss: 0.163932746040056, test loss: 0.2697396680102132\n",
      "epoch 3210: train loss: 0.16391673482961402, test loss: 0.2697342937010893\n",
      "epoch 3211: train loss: 0.16390073114152975, test loss: 0.2697289236695572\n",
      "epoch 3212: train loss: 0.16388473496980013, test loss: 0.2697235579113566\n",
      "epoch 3213: train loss: 0.1638687463084288, test loss: 0.2697181964222323\n",
      "epoch 3214: train loss: 0.16385276515142608, test loss: 0.26971283919793443\n",
      "epoch 3215: train loss: 0.1638367914928092, test loss: 0.2697074862342179\n",
      "epoch 3216: train loss: 0.16382082532660186, test loss: 0.269702137526843\n",
      "epoch 3217: train loss: 0.1638048666468346, test loss: 0.26969679307157507\n",
      "epoch 3218: train loss: 0.1637889154475446, test loss: 0.2696914528641846\n",
      "epoch 3219: train loss: 0.16377297172277575, test loss: 0.269686116900447\n",
      "epoch 3220: train loss: 0.16375703546657863, test loss: 0.2696807851761431\n",
      "epoch 3221: train loss: 0.16374110667301042, test loss: 0.2696754576870585\n",
      "epoch 3222: train loss: 0.16372518533613495, test loss: 0.2696701344289841\n",
      "epoch 3223: train loss: 0.1637092714500227, test loss: 0.2696648153977156\n",
      "epoch 3224: train loss: 0.16369336500875079, test loss: 0.2696595005890541\n",
      "epoch 3225: train loss: 0.16367746600640307, test loss: 0.26965418999880564\n",
      "epoch 3226: train loss: 0.16366157443706975, test loss: 0.2696488836227811\n",
      "epoch 3227: train loss: 0.1636456902948479, test loss: 0.26964358145679673\n",
      "epoch 3228: train loss: 0.16362981357384102, test loss: 0.2696382834966737\n",
      "epoch 3229: train loss: 0.16361394426815923, test loss: 0.26963298973823796\n",
      "epoch 3230: train loss: 0.16359808237191928, test loss: 0.26962770017732085\n",
      "epoch 3231: train loss: 0.1635822278792444, test loss: 0.26962241480975857\n",
      "epoch 3232: train loss: 0.16356638078426447, test loss: 0.26961713363139234\n",
      "epoch 3233: train loss: 0.16355054108111575, test loss: 0.26961185663806825\n",
      "epoch 3234: train loss: 0.16353470876394124, test loss: 0.2696065838256376\n",
      "epoch 3235: train loss: 0.16351888382689028, test loss: 0.2696013151899566\n",
      "epoch 3236: train loss: 0.16350306626411892, test loss: 0.2695960507268864\n",
      "epoch 3237: train loss: 0.16348725606978953, test loss: 0.2695907904322931\n",
      "epoch 3238: train loss: 0.16347145323807105, test loss: 0.26958553430204785\n",
      "epoch 3239: train loss: 0.16345565776313897, test loss: 0.26958028233202674\n",
      "epoch 3240: train loss: 0.1634398696391751, test loss: 0.2695750345181107\n",
      "epoch 3241: train loss: 0.16342408886036794, test loss: 0.26956979085618576\n",
      "epoch 3242: train loss: 0.16340831542091222, test loss: 0.26956455134214286\n",
      "epoch 3243: train loss: 0.16339254931500932, test loss: 0.26955931597187777\n",
      "epoch 3244: train loss: 0.16337679053686693, test loss: 0.2695540847412912\n",
      "epoch 3245: train loss: 0.16336103908069924, test loss: 0.2695488576462889\n",
      "epoch 3246: train loss: 0.16334529494072675, test loss: 0.2695436346827815\n",
      "epoch 3247: train loss: 0.1633295581111765, test loss: 0.26953841584668436\n",
      "epoch 3248: train loss: 0.16331382858628193, test loss: 0.269533201133918\n",
      "epoch 3249: train loss: 0.16329810636028277, test loss: 0.26952799054040755\n",
      "epoch 3250: train loss: 0.16328239142742518, test loss: 0.26952278406208324\n",
      "epoch 3251: train loss: 0.16326668378196182, test loss: 0.2695175816948801\n",
      "epoch 3252: train loss: 0.1632509834181515, test loss: 0.26951238343473805\n",
      "epoch 3253: train loss: 0.1632352903302595, test loss: 0.26950718927760176\n",
      "epoch 3254: train loss: 0.16321960451255751, test loss: 0.2695019992194209\n",
      "epoch 3255: train loss: 0.16320392595932348, test loss: 0.2694968132561501\n",
      "epoch 3256: train loss: 0.16318825466484166, test loss: 0.2694916313837484\n",
      "epoch 3257: train loss: 0.1631725906234027, test loss: 0.26948645359818\n",
      "epoch 3258: train loss: 0.1631569338293035, test loss: 0.2694812798954139\n",
      "epoch 3259: train loss: 0.1631412842768473, test loss: 0.269476110271424\n",
      "epoch 3260: train loss: 0.16312564196034363, test loss: 0.2694709447221888\n",
      "epoch 3261: train loss: 0.1631100068741083, test loss: 0.26946578324369164\n",
      "epoch 3262: train loss: 0.16309437901246326, test loss: 0.2694606258319208\n",
      "epoch 3263: train loss: 0.16307875836973706, test loss: 0.26945547248286933\n",
      "epoch 3264: train loss: 0.1630631449402642, test loss: 0.269450323192535\n",
      "epoch 3265: train loss: 0.16304753871838554, test loss: 0.26944517795692036\n",
      "epoch 3266: train loss: 0.16303193969844818, test loss: 0.26944003677203276\n",
      "epoch 3267: train loss: 0.1630163478748054, test loss: 0.2694348996338843\n",
      "epoch 3268: train loss: 0.1630007632418168, test loss: 0.26942976653849193\n",
      "epoch 3269: train loss: 0.16298518579384813, test loss: 0.2694246374818771\n",
      "epoch 3270: train loss: 0.16296961552527128, test loss: 0.2694195124600664\n",
      "epoch 3271: train loss: 0.16295405243046449, test loss: 0.2694143914690909\n",
      "epoch 3272: train loss: 0.16293849650381204, test loss: 0.2694092745049863\n",
      "epoch 3273: train loss: 0.16292294773970448, test loss: 0.2694041615637933\n",
      "epoch 3274: train loss: 0.16290740613253843, test loss: 0.2693990526415572\n",
      "epoch 3275: train loss: 0.16289187167671684, test loss: 0.269393947734328\n",
      "epoch 3276: train loss: 0.1628763443666486, test loss: 0.26938884683816045\n",
      "epoch 3277: train loss: 0.1628608241967489, test loss: 0.26938374994911385\n",
      "epoch 3278: train loss: 0.162845311161439, test loss: 0.2693786570632525\n",
      "epoch 3279: train loss: 0.1628298052551463, test loss: 0.269373568176645\n",
      "epoch 3280: train loss: 0.16281430647230422, test loss: 0.269368483285365\n",
      "epoch 3281: train loss: 0.16279881480735248, test loss: 0.26936340238549056\n",
      "epoch 3282: train loss: 0.1627833302547368, test loss: 0.2693583254731046\n",
      "epoch 3283: train loss: 0.16276785280890885, test loss: 0.2693532525442945\n",
      "epoch 3284: train loss: 0.16275238246432663, test loss: 0.26934818359515256\n",
      "epoch 3285: train loss: 0.16273691921545402, test loss: 0.2693431186217754\n",
      "epoch 3286: train loss: 0.1627214630567611, test loss: 0.2693380576202645\n",
      "epoch 3287: train loss: 0.1627060139827239, test loss: 0.269333000586726\n",
      "epoch 3288: train loss: 0.1626905719878245, test loss: 0.26932794751727057\n",
      "epoch 3289: train loss: 0.1626751370665511, test loss: 0.26932289840801354\n",
      "epoch 3290: train loss: 0.1626597092133979, test loss: 0.26931785325507485\n",
      "epoch 3291: train loss: 0.16264428842286505, test loss: 0.269312812054579\n",
      "epoch 3292: train loss: 0.16262887468945877, test loss: 0.2693077748026552\n",
      "epoch 3293: train loss: 0.16261346800769125, test loss: 0.26930274149543726\n",
      "epoch 3294: train loss: 0.16259806837208074, test loss: 0.2692977121290634\n",
      "epoch 3295: train loss: 0.16258267577715146, test loss: 0.26929268669967654\n",
      "epoch 3296: train loss: 0.16256729021743355, test loss: 0.26928766520342434\n",
      "epoch 3297: train loss: 0.16255191168746308, test loss: 0.2692826476364587\n",
      "epoch 3298: train loss: 0.1625365401817822, test loss: 0.2692776339949364\n",
      "epoch 3299: train loss: 0.16252117569493899, test loss: 0.2692726242750186\n",
      "epoch 3300: train loss: 0.16250581822148746, test loss: 0.26926761847287106\n",
      "epoch 3301: train loss: 0.16249046775598747, test loss: 0.269262616584664\n",
      "epoch 3302: train loss: 0.16247512429300484, test loss: 0.2692576186065723\n",
      "epoch 3303: train loss: 0.16245978782711148, test loss: 0.26925262453477544\n",
      "epoch 3304: train loss: 0.162444458352885, test loss: 0.26924763436545723\n",
      "epoch 3305: train loss: 0.16242913586490895, test loss: 0.26924264809480614\n",
      "epoch 3306: train loss: 0.16241382035777285, test loss: 0.26923766571901503\n",
      "epoch 3307: train loss: 0.16239851182607204, test loss: 0.2692326872342814\n",
      "epoch 3308: train loss: 0.1623832102644078, test loss: 0.26922771263680717\n",
      "epoch 3309: train loss: 0.16236791566738717, test loss: 0.2692227419227989\n",
      "epoch 3310: train loss: 0.16235262802962316, test loss: 0.2692177750884674\n",
      "epoch 3311: train loss: 0.1623373473457346, test loss: 0.26921281213002807\n",
      "epoch 3312: train loss: 0.16232207361034606, test loss: 0.269207853043701\n",
      "epoch 3313: train loss: 0.1623068068180881, test loss: 0.2692028978257103\n",
      "epoch 3314: train loss: 0.16229154696359707, test loss: 0.269197946472285\n",
      "epoch 3315: train loss: 0.16227629404151503, test loss: 0.2691929989796583\n",
      "epoch 3316: train loss: 0.16226104804648997, test loss: 0.26918805534406803\n",
      "epoch 3317: train loss: 0.16224580897317561, test loss: 0.26918311556175634\n",
      "epoch 3318: train loss: 0.1622305768162316, test loss: 0.26917817962896984\n",
      "epoch 3319: train loss: 0.16221535157032307, test loss: 0.2691732475419596\n",
      "epoch 3320: train loss: 0.16220013323012133, test loss: 0.26916831929698115\n",
      "epoch 3321: train loss: 0.16218492179030314, test loss: 0.26916339489029445\n",
      "epoch 3322: train loss: 0.16216971724555113, test loss: 0.26915847431816375\n",
      "epoch 3323: train loss: 0.16215451959055371, test loss: 0.2691535575768578\n",
      "epoch 3324: train loss: 0.16213932882000506, test loss: 0.2691486446626498\n",
      "epoch 3325: train loss: 0.16212414492860497, test loss: 0.2691437355718173\n",
      "epoch 3326: train loss: 0.1621089679110591, test loss: 0.26913883030064223\n",
      "epoch 3327: train loss: 0.16209379776207875, test loss: 0.2691339288454109\n",
      "epoch 3328: train loss: 0.16207863447638093, test loss: 0.26912903120241405\n",
      "epoch 3329: train loss: 0.16206347804868837, test loss: 0.2691241373679467\n",
      "epoch 3330: train loss: 0.16204832847372955, test loss: 0.2691192473383084\n",
      "epoch 3331: train loss: 0.16203318574623854, test loss: 0.2691143611098028\n",
      "epoch 3332: train loss: 0.16201804986095517, test loss: 0.26910947867873825\n",
      "epoch 3333: train loss: 0.1620029208126249, test loss: 0.26910460004142706\n",
      "epoch 3334: train loss: 0.16198779859599888, test loss: 0.2690997251941863\n",
      "epoch 3335: train loss: 0.16197268320583383, test loss: 0.2690948541333371\n",
      "epoch 3336: train loss: 0.16195757463689234, test loss: 0.26908998685520497\n",
      "epoch 3337: train loss: 0.16194247288394234, test loss: 0.2690851233561197\n",
      "epoch 3338: train loss: 0.16192737794175763, test loss: 0.26908026363241555\n",
      "epoch 3339: train loss: 0.16191228980511752, test loss: 0.269075407680431\n",
      "epoch 3340: train loss: 0.16189720846880695, test loss: 0.2690705554965088\n",
      "epoch 3341: train loss: 0.16188213392761658, test loss: 0.26906570707699606\n",
      "epoch 3342: train loss: 0.16186706617634247, test loss: 0.26906086241824434\n",
      "epoch 3343: train loss: 0.1618520052097864, test loss: 0.269056021516609\n",
      "epoch 3344: train loss: 0.16183695102275575, test loss: 0.26905118436845027\n",
      "epoch 3345: train loss: 0.1618219036100634, test loss: 0.26904635097013235\n",
      "epoch 3346: train loss: 0.16180686296652788, test loss: 0.2690415213180238\n",
      "epoch 3347: train loss: 0.1617918290869733, test loss: 0.26903669540849723\n",
      "epoch 3348: train loss: 0.1617768019662291, test loss: 0.2690318732379298\n",
      "epoch 3349: train loss: 0.16176178159913057, test loss: 0.2690270548027029\n",
      "epoch 3350: train loss: 0.16174676798051837, test loss: 0.26902224009920195\n",
      "epoch 3351: train loss: 0.16173176110523876, test loss: 0.2690174291238169\n",
      "epoch 3352: train loss: 0.16171676096814344, test loss: 0.26901262187294156\n",
      "epoch 3353: train loss: 0.16170176756408966, test loss: 0.2690078183429745\n",
      "epoch 3354: train loss: 0.16168678088794028, test loss: 0.26900301853031783\n",
      "epoch 3355: train loss: 0.1616718009345635, test loss: 0.26899822243137866\n",
      "epoch 3356: train loss: 0.1616568276988331, test loss: 0.26899343004256765\n",
      "epoch 3357: train loss: 0.1616418611756283, test loss: 0.2689886413603001\n",
      "epoch 3358: train loss: 0.1616269013598339, test loss: 0.26898385638099526\n",
      "epoch 3359: train loss: 0.16161194824634006, test loss: 0.26897907510107666\n",
      "epoch 3360: train loss: 0.1615970018300424, test loss: 0.2689742975169721\n",
      "epoch 3361: train loss: 0.1615820621058421, test loss: 0.2689695236251135\n",
      "epoch 3362: train loss: 0.1615671290686457, test loss: 0.2689647534219369\n",
      "epoch 3363: train loss: 0.16155220271336518, test loss: 0.26895998690388256\n",
      "epoch 3364: train loss: 0.16153728303491796, test loss: 0.26895522406739486\n",
      "epoch 3365: train loss: 0.16152237002822697, test loss: 0.2689504649089226\n",
      "epoch 3366: train loss: 0.16150746368822042, test loss: 0.26894570942491847\n",
      "epoch 3367: train loss: 0.16149256400983195, test loss: 0.26894095761183934\n",
      "epoch 3368: train loss: 0.16147767098800073, test loss: 0.2689362094661462\n",
      "epoch 3369: train loss: 0.16146278461767125, test loss: 0.2689314649843045\n",
      "epoch 3370: train loss: 0.16144790489379324, test loss: 0.26892672416278324\n",
      "epoch 3371: train loss: 0.1614330318113221, test loss: 0.26892198699805614\n",
      "epoch 3372: train loss: 0.16141816536521836, test loss: 0.26891725348660067\n",
      "epoch 3373: train loss: 0.16140330555044802, test loss: 0.2689125236248986\n",
      "epoch 3374: train loss: 0.1613884523619824, test loss: 0.26890779740943566\n",
      "epoch 3375: train loss: 0.16137360579479818, test loss: 0.2689030748367019\n",
      "epoch 3376: train loss: 0.16135876584387743, test loss: 0.2688983559031912\n",
      "epoch 3377: train loss: 0.16134393250420748, test loss: 0.26889364060540183\n",
      "epoch 3378: train loss: 0.161329105770781, test loss: 0.26888892893983596\n",
      "epoch 3379: train loss: 0.16131428563859607, test loss: 0.26888422090299974\n",
      "epoch 3380: train loss: 0.16129947210265594, test loss: 0.26887951649140374\n",
      "epoch 3381: train loss: 0.16128466515796927, test loss: 0.26887481570156235\n",
      "epoch 3382: train loss: 0.16126986479954994, test loss: 0.2688701185299941\n",
      "epoch 3383: train loss: 0.1612550710224172, test loss: 0.26886542497322147\n",
      "epoch 3384: train loss: 0.16124028382159558, test loss: 0.26886073502777125\n",
      "epoch 3385: train loss: 0.16122550319211484, test loss: 0.268856048690174\n",
      "epoch 3386: train loss: 0.16121072912900997, test loss: 0.26885136595696446\n",
      "epoch 3387: train loss: 0.1611959616273213, test loss: 0.26884668682468155\n",
      "epoch 3388: train loss: 0.1611812006820944, test loss: 0.2688420112898679\n",
      "epoch 3389: train loss: 0.1611664462883801, test loss: 0.2688373393490704\n",
      "epoch 3390: train loss: 0.16115169844123436, test loss: 0.26883267099884\n",
      "epoch 3391: train loss: 0.16113695713571854, test loss: 0.26882800623573155\n",
      "epoch 3392: train loss: 0.16112222236689913, test loss: 0.2688233450563039\n",
      "epoch 3393: train loss: 0.16110749412984782, test loss: 0.26881868745711995\n",
      "epoch 3394: train loss: 0.1610927724196415, test loss: 0.26881403343474664\n",
      "epoch 3395: train loss: 0.16107805723136237, test loss: 0.2688093829857548\n",
      "epoch 3396: train loss: 0.1610633485600978, test loss: 0.2688047361067194\n",
      "epoch 3397: train loss: 0.16104864640094024, test loss: 0.26880009279421935\n",
      "epoch 3398: train loss: 0.1610339507489874, test loss: 0.26879545304483743\n",
      "epoch 3399: train loss: 0.16101926159934218, test loss: 0.2687908168551605\n",
      "epoch 3400: train loss: 0.1610045789471126, test loss: 0.26878618422177936\n",
      "epoch 3401: train loss: 0.16098990278741185, test loss: 0.2687815551412889\n",
      "epoch 3402: train loss: 0.16097523311535833, test loss: 0.26877692961028754\n",
      "epoch 3403: train loss: 0.16096056992607552, test loss: 0.2687723076253783\n",
      "epoch 3404: train loss: 0.1609459132146921, test loss: 0.26876768918316757\n",
      "epoch 3405: train loss: 0.16093126297634178, test loss: 0.26876307428026613\n",
      "epoch 3406: train loss: 0.1609166192061636, test loss: 0.2687584629132883\n",
      "epoch 3407: train loss: 0.1609019818993014, test loss: 0.26875385507885263\n",
      "epoch 3408: train loss: 0.16088735105090443, test loss: 0.26874925077358147\n",
      "epoch 3409: train loss: 0.16087272665612695, test loss: 0.2687446499941012\n",
      "epoch 3410: train loss: 0.1608581087101282, test loss: 0.2687400527370418\n",
      "epoch 3411: train loss: 0.16084349720807267, test loss: 0.26873545899903767\n",
      "epoch 3412: train loss: 0.16082889214512988, test loss: 0.2687308687767267\n",
      "epoch 3413: train loss: 0.16081429351647436, test loss: 0.26872628206675075\n",
      "epoch 3414: train loss: 0.16079970131728583, test loss: 0.26872169886575575\n",
      "epoch 3415: train loss: 0.16078511554274896, test loss: 0.2687171191703914\n",
      "epoch 3416: train loss: 0.16077053618805354, test loss: 0.2687125429773113\n",
      "epoch 3417: train loss: 0.16075596324839442, test loss: 0.2687079702831729\n",
      "epoch 3418: train loss: 0.16074139671897142, test loss: 0.2687034010846376\n",
      "epoch 3419: train loss: 0.1607268365949895, test loss: 0.2686988353783705\n",
      "epoch 3420: train loss: 0.1607122828716585, test loss: 0.2686942731610409\n",
      "epoch 3421: train loss: 0.16069773554419345, test loss: 0.26868971442932155\n",
      "epoch 3422: train loss: 0.16068319460781433, test loss: 0.2686851591798894\n",
      "epoch 3423: train loss: 0.160668660057746, test loss: 0.26868060740942495\n",
      "epoch 3424: train loss: 0.16065413188921857, test loss: 0.26867605911461284\n",
      "epoch 3425: train loss: 0.1606396100974669, test loss: 0.26867151429214126\n",
      "epoch 3426: train loss: 0.160625094677731, test loss: 0.2686669729387025\n",
      "epoch 3427: train loss: 0.16061058562525582, test loss: 0.26866243505099247\n",
      "epoch 3428: train loss: 0.16059608293529123, test loss: 0.2686579006257111\n",
      "epoch 3429: train loss: 0.1605815866030921, test loss: 0.2686533696595619\n",
      "epoch 3430: train loss: 0.16056709662391833, test loss: 0.2686488421492524\n",
      "epoch 3431: train loss: 0.16055261299303464, test loss: 0.26864431809149375\n",
      "epoch 3432: train loss: 0.1605381357057108, test loss: 0.2686397974830011\n",
      "epoch 3433: train loss: 0.16052366475722146, test loss: 0.2686352803204932\n",
      "epoch 3434: train loss: 0.16050920014284628, test loss: 0.26863076660069285\n",
      "epoch 3435: train loss: 0.16049474185786974, test loss: 0.26862625632032644\n",
      "epoch 3436: train loss: 0.1604802898975813, test loss: 0.2686217494761241\n",
      "epoch 3437: train loss: 0.16046584425727542, test loss: 0.26861724606481985\n",
      "epoch 3438: train loss: 0.16045140493225127, test loss: 0.2686127460831515\n",
      "epoch 3439: train loss: 0.16043697191781306, test loss: 0.2686082495278606\n",
      "epoch 3440: train loss: 0.16042254520926985, test loss: 0.26860375639569234\n",
      "epoch 3441: train loss: 0.16040812480193567, test loss: 0.2685992666833959\n",
      "epoch 3442: train loss: 0.16039371069112923, test loss: 0.268594780387724\n",
      "epoch 3443: train loss: 0.1603793028721743, test loss: 0.2685902975054331\n",
      "epoch 3444: train loss: 0.16036490134039952, test loss: 0.2685858180332837\n",
      "epoch 3445: train loss: 0.1603505060911382, test loss: 0.2685813419680397\n",
      "epoch 3446: train loss: 0.16033611711972873, test loss: 0.2685768693064689\n",
      "epoch 3447: train loss: 0.1603217344215142, test loss: 0.26857240004534266\n",
      "epoch 3448: train loss: 0.1603073579918426, test loss: 0.26856793418143643\n",
      "epoch 3449: train loss: 0.16029298782606677, test loss: 0.268563471711529\n",
      "epoch 3450: train loss: 0.16027862391954426, test loss: 0.26855901263240295\n",
      "epoch 3451: train loss: 0.16026426626763757, test loss: 0.26855455694084474\n",
      "epoch 3452: train loss: 0.16024991486571408, test loss: 0.2685501046336443\n",
      "epoch 3453: train loss: 0.16023556970914574, test loss: 0.2685456557075955\n",
      "epoch 3454: train loss: 0.16022123079330947, test loss: 0.26854121015949567\n",
      "epoch 3455: train loss: 0.16020689811358693, test loss: 0.26853676798614595\n",
      "epoch 3456: train loss: 0.16019257166536463, test loss: 0.2685323291843512\n",
      "epoch 3457: train loss: 0.16017825144403383, test loss: 0.2685278937509199\n",
      "epoch 3458: train loss: 0.16016393744499044, test loss: 0.26852346168266406\n",
      "epoch 3459: train loss: 0.16014962966363538, test loss: 0.26851903297639973\n",
      "epoch 3460: train loss: 0.16013532809537417, test loss: 0.2685146076289463\n",
      "epoch 3461: train loss: 0.1601210327356171, test loss: 0.26851018563712686\n",
      "epoch 3462: train loss: 0.16010674357977928, test loss: 0.2685057669977683\n",
      "epoch 3463: train loss: 0.1600924606232804, test loss: 0.26850135170770095\n",
      "epoch 3464: train loss: 0.16007818386154513, test loss: 0.26849693976375894\n",
      "epoch 3465: train loss: 0.16006391329000266, test loss: 0.26849253116278016\n",
      "epoch 3466: train loss: 0.16004964890408704, test loss: 0.2684881259016057\n",
      "epoch 3467: train loss: 0.16003539069923697, test loss: 0.2684837239770807\n",
      "epoch 3468: train loss: 0.1600211386708959, test loss: 0.2684793253860538\n",
      "epoch 3469: train loss: 0.16000689281451186, test loss: 0.2684749301253771\n",
      "epoch 3470: train loss: 0.15999265312553784, test loss: 0.26847053819190664\n",
      "epoch 3471: train loss: 0.15997841959943124, test loss: 0.2684661495825017\n",
      "epoch 3472: train loss: 0.1599641922316543, test loss: 0.26846176429402546\n",
      "epoch 3473: train loss: 0.15994997101767391, test loss: 0.26845738232334443\n",
      "epoch 3474: train loss: 0.1599357559529617, test loss: 0.26845300366732905\n",
      "epoch 3475: train loss: 0.1599215470329938, test loss: 0.2684486283228531\n",
      "epoch 3476: train loss: 0.15990734425325115, test loss: 0.26844425628679397\n",
      "epoch 3477: train loss: 0.15989314760921933, test loss: 0.2684398875560327\n",
      "epoch 3478: train loss: 0.1598789570963885, test loss: 0.26843552212745386\n",
      "epoch 3479: train loss: 0.15986477271025348, test loss: 0.2684311599979457\n",
      "epoch 3480: train loss: 0.15985059444631383, test loss: 0.2684268011643998\n",
      "epoch 3481: train loss: 0.15983642230007356, test loss: 0.26842244562371165\n",
      "epoch 3482: train loss: 0.1598222562670414, test loss: 0.2684180933727799\n",
      "epoch 3483: train loss: 0.1598080963427308, test loss: 0.268413744408507\n",
      "epoch 3484: train loss: 0.15979394252265963, test loss: 0.268409398727799\n",
      "epoch 3485: train loss: 0.1597797948023505, test loss: 0.26840505632756534\n",
      "epoch 3486: train loss: 0.15976565317733057, test loss: 0.268400717204719\n",
      "epoch 3487: train loss: 0.15975151764313153, test loss: 0.26839638135617655\n",
      "epoch 3488: train loss: 0.15973738819528985, test loss: 0.26839204877885814\n",
      "epoch 3489: train loss: 0.15972326482934635, test loss: 0.2683877194696873\n",
      "epoch 3490: train loss: 0.15970914754084659, test loss: 0.2683833934255913\n",
      "epoch 3491: train loss: 0.1596950363253406, test loss: 0.2683790706435007\n",
      "epoch 3492: train loss: 0.15968093117838303, test loss: 0.26837475112034964\n",
      "epoch 3493: train loss: 0.15966683209553306, test loss: 0.2683704348530759\n",
      "epoch 3494: train loss: 0.1596527390723545, test loss: 0.2683661218386205\n",
      "epoch 3495: train loss: 0.1596386521044155, test loss: 0.2683618120739282\n",
      "epoch 3496: train loss: 0.15962457118728898, test loss: 0.2683575055559471\n",
      "epoch 3497: train loss: 0.1596104963165523, test loss: 0.268353202281629\n",
      "epoch 3498: train loss: 0.15959642748778732, test loss: 0.26834890224792884\n",
      "epoch 3499: train loss: 0.15958236469658038, test loss: 0.2683446054518054\n",
      "epoch 3500: train loss: 0.15956830793852247, test loss: 0.2683403118902206\n",
      "epoch 3501: train loss: 0.15955425720920902, test loss: 0.2683360215601401\n",
      "epoch 3502: train loss: 0.15954021250423991, test loss: 0.26833173445853276\n",
      "epoch 3503: train loss: 0.1595261738192196, test loss: 0.2683274505823713\n",
      "epoch 3504: train loss: 0.15951214114975695, test loss: 0.2683231699286315\n",
      "epoch 3505: train loss: 0.1594981144914654, test loss: 0.2683188924942927\n",
      "epoch 3506: train loss: 0.15948409383996284, test loss: 0.2683146182763378\n",
      "epoch 3507: train loss: 0.15947007919087158, test loss: 0.268310347271753\n",
      "epoch 3508: train loss: 0.15945607053981842, test loss: 0.26830607947752816\n",
      "epoch 3509: train loss: 0.15944206788243473, test loss: 0.2683018148906563\n",
      "epoch 3510: train loss: 0.15942807121435612, test loss: 0.26829755350813395\n",
      "epoch 3511: train loss: 0.15941408053122286, test loss: 0.2682932953269612\n",
      "epoch 3512: train loss: 0.1594000958286795, test loss: 0.2682890403441414\n",
      "epoch 3513: train loss: 0.15938611710237516, test loss: 0.2682847885566815\n",
      "epoch 3514: train loss: 0.15937214434796332, test loss: 0.2682805399615915\n",
      "epoch 3515: train loss: 0.15935817756110185, test loss: 0.2682762945558852\n",
      "epoch 3516: train loss: 0.15934421673745316, test loss: 0.2682720523365796\n",
      "epoch 3517: train loss: 0.1593302618726839, test loss: 0.26826781330069516\n",
      "epoch 3518: train loss: 0.15931631296246532, test loss: 0.26826357744525575\n",
      "epoch 3519: train loss: 0.1593023700024729, test loss: 0.26825934476728847\n",
      "epoch 3520: train loss: 0.15928843298838666, test loss: 0.268255115263824\n",
      "epoch 3521: train loss: 0.1592745019158909, test loss: 0.26825088893189636\n",
      "epoch 3522: train loss: 0.1592605767806744, test loss: 0.2682466657685428\n",
      "epoch 3523: train loss: 0.1592466575784302, test loss: 0.26824244577080414\n",
      "epoch 3524: train loss: 0.15923274430485584, test loss: 0.2682382289357244\n",
      "epoch 3525: train loss: 0.15921883695565314, test loss: 0.26823401526035107\n",
      "epoch 3526: train loss: 0.1592049355265283, test loss: 0.26822980474173497\n",
      "epoch 3527: train loss: 0.1591910400131919, test loss: 0.26822559737693025\n",
      "epoch 3528: train loss: 0.15917715041135885, test loss: 0.2682213931629944\n",
      "epoch 3529: train loss: 0.15916326671674844, test loss: 0.26821719209698835\n",
      "epoch 3530: train loss: 0.1591493889250842, test loss: 0.2682129941759762\n",
      "epoch 3531: train loss: 0.1591355170320941, test loss: 0.2682087993970256\n",
      "epoch 3532: train loss: 0.15912165103351042, test loss: 0.26820460775720717\n",
      "epoch 3533: train loss: 0.15910779092506971, test loss: 0.2682004192535954\n",
      "epoch 3534: train loss: 0.1590939367025129, test loss: 0.2681962338832675\n",
      "epoch 3535: train loss: 0.15908008836158513, test loss: 0.26819205164330456\n",
      "epoch 3536: train loss: 0.15906624589803597, test loss: 0.26818787253079057\n",
      "epoch 3537: train loss: 0.1590524093076192, test loss: 0.26818369654281304\n",
      "epoch 3538: train loss: 0.15903857858609288, test loss: 0.26817952367646264\n",
      "epoch 3539: train loss: 0.1590247537292195, test loss: 0.2681753539288335\n",
      "epoch 3540: train loss: 0.15901093473276565, test loss: 0.2681711872970229\n",
      "epoch 3541: train loss: 0.1589971215925023, test loss: 0.26816702377813156\n",
      "epoch 3542: train loss: 0.15898331430420468, test loss: 0.26816286336926337\n",
      "epoch 3543: train loss: 0.15896951286365224, test loss: 0.26815870606752557\n",
      "epoch 3544: train loss: 0.15895571726662874, test loss: 0.2681545518700285\n",
      "epoch 3545: train loss: 0.1589419275089222, test loss: 0.2681504007738861\n",
      "epoch 3546: train loss: 0.1589281435863248, test loss: 0.2681462527762154\n",
      "epoch 3547: train loss: 0.1589143654946331, test loss: 0.26814210787413656\n",
      "epoch 3548: train loss: 0.15890059322964775, test loss: 0.26813796606477325\n",
      "epoch 3549: train loss: 0.15888682678717375, test loss: 0.26813382734525226\n",
      "epoch 3550: train loss: 0.1588730661630203, test loss: 0.2681296917127037\n",
      "epoch 3551: train loss: 0.15885931135300074, test loss: 0.2681255591642608\n",
      "epoch 3552: train loss: 0.15884556235293273, test loss: 0.2681214296970602\n",
      "epoch 3553: train loss: 0.1588318191586381, test loss: 0.26811730330824174\n",
      "epoch 3554: train loss: 0.15881808176594284, test loss: 0.2681131799949483\n",
      "epoch 3555: train loss: 0.1588043501706772, test loss: 0.2681090597543263\n",
      "epoch 3556: train loss: 0.1587906243686756, test loss: 0.26810494258352524\n",
      "epoch 3557: train loss: 0.15877690435577668, test loss: 0.2681008284796978\n",
      "epoch 3558: train loss: 0.15876319012782317, test loss: 0.2680967174399999\n",
      "epoch 3559: train loss: 0.1587494816806621, test loss: 0.26809260946159075\n",
      "epoch 3560: train loss: 0.15873577901014457, test loss: 0.2680885045416327\n",
      "epoch 3561: train loss: 0.15872208211212588, test loss: 0.26808440267729133\n",
      "epoch 3562: train loss: 0.15870839098246547, test loss: 0.2680803038657355\n",
      "epoch 3563: train loss: 0.15869470561702706, test loss: 0.268076208104137\n",
      "epoch 3564: train loss: 0.1586810260116783, test loss: 0.26807211538967124\n",
      "epoch 3565: train loss: 0.1586673521622912, test loss: 0.26806802571951643\n",
      "epoch 3566: train loss: 0.15865368406474167, test loss: 0.26806393909085413\n",
      "epoch 3567: train loss: 0.1586400217149101, test loss: 0.26805985550086914\n",
      "epoch 3568: train loss: 0.1586263651086806, test loss: 0.2680557749467493\n",
      "epoch 3569: train loss: 0.15861271424194176, test loss: 0.26805169742568574\n",
      "epoch 3570: train loss: 0.15859906911058602, test loss: 0.2680476229348727\n",
      "epoch 3571: train loss: 0.15858542971051012, test loss: 0.2680435514715076\n",
      "epoch 3572: train loss: 0.15857179603761482, test loss: 0.26803948303279107\n",
      "epoch 3573: train loss: 0.1585581680878049, test loss: 0.2680354176159268\n",
      "epoch 3574: train loss: 0.15854454585698946, test loss: 0.2680313552181216\n",
      "epoch 3575: train loss: 0.15853092934108148, test loss: 0.2680272958365857\n",
      "epoch 3576: train loss: 0.1585173185359981, test loss: 0.2680232394685321\n",
      "epoch 3577: train loss: 0.15850371343766056, test loss: 0.26801918611117725\n",
      "epoch 3578: train loss: 0.15849011404199417, test loss: 0.2680151357617406\n",
      "epoch 3579: train loss: 0.15847652034492826, test loss: 0.2680110884174447\n",
      "epoch 3580: train loss: 0.15846293234239628, test loss: 0.2680070440755153\n",
      "epoch 3581: train loss: 0.15844935003033575, test loss: 0.2680030027331813\n",
      "epoch 3582: train loss: 0.1584357734046882, test loss: 0.26799896438767457\n",
      "epoch 3583: train loss: 0.15842220246139915, test loss: 0.2679949290362302\n",
      "epoch 3584: train loss: 0.15840863719641832, test loss: 0.2679908966760865\n",
      "epoch 3585: train loss: 0.1583950776056993, test loss: 0.26798686730448473\n",
      "epoch 3586: train loss: 0.1583815236851999, test loss: 0.2679828409186693\n",
      "epoch 3587: train loss: 0.15836797543088174, test loss: 0.2679788175158877\n",
      "epoch 3588: train loss: 0.15835443283871065, test loss: 0.26797479709339056\n",
      "epoch 3589: train loss: 0.15834089590465641, test loss: 0.26797077964843163\n",
      "epoch 3590: train loss: 0.15832736462469274, test loss: 0.2679667651782677\n",
      "epoch 3591: train loss: 0.15831383899479742, test loss: 0.26796275368015854\n",
      "epoch 3592: train loss: 0.15830031901095232, test loss: 0.2679587451513672\n",
      "epoch 3593: train loss: 0.15828680466914316, test loss: 0.2679547395891596\n",
      "epoch 3594: train loss: 0.15827329596535974, test loss: 0.267950736990805\n",
      "epoch 3595: train loss: 0.15825979289559583, test loss: 0.2679467373535756\n",
      "epoch 3596: train loss: 0.15824629545584915, test loss: 0.26794274067474655\n",
      "epoch 3597: train loss: 0.15823280364212144, test loss: 0.2679387469515961\n",
      "epoch 3598: train loss: 0.1582193174504184, test loss: 0.26793475618140566\n",
      "epoch 3599: train loss: 0.15820583687674963, test loss: 0.26793076836145974\n",
      "epoch 3600: train loss: 0.1581923619171288, test loss: 0.26792678348904564\n",
      "epoch 3601: train loss: 0.15817889256757345, test loss: 0.2679228015614539\n",
      "epoch 3602: train loss: 0.15816542882410511, test loss: 0.2679188225759782\n",
      "epoch 3603: train loss: 0.15815197068274928, test loss: 0.26791484652991504\n",
      "epoch 3604: train loss: 0.1581385181395353, test loss: 0.26791087342056397\n",
      "epoch 3605: train loss: 0.15812507119049657, test loss: 0.26790690324522776\n",
      "epoch 3606: train loss: 0.15811162983167035, test loss: 0.267902936001212\n",
      "epoch 3607: train loss: 0.15809819405909778, test loss: 0.26789897168582544\n",
      "epoch 3608: train loss: 0.15808476386882403, test loss: 0.2678950102963797\n",
      "epoch 3609: train loss: 0.1580713392568981, test loss: 0.26789105183018963\n",
      "epoch 3610: train loss: 0.15805792021937293, test loss: 0.26788709628457286\n",
      "epoch 3611: train loss: 0.1580445067523054, test loss: 0.2678831436568502\n",
      "epoch 3612: train loss: 0.1580310988517562, test loss: 0.26787919394434534\n",
      "epoch 3613: train loss: 0.15801769651379002, test loss: 0.26787524714438504\n",
      "epoch 3614: train loss: 0.15800429973447536, test loss: 0.2678713032542991\n",
      "epoch 3615: train loss: 0.1579909085098846, test loss: 0.2678673622714202\n",
      "epoch 3616: train loss: 0.15797752283609412, test loss: 0.2678634241930841\n",
      "epoch 3617: train loss: 0.15796414270918396, test loss: 0.2678594890166294\n",
      "epoch 3618: train loss: 0.1579507681252383, test loss: 0.26785555673939787\n",
      "epoch 3619: train loss: 0.15793739908034493, test loss: 0.26785162735873413\n",
      "epoch 3620: train loss: 0.15792403557059567, test loss: 0.2678477008719858\n",
      "epoch 3621: train loss: 0.1579106775920861, test loss: 0.26784377727650344\n",
      "epoch 3622: train loss: 0.15789732514091567, test loss: 0.2678398565696407\n",
      "epoch 3623: train loss: 0.1578839782131877, test loss: 0.26783593874875405\n",
      "epoch 3624: train loss: 0.15787063680500946, test loss: 0.2678320238112029\n",
      "epoch 3625: train loss: 0.15785730091249175, test loss: 0.26782811175434984\n",
      "epoch 3626: train loss: 0.15784397053174945, test loss: 0.26782420257556006\n",
      "epoch 3627: train loss: 0.1578306456589013, test loss: 0.267820296272202\n",
      "epoch 3628: train loss: 0.15781732629006961, test loss: 0.2678163928416468\n",
      "epoch 3629: train loss: 0.15780401242138076, test loss: 0.2678124922812688\n",
      "epoch 3630: train loss: 0.1577907040489648, test loss: 0.2678085945884451\n",
      "epoch 3631: train loss: 0.15777740116895564, test loss: 0.2678046997605557\n",
      "epoch 3632: train loss: 0.15776410377749095, test loss: 0.2678008077949836\n",
      "epoch 3633: train loss: 0.15775081187071222, test loss: 0.2677969186891148\n",
      "epoch 3634: train loss: 0.15773752544476477, test loss: 0.26779303244033803\n",
      "epoch 3635: train loss: 0.15772424449579764, test loss: 0.26778914904604506\n",
      "epoch 3636: train loss: 0.15771096901996373, test loss: 0.2677852685036306\n",
      "epoch 3637: train loss: 0.15769769901341957, test loss: 0.2677813908104921\n",
      "epoch 3638: train loss: 0.15768443447232564, test loss: 0.26777751596403015\n",
      "epoch 3639: train loss: 0.15767117539284609, test loss: 0.26777364396164804\n",
      "epoch 3640: train loss: 0.15765792177114882, test loss: 0.26776977480075215\n",
      "epoch 3641: train loss: 0.15764467360340553, test loss: 0.2677659084787515\n",
      "epoch 3642: train loss: 0.15763143088579168, test loss: 0.2677620449930582\n",
      "epoch 3643: train loss: 0.15761819361448648, test loss: 0.2677581843410871\n",
      "epoch 3644: train loss: 0.1576049617856728, test loss: 0.26775432652025616\n",
      "epoch 3645: train loss: 0.15759173539553734, test loss: 0.267750471527986\n",
      "epoch 3646: train loss: 0.1575785144402705, test loss: 0.2677466193617001\n",
      "epoch 3647: train loss: 0.15756529891606644, test loss: 0.26774277001882507\n",
      "epoch 3648: train loss: 0.15755208881912297, test loss: 0.26773892349679007\n",
      "epoch 3649: train loss: 0.15753888414564166, test loss: 0.26773507979302735\n",
      "epoch 3650: train loss: 0.1575256848918279, test loss: 0.26773123890497186\n",
      "epoch 3651: train loss: 0.15751249105389056, test loss: 0.2677274008300616\n",
      "epoch 3652: train loss: 0.1574993026280425, test loss: 0.2677235655657372\n",
      "epoch 3653: train loss: 0.15748611961049996, test loss: 0.26771973310944225\n",
      "epoch 3654: train loss: 0.15747294199748318, test loss: 0.2677159034586233\n",
      "epoch 3655: train loss: 0.1574597697852159, test loss: 0.26771207661072943\n",
      "epoch 3656: train loss: 0.1574466029699256, test loss: 0.2677082525632129\n",
      "epoch 3657: train loss: 0.15743344154784342, test loss: 0.2677044313135286\n",
      "epoch 3658: train loss: 0.1574202855152043, test loss: 0.26770061285913427\n",
      "epoch 3659: train loss: 0.15740713486824665, test loss: 0.26769679719749057\n",
      "epoch 3660: train loss: 0.15739398960321271, test loss: 0.2676929843260609\n",
      "epoch 3661: train loss: 0.15738084971634833, test loss: 0.26768917424231153\n",
      "epoch 3662: train loss: 0.157367715203903, test loss: 0.2676853669437115\n",
      "epoch 3663: train loss: 0.1573545860621299, test loss: 0.26768156242773267\n",
      "epoch 3664: train loss: 0.1573414622872858, test loss: 0.26767776069184973\n",
      "epoch 3665: train loss: 0.1573283438756312, test loss: 0.26767396173354024\n",
      "epoch 3666: train loss: 0.15731523082343016, test loss: 0.2676701655502844\n",
      "epoch 3667: train loss: 0.15730212312695044, test loss: 0.26766637213956523\n",
      "epoch 3668: train loss: 0.1572890207824634, test loss: 0.2676625814988688\n",
      "epoch 3669: train loss: 0.15727592378624405, test loss: 0.2676587936256837\n",
      "epoch 3670: train loss: 0.15726283213457093, test loss: 0.26765500851750135\n",
      "epoch 3671: train loss: 0.15724974582372636, test loss: 0.267651226171816\n",
      "epoch 3672: train loss: 0.15723666484999613, test loss: 0.26764744658612477\n",
      "epoch 3673: train loss: 0.15722358920966967, test loss: 0.2676436697579274\n",
      "epoch 3674: train loss: 0.1572105188990401, test loss: 0.26763989568472657\n",
      "epoch 3675: train loss: 0.15719745391440404, test loss: 0.26763612436402745\n",
      "epoch 3676: train loss: 0.15718439425206174, test loss: 0.2676323557933382\n",
      "epoch 3677: train loss: 0.15717133990831705, test loss: 0.26762858997016986\n",
      "epoch 3678: train loss: 0.15715829087947739, test loss: 0.2676248268920359\n",
      "epoch 3679: train loss: 0.1571452471618538, test loss: 0.26762106655645274\n",
      "epoch 3680: train loss: 0.15713220875176082, test loss: 0.26761730896093966\n",
      "epoch 3681: train loss: 0.15711917564551658, test loss: 0.2676135541030184\n",
      "epoch 3682: train loss: 0.15710614783944288, test loss: 0.2676098019802137\n",
      "epoch 3683: train loss: 0.15709312532986497, test loss: 0.2676060525900528\n",
      "epoch 3684: train loss: 0.15708010811311168, test loss: 0.2676023059300661\n",
      "epoch 3685: train loss: 0.15706709618551548, test loss: 0.26759856199778626\n",
      "epoch 3686: train loss: 0.15705408954341224, test loss: 0.2675948207907488\n",
      "epoch 3687: train loss: 0.1570410881831415, test loss: 0.2675910823064922\n",
      "epoch 3688: train loss: 0.15702809210104626, test loss: 0.26758734654255756\n",
      "epoch 3689: train loss: 0.15701510129347315, test loss: 0.26758361349648846\n",
      "epoch 3690: train loss: 0.15700211575677228, test loss: 0.2675798831658314\n",
      "epoch 3691: train loss: 0.15698913548729723, test loss: 0.26757615554813574\n",
      "epoch 3692: train loss: 0.1569761604814052, test loss: 0.26757243064095326\n",
      "epoch 3693: train loss: 0.1569631907354568, test loss: 0.26756870844183867\n",
      "epoch 3694: train loss: 0.15695022624581634, test loss: 0.2675649889483492\n",
      "epoch 3695: train loss: 0.15693726700885144, test loss: 0.2675612721580449\n",
      "epoch 3696: train loss: 0.15692431302093332, test loss: 0.2675575580684885\n",
      "epoch 3697: train loss: 0.15691136427843666, test loss: 0.26755384667724547\n",
      "epoch 3698: train loss: 0.15689842077773974, test loss: 0.2675501379818839\n",
      "epoch 3699: train loss: 0.1568854825152242, test loss: 0.26754643197997446\n",
      "epoch 3700: train loss: 0.15687254948727525, test loss: 0.2675427286690908\n",
      "epoch 3701: train loss: 0.15685962169028156, test loss: 0.26753902804680896\n",
      "epoch 3702: train loss: 0.15684669912063529, test loss: 0.26753533011070785\n",
      "epoch 3703: train loss: 0.15683378177473206, test loss: 0.26753163485836895\n",
      "epoch 3704: train loss: 0.15682086964897093, test loss: 0.2675279422873764\n",
      "epoch 3705: train loss: 0.1568079627397545, test loss: 0.26752425239531713\n",
      "epoch 3706: train loss: 0.1567950610434888, test loss: 0.26752056517978057\n",
      "epoch 3707: train loss: 0.1567821645565833, test loss: 0.267516880638359\n",
      "epoch 3708: train loss: 0.15676927327545095, test loss: 0.26751319876864704\n",
      "epoch 3709: train loss: 0.15675638719650817, test loss: 0.26750951956824254\n",
      "epoch 3710: train loss: 0.1567435063161747, test loss: 0.2675058430347452\n",
      "epoch 3711: train loss: 0.15673063063087386, test loss: 0.26750216916575814\n",
      "epoch 3712: train loss: 0.15671776013703237, test loss: 0.2674984979588866\n",
      "epoch 3713: train loss: 0.15670489483108038, test loss: 0.2674948294117387\n",
      "epoch 3714: train loss: 0.15669203470945145, test loss: 0.26749116352192515\n",
      "epoch 3715: train loss: 0.15667917976858256, test loss: 0.2674875002870592\n",
      "epoch 3716: train loss: 0.15666633000491412, test loss: 0.267483839704757\n",
      "epoch 3717: train loss: 0.15665348541488996, test loss: 0.26748018177263694\n",
      "epoch 3718: train loss: 0.15664064599495733, test loss: 0.26747652648832043\n",
      "epoch 3719: train loss: 0.15662781174156692, test loss: 0.26747287384943114\n",
      "epoch 3720: train loss: 0.15661498265117269, test loss: 0.2674692238535956\n",
      "epoch 3721: train loss: 0.1566021587202321, test loss: 0.2674655764984428\n",
      "epoch 3722: train loss: 0.15658933994520605, test loss: 0.2674619317816046\n",
      "epoch 3723: train loss: 0.15657652632255872, test loss: 0.26745828970071517\n",
      "epoch 3724: train loss: 0.15656371784875772, test loss: 0.2674546502534113\n",
      "epoch 3725: train loss: 0.1565509145202741, test loss: 0.26745101343733263\n",
      "epoch 3726: train loss: 0.1565381163335822, test loss: 0.2674473792501213\n",
      "epoch 3727: train loss: 0.15652532328515978, test loss: 0.26744374768942175\n",
      "epoch 3728: train loss: 0.1565125353714879, test loss: 0.26744011875288154\n",
      "epoch 3729: train loss: 0.15649975258905113, test loss: 0.2674364924381503\n",
      "epoch 3730: train loss: 0.15648697493433728, test loss: 0.2674328687428806\n",
      "epoch 3731: train loss: 0.15647420240383747, test loss: 0.2674292476647275\n",
      "epoch 3732: train loss: 0.15646143499404636, test loss: 0.26742562920134844\n",
      "epoch 3733: train loss: 0.1564486727014618, test loss: 0.2674220133504038\n",
      "epoch 3734: train loss: 0.156435915522585, test loss: 0.26741840010955614\n",
      "epoch 3735: train loss: 0.15642316345392063, test loss: 0.26741478947647096\n",
      "epoch 3736: train loss: 0.1564104164919765, test loss: 0.26741118144881604\n",
      "epoch 3737: train loss: 0.15639767463326396, test loss: 0.26740757602426185\n",
      "epoch 3738: train loss: 0.1563849378742975, test loss: 0.2674039732004815\n",
      "epoch 3739: train loss: 0.15637220621159506, test loss: 0.2674003729751503\n",
      "epoch 3740: train loss: 0.1563594796416779, test loss: 0.26739677534594664\n",
      "epoch 3741: train loss: 0.1563467581610705, test loss: 0.26739318031055104\n",
      "epoch 3742: train loss: 0.1563340417663007, test loss: 0.26738958786664674\n",
      "epoch 3743: train loss: 0.15632133045389968, test loss: 0.2673859980119195\n",
      "epoch 3744: train loss: 0.15630862422040182, test loss: 0.2673824107440576\n",
      "epoch 3745: train loss: 0.15629592306234502, test loss: 0.2673788260607519\n",
      "epoch 3746: train loss: 0.15628322697627023, test loss: 0.2673752439596957\n",
      "epoch 3747: train loss: 0.1562705359587218, test loss: 0.267371664438585\n",
      "epoch 3748: train loss: 0.15625785000624737, test loss: 0.26736808749511815\n",
      "epoch 3749: train loss: 0.15624516911539782, test loss: 0.267364513126996\n",
      "epoch 3750: train loss: 0.15623249328272737, test loss: 0.2673609413319222\n",
      "epoch 3751: train loss: 0.1562198225047935, test loss: 0.26735737210760263\n",
      "epoch 3752: train loss: 0.1562071567781569, test loss: 0.2673538054517458\n",
      "epoch 3753: train loss: 0.15619449609938157, test loss: 0.26735024136206276\n",
      "epoch 3754: train loss: 0.15618184046503478, test loss: 0.2673466798362669\n",
      "epoch 3755: train loss: 0.15616918987168707, test loss: 0.2673431208720744\n",
      "epoch 3756: train loss: 0.15615654431591217, test loss: 0.2673395644672037\n",
      "epoch 3757: train loss: 0.15614390379428716, test loss: 0.26733601061937584\n",
      "epoch 3758: train loss: 0.15613126830339227, test loss: 0.2673324593263143\n",
      "epoch 3759: train loss: 0.15611863783981106, test loss: 0.26732891058574515\n",
      "epoch 3760: train loss: 0.15610601240013022, test loss: 0.2673253643953969\n",
      "epoch 3761: train loss: 0.15609339198093977, test loss: 0.26732182075300037\n",
      "epoch 3762: train loss: 0.15608077657883293, test loss: 0.2673182796562893\n",
      "epoch 3763: train loss: 0.15606816619040614, test loss: 0.26731474110299946\n",
      "epoch 3764: train loss: 0.1560555608122591, test loss: 0.26731120509086925\n",
      "epoch 3765: train loss: 0.15604296044099464, test loss: 0.2673076716176398\n",
      "epoch 3766: train loss: 0.15603036507321885, test loss: 0.2673041406810541\n",
      "epoch 3767: train loss: 0.1560177747055411, test loss: 0.26730061227885843\n",
      "epoch 3768: train loss: 0.15600518933457386, test loss: 0.2672970864088007\n",
      "epoch 3769: train loss: 0.1559926089569329, test loss: 0.26729356306863195\n",
      "epoch 3770: train loss: 0.15598003356923712, test loss: 0.26729004225610514\n",
      "epoch 3771: train loss: 0.1559674631681086, test loss: 0.2672865239689763\n",
      "epoch 3772: train loss: 0.15595489775017274, test loss: 0.2672830082050033\n",
      "epoch 3773: train loss: 0.15594233731205792, test loss: 0.26727949496194675\n",
      "epoch 3774: train loss: 0.15592978185039588, test loss: 0.2672759842375698\n",
      "epoch 3775: train loss: 0.15591723136182145, test loss: 0.2672724760296378\n",
      "epoch 3776: train loss: 0.1559046858429727, test loss: 0.2672689703359187\n",
      "epoch 3777: train loss: 0.15589214529049078, test loss: 0.26726546715418287\n",
      "epoch 3778: train loss: 0.15587960970102016, test loss: 0.2672619664822031\n",
      "epoch 3779: train loss: 0.15586707907120828, test loss: 0.26725846831775457\n",
      "epoch 3780: train loss: 0.15585455339770588, test loss: 0.2672549726586149\n",
      "epoch 3781: train loss: 0.1558420326771668, test loss: 0.2672514795025643\n",
      "epoch 3782: train loss: 0.15582951690624802, test loss: 0.2672479888473851\n",
      "epoch 3783: train loss: 0.15581700608160975, test loss: 0.2672445006908622\n",
      "epoch 3784: train loss: 0.15580450019991526, test loss: 0.26724101503078307\n",
      "epoch 3785: train loss: 0.15579199925783105, test loss: 0.26723753186493737\n",
      "epoch 3786: train loss: 0.15577950325202658, test loss: 0.2672340511911172\n",
      "epoch 3787: train loss: 0.15576701217917466, test loss: 0.2672305730071173\n",
      "epoch 3788: train loss: 0.1557545260359511, test loss: 0.26722709731073435\n",
      "epoch 3789: train loss: 0.15574204481903486, test loss: 0.2672236240997679\n",
      "epoch 3790: train loss: 0.15572956852510805, test loss: 0.26722015337201976\n",
      "epoch 3791: train loss: 0.15571709715085583, test loss: 0.2672166851252939\n",
      "epoch 3792: train loss: 0.15570463069296658, test loss: 0.26721321935739695\n",
      "epoch 3793: train loss: 0.1556921691481317, test loss: 0.267209756066138\n",
      "epoch 3794: train loss: 0.1556797125130457, test loss: 0.26720629524932815\n",
      "epoch 3795: train loss: 0.15566726078440626, test loss: 0.2672028369047812\n",
      "epoch 3796: train loss: 0.15565481395891417, test loss: 0.26719938103031327\n",
      "epoch 3797: train loss: 0.1556423720332732, test loss: 0.2671959276237428\n",
      "epoch 3798: train loss: 0.15562993500419028, test loss: 0.2671924766828906\n",
      "epoch 3799: train loss: 0.15561750286837545, test loss: 0.2671890282055799\n",
      "epoch 3800: train loss: 0.15560507562254178, test loss: 0.26718558218963623\n",
      "epoch 3801: train loss: 0.15559265326340552, test loss: 0.2671821386328877\n",
      "epoch 3802: train loss: 0.15558023578768584, test loss: 0.2671786975331645\n",
      "epoch 3803: train loss: 0.15556782319210513, test loss: 0.26717525888829924\n",
      "epoch 3804: train loss: 0.15555541547338878, test loss: 0.26717182269612705\n",
      "epoch 3805: train loss: 0.15554301262826523, test loss: 0.2671683889544853\n",
      "epoch 3806: train loss: 0.15553061465346604, test loss: 0.2671649576612138\n",
      "epoch 3807: train loss: 0.1555182215457258, test loss: 0.2671615288141544\n",
      "epoch 3808: train loss: 0.15550583330178216, test loss: 0.2671581024111517\n",
      "epoch 3809: train loss: 0.15549344991837574, test loss: 0.2671546784500524\n",
      "epoch 3810: train loss: 0.1554810713922504, test loss: 0.26715125692870567\n",
      "epoch 3811: train loss: 0.15546869772015282, test loss: 0.267147837844963\n",
      "epoch 3812: train loss: 0.15545632889883287, test loss: 0.26714442119667803\n",
      "epoch 3813: train loss: 0.15544396492504337, test loss: 0.26714100698170695\n",
      "epoch 3814: train loss: 0.15543160579554027, test loss: 0.2671375951979082\n",
      "epoch 3815: train loss: 0.15541925150708244, test loss: 0.2671341858431425\n",
      "epoch 3816: train loss: 0.15540690205643187, test loss: 0.267130778915273\n",
      "epoch 3817: train loss: 0.1553945574403535, test loss: 0.26712737441216505\n",
      "epoch 3818: train loss: 0.1553822176556153, test loss: 0.2671239723316864\n",
      "epoch 3819: train loss: 0.15536988269898833, test loss: 0.2671205726717071\n",
      "epoch 3820: train loss: 0.1553575525672465, test loss: 0.26711717543009944\n",
      "epoch 3821: train loss: 0.1553452272571669, test loss: 0.26711378060473817\n",
      "epoch 3822: train loss: 0.15533290676552958, test loss: 0.2671103881935002\n",
      "epoch 3823: train loss: 0.1553205910891175, test loss: 0.26710699819426476\n",
      "epoch 3824: train loss: 0.15530828022471665, test loss: 0.26710361060491344\n",
      "epoch 3825: train loss: 0.15529597416911614, test loss: 0.2671002254233301\n",
      "epoch 3826: train loss: 0.1552836729191079, test loss: 0.2670968426474009\n",
      "epoch 3827: train loss: 0.1552713764714869, test loss: 0.2670934622750143\n",
      "epoch 3828: train loss: 0.15525908482305115, test loss: 0.26709008430406106\n",
      "epoch 3829: train loss: 0.15524679797060156, test loss: 0.2670867087324341\n",
      "epoch 3830: train loss: 0.1552345159109421, test loss: 0.26708333555802877\n",
      "epoch 3831: train loss: 0.15522223864087956, test loss: 0.2670799647787427\n",
      "epoch 3832: train loss: 0.15520996615722388, test loss: 0.2670765963924756\n",
      "epoch 3833: train loss: 0.15519769845678785, test loss: 0.2670732303971299\n",
      "epoch 3834: train loss: 0.15518543553638728, test loss: 0.2670698667906097\n",
      "epoch 3835: train loss: 0.15517317739284084, test loss: 0.26706650557082184\n",
      "epoch 3836: train loss: 0.15516092402297033, test loss: 0.2670631467356752\n",
      "epoch 3837: train loss: 0.1551486754236003, test loss: 0.267059790283081\n",
      "epoch 3838: train loss: 0.15513643159155832, test loss: 0.26705643621095265\n",
      "epoch 3839: train loss: 0.15512419252367501, test loss: 0.267053084517206\n",
      "epoch 3840: train loss: 0.15511195821678378, test loss: 0.2670497351997589\n",
      "epoch 3841: train loss: 0.15509972866772104, test loss: 0.2670463882565316\n",
      "epoch 3842: train loss: 0.15508750387332612, test loss: 0.2670430436854467\n",
      "epoch 3843: train loss: 0.15507528383044128, test loss: 0.2670397014844286\n",
      "epoch 3844: train loss: 0.15506306853591176, test loss: 0.2670363616514047\n",
      "epoch 3845: train loss: 0.15505085798658563, test loss: 0.26703302418430397\n",
      "epoch 3846: train loss: 0.15503865217931392, test loss: 0.2670296890810578\n",
      "epoch 3847: train loss: 0.1550264511109506, test loss: 0.2670263563396001\n",
      "epoch 3848: train loss: 0.15501425477835246, test loss: 0.2670230259578665\n",
      "epoch 3849: train loss: 0.15500206317837933, test loss: 0.2670196979337953\n",
      "epoch 3850: train loss: 0.15498987630789388, test loss: 0.2670163722653269\n",
      "epoch 3851: train loss: 0.15497769416376161, test loss: 0.2670130489504039\n",
      "epoch 3852: train loss: 0.15496551674285106, test loss: 0.26700972798697103\n",
      "epoch 3853: train loss: 0.1549533440420335, test loss: 0.2670064093729754\n",
      "epoch 3854: train loss: 0.15494117605818328, test loss: 0.26700309310636616\n",
      "epoch 3855: train loss: 0.15492901278817747, test loss: 0.2669997791850949\n",
      "epoch 3856: train loss: 0.1549168542288961, test loss: 0.26699646760711526\n",
      "epoch 3857: train loss: 0.15490470037722212, test loss: 0.2669931583703831\n",
      "epoch 3858: train loss: 0.15489255123004123, test loss: 0.26698985147285664\n",
      "epoch 3859: train loss: 0.15488040678424211, test loss: 0.266986546912496\n",
      "epoch 3860: train loss: 0.1548682670367163, test loss: 0.26698324468726375\n",
      "epoch 3861: train loss: 0.15485613198435813, test loss: 0.2669799447951246\n",
      "epoch 3862: train loss: 0.15484400162406492, test loss: 0.2669766472340454\n",
      "epoch 3863: train loss: 0.15483187595273676, test loss: 0.2669733520019953\n",
      "epoch 3864: train loss: 0.15481975496727654, test loss: 0.26697005909694554\n",
      "epoch 3865: train loss: 0.15480763866459013, test loss: 0.2669667685168695\n",
      "epoch 3866: train loss: 0.15479552704158625, test loss: 0.26696348025974287\n",
      "epoch 3867: train loss: 0.15478342009517632, test loss: 0.26696019432354356\n",
      "epoch 3868: train loss: 0.15477131782227474, test loss: 0.26695691070625144\n",
      "epoch 3869: train loss: 0.15475922021979868, test loss: 0.26695362940584877\n",
      "epoch 3870: train loss: 0.1547471272846682, test loss: 0.26695035042031995\n",
      "epoch 3871: train loss: 0.1547350390138061, test loss: 0.2669470737476513\n",
      "epoch 3872: train loss: 0.15472295540413816, test loss: 0.2669437993858318\n",
      "epoch 3873: train loss: 0.15471087645259285, test loss: 0.26694052733285206\n",
      "epoch 3874: train loss: 0.1546988021561015, test loss: 0.2669372575867051\n",
      "epoch 3875: train loss: 0.15468673251159826, test loss: 0.2669339901453863\n",
      "epoch 3876: train loss: 0.1546746675160201, test loss: 0.26693072500689285\n",
      "epoch 3877: train loss: 0.1546626071663068, test loss: 0.26692746216922436\n",
      "epoch 3878: train loss: 0.15465055145940107, test loss: 0.2669242016303823\n",
      "epoch 3879: train loss: 0.15463850039224813, test loss: 0.2669209433883706\n",
      "epoch 3880: train loss: 0.1546264539617963, test loss: 0.2669176874411951\n",
      "epoch 3881: train loss: 0.15461441216499658, test loss: 0.266914433786864\n",
      "epoch 3882: train loss: 0.1546023749988027, test loss: 0.26691118242338746\n",
      "epoch 3883: train loss: 0.15459034246017125, test loss: 0.2669079333487778\n",
      "epoch 3884: train loss: 0.1545783145460617, test loss: 0.26690468656104965\n",
      "epoch 3885: train loss: 0.15456629125343613, test loss: 0.2669014420582196\n",
      "epoch 3886: train loss: 0.15455427257925952, test loss: 0.26689819983830637\n",
      "epoch 3887: train loss: 0.15454225852049958, test loss: 0.2668949598993309\n",
      "epoch 3888: train loss: 0.1545302490741268, test loss: 0.2668917222393161\n",
      "epoch 3889: train loss: 0.1545182442371145, test loss: 0.26688848685628735\n",
      "epoch 3890: train loss: 0.1545062440064387, test loss: 0.26688525374827177\n",
      "epoch 3891: train loss: 0.15449424837907821, test loss: 0.2668820229132987\n",
      "epoch 3892: train loss: 0.15448225735201457, test loss: 0.2668787943493998\n",
      "epoch 3893: train loss: 0.15447027092223217, test loss: 0.26687556805460855\n",
      "epoch 3894: train loss: 0.15445828908671808, test loss: 0.26687234402696075\n",
      "epoch 3895: train loss: 0.15444631184246208, test loss: 0.2668691222644942\n",
      "epoch 3896: train loss: 0.15443433918645685, test loss: 0.2668659027652489\n",
      "epoch 3897: train loss: 0.15442237111569768, test loss: 0.26686268552726683\n",
      "epoch 3898: train loss: 0.1544104076271827, test loss: 0.2668594705485921\n",
      "epoch 3899: train loss: 0.15439844871791264, test loss: 0.26685625782727107\n",
      "epoch 3900: train loss: 0.15438649438489116, test loss: 0.266853047361352\n",
      "epoch 3901: train loss: 0.15437454462512445, test loss: 0.2668498391488853\n",
      "epoch 3902: train loss: 0.15436259943562164, test loss: 0.2668466331879237\n",
      "epoch 3903: train loss: 0.1543506588133944, test loss: 0.26684342947652145\n",
      "epoch 3904: train loss: 0.15433872275545724, test loss: 0.26684022801273566\n",
      "epoch 3905: train loss: 0.1543267912588273, test loss: 0.26683702879462473\n",
      "epoch 3906: train loss: 0.1543148643205246, test loss: 0.2668338318202497\n",
      "epoch 3907: train loss: 0.1543029419375717, test loss: 0.2668306370876736\n",
      "epoch 3908: train loss: 0.15429102410699388, test loss: 0.26682744459496127\n",
      "epoch 3909: train loss: 0.15427911082581933, test loss: 0.26682425434017987\n",
      "epoch 3910: train loss: 0.15426720209107866, test loss: 0.2668210663213985\n",
      "epoch 3911: train loss: 0.1542552978998054, test loss: 0.2668178805366885\n",
      "epoch 3912: train loss: 0.1542433982490357, test loss: 0.266814696984123\n",
      "epoch 3913: train loss: 0.15423150313580838, test loss: 0.2668115156617775\n",
      "epoch 3914: train loss: 0.154219612557165, test loss: 0.26680833656772934\n",
      "epoch 3915: train loss: 0.15420772651014975, test loss: 0.266805159700058\n",
      "epoch 3916: train loss: 0.15419584499180958, test loss: 0.266801985056845\n",
      "epoch 3917: train loss: 0.15418396799919407, test loss: 0.2667988126361739\n",
      "epoch 3918: train loss: 0.15417209552935554, test loss: 0.26679564243613035\n",
      "epoch 3919: train loss: 0.15416022757934883, test loss: 0.2667924744548021\n",
      "epoch 3920: train loss: 0.1541483641462317, test loss: 0.2667893086902787\n",
      "epoch 3921: train loss: 0.1541365052270644, test loss: 0.2667861451406521\n",
      "epoch 3922: train loss: 0.15412465081890986, test loss: 0.26678298380401594\n",
      "epoch 3923: train loss: 0.15411280091883373, test loss: 0.2667798246784661\n",
      "epoch 3924: train loss: 0.1541009555239043, test loss: 0.2667766677621006\n",
      "epoch 3925: train loss: 0.15408911463119251, test loss: 0.2667735130530191\n",
      "epoch 3926: train loss: 0.15407727823777198, test loss: 0.2667703605493238\n",
      "epoch 3927: train loss: 0.15406544634071892, test loss: 0.2667672102491185\n",
      "epoch 3928: train loss: 0.15405361893711228, test loss: 0.26676406215050924\n",
      "epoch 3929: train loss: 0.1540417960240336, test loss: 0.26676091625160403\n",
      "epoch 3930: train loss: 0.15402997759856699, test loss: 0.2667577725505129\n",
      "epoch 3931: train loss: 0.1540181636577994, test loss: 0.2667546310453479\n",
      "epoch 3932: train loss: 0.15400635419882022, test loss: 0.2667514917342232\n",
      "epoch 3933: train loss: 0.15399454921872155, test loss: 0.2667483546152548\n",
      "epoch 3934: train loss: 0.15398274871459813, test loss: 0.2667452196865608\n",
      "epoch 3935: train loss: 0.15397095268354735, test loss: 0.2667420869462613\n",
      "epoch 3936: train loss: 0.1539591611226691, test loss: 0.2667389563924784\n",
      "epoch 3937: train loss: 0.15394737402906603, test loss: 0.26673582802333623\n",
      "epoch 3938: train loss: 0.15393559139984336, test loss: 0.2667327018369609\n",
      "epoch 3939: train loss: 0.1539238132321089, test loss: 0.2667295778314806\n",
      "epoch 3940: train loss: 0.15391203952297314, test loss: 0.26672645600502526\n",
      "epoch 3941: train loss: 0.15390027026954908, test loss: 0.26672333635572726\n",
      "epoch 3942: train loss: 0.15388850546895239, test loss: 0.2667202188817204\n",
      "epoch 3943: train loss: 0.15387674511830135, test loss: 0.2667171035811409\n",
      "epoch 3944: train loss: 0.15386498921471678, test loss: 0.26671399045212696\n",
      "epoch 3945: train loss: 0.15385323775532217, test loss: 0.2667108794928184\n",
      "epoch 3946: train loss: 0.15384149073724354, test loss: 0.2667077707013574\n",
      "epoch 3947: train loss: 0.15382974815760955, test loss: 0.266704664075888\n",
      "epoch 3948: train loss: 0.15381801001355144, test loss: 0.26670155961455605\n",
      "epoch 3949: train loss: 0.15380627630220295, test loss: 0.26669845731550973\n",
      "epoch 3950: train loss: 0.15379454702070058, test loss: 0.26669535717689885\n",
      "epoch 3951: train loss: 0.15378282216618322, test loss: 0.2666922591968753\n",
      "epoch 3952: train loss: 0.15377110173579245, test loss: 0.266689163373593\n",
      "epoch 3953: train loss: 0.15375938572667236, test loss: 0.26668606970520786\n",
      "epoch 3954: train loss: 0.15374767413596968, test loss: 0.2666829781898776\n",
      "epoch 3955: train loss: 0.1537359669608336, test loss: 0.266679888825762\n",
      "epoch 3956: train loss: 0.15372426419841603, test loss: 0.26667680161102286\n",
      "epoch 3957: train loss: 0.15371256584587129, test loss: 0.26667371654382377\n",
      "epoch 3958: train loss: 0.1537008719003563, test loss: 0.26667063362233046\n",
      "epoch 3959: train loss: 0.15368918235903062, test loss: 0.2666675528447104\n",
      "epoch 3960: train loss: 0.1536774972190562, test loss: 0.2666644742091333\n",
      "epoch 3961: train loss: 0.15366581647759772, test loss: 0.2666613977137705\n",
      "epoch 3962: train loss: 0.15365414013182227, test loss: 0.2666583233567955\n",
      "epoch 3963: train loss: 0.15364246817889954, test loss: 0.2666552511363836\n",
      "epoch 3964: train loss: 0.15363080061600176, test loss: 0.26665218105071226\n",
      "epoch 3965: train loss: 0.15361913744030364, test loss: 0.2666491130979605\n",
      "epoch 3966: train loss: 0.15360747864898255, test loss: 0.26664604727630975\n",
      "epoch 3967: train loss: 0.15359582423921825, test loss: 0.266642983583943\n",
      "epoch 3968: train loss: 0.15358417420819312, test loss: 0.2666399220190453\n",
      "epoch 3969: train loss: 0.15357252855309203, test loss: 0.2666368625798035\n",
      "epoch 3970: train loss: 0.15356088727110234, test loss: 0.2666338052644068\n",
      "epoch 3971: train loss: 0.153549250359414, test loss: 0.26663075007104575\n",
      "epoch 3972: train loss: 0.1535376178152195, test loss: 0.26662769699791317\n",
      "epoch 3973: train loss: 0.15352598963571368, test loss: 0.26662464604320385\n",
      "epoch 3974: train loss: 0.15351436581809405, test loss: 0.26662159720511425\n",
      "epoch 3975: train loss: 0.1535027463595606, test loss: 0.2666185504818428\n",
      "epoch 3976: train loss: 0.15349113125731578, test loss: 0.26661550587159005\n",
      "epoch 3977: train loss: 0.15347952050856456, test loss: 0.2666124633725582\n",
      "epoch 3978: train loss: 0.15346791411051441, test loss: 0.2666094229829516\n",
      "epoch 3979: train loss: 0.1534563120603753, test loss: 0.26660638470097625\n",
      "epoch 3980: train loss: 0.1534447143553597, test loss: 0.2666033485248402\n",
      "epoch 3981: train loss: 0.15343312099268258, test loss: 0.26660031445275345\n",
      "epoch 3982: train loss: 0.15342153196956132, test loss: 0.26659728248292774\n",
      "epoch 3983: train loss: 0.1534099472832159, test loss: 0.26659425261357683\n",
      "epoch 3984: train loss: 0.1533983669308687, test loss: 0.2665912248429164\n",
      "epoch 3985: train loss: 0.1533867909097446, test loss: 0.2665881991691639\n",
      "epoch 3986: train loss: 0.153375219217071, test loss: 0.2665851755905387\n",
      "epoch 3987: train loss: 0.15336365185007772, test loss: 0.2665821541052621\n",
      "epoch 3988: train loss: 0.15335208880599704, test loss: 0.2665791347115573\n",
      "epoch 3989: train loss: 0.15334053008206383, test loss: 0.26657611740764936\n",
      "epoch 3990: train loss: 0.1533289756755152, test loss: 0.26657310219176517\n",
      "epoch 3991: train loss: 0.15331742558359088, test loss: 0.2665700890621335\n",
      "epoch 3992: train loss: 0.1533058798035331, test loss: 0.26656707801698515\n",
      "epoch 3993: train loss: 0.1532943383325864, test loss: 0.26656406905455254\n",
      "epoch 3994: train loss: 0.15328280116799792, test loss: 0.2665610621730703\n",
      "epoch 3995: train loss: 0.15327126830701712, test loss: 0.2665580573707745\n",
      "epoch 3996: train loss: 0.15325973974689602, test loss: 0.26655505464590346\n",
      "epoch 3997: train loss: 0.15324821548488898, test loss: 0.2665520539966972\n",
      "epoch 3998: train loss: 0.15323669551825295, test loss: 0.26654905542139745\n",
      "epoch 3999: train loss: 0.15322517984424708, test loss: 0.2665460589182482\n",
      "epoch 4000: train loss: 0.15321366846013323, test loss: 0.26654306448549486\n",
      "epoch 4001: train loss: 0.15320216136317555, test loss: 0.266540072121385\n",
      "epoch 4002: train loss: 0.1531906585506406, test loss: 0.26653708182416785\n",
      "epoch 4003: train loss: 0.1531791600197974, test loss: 0.2665340935920947\n",
      "epoch 4004: train loss: 0.15316766576791743, test loss: 0.26653110742341846\n",
      "epoch 4005: train loss: 0.15315617579227458, test loss: 0.26652812331639403\n",
      "epoch 4006: train loss: 0.1531446900901451, test loss: 0.2665251412692781\n",
      "epoch 4007: train loss: 0.1531332086588078, test loss: 0.26652216128032935\n",
      "epoch 4008: train loss: 0.15312173149554367, test loss: 0.266519183347808\n",
      "epoch 4009: train loss: 0.15311025859763636, test loss: 0.2665162074699763\n",
      "epoch 4010: train loss: 0.15309878996237175, test loss: 0.26651323364509844\n",
      "epoch 4011: train loss: 0.15308732558703825, test loss: 0.2665102618714401\n",
      "epoch 4012: train loss: 0.15307586546892654, test loss: 0.2665072921472692\n",
      "epoch 4013: train loss: 0.15306440960532988, test loss: 0.2665043244708552\n",
      "epoch 4014: train loss: 0.1530529579935438, test loss: 0.2665013588404696\n",
      "epoch 4015: train loss: 0.15304151063086618, test loss: 0.2664983952543855\n",
      "epoch 4016: train loss: 0.15303006751459744, test loss: 0.26649543371087797\n",
      "epoch 4017: train loss: 0.15301862864204036, test loss: 0.26649247420822375\n",
      "epoch 4018: train loss: 0.1530071940104999, test loss: 0.26648951674470167\n",
      "epoch 4019: train loss: 0.15299576361728373, test loss: 0.2664865613185922\n",
      "epoch 4020: train loss: 0.15298433745970164, test loss: 0.26648360792817755\n",
      "epoch 4021: train loss: 0.15297291553506595, test loss: 0.26648065657174186\n",
      "epoch 4022: train loss: 0.1529614978406913, test loss: 0.2664777072475711\n",
      "epoch 4023: train loss: 0.1529500843738947, test loss: 0.2664747599539529\n",
      "epoch 4024: train loss: 0.15293867513199552, test loss: 0.26647181468917697\n",
      "epoch 4025: train loss: 0.15292727011231558, test loss: 0.2664688714515345\n",
      "epoch 4026: train loss: 0.1529158693121789, test loss: 0.26646593023931864\n",
      "epoch 4027: train loss: 0.15290447272891206, test loss: 0.2664629910508243\n",
      "epoch 4028: train loss: 0.1528930803598439, test loss: 0.26646005388434835\n",
      "epoch 4029: train loss: 0.15288169220230555, test loss: 0.26645711873818917\n",
      "epoch 4030: train loss: 0.15287030825363065, test loss: 0.2664541856106471\n",
      "epoch 4031: train loss: 0.15285892851115507, test loss: 0.26645125450002444\n",
      "epoch 4032: train loss: 0.15284755297221708, test loss: 0.2664483254046248\n",
      "epoch 4033: train loss: 0.1528361816341573, test loss: 0.26644539832275405\n",
      "epoch 4034: train loss: 0.15282481449431862, test loss: 0.2664424732527197\n",
      "epoch 4035: train loss: 0.15281345155004644, test loss: 0.2664395501928308\n",
      "epoch 4036: train loss: 0.15280209279868828, test loss: 0.2664366291413985\n",
      "epoch 4037: train loss: 0.1527907382375942, test loss: 0.26643371009673555\n",
      "epoch 4038: train loss: 0.15277938786411646, test loss: 0.26643079305715667\n",
      "epoch 4039: train loss: 0.1527680416756097, test loss: 0.2664278780209781\n",
      "epoch 4040: train loss: 0.15275669966943087, test loss: 0.266424964986518\n",
      "epoch 4041: train loss: 0.15274536184293927, test loss: 0.2664220539520963\n",
      "epoch 4042: train loss: 0.1527340281934965, test loss: 0.2664191449160345\n",
      "epoch 4043: train loss: 0.1527226987184665, test loss: 0.2664162378766563\n",
      "epoch 4044: train loss: 0.15271137341521554, test loss: 0.2664133328322867\n",
      "epoch 4045: train loss: 0.15270005228111214, test loss: 0.2664104297812527\n",
      "epoch 4046: train loss: 0.15268873531352717, test loss: 0.266407528721883\n",
      "epoch 4047: train loss: 0.15267742250983385, test loss: 0.26640462965250805\n",
      "epoch 4048: train loss: 0.1526661138674077, test loss: 0.26640173257146016\n",
      "epoch 4049: train loss: 0.15265480938362644, test loss: 0.26639883747707316\n",
      "epoch 4050: train loss: 0.15264350905587018, test loss: 0.26639594436768294\n",
      "epoch 4051: train loss: 0.15263221288152143, test loss: 0.2663930532416268\n",
      "epoch 4052: train loss: 0.15262092085796475, test loss: 0.26639016409724414\n",
      "epoch 4053: train loss: 0.1526096329825872, test loss: 0.2663872769328758\n",
      "epoch 4054: train loss: 0.15259834925277804, test loss: 0.26638439174686446\n",
      "epoch 4055: train loss: 0.15258706966592886, test loss: 0.2663815085375547\n",
      "epoch 4056: train loss: 0.1525757942194335, test loss: 0.26637862730329254\n",
      "epoch 4057: train loss: 0.15256452291068814, test loss: 0.26637574804242603\n",
      "epoch 4058: train loss: 0.15255325573709114, test loss: 0.26637287075330485\n",
      "epoch 4059: train loss: 0.15254199269604327, test loss: 0.2663699954342802\n",
      "epoch 4060: train loss: 0.15253073378494744, test loss: 0.26636712208370533\n",
      "epoch 4061: train loss: 0.15251947900120896, test loss: 0.2663642506999351\n",
      "epoch 4062: train loss: 0.15250822834223535, test loss: 0.266361381281326\n",
      "epoch 4063: train loss: 0.15249698180543633, test loss: 0.2663585138262363\n",
      "epoch 4064: train loss: 0.15248573938822407, test loss: 0.2663556483330262\n",
      "epoch 4065: train loss: 0.1524745010880128, test loss: 0.26635278480005725\n",
      "epoch 4066: train loss: 0.15246326690221912, test loss: 0.26634992322569295\n",
      "epoch 4067: train loss: 0.15245203682826192, test loss: 0.2663470636082985\n",
      "epoch 4068: train loss: 0.15244081086356226, test loss: 0.2663442059462407\n",
      "epoch 4069: train loss: 0.1524295890055435, test loss: 0.26634135023788824\n",
      "epoch 4070: train loss: 0.15241837125163119, test loss: 0.26633849648161123\n",
      "epoch 4071: train loss: 0.15240715759925327, test loss: 0.2663356446757819\n",
      "epoch 4072: train loss: 0.1523959480458398, test loss: 0.2663327948187739\n",
      "epoch 4073: train loss: 0.1523847425888231, test loss: 0.2663299469089626\n",
      "epoch 4074: train loss: 0.15237354122563773, test loss: 0.26632710094472517\n",
      "epoch 4075: train loss: 0.15236234395372059, test loss: 0.2663242569244404\n",
      "epoch 4076: train loss: 0.15235115077051062, test loss: 0.26632141484648875\n",
      "epoch 4077: train loss: 0.1523399616734492, test loss: 0.26631857470925246\n",
      "epoch 4078: train loss: 0.15232877665997982, test loss: 0.26631573651111545\n",
      "epoch 4079: train loss: 0.1523175957275482, test loss: 0.2663129002504634\n",
      "epoch 4080: train loss: 0.15230641887360233, test loss: 0.26631006592568346\n",
      "epoch 4081: train loss: 0.15229524609559242, test loss: 0.2663072335351646\n",
      "epoch 4082: train loss: 0.15228407739097086, test loss: 0.2663044030772976\n",
      "epoch 4083: train loss: 0.1522729127571923, test loss: 0.26630157455047465\n",
      "epoch 4084: train loss: 0.15226175219171356, test loss: 0.26629874795308983\n",
      "epoch 4085: train loss: 0.15225059569199373, test loss: 0.26629592328353885\n",
      "epoch 4086: train loss: 0.1522394432554941, test loss: 0.26629310054021915\n",
      "epoch 4087: train loss: 0.15222829487967812, test loss: 0.26629027972152963\n",
      "epoch 4088: train loss: 0.1522171505620115, test loss: 0.2662874608258712\n",
      "epoch 4089: train loss: 0.15220601029996206, test loss: 0.26628464385164613\n",
      "epoch 4090: train loss: 0.15219487409100002, test loss: 0.2662818287972585\n",
      "epoch 4091: train loss: 0.15218374193259757, test loss: 0.26627901566111406\n",
      "epoch 4092: train loss: 0.1521726138222292, test loss: 0.2662762044416202\n",
      "epoch 4093: train loss: 0.15216148975737165, test loss: 0.266273395137186\n",
      "epoch 4094: train loss: 0.15215036973550375, test loss: 0.26627058774622214\n",
      "epoch 4095: train loss: 0.15213925375410653, test loss: 0.2662677822671411\n",
      "epoch 4096: train loss: 0.15212814181066334, test loss: 0.26626497869835686\n",
      "epoch 4097: train loss: 0.1521170339026595, test loss: 0.26626217703828514\n",
      "epoch 4098: train loss: 0.15210593002758266, test loss: 0.2662593772853433\n",
      "epoch 4099: train loss: 0.15209483018292264, test loss: 0.2662565794379503\n",
      "epoch 4100: train loss: 0.15208373436617134, test loss: 0.2662537834945267\n",
      "epoch 4101: train loss: 0.15207264257482295, test loss: 0.26625098945349507\n",
      "epoch 4102: train loss: 0.1520615548063738, test loss: 0.2662481973132791\n",
      "epoch 4103: train loss: 0.1520504710583223, test loss: 0.2662454070723046\n",
      "epoch 4104: train loss: 0.15203939132816918, test loss: 0.26624261872899857\n",
      "epoch 4105: train loss: 0.1520283156134172, test loss: 0.26623983228179005\n",
      "epoch 4106: train loss: 0.15201724391157134, test loss: 0.2662370477291096\n",
      "epoch 4107: train loss: 0.15200617622013876, test loss: 0.2662342650693892\n",
      "epoch 4108: train loss: 0.15199511253662876, test loss: 0.2662314843010627\n",
      "epoch 4109: train loss: 0.15198405285855274, test loss: 0.26622870542256555\n",
      "epoch 4110: train loss: 0.15197299718342433, test loss: 0.2662259284323348\n",
      "epoch 4111: train loss: 0.15196194550875924, test loss: 0.266223153328809\n",
      "epoch 4112: train loss: 0.15195089783207544, test loss: 0.26622038011042864\n",
      "epoch 4113: train loss: 0.1519398541508929, test loss: 0.2662176087756356\n",
      "epoch 4114: train loss: 0.15192881446273382, test loss: 0.2662148393228733\n",
      "epoch 4115: train loss: 0.15191777876512258, test loss: 0.2662120717505871\n",
      "epoch 4116: train loss: 0.1519067470555856, test loss: 0.26620930605722365\n",
      "epoch 4117: train loss: 0.1518957193316515, test loss: 0.2662065422412314\n",
      "epoch 4118: train loss: 0.15188469559085097, test loss: 0.2662037803010604\n",
      "epoch 4119: train loss: 0.1518736758307169, test loss: 0.26620102023516223\n",
      "epoch 4120: train loss: 0.15186266004878426, test loss: 0.2661982620419902\n",
      "epoch 4121: train loss: 0.15185164824259026, test loss: 0.2661955057199991\n",
      "epoch 4122: train loss: 0.15184064040967404, test loss: 0.26619275126764547\n",
      "epoch 4123: train loss: 0.15182963654757697, test loss: 0.26618999868338744\n",
      "epoch 4124: train loss: 0.15181863665384257, test loss: 0.2661872479656845\n",
      "epoch 4125: train loss: 0.15180764072601646, test loss: 0.2661844991129981\n",
      "epoch 4126: train loss: 0.15179664876164634, test loss: 0.266181752123791\n",
      "epoch 4127: train loss: 0.15178566075828198, test loss: 0.26617900699652774\n",
      "epoch 4128: train loss: 0.1517746767134754, test loss: 0.26617626372967435\n",
      "epoch 4129: train loss: 0.15176369662478056, test loss: 0.26617352232169855\n",
      "epoch 4130: train loss: 0.1517527204897537, test loss: 0.2661707827710696\n",
      "epoch 4131: train loss: 0.15174174830595302, test loss: 0.2661680450762583\n",
      "epoch 4132: train loss: 0.15173078007093888, test loss: 0.2661653092357371\n",
      "epoch 4133: train loss: 0.15171981578227373, test loss: 0.26616257524798015\n",
      "epoch 4134: train loss: 0.15170885543752213, test loss: 0.26615984311146285\n",
      "epoch 4135: train loss: 0.15169789903425074, test loss: 0.2661571128246626\n",
      "epoch 4136: train loss: 0.1516869465700282, test loss: 0.2661543843860581\n",
      "epoch 4137: train loss: 0.15167599804242546, test loss: 0.2661516577941297\n",
      "epoch 4138: train loss: 0.15166505344901535, test loss: 0.26614893304735926\n",
      "epoch 4139: train loss: 0.15165411278737287, test loss: 0.26614621014423046\n",
      "epoch 4140: train loss: 0.15164317605507513, test loss: 0.2661434890832283\n",
      "epoch 4141: train loss: 0.15163224324970126, test loss: 0.26614076986283935\n",
      "epoch 4142: train loss: 0.15162131436883247, test loss: 0.26613805248155203\n",
      "epoch 4143: train loss: 0.1516103894100521, test loss: 0.266135336937856\n",
      "epoch 4144: train loss: 0.15159946837094554, test loss: 0.2661326232302427\n",
      "epoch 4145: train loss: 0.1515885512491002, test loss: 0.26612991135720504\n",
      "epoch 4146: train loss: 0.15157763804210564, test loss: 0.2661272013172375\n",
      "epoch 4147: train loss: 0.1515667287475534, test loss: 0.2661244931088362\n",
      "epoch 4148: train loss: 0.15155582336303716, test loss: 0.26612178673049863\n",
      "epoch 4149: train loss: 0.15154492188615265, test loss: 0.266119082180724\n",
      "epoch 4150: train loss: 0.15153402431449764, test loss: 0.26611637945801314\n",
      "epoch 4151: train loss: 0.15152313064567188, test loss: 0.2661136785608683\n",
      "epoch 4152: train loss: 0.15151224087727733, test loss: 0.26611097948779316\n",
      "epoch 4153: train loss: 0.15150135500691791, test loss: 0.2661082822372933\n",
      "epoch 4154: train loss: 0.15149047303219962, test loss: 0.26610558680787566\n",
      "epoch 4155: train loss: 0.15147959495073046, test loss: 0.2661028931980486\n",
      "epoch 4156: train loss: 0.15146872076012052, test loss: 0.2661002014063222\n",
      "epoch 4157: train loss: 0.1514578504579819, test loss: 0.266097511431208\n",
      "epoch 4158: train loss: 0.15144698404192883, test loss: 0.2660948232712192\n",
      "epoch 4159: train loss: 0.15143612150957747, test loss: 0.2660921369248703\n",
      "epoch 4160: train loss: 0.15142526285854602, test loss: 0.2660894523906777\n",
      "epoch 4161: train loss: 0.15141440808645484, test loss: 0.266086769667159\n",
      "epoch 4162: train loss: 0.15140355719092616, test loss: 0.2660840887528334\n",
      "epoch 4163: train loss: 0.1513927101695844, test loss: 0.2660814096462218\n",
      "epoch 4164: train loss: 0.1513818670200558, test loss: 0.2660787323458465\n",
      "epoch 4165: train loss: 0.15137102773996886, test loss: 0.26607605685023134\n",
      "epoch 4166: train loss: 0.15136019232695397, test loss: 0.2660733831579017\n",
      "epoch 4167: train loss: 0.15134936077864353, test loss: 0.26607071126738446\n",
      "epoch 4168: train loss: 0.15133853309267203, test loss: 0.2660680411772082\n",
      "epoch 4169: train loss: 0.15132770926667594, test loss: 0.2660653728859026\n",
      "epoch 4170: train loss: 0.15131688929829373, test loss: 0.26606270639199936\n",
      "epoch 4171: train loss: 0.15130607318516592, test loss: 0.26606004169403136\n",
      "epoch 4172: train loss: 0.15129526092493495, test loss: 0.2660573787905332\n",
      "epoch 4173: train loss: 0.15128445251524542, test loss: 0.2660547176800408\n",
      "epoch 4174: train loss: 0.15127364795374384, test loss: 0.2660520583610918\n",
      "epoch 4175: train loss: 0.1512628472380787, test loss: 0.266049400832225\n",
      "epoch 4176: train loss: 0.15125205036590056, test loss: 0.26604674509198134\n",
      "epoch 4177: train loss: 0.15124125733486188, test loss: 0.26604409113890254\n",
      "epoch 4178: train loss: 0.15123046814261726, test loss: 0.2660414389715324\n",
      "epoch 4179: train loss: 0.1512196827868232, test loss: 0.26603878858841584\n",
      "epoch 4180: train loss: 0.1512089012651382, test loss: 0.2660361399880995\n",
      "epoch 4181: train loss: 0.15119812357522278, test loss: 0.26603349316913144\n",
      "epoch 4182: train loss: 0.15118734971473938, test loss: 0.2660308481300613\n",
      "epoch 4183: train loss: 0.15117657968135254, test loss: 0.26602820486944\n",
      "epoch 4184: train loss: 0.15116581347272867, test loss: 0.2660255633858203\n",
      "epoch 4185: train loss: 0.15115505108653626, test loss: 0.2660229236777561\n",
      "epoch 4186: train loss: 0.15114429252044573, test loss: 0.26602028574380304\n",
      "epoch 4187: train loss: 0.15113353777212943, test loss: 0.2660176495825182\n",
      "epoch 4188: train loss: 0.15112278683926175, test loss: 0.26601501519246\n",
      "epoch 4189: train loss: 0.1511120397195191, test loss: 0.2660123825721885\n",
      "epoch 4190: train loss: 0.1511012964105797, test loss: 0.2660097517202653\n",
      "epoch 4191: train loss: 0.15109055691012394, test loss: 0.2660071226352532\n",
      "epoch 4192: train loss: 0.15107982121583396, test loss: 0.2660044953157169\n",
      "epoch 4193: train loss: 0.15106908932539415, test loss: 0.26600186976022216\n",
      "epoch 4194: train loss: 0.1510583612364905, test loss: 0.2659992459673365\n",
      "epoch 4195: train loss: 0.15104763694681125, test loss: 0.2659966239356288\n",
      "epoch 4196: train loss: 0.15103691645404652, test loss: 0.2659940036636695\n",
      "epoch 4197: train loss: 0.15102619975588832, test loss: 0.2659913851500303\n",
      "epoch 4198: train loss: 0.15101548685003074, test loss: 0.2659887683932846\n",
      "epoch 4199: train loss: 0.15100477773416965, test loss: 0.26598615339200726\n",
      "epoch 4200: train loss: 0.150994072406003, test loss: 0.2659835401447745\n",
      "epoch 4201: train loss: 0.15098337086323066, test loss: 0.26598092865016404\n",
      "epoch 4202: train loss: 0.15097267310355444, test loss: 0.26597831890675505\n",
      "epoch 4203: train loss: 0.15096197912467807, test loss: 0.2659757109131282\n",
      "epoch 4204: train loss: 0.1509512889243073, test loss: 0.26597310466786567\n",
      "epoch 4205: train loss: 0.15094060250014965, test loss: 0.2659705001695509\n",
      "epoch 4206: train loss: 0.15092991984991477, test loss: 0.2659678974167691\n",
      "epoch 4207: train loss: 0.15091924097131415, test loss: 0.26596529640810657\n",
      "epoch 4208: train loss: 0.15090856586206125, test loss: 0.2659626971421515\n",
      "epoch 4209: train loss: 0.1508978945198714, test loss: 0.265960099617493\n",
      "epoch 4210: train loss: 0.15088722694246187, test loss: 0.2659575038327221\n",
      "epoch 4211: train loss: 0.15087656312755196, test loss: 0.265954909786431\n",
      "epoch 4212: train loss: 0.15086590307286277, test loss: 0.2659523174772135\n",
      "epoch 4213: train loss: 0.15085524677611736, test loss: 0.2659497269036648\n",
      "epoch 4214: train loss: 0.15084459423504076, test loss: 0.2659471380643816\n",
      "epoch 4215: train loss: 0.15083394544735987, test loss: 0.2659445509579619\n",
      "epoch 4216: train loss: 0.1508233004108035, test loss: 0.26594196558300504\n",
      "epoch 4217: train loss: 0.15081265912310238, test loss: 0.2659393819381123\n",
      "epoch 4218: train loss: 0.15080202158198916, test loss: 0.26593680002188586\n",
      "epoch 4219: train loss: 0.15079138778519843, test loss: 0.2659342198329297\n",
      "epoch 4220: train loss: 0.15078075773046667, test loss: 0.265931641369849\n",
      "epoch 4221: train loss: 0.1507701314155322, test loss: 0.2659290646312504\n",
      "epoch 4222: train loss: 0.15075950883813535, test loss: 0.26592648961574217\n",
      "epoch 4223: train loss: 0.15074888999601826, test loss: 0.26592391632193374\n",
      "epoch 4224: train loss: 0.15073827488692504, test loss: 0.2659213447484362\n",
      "epoch 4225: train loss: 0.1507276635086017, test loss: 0.2659187748938619\n",
      "epoch 4226: train loss: 0.15071705585879605, test loss: 0.26591620675682476\n",
      "epoch 4227: train loss: 0.15070645193525792, test loss: 0.2659136403359399\n",
      "epoch 4228: train loss: 0.1506958517357389, test loss: 0.2659110756298241\n",
      "epoch 4229: train loss: 0.15068525525799262, test loss: 0.2659085126370955\n",
      "epoch 4230: train loss: 0.15067466249977446, test loss: 0.26590595135637346\n",
      "epoch 4231: train loss: 0.15066407345884175, test loss: 0.2659033917862791\n",
      "epoch 4232: train loss: 0.15065348813295373, test loss: 0.2659008339254346\n",
      "epoch 4233: train loss: 0.15064290651987147, test loss: 0.2658982777724639\n",
      "epoch 4234: train loss: 0.15063232861735792, test loss: 0.265895723325992\n",
      "epoch 4235: train loss: 0.1506217544231779, test loss: 0.26589317058464557\n",
      "epoch 4236: train loss: 0.15061118393509826, test loss: 0.2658906195470527\n",
      "epoch 4237: train loss: 0.15060061715088746, test loss: 0.2658880702118426\n",
      "epoch 4238: train loss: 0.150590054068316, test loss: 0.26588552257764614\n",
      "epoch 4239: train loss: 0.15057949468515622, test loss: 0.2658829766430955\n",
      "epoch 4240: train loss: 0.15056893899918233, test loss: 0.26588043240682446\n",
      "epoch 4241: train loss: 0.15055838700817042, test loss: 0.26587788986746785\n",
      "epoch 4242: train loss: 0.1505478387098984, test loss: 0.26587534902366206\n",
      "epoch 4243: train loss: 0.15053729410214603, test loss: 0.26587280987404505\n",
      "epoch 4244: train loss: 0.150526753182695, test loss: 0.26587027241725586\n",
      "epoch 4245: train loss: 0.15051621594932885, test loss: 0.2658677366519352\n",
      "epoch 4246: train loss: 0.15050568239983286, test loss: 0.2658652025767249\n",
      "epoch 4247: train loss: 0.15049515253199433, test loss: 0.26586267019026855\n",
      "epoch 4248: train loss: 0.15048462634360227, test loss: 0.2658601394912107\n",
      "epoch 4249: train loss: 0.15047410383244764, test loss: 0.26585761047819767\n",
      "epoch 4250: train loss: 0.15046358499632315, test loss: 0.26585508314987677\n",
      "epoch 4251: train loss: 0.15045306983302348, test loss: 0.2658525575048972\n",
      "epoch 4252: train loss: 0.15044255834034506, test loss: 0.2658500335419091\n",
      "epoch 4253: train loss: 0.15043205051608616, test loss: 0.2658475112595642\n",
      "epoch 4254: train loss: 0.15042154635804697, test loss: 0.2658449906565155\n",
      "epoch 4255: train loss: 0.15041104586402942, test loss: 0.26584247173141745\n",
      "epoch 4256: train loss: 0.15040054903183733, test loss: 0.26583995448292597\n",
      "epoch 4257: train loss: 0.15039005585927634, test loss: 0.2658374389096981\n",
      "epoch 4258: train loss: 0.15037956634415398, test loss: 0.26583492501039246\n",
      "epoch 4259: train loss: 0.15036908048427947, test loss: 0.26583241278366904\n",
      "epoch 4260: train loss: 0.15035859827746398, test loss: 0.2658299022281891\n",
      "epoch 4261: train loss: 0.15034811972152046, test loss: 0.26582739334261524\n",
      "epoch 4262: train loss: 0.15033764481426373, test loss: 0.26582488612561156\n",
      "epoch 4263: train loss: 0.15032717355351036, test loss: 0.2658223805758435\n",
      "epoch 4264: train loss: 0.15031670593707877, test loss: 0.2658198766919778\n",
      "epoch 4265: train loss: 0.15030624196278922, test loss: 0.26581737447268255\n",
      "epoch 4266: train loss: 0.15029578162846374, test loss: 0.2658148739166272\n",
      "epoch 4267: train loss: 0.15028532493192623, test loss: 0.2658123750224827\n",
      "epoch 4268: train loss: 0.15027487187100239, test loss: 0.26580987778892123\n",
      "epoch 4269: train loss: 0.15026442244351965, test loss: 0.2658073822146163\n",
      "epoch 4270: train loss: 0.15025397664730739, test loss: 0.26580488829824284\n",
      "epoch 4271: train loss: 0.1502435344801967, test loss: 0.2658023960384771\n",
      "epoch 4272: train loss: 0.1502330959400205, test loss: 0.26579990543399673\n",
      "epoch 4273: train loss: 0.15022266102461346, test loss: 0.26579741648348065\n",
      "epoch 4274: train loss: 0.1502122297318121, test loss: 0.2657949291856092\n",
      "epoch 4275: train loss: 0.1502018020594548, test loss: 0.2657924435390641\n",
      "epoch 4276: train loss: 0.15019137800538165, test loss: 0.2657899595425282\n",
      "epoch 4277: train loss: 0.15018095756743452, test loss: 0.26578747719468604\n",
      "epoch 4278: train loss: 0.15017054074345715, test loss: 0.26578499649422305\n",
      "epoch 4279: train loss: 0.150160127531295, test loss: 0.2657825174398265\n",
      "epoch 4280: train loss: 0.1501497179287954, test loss: 0.2657800400301846\n",
      "epoch 4281: train loss: 0.15013931193380736, test loss: 0.2657775642639872\n",
      "epoch 4282: train loss: 0.1501289095441818, test loss: 0.26577509013992523\n",
      "epoch 4283: train loss: 0.15011851075777127, test loss: 0.2657726176566911\n",
      "epoch 4284: train loss: 0.1501081155724303, test loss: 0.26577014681297845\n",
      "epoch 4285: train loss: 0.150097723986015, test loss: 0.2657676776074824\n",
      "epoch 4286: train loss: 0.1500873359963834, test loss: 0.2657652100388992\n",
      "epoch 4287: train loss: 0.15007695160139525, test loss: 0.26576274410592665\n",
      "epoch 4288: train loss: 0.15006657079891203, test loss: 0.2657602798072638\n",
      "epoch 4289: train loss: 0.1500561935867971, test loss: 0.2657578171416109\n",
      "epoch 4290: train loss: 0.15004581996291544, test loss: 0.2657553561076696\n",
      "epoch 4291: train loss: 0.150035449925134, test loss: 0.265752896704143\n",
      "epoch 4292: train loss: 0.15002508347132135, test loss: 0.26575043892973527\n",
      "epoch 4293: train loss: 0.1500147205993478, test loss: 0.26574798278315215\n",
      "epoch 4294: train loss: 0.15000436130708555, test loss: 0.26574552826310055\n",
      "epoch 4295: train loss: 0.1499940055924085, test loss: 0.2657430753682887\n",
      "epoch 4296: train loss: 0.1499836534531922, test loss: 0.26574062409742616\n",
      "epoch 4297: train loss: 0.14997330488731417, test loss: 0.2657381744492239\n",
      "epoch 4298: train loss: 0.14996295989265357, test loss: 0.26573572642239396\n",
      "epoch 4299: train loss: 0.1499526184670912, test loss: 0.2657332800156501\n",
      "epoch 4300: train loss: 0.14994228060850986, test loss: 0.26573083522770685\n",
      "epoch 4301: train loss: 0.14993194631479395, test loss: 0.2657283920572806\n",
      "epoch 4302: train loss: 0.14992161558382958, test loss: 0.26572595050308856\n",
      "epoch 4303: train loss: 0.1499112884135047, test loss: 0.2657235105638495\n",
      "epoch 4304: train loss: 0.14990096480170897, test loss: 0.26572107223828356\n",
      "epoch 4305: train loss: 0.14989064474633382, test loss: 0.26571863552511205\n",
      "epoch 4306: train loss: 0.1498803282452723, test loss: 0.26571620042305755\n",
      "epoch 4307: train loss: 0.14987001529641938, test loss: 0.26571376693084386\n",
      "epoch 4308: train loss: 0.14985970589767164, test loss: 0.26571133504719646\n",
      "epoch 4309: train loss: 0.14984940004692743, test loss: 0.26570890477084175\n",
      "epoch 4310: train loss: 0.14983909774208684, test loss: 0.2657064761005075\n",
      "epoch 4311: train loss: 0.14982879898105173, test loss: 0.26570404903492295\n",
      "epoch 4312: train loss: 0.14981850376172554, test loss: 0.2657016235728183\n",
      "epoch 4313: train loss: 0.14980821208201361, test loss: 0.2656991997129253\n",
      "epoch 4314: train loss: 0.14979792393982297, test loss: 0.26569677745397713\n",
      "epoch 4315: train loss: 0.1497876393330623, test loss: 0.26569435679470776\n",
      "epoch 4316: train loss: 0.14977735825964206, test loss: 0.2656919377338529\n",
      "epoch 4317: train loss: 0.1497670807174744, test loss: 0.26568952027014936\n",
      "epoch 4318: train loss: 0.14975680670447325, test loss: 0.2656871044023352\n",
      "epoch 4319: train loss: 0.14974653621855416, test loss: 0.26568469012914975\n",
      "epoch 4320: train loss: 0.14973626925763447, test loss: 0.26568227744933376\n",
      "epoch 4321: train loss: 0.14972600581963322, test loss: 0.2656798663616292\n",
      "epoch 4322: train loss: 0.14971574590247116, test loss: 0.26567745686477917\n",
      "epoch 4323: train loss: 0.14970548950407067, test loss: 0.26567504895752825\n",
      "epoch 4324: train loss: 0.149695236622356, test loss: 0.2656726426386222\n",
      "epoch 4325: train loss: 0.14968498725525298, test loss: 0.2656702379068081\n",
      "epoch 4326: train loss: 0.14967474140068918, test loss: 0.2656678347608341\n",
      "epoch 4327: train loss: 0.14966449905659385, test loss: 0.2656654331994499\n",
      "epoch 4328: train loss: 0.149654260220898, test loss: 0.26566303322140633\n",
      "epoch 4329: train loss: 0.14964402489153428, test loss: 0.26566063482545543\n",
      "epoch 4330: train loss: 0.14963379306643707, test loss: 0.2656582380103506\n",
      "epoch 4331: train loss: 0.14962356474354244, test loss: 0.26565584277484655\n",
      "epoch 4332: train loss: 0.14961333992078812, test loss: 0.26565344911769906\n",
      "epoch 4333: train loss: 0.14960311859611358, test loss: 0.26565105703766534\n",
      "epoch 4334: train loss: 0.14959290076745996, test loss: 0.2656486665335038\n",
      "epoch 4335: train loss: 0.14958268643277006, test loss: 0.265646277603974\n",
      "epoch 4336: train loss: 0.14957247558998843, test loss: 0.265643890247837\n",
      "epoch 4337: train loss: 0.14956226823706129, test loss: 0.26564150446385504\n",
      "epoch 4338: train loss: 0.14955206437193647, test loss: 0.26563912025079134\n",
      "epoch 4339: train loss: 0.14954186399256353, test loss: 0.26563673760741074\n",
      "epoch 4340: train loss: 0.14953166709689375, test loss: 0.2656343565324791\n",
      "epoch 4341: train loss: 0.14952147368288005, test loss: 0.2656319770247636\n",
      "epoch 4342: train loss: 0.149511283748477, test loss: 0.26562959908303263\n",
      "epoch 4343: train loss: 0.14950109729164088, test loss: 0.265627222706056\n",
      "epoch 4344: train loss: 0.14949091431032963, test loss: 0.26562484789260443\n",
      "epoch 4345: train loss: 0.14948073480250285, test loss: 0.2656224746414502\n",
      "epoch 4346: train loss: 0.14947055876612186, test loss: 0.26562010295136673\n",
      "epoch 4347: train loss: 0.1494603861991496, test loss: 0.2656177328211286\n",
      "epoch 4348: train loss: 0.14945021709955061, test loss: 0.26561536424951165\n",
      "epoch 4349: train loss: 0.1494400514652913, test loss: 0.2656129972352931\n",
      "epoch 4350: train loss: 0.1494298892943395, test loss: 0.2656106317772512\n",
      "epoch 4351: train loss: 0.14941973058466484, test loss: 0.2656082678741656\n",
      "epoch 4352: train loss: 0.14940957533423863, test loss: 0.26560590552481705\n",
      "epoch 4353: train loss: 0.14939942354103372, test loss: 0.2656035447279877\n",
      "epoch 4354: train loss: 0.1493892752030247, test loss: 0.26560118548246064\n",
      "epoch 4355: train loss: 0.1493791303181878, test loss: 0.2655988277870206\n",
      "epoch 4356: train loss: 0.14936898888450093, test loss: 0.26559647164045314\n",
      "epoch 4357: train loss: 0.1493588508999435, test loss: 0.2655941170415453\n",
      "epoch 4358: train loss: 0.1493487163624968, test loss: 0.2655917639890852\n",
      "epoch 4359: train loss: 0.14933858527014363, test loss: 0.26558941248186235\n",
      "epoch 4360: train loss: 0.1493284576208684, test loss: 0.2655870625186672\n",
      "epoch 4361: train loss: 0.1493183334126573, test loss: 0.2655847140982918\n",
      "epoch 4362: train loss: 0.14930821264349797, test loss: 0.2655823672195291\n",
      "epoch 4363: train loss: 0.14929809531137989, test loss: 0.26558002188117336\n",
      "epoch 4364: train loss: 0.14928798141429403, test loss: 0.26557767808202015\n",
      "epoch 4365: train loss: 0.1492778709502331, test loss: 0.2655753358208661\n",
      "epoch 4366: train loss: 0.14926776391719135, test loss: 0.26557299509650933\n",
      "epoch 4367: train loss: 0.1492576603131647, test loss: 0.2655706559077487\n",
      "epoch 4368: train loss: 0.1492475601361508, test loss: 0.2655683182533847\n",
      "epoch 4369: train loss: 0.14923746338414873, test loss: 0.2655659821322189\n",
      "epoch 4370: train loss: 0.14922737005515935, test loss: 0.265563647543054\n",
      "epoch 4371: train loss: 0.1492172801471851, test loss: 0.26556131448469406\n",
      "epoch 4372: train loss: 0.14920719365823007, test loss: 0.26555898295594416\n",
      "epoch 4373: train loss: 0.14919711058629995, test loss: 0.26555665295561076\n",
      "epoch 4374: train loss: 0.149187030929402, test loss: 0.26555432448250144\n",
      "epoch 4375: train loss: 0.14917695468554515, test loss: 0.2655519975354249\n",
      "epoch 4376: train loss: 0.14916688185274005, test loss: 0.26554967211319114\n",
      "epoch 4377: train loss: 0.14915681242899873, test loss: 0.2655473482146114\n",
      "epoch 4378: train loss: 0.14914674641233505, test loss: 0.26554502583849804\n",
      "epoch 4379: train loss: 0.14913668380076436, test loss: 0.26554270498366456\n",
      "epoch 4380: train loss: 0.1491266245923037, test loss: 0.26554038564892585\n",
      "epoch 4381: train loss: 0.1491165687849716, test loss: 0.26553806783309775\n",
      "epoch 4382: train loss: 0.14910651637678837, test loss: 0.2655357515349975\n",
      "epoch 4383: train loss: 0.14909646736577578, test loss: 0.2655334367534433\n",
      "epoch 4384: train loss: 0.14908642174995726, test loss: 0.2655311234872549\n",
      "epoch 4385: train loss: 0.14907637952735783, test loss: 0.26552881173525283\n",
      "epoch 4386: train loss: 0.1490663406960042, test loss: 0.26552650149625906\n",
      "epoch 4387: train loss: 0.14905630525392444, test loss: 0.26552419276909667\n",
      "epoch 4388: train loss: 0.14904627319914854, test loss: 0.2655218855525899\n",
      "epoch 4389: train loss: 0.14903624452970782, test loss: 0.2655195798455643\n",
      "epoch 4390: train loss: 0.14902621924363532, test loss: 0.26551727564684635\n",
      "epoch 4391: train loss: 0.14901619733896568, test loss: 0.26551497295526394\n",
      "epoch 4392: train loss: 0.14900617881373507, test loss: 0.2655126717696461\n",
      "epoch 4393: train loss: 0.14899616366598123, test loss: 0.265510372088823\n",
      "epoch 4394: train loss: 0.14898615189374365, test loss: 0.26550807391162584\n",
      "epoch 4395: train loss: 0.14897614349506325, test loss: 0.26550577723688723\n",
      "epoch 4396: train loss: 0.14896613846798248, test loss: 0.26550348206344093\n",
      "epoch 4397: train loss: 0.14895613681054562, test loss: 0.2655011883901217\n",
      "epoch 4398: train loss: 0.14894613852079824, test loss: 0.2654988962157656\n",
      "epoch 4399: train loss: 0.14893614359678772, test loss: 0.2654966055392098\n",
      "epoch 4400: train loss: 0.14892615203656295, test loss: 0.2654943163592927\n",
      "epoch 4401: train loss: 0.14891616383817427, test loss: 0.26549202867485383\n",
      "epoch 4402: train loss: 0.1489061789996738, test loss: 0.26548974248473384\n",
      "epoch 4403: train loss: 0.14889619751911504, test loss: 0.2654874577877746\n",
      "epoch 4404: train loss: 0.14888621939455318, test loss: 0.2654851745828192\n",
      "epoch 4405: train loss: 0.14887624462404506, test loss: 0.2654828928687117\n",
      "epoch 4406: train loss: 0.1488662732056488, test loss: 0.26548061264429745\n",
      "epoch 4407: train loss: 0.1488563051374244, test loss: 0.2654783339084231\n",
      "epoch 4408: train loss: 0.14884634041743322, test loss: 0.26547605665993623\n",
      "epoch 4409: train loss: 0.1488363790437383, test loss: 0.2654737808976856\n",
      "epoch 4410: train loss: 0.1488264210144042, test loss: 0.26547150662052116\n",
      "epoch 4411: train loss: 0.14881646632749695, test loss: 0.2654692338272941\n",
      "epoch 4412: train loss: 0.14880651498108435, test loss: 0.2654669625168566\n",
      "epoch 4413: train loss: 0.1487965669732355, test loss: 0.2654646926880622\n",
      "epoch 4414: train loss: 0.1487866223020213, test loss: 0.26546242433976536\n",
      "epoch 4415: train loss: 0.14877668096551402, test loss: 0.26546015747082186\n",
      "epoch 4416: train loss: 0.1487667429617876, test loss: 0.26545789208008846\n",
      "epoch 4417: train loss: 0.1487568082889174, test loss: 0.2654556281664234\n",
      "epoch 4418: train loss: 0.1487468769449805, test loss: 0.26545336572868555\n",
      "epoch 4419: train loss: 0.1487369489280554, test loss: 0.26545110476573536\n",
      "epoch 4420: train loss: 0.1487270242362222, test loss: 0.2654488452764342\n",
      "epoch 4421: train loss: 0.14871710286756248, test loss: 0.26544658725964465\n",
      "epoch 4422: train loss: 0.14870718482015946, test loss: 0.26544433071423057\n",
      "epoch 4423: train loss: 0.14869727009209785, test loss: 0.2654420756390566\n",
      "epoch 4424: train loss: 0.14868735868146385, test loss: 0.26543982203298877\n",
      "epoch 4425: train loss: 0.14867745058634532, test loss: 0.2654375698948943\n",
      "epoch 4426: train loss: 0.14866754580483155, test loss: 0.2654353192236414\n",
      "epoch 4427: train loss: 0.14865764433501333, test loss: 0.26543307001809935\n",
      "epoch 4428: train loss: 0.14864774617498316, test loss: 0.2654308222771388\n",
      "epoch 4429: train loss: 0.1486378513228349, test loss: 0.26542857599963143\n",
      "epoch 4430: train loss: 0.14862795977666401, test loss: 0.2654263311844498\n",
      "epoch 4431: train loss: 0.1486180715345675, test loss: 0.26542408783046795\n",
      "epoch 4432: train loss: 0.1486081865946438, test loss: 0.265421845936561\n",
      "epoch 4433: train loss: 0.14859830495499307, test loss: 0.2654196055016049\n",
      "epoch 4434: train loss: 0.14858842661371674, test loss: 0.2654173665244771\n",
      "epoch 4435: train loss: 0.14857855156891797, test loss: 0.26541512900405584\n",
      "epoch 4436: train loss: 0.14856867981870134, test loss: 0.2654128929392208\n",
      "epoch 4437: train loss: 0.14855881136117294, test loss: 0.2654106583288525\n",
      "epoch 4438: train loss: 0.1485489461944404, test loss: 0.26540842517183283\n",
      "epoch 4439: train loss: 0.14853908431661292, test loss: 0.26540619346704447\n",
      "epoch 4440: train loss: 0.1485292257258011, test loss: 0.26540396321337156\n",
      "epoch 4441: train loss: 0.14851937042011717, test loss: 0.2654017344096991\n",
      "epoch 4442: train loss: 0.14850951839767482, test loss: 0.26539950705491333\n",
      "epoch 4443: train loss: 0.14849966965658917, test loss: 0.2653972811479016\n",
      "epoch 4444: train loss: 0.14848982419497703, test loss: 0.2653950566875524\n",
      "epoch 4445: train loss: 0.14847998201095652, test loss: 0.2653928336727552\n",
      "epoch 4446: train loss: 0.14847014310264744, test loss: 0.2653906121024006\n",
      "epoch 4447: train loss: 0.14846030746817096, test loss: 0.26538839197538056\n",
      "epoch 4448: train loss: 0.14845047510564982, test loss: 0.2653861732905878\n",
      "epoch 4449: train loss: 0.1484406460132082, test loss: 0.2653839560469163\n",
      "epoch 4450: train loss: 0.14843082018897188, test loss: 0.26538174024326117\n",
      "epoch 4451: train loss: 0.14842099763106809, test loss: 0.2653795258785185\n",
      "epoch 4452: train loss: 0.14841117833762546, test loss: 0.26537731295158573\n",
      "epoch 4453: train loss: 0.14840136230677425, test loss: 0.2653751014613611\n",
      "epoch 4454: train loss: 0.14839154953664618, test loss: 0.2653728914067441\n",
      "epoch 4455: train loss: 0.14838174002537444, test loss: 0.2653706827866354\n",
      "epoch 4456: train loss: 0.14837193377109364, test loss: 0.2653684755999366\n",
      "epoch 4457: train loss: 0.14836213077194005, test loss: 0.2653662698455504\n",
      "epoch 4458: train loss: 0.14835233102605128, test loss: 0.26536406552238084\n",
      "epoch 4459: train loss: 0.14834253453156646, test loss: 0.26536186262933265\n",
      "epoch 4460: train loss: 0.14833274128662624, test loss: 0.265359661165312\n",
      "epoch 4461: train loss: 0.14832295128937273, test loss: 0.265357461129226\n",
      "epoch 4462: train loss: 0.14831316453794946, test loss: 0.2653552625199828\n",
      "epoch 4463: train loss: 0.14830338103050156, test loss: 0.26535306533649183\n",
      "epoch 4464: train loss: 0.1482936007651756, test loss: 0.2653508695776633\n",
      "epoch 4465: train loss: 0.1482838237401195, test loss: 0.2653486752424089\n",
      "epoch 4466: train loss: 0.14827404995348287, test loss: 0.2653464823296411\n",
      "epoch 4467: train loss: 0.1482642794034166, test loss: 0.26534429083827343\n",
      "epoch 4468: train loss: 0.14825451208807316, test loss: 0.2653421007672207\n",
      "epoch 4469: train loss: 0.14824474800560647, test loss: 0.2653399121153988\n",
      "epoch 4470: train loss: 0.14823498715417188, test loss: 0.26533772488172447\n",
      "epoch 4471: train loss: 0.14822522953192624, test loss: 0.2653355390651157\n",
      "epoch 4472: train loss: 0.1482154751370279, test loss: 0.2653333546644916\n",
      "epoch 4473: train loss: 0.14820572396763657, test loss: 0.2653311716787722\n",
      "epoch 4474: train loss: 0.14819597602191356, test loss: 0.26532899010687866\n",
      "epoch 4475: train loss: 0.14818623129802153, test loss: 0.2653268099477333\n",
      "epoch 4476: train loss: 0.14817648979412465, test loss: 0.26532463120025945\n",
      "epoch 4477: train loss: 0.14816675150838854, test loss: 0.2653224538633814\n",
      "epoch 4478: train loss: 0.14815701643898027, test loss: 0.26532027793602475\n",
      "epoch 4479: train loss: 0.14814728458406834, test loss: 0.265318103417116\n",
      "epoch 4480: train loss: 0.1481375559418228, test loss: 0.26531593030558265\n",
      "epoch 4481: train loss: 0.14812783051041503, test loss: 0.26531375860035356\n",
      "epoch 4482: train loss: 0.14811810828801794, test loss: 0.2653115883003582\n",
      "epoch 4483: train loss: 0.14810838927280584, test loss: 0.26530941940452757\n",
      "epoch 4484: train loss: 0.14809867346295455, test loss: 0.2653072519117935\n",
      "epoch 4485: train loss: 0.1480889608566413, test loss: 0.2653050858210888\n",
      "epoch 4486: train loss: 0.14807925145204473, test loss: 0.2653029211313475\n",
      "epoch 4487: train loss: 0.148069545247345, test loss: 0.26530075784150464\n",
      "epoch 4488: train loss: 0.14805984224072363, test loss: 0.2652985959504964\n",
      "epoch 4489: train loss: 0.14805014243036363, test loss: 0.26529643545725967\n",
      "epoch 4490: train loss: 0.14804044581444947, test loss: 0.26529427636073283\n",
      "epoch 4491: train loss: 0.148030752391167, test loss: 0.2652921186598552\n",
      "epoch 4492: train loss: 0.14802106215870356, test loss: 0.26528996235356694\n",
      "epoch 4493: train loss: 0.14801137511524787, test loss: 0.2652878074408094\n",
      "epoch 4494: train loss: 0.1480016912589901, test loss: 0.2652856539205251\n",
      "epoch 4495: train loss: 0.14799201058812195, test loss: 0.2652835017916575\n",
      "epoch 4496: train loss: 0.14798233310083636, test loss: 0.265281351053151\n",
      "epoch 4497: train loss: 0.14797265879532787, test loss: 0.2652792017039512\n",
      "epoch 4498: train loss: 0.14796298766979235, test loss: 0.2652770537430047\n",
      "epoch 4499: train loss: 0.1479533197224271, test loss: 0.26527490716925906\n",
      "epoch 4500: train loss: 0.14794365495143094, test loss: 0.265272761981663\n",
      "epoch 4501: train loss: 0.147933993355004, test loss: 0.2652706181791664\n",
      "epoch 4502: train loss: 0.1479243349313479, test loss: 0.26526847576071977\n",
      "epoch 4503: train loss: 0.14791467967866567, test loss: 0.26526633472527517\n",
      "epoch 4504: train loss: 0.14790502759516172, test loss: 0.2652641950717851\n",
      "epoch 4505: train loss: 0.14789537867904187, test loss: 0.2652620567992038\n",
      "epoch 4506: train loss: 0.14788573292851348, test loss: 0.265259919906486\n",
      "epoch 4507: train loss: 0.14787609034178517, test loss: 0.2652577843925876\n",
      "epoch 4508: train loss: 0.147866450917067, test loss: 0.2652556502564658\n",
      "epoch 4509: train loss: 0.14785681465257058, test loss: 0.26525351749707843\n",
      "epoch 4510: train loss: 0.14784718154650878, test loss: 0.26525138611338445\n",
      "epoch 4511: train loss: 0.1478375515970959, test loss: 0.26524925610434424\n",
      "epoch 4512: train loss: 0.14782792480254772, test loss: 0.2652471274689186\n",
      "epoch 4513: train loss: 0.14781830116108138, test loss: 0.26524500020606984\n",
      "epoch 4514: train loss: 0.1478086806709154, test loss: 0.265242874314761\n",
      "epoch 4515: train loss: 0.14779906333026976, test loss: 0.2652407497939564\n",
      "epoch 4516: train loss: 0.1477894491373658, test loss: 0.2652386266426212\n",
      "epoch 4517: train loss: 0.14777983809042625, test loss: 0.26523650485972167\n",
      "epoch 4518: train loss: 0.14777023018767532, test loss: 0.265234384444225\n",
      "epoch 4519: train loss: 0.1477606254273385, test loss: 0.26523226539509936\n",
      "epoch 4520: train loss: 0.14775102380764277, test loss: 0.2652301477113142\n",
      "epoch 4521: train loss: 0.14774142532681644, test loss: 0.26522803139183987\n",
      "epoch 4522: train loss: 0.14773182998308929, test loss: 0.2652259164356476\n",
      "epoch 4523: train loss: 0.1477222377746924, test loss: 0.2652238028417097\n",
      "epoch 4524: train loss: 0.14771264869985834, test loss: 0.2652216906089997\n",
      "epoch 4525: train loss: 0.14770306275682096, test loss: 0.2652195797364918\n",
      "epoch 4526: train loss: 0.14769347994381557, test loss: 0.2652174702231615\n",
      "epoch 4527: train loss: 0.1476839002590789, test loss: 0.2652153620679851\n",
      "epoch 4528: train loss: 0.147674323700849, test loss: 0.2652132552699402\n",
      "epoch 4529: train loss: 0.14766475026736522, test loss: 0.26521114982800514\n",
      "epoch 4530: train loss: 0.1476551799568685, test loss: 0.2652090457411592\n",
      "epoch 4531: train loss: 0.14764561276760102, test loss: 0.265206943008383\n",
      "epoch 4532: train loss: 0.1476360486978064, test loss: 0.265204841628658\n",
      "epoch 4533: train loss: 0.14762648774572956, test loss: 0.26520274160096646\n",
      "epoch 4534: train loss: 0.14761692990961686, test loss: 0.26520064292429196\n",
      "epoch 4535: train loss: 0.14760737518771605, test loss: 0.2651985455976189\n",
      "epoch 4536: train loss: 0.14759782357827622, test loss: 0.26519644961993283\n",
      "epoch 4537: train loss: 0.14758827507954783, test loss: 0.26519435499022015\n",
      "epoch 4538: train loss: 0.14757872968978272, test loss: 0.26519226170746835\n",
      "epoch 4539: train loss: 0.14756918740723413, test loss: 0.26519016977066584\n",
      "epoch 4540: train loss: 0.14755964823015655, test loss: 0.2651880791788021\n",
      "epoch 4541: train loss: 0.14755011215680605, test loss: 0.26518598993086756\n",
      "epoch 4542: train loss: 0.1475405791854399, test loss: 0.26518390202585373\n",
      "epoch 4543: train loss: 0.1475310493143167, test loss: 0.265181815462753\n",
      "epoch 4544: train loss: 0.1475215225416966, test loss: 0.26517973024055885\n",
      "epoch 4545: train loss: 0.14751199886584093, test loss: 0.2651776463582655\n",
      "epoch 4546: train loss: 0.14750247828501248, test loss: 0.2651755638148687\n",
      "epoch 4547: train loss: 0.14749296079747537, test loss: 0.2651734826093646\n",
      "epoch 4548: train loss: 0.14748344640149508, test loss: 0.2651714027407507\n",
      "epoch 4549: train loss: 0.14747393509533843, test loss: 0.2651693242080253\n",
      "epoch 4550: train loss: 0.14746442687727362, test loss: 0.2651672470101878\n",
      "epoch 4551: train loss: 0.1474549217455702, test loss: 0.2651651711462386\n",
      "epoch 4552: train loss: 0.1474454196984991, test loss: 0.265163096615179\n",
      "epoch 4553: train loss: 0.14743592073433248, test loss: 0.2651610234160114\n",
      "epoch 4554: train loss: 0.147426424851344, test loss: 0.26515895154773894\n",
      "epoch 4555: train loss: 0.14741693204780862, test loss: 0.2651568810093659\n",
      "epoch 4556: train loss: 0.1474074423220026, test loss: 0.26515481179989775\n",
      "epoch 4557: train loss: 0.14739795567220354, test loss: 0.2651527439183406\n",
      "epoch 4558: train loss: 0.1473884720966905, test loss: 0.2651506773637016\n",
      "epoch 4559: train loss: 0.14737899159374376, test loss: 0.265148612134989\n",
      "epoch 4560: train loss: 0.147369514161645, test loss: 0.26514654823121203\n",
      "epoch 4561: train loss: 0.1473600397986772, test loss: 0.26514448565138077\n",
      "epoch 4562: train loss: 0.14735056850312475, test loss: 0.26514242439450636\n",
      "epoch 4563: train loss: 0.1473411002732733, test loss: 0.2651403644596008\n",
      "epoch 4564: train loss: 0.14733163510740988, test loss: 0.26513830584567727\n",
      "epoch 4565: train loss: 0.14732217300382286, test loss: 0.2651362485517497\n",
      "epoch 4566: train loss: 0.14731271396080187, test loss: 0.2651341925768332\n",
      "epoch 4567: train loss: 0.147303257976638, test loss: 0.2651321379199436\n",
      "epoch 4568: train loss: 0.14729380504962358, test loss: 0.26513008458009796\n",
      "epoch 4569: train loss: 0.14728435517805225, test loss: 0.2651280325563141\n",
      "epoch 4570: train loss: 0.14727490836021906, test loss: 0.2651259818476109\n",
      "epoch 4571: train loss: 0.14726546459442033, test loss: 0.26512393245300814\n",
      "epoch 4572: train loss: 0.14725602387895376, test loss: 0.2651218843715267\n",
      "epoch 4573: train loss: 0.14724658621211828, test loss: 0.2651198376021882\n",
      "epoch 4574: train loss: 0.1472371515922142, test loss: 0.2651177921440155\n",
      "epoch 4575: train loss: 0.1472277200175432, test loss: 0.26511574799603216\n",
      "epoch 4576: train loss: 0.14721829148640822, test loss: 0.26511370515726285\n",
      "epoch 4577: train loss: 0.14720886599711347, test loss: 0.26511166362673316\n",
      "epoch 4578: train loss: 0.1471994435479646, test loss: 0.26510962340346955\n",
      "epoch 4579: train loss: 0.14719002413726848, test loss: 0.2651075844864996\n",
      "epoch 4580: train loss: 0.14718060776333336, test loss: 0.26510554687485177\n",
      "epoch 4581: train loss: 0.14717119442446872, test loss: 0.2651035105675554\n",
      "epoch 4582: train loss: 0.14716178411898548, test loss: 0.26510147556364083\n",
      "epoch 4583: train loss: 0.14715237684519575, test loss: 0.2650994418621394\n",
      "epoch 4584: train loss: 0.14714297260141296, test loss: 0.2650974094620834\n",
      "epoch 4585: train loss: 0.14713357138595198, test loss: 0.26509537836250596\n",
      "epoch 4586: train loss: 0.14712417319712884, test loss: 0.2650933485624412\n",
      "epoch 4587: train loss: 0.14711477803326092, test loss: 0.26509132006092434\n",
      "epoch 4588: train loss: 0.14710538589266695, test loss: 0.26508929285699134\n",
      "epoch 4589: train loss: 0.14709599677366686, test loss: 0.26508726694967916\n",
      "epoch 4590: train loss: 0.147086610674582, test loss: 0.26508524233802583\n",
      "epoch 4591: train loss: 0.147077227593735, test loss: 0.26508321902107007\n",
      "epoch 4592: train loss: 0.1470678475294497, test loss: 0.2650811969978518\n",
      "epoch 4593: train loss: 0.14705847048005133, test loss: 0.26507917626741173\n",
      "epoch 4594: train loss: 0.14704909644386638, test loss: 0.2650771568287916\n",
      "epoch 4595: train loss: 0.1470397254192226, test loss: 0.26507513868103394\n",
      "epoch 4596: train loss: 0.14703035740444917, test loss: 0.2650731218231825\n",
      "epoch 4597: train loss: 0.1470209923978764, test loss: 0.2650711062542815\n",
      "epoch 4598: train loss: 0.147011630397836, test loss: 0.26506909197337664\n",
      "epoch 4599: train loss: 0.14700227140266087, test loss: 0.2650670789795141\n",
      "epoch 4600: train loss: 0.1469929154106853, test loss: 0.26506506727174134\n",
      "epoch 4601: train loss: 0.1469835624202448, test loss: 0.26506305684910647\n",
      "epoch 4602: train loss: 0.1469742124296763, test loss: 0.2650610477106587\n",
      "epoch 4603: train loss: 0.1469648654373178, test loss: 0.2650590398554482\n",
      "epoch 4604: train loss: 0.1469555214415087, test loss: 0.2650570332825259\n",
      "epoch 4605: train loss: 0.14694618044058977, test loss: 0.2650550279909437\n",
      "epoch 4606: train loss: 0.1469368424329029, test loss: 0.2650530239797546\n",
      "epoch 4607: train loss: 0.1469275074167913, test loss: 0.2650510212480124\n",
      "epoch 4608: train loss: 0.1469181753905996, test loss: 0.2650490197947718\n",
      "epoch 4609: train loss: 0.14690884635267348, test loss: 0.2650470196190884\n",
      "epoch 4610: train loss: 0.14689952030136008, test loss: 0.2650450207200188\n",
      "epoch 4611: train loss: 0.14689019723500776, test loss: 0.26504302309662053\n",
      "epoch 4612: train loss: 0.14688087715196615, test loss: 0.265041026747952\n",
      "epoch 4613: train loss: 0.1468715600505861, test loss: 0.2650390316730725\n",
      "epoch 4614: train loss: 0.14686224592921984, test loss: 0.2650370378710424\n",
      "epoch 4615: train loss: 0.14685293478622077, test loss: 0.26503504534092265\n",
      "epoch 4616: train loss: 0.1468436266199436, test loss: 0.2650330540817756\n",
      "epoch 4617: train loss: 0.1468343214287443, test loss: 0.2650310640926641\n",
      "epoch 4618: train loss: 0.14682501921098018, test loss: 0.2650290753726521\n",
      "epoch 4619: train loss: 0.14681571996500967, test loss: 0.2650270879208045\n",
      "epoch 4620: train loss: 0.1468064236891926, test loss: 0.2650251017361869\n",
      "epoch 4621: train loss: 0.14679713038188996, test loss: 0.265023116817866\n",
      "epoch 4622: train loss: 0.1467878400414641, test loss: 0.2650211331649095\n",
      "epoch 4623: train loss: 0.14677855266627857, test loss: 0.26501915077638577\n",
      "epoch 4624: train loss: 0.14676926825469816, test loss: 0.2650171696513642\n",
      "epoch 4625: train loss: 0.14675998680508898, test loss: 0.26501518978891503\n",
      "epoch 4626: train loss: 0.14675070831581835, test loss: 0.26501321118810955\n",
      "epoch 4627: train loss: 0.14674143278525487, test loss: 0.2650112338480199\n",
      "epoch 4628: train loss: 0.14673216021176835, test loss: 0.26500925776771894\n",
      "epoch 4629: train loss: 0.14672289059372992, test loss: 0.2650072829462806\n",
      "epoch 4630: train loss: 0.14671362392951195, test loss: 0.2650053093827799\n",
      "epoch 4631: train loss: 0.14670436021748798, test loss: 0.26500333707629226\n",
      "epoch 4632: train loss: 0.14669509945603296, test loss: 0.2650013660258945\n",
      "epoch 4633: train loss: 0.14668584164352286, test loss: 0.2649993962306641\n",
      "epoch 4634: train loss: 0.14667658677833514, test loss: 0.26499742768967943\n",
      "epoch 4635: train loss: 0.1466673348588483, test loss: 0.2649954604020198\n",
      "epoch 4636: train loss: 0.14665808588344226, test loss: 0.26499349436676545\n",
      "epoch 4637: train loss: 0.14664883985049804, test loss: 0.26499152958299754\n",
      "epoch 4638: train loss: 0.146639596758398, test loss: 0.2649895660497979\n",
      "epoch 4639: train loss: 0.14663035660552567, test loss: 0.2649876037662495\n",
      "epoch 4640: train loss: 0.14662111939026587, test loss: 0.2649856427314362\n",
      "epoch 4641: train loss: 0.14661188511100462, test loss: 0.26498368294444263\n",
      "epoch 4642: train loss: 0.14660265376612924, test loss: 0.2649817244043543\n",
      "epoch 4643: train loss: 0.1465934253540282, test loss: 0.2649797671102577\n",
      "epoch 4644: train loss: 0.14658419987309126, test loss: 0.2649778110612402\n",
      "epoch 4645: train loss: 0.14657497732170946, test loss: 0.26497585625639\n",
      "epoch 4646: train loss: 0.14656575769827496, test loss: 0.2649739026947962\n",
      "epoch 4647: train loss: 0.1465565410011812, test loss: 0.2649719503755488\n",
      "epoch 4648: train loss: 0.14654732722882285, test loss: 0.26496999929773873\n",
      "epoch 4649: train loss: 0.1465381163795959, test loss: 0.26496804946045777\n",
      "epoch 4650: train loss: 0.14652890845189737, test loss: 0.26496610086279854\n",
      "epoch 4651: train loss: 0.1465197034441257, test loss: 0.26496415350385455\n",
      "epoch 4652: train loss: 0.14651050135468047, test loss: 0.26496220738272025\n",
      "epoch 4653: train loss: 0.14650130218196245, test loss: 0.264960262498491\n",
      "epoch 4654: train loss: 0.1464921059243737, test loss: 0.2649583188502628\n",
      "epoch 4655: train loss: 0.14648291258031748, test loss: 0.2649563764371328\n",
      "epoch 4656: train loss: 0.14647372214819826, test loss: 0.26495443525819895\n",
      "epoch 4657: train loss: 0.1464645346264217, test loss: 0.2649524953125601\n",
      "epoch 4658: train loss: 0.14645535001339477, test loss: 0.2649505565993158\n",
      "epoch 4659: train loss: 0.14644616830752555, test loss: 0.26494861911756673\n",
      "epoch 4660: train loss: 0.14643698950722342, test loss: 0.2649466828664142\n",
      "epoch 4661: train loss: 0.1464278136108989, test loss: 0.26494474784496064\n",
      "epoch 4662: train loss: 0.14641864061696386, test loss: 0.2649428140523091\n",
      "epoch 4663: train loss: 0.1464094705238312, test loss: 0.2649408814875637\n",
      "epoch 4664: train loss: 0.1464003033299151, test loss: 0.26493895014982927\n",
      "epoch 4665: train loss: 0.146391139033631, test loss: 0.2649370200382116\n",
      "epoch 4666: train loss: 0.14638197763339553, test loss: 0.26493509115181746\n",
      "epoch 4667: train loss: 0.14637281912762648, test loss: 0.2649331634897542\n",
      "epoch 4668: train loss: 0.14636366351474295, test loss: 0.2649312370511303\n",
      "epoch 4669: train loss: 0.1463545107931651, test loss: 0.2649293118350549\n",
      "epoch 4670: train loss: 0.14634536096131442, test loss: 0.2649273878406382\n",
      "epoch 4671: train loss: 0.14633621401761351, test loss: 0.26492546506699105\n",
      "epoch 4672: train loss: 0.14632706996048622, test loss: 0.26492354351322545\n",
      "epoch 4673: train loss: 0.14631792878835762, test loss: 0.26492162317845397\n",
      "epoch 4674: train loss: 0.14630879049965395, test loss: 0.26491970406179016\n",
      "epoch 4675: train loss: 0.14629965509280266, test loss: 0.2649177861623485\n",
      "epoch 4676: train loss: 0.14629052256623232, test loss: 0.26491586947924417\n",
      "epoch 4677: train loss: 0.14628139291837286, test loss: 0.2649139540115933\n",
      "epoch 4678: train loss: 0.14627226614765523, test loss: 0.2649120397585129\n",
      "epoch 4679: train loss: 0.14626314225251172, test loss: 0.26491012671912073\n",
      "epoch 4680: train loss: 0.14625402123137568, test loss: 0.26490821489253563\n",
      "epoch 4681: train loss: 0.1462449030826818, test loss: 0.264906304277877\n",
      "epoch 4682: train loss: 0.1462357878048658, test loss: 0.26490439487426526\n",
      "epoch 4683: train loss: 0.14622667539636466, test loss: 0.26490248668082167\n",
      "epoch 4684: train loss: 0.14621756585561665, test loss: 0.2649005796966683\n",
      "epoch 4685: train loss: 0.14620845918106107, test loss: 0.26489867392092814\n",
      "epoch 4686: train loss: 0.14619935537113843, test loss: 0.2648967693527248\n",
      "epoch 4687: train loss: 0.1461902544242905, test loss: 0.2648948659911832\n",
      "epoch 4688: train loss: 0.14618115633896023, test loss: 0.26489296383542865\n",
      "epoch 4689: train loss: 0.14617206111359163, test loss: 0.2648910628845876\n",
      "epoch 4690: train loss: 0.14616296874663007, test loss: 0.26488916313778693\n",
      "epoch 4691: train loss: 0.14615387923652193, test loss: 0.264887264594155\n",
      "epoch 4692: train loss: 0.1461447925817149, test loss: 0.2648853672528205\n",
      "epoch 4693: train loss: 0.14613570878065774, test loss: 0.26488347111291316\n",
      "epoch 4694: train loss: 0.14612662783180047, test loss: 0.2648815761735635\n",
      "epoch 4695: train loss: 0.14611754973359428, test loss: 0.2648796824339029\n",
      "epoch 4696: train loss: 0.14610847448449146, test loss: 0.26487778989306365\n",
      "epoch 4697: train loss: 0.14609940208294553, test loss: 0.26487589855017873\n",
      "epoch 4698: train loss: 0.14609033252741122, test loss: 0.26487400840438213\n",
      "epoch 4699: train loss: 0.14608126581634437, test loss: 0.26487211945480843\n",
      "epoch 4700: train loss: 0.14607220194820192, test loss: 0.2648702317005933\n",
      "epoch 4701: train loss: 0.14606314092144218, test loss: 0.26486834514087315\n",
      "epoch 4702: train loss: 0.14605408273452444, test loss: 0.26486645977478523\n",
      "epoch 4703: train loss: 0.14604502738590924, test loss: 0.26486457560146753\n",
      "epoch 4704: train loss: 0.14603597487405828, test loss: 0.26486269262005896\n",
      "epoch 4705: train loss: 0.14602692519743443, test loss: 0.26486081082969937\n",
      "epoch 4706: train loss: 0.14601787835450164, test loss: 0.26485893022952917\n",
      "epoch 4707: train loss: 0.14600883434372516, test loss: 0.2648570508186898\n",
      "epoch 4708: train loss: 0.1459997931635713, test loss: 0.2648551725963236\n",
      "epoch 4709: train loss: 0.14599075481250756, test loss: 0.2648532955615734\n",
      "epoch 4710: train loss: 0.1459817192890026, test loss: 0.26485141971358317\n",
      "epoch 4711: train loss: 0.14597268659152632, test loss: 0.26484954505149766\n",
      "epoch 4712: train loss: 0.14596365671854955, test loss: 0.2648476715744623\n",
      "epoch 4713: train loss: 0.14595462966854447, test loss: 0.2648457992816235\n",
      "epoch 4714: train loss: 0.14594560543998436, test loss: 0.26484392817212843\n",
      "epoch 4715: train loss: 0.1459365840313437, test loss: 0.264842058245125\n",
      "epoch 4716: train loss: 0.14592756544109803, test loss: 0.2648401894997621\n",
      "epoch 4717: train loss: 0.1459185496677241, test loss: 0.26483832193518936\n",
      "epoch 4718: train loss: 0.14590953670969975, test loss: 0.2648364555505572\n",
      "epoch 4719: train loss: 0.1459005265655041, test loss: 0.2648345903450168\n",
      "epoch 4720: train loss: 0.14589151923361726, test loss: 0.26483272631772037\n",
      "epoch 4721: train loss: 0.14588251471252056, test loss: 0.26483086346782075\n",
      "epoch 4722: train loss: 0.14587351300069654, test loss: 0.2648290017944717\n",
      "epoch 4723: train loss: 0.1458645140966287, test loss: 0.2648271412968278\n",
      "epoch 4724: train loss: 0.14585551799880192, test loss: 0.2648252819740442\n",
      "epoch 4725: train loss: 0.14584652470570206, test loss: 0.2648234238252772\n",
      "epoch 4726: train loss: 0.14583753421581613, test loss: 0.2648215668496837\n",
      "epoch 4727: train loss: 0.14582854652763236, test loss: 0.2648197110464216\n",
      "epoch 4728: train loss: 0.14581956163963997, test loss: 0.26481785641464933\n",
      "epoch 4729: train loss: 0.14581057955032958, test loss: 0.2648160029535265\n",
      "epoch 4730: train loss: 0.1458016002581926, test loss: 0.264814150662213\n",
      "epoch 4731: train loss: 0.14579262376172192, test loss: 0.2648122995398701\n",
      "epoch 4732: train loss: 0.14578365005941132, test loss: 0.2648104495856596\n",
      "epoch 4733: train loss: 0.1457746791497558, test loss: 0.26480860079874396\n",
      "epoch 4734: train loss: 0.1457657110312515, test loss: 0.2648067531782868\n",
      "epoch 4735: train loss: 0.14575674570239566, test loss: 0.26480490672345225\n",
      "epoch 4736: train loss: 0.1457477831616867, test loss: 0.26480306143340543\n",
      "epoch 4737: train loss: 0.14573882340762415, test loss: 0.26480121730731204\n",
      "epoch 4738: train loss: 0.1457298664387086, test loss: 0.2647993743443388\n",
      "epoch 4739: train loss: 0.1457209122534419, test loss: 0.2647975325436532\n",
      "epoch 4740: train loss: 0.1457119608503268, test loss: 0.2647956919044234\n",
      "epoch 4741: train loss: 0.1457030122278675, test loss: 0.26479385242581854\n",
      "epoch 4742: train loss: 0.14569406638456905, test loss: 0.26479201410700837\n",
      "epoch 4743: train loss: 0.1456851233189377, test loss: 0.26479017694716345\n",
      "epoch 4744: train loss: 0.1456761830294809, test loss: 0.2647883409454554\n",
      "epoch 4745: train loss: 0.14566724551470714, test loss: 0.2647865061010563\n",
      "epoch 4746: train loss: 0.14565831077312602, test loss: 0.2647846724131393\n",
      "epoch 4747: train loss: 0.14564937880324835, test loss: 0.264782839880878\n",
      "epoch 4748: train loss: 0.14564044960358588, test loss: 0.2647810085034472\n",
      "epoch 4749: train loss: 0.14563152317265168, test loss: 0.2647791782800223\n",
      "epoch 4750: train loss: 0.1456225995089598, test loss: 0.2647773492097793\n",
      "epoch 4751: train loss: 0.1456136786110255, test loss: 0.26477552129189535\n",
      "epoch 4752: train loss: 0.14560476047736504, test loss: 0.2647736945255482\n",
      "epoch 4753: train loss: 0.1455958451064959, test loss: 0.26477186890991633\n",
      "epoch 4754: train loss: 0.14558693249693658, test loss: 0.2647700444441791\n",
      "epoch 4755: train loss: 0.14557802264720673, test loss: 0.26476822112751675\n",
      "epoch 4756: train loss: 0.1455691155558272, test loss: 0.26476639895910997\n",
      "epoch 4757: train loss: 0.14556021122131974, test loss: 0.2647645779381407\n",
      "epoch 4758: train loss: 0.14555130964220736, test loss: 0.26476275806379124\n",
      "epoch 4759: train loss: 0.1455424108170142, test loss: 0.26476093933524497\n",
      "epoch 4760: train loss: 0.14553351474426535, test loss: 0.26475912175168587\n",
      "epoch 4761: train loss: 0.14552462142248718, test loss: 0.2647573053122988\n",
      "epoch 4762: train loss: 0.145515730850207, test loss: 0.26475549001626936\n",
      "epoch 4763: train loss: 0.14550684302595335, test loss: 0.264753675862784\n",
      "epoch 4764: train loss: 0.14549795794825585, test loss: 0.2647518628510298\n",
      "epoch 4765: train loss: 0.1454890756156451, test loss: 0.2647500509801947\n",
      "epoch 4766: train loss: 0.14548019602665296, test loss: 0.26474824024946747\n",
      "epoch 4767: train loss: 0.14547131917981226, test loss: 0.26474643065803766\n",
      "epoch 4768: train loss: 0.14546244507365702, test loss: 0.2647446222050954\n",
      "epoch 4769: train loss: 0.14545357370672232, test loss: 0.26474281488983187\n",
      "epoch 4770: train loss: 0.14544470507754428, test loss: 0.26474100871143885\n",
      "epoch 4771: train loss: 0.14543583918466024, test loss: 0.26473920366910897\n",
      "epoch 4772: train loss: 0.14542697602660845, test loss: 0.2647373997620355\n",
      "epoch 4773: train loss: 0.14541811560192844, test loss: 0.2647355969894127\n",
      "epoch 4774: train loss: 0.1454092579091607, test loss: 0.26473379535043534\n",
      "epoch 4775: train loss: 0.14540040294684686, test loss: 0.2647319948442992\n",
      "epoch 4776: train loss: 0.14539155071352966, test loss: 0.2647301954702007\n",
      "epoch 4777: train loss: 0.1453827012077528, test loss: 0.2647283972273371\n",
      "epoch 4778: train loss: 0.14537385442806128, test loss: 0.26472660011490623\n",
      "epoch 4779: train loss: 0.14536501037300095, test loss: 0.2647248041321071\n",
      "epoch 4780: train loss: 0.14535616904111898, test loss: 0.26472300927813897\n",
      "epoch 4781: train loss: 0.14534733043096343, test loss: 0.2647212155522023\n",
      "epoch 4782: train loss: 0.14533849454108352, test loss: 0.26471942295349793\n",
      "epoch 4783: train loss: 0.1453296613700295, test loss: 0.2647176314812279\n",
      "epoch 4784: train loss: 0.14532083091635287, test loss: 0.2647158411345945\n",
      "epoch 4785: train loss: 0.14531200317860596, test loss: 0.26471405191280134\n",
      "epoch 4786: train loss: 0.14530317815534233, test loss: 0.2647122638150524\n",
      "epoch 4787: train loss: 0.14529435584511657, test loss: 0.26471047684055243\n",
      "epoch 4788: train loss: 0.1452855362464844, test loss: 0.2647086909885072\n",
      "epoch 4789: train loss: 0.1452767193580026, test loss: 0.26470690625812293\n",
      "epoch 4790: train loss: 0.1452679051782289, test loss: 0.2647051226486068\n",
      "epoch 4791: train loss: 0.14525909370572226, test loss: 0.2647033401591667\n",
      "epoch 4792: train loss: 0.14525028493904266, test loss: 0.26470155878901125\n",
      "epoch 4793: train loss: 0.1452414788767511, test loss: 0.26469977853734983\n",
      "epoch 4794: train loss: 0.1452326755174097, test loss: 0.26469799940339256\n",
      "epoch 4795: train loss: 0.14522387485958166, test loss: 0.2646962213863504\n",
      "epoch 4796: train loss: 0.14521507690183125, test loss: 0.2646944444854349\n",
      "epoch 4797: train loss: 0.1452062816427237, test loss: 0.2646926686998585\n",
      "epoch 4798: train loss: 0.14519748908082544, test loss: 0.2646908940288344\n",
      "epoch 4799: train loss: 0.1451886992147039, test loss: 0.2646891204715764\n",
      "epoch 4800: train loss: 0.1451799120429276, test loss: 0.2646873480272992\n",
      "epoch 4801: train loss: 0.14517112756406608, test loss: 0.26468557669521814\n",
      "epoch 4802: train loss: 0.14516234577669, test loss: 0.2646838064745495\n",
      "epoch 4803: train loss: 0.145153566679371, test loss: 0.26468203736451\n",
      "epoch 4804: train loss: 0.14514479027068183, test loss: 0.26468026936431743\n",
      "epoch 4805: train loss: 0.14513601654919633, test loss: 0.26467850247318997\n",
      "epoch 4806: train loss: 0.14512724551348935, test loss: 0.26467673669034686\n",
      "epoch 4807: train loss: 0.14511847716213683, test loss: 0.264674972015008\n",
      "epoch 4808: train loss: 0.14510971149371568, test loss: 0.26467320844639397\n",
      "epoch 4809: train loss: 0.14510094850680397, test loss: 0.26467144598372605\n",
      "epoch 4810: train loss: 0.14509218819998076, test loss: 0.2646696846262263\n",
      "epoch 4811: train loss: 0.14508343057182618, test loss: 0.26466792437311776\n",
      "epoch 4812: train loss: 0.14507467562092147, test loss: 0.26466616522362374\n",
      "epoch 4813: train loss: 0.14506592334584878, test loss: 0.2646644071769687\n",
      "epoch 4814: train loss: 0.14505717374519145, test loss: 0.26466265023237767\n",
      "epoch 4815: train loss: 0.14504842681753377, test loss: 0.2646608943890763\n",
      "epoch 4816: train loss: 0.14503968256146116, test loss: 0.2646591396462913\n",
      "epoch 4817: train loss: 0.14503094097556002, test loss: 0.26465738600324973\n",
      "epoch 4818: train loss: 0.14502220205841782, test loss: 0.2646556334591798\n",
      "epoch 4819: train loss: 0.14501346580862307, test loss: 0.26465388201331\n",
      "epoch 4820: train loss: 0.14500473222476531, test loss: 0.26465213166486995\n",
      "epoch 4821: train loss: 0.1449960013054352, test loss: 0.26465038241308964\n",
      "epoch 4822: train loss: 0.14498727304922432, test loss: 0.2646486342572002\n",
      "epoch 4823: train loss: 0.14497854745472538, test loss: 0.2646468871964331\n",
      "epoch 4824: train loss: 0.1449698245205321, test loss: 0.2646451412300208\n",
      "epoch 4825: train loss: 0.14496110424523923, test loss: 0.2646433963571964\n",
      "epoch 4826: train loss: 0.14495238662744256, test loss: 0.26464165257719374\n",
      "epoch 4827: train loss: 0.1449436716657389, test loss: 0.26463990988924735\n",
      "epoch 4828: train loss: 0.14493495935872616, test loss: 0.26463816829259246\n",
      "epoch 4829: train loss: 0.14492624970500323, test loss: 0.2646364277864652\n",
      "epoch 4830: train loss: 0.14491754270317006, test loss: 0.2646346883701021\n",
      "epoch 4831: train loss: 0.14490883835182758, test loss: 0.2646329500427409\n",
      "epoch 4832: train loss: 0.1449001366495778, test loss: 0.26463121280361956\n",
      "epoch 4833: train loss: 0.1448914375950238, test loss: 0.264629476651977\n",
      "epoch 4834: train loss: 0.14488274118676955, test loss: 0.264627741587053\n",
      "epoch 4835: train loss: 0.14487404742342025, test loss: 0.26462600760808774\n",
      "epoch 4836: train loss: 0.14486535630358188, test loss: 0.26462427471432237\n",
      "epoch 4837: train loss: 0.1448566678258617, test loss: 0.26462254290499865\n",
      "epoch 4838: train loss: 0.14484798198886786, test loss: 0.264620812179359\n",
      "epoch 4839: train loss: 0.1448392987912095, test loss: 0.2646190825366468\n",
      "epoch 4840: train loss: 0.14483061823149687, test loss: 0.264617353976106\n",
      "epoch 4841: train loss: 0.1448219403083412, test loss: 0.264615626496981\n",
      "epoch 4842: train loss: 0.1448132650203548, test loss: 0.2646139000985174\n",
      "epoch 4843: train loss: 0.14480459236615087, test loss: 0.26461217477996113\n",
      "epoch 4844: train loss: 0.14479592234434377, test loss: 0.26461045054055915\n",
      "epoch 4845: train loss: 0.1447872549535488, test loss: 0.2646087273795588\n",
      "epoch 4846: train loss: 0.14477859019238232, test loss: 0.26460700529620845\n",
      "epoch 4847: train loss: 0.1447699280594617, test loss: 0.264605284289757\n",
      "epoch 4848: train loss: 0.1447612685534053, test loss: 0.26460356435945404\n",
      "epoch 4849: train loss: 0.14475261167283243, test loss: 0.2646018455045499\n",
      "epoch 4850: train loss: 0.14474395741636362, test loss: 0.2646001277242957\n",
      "epoch 4851: train loss: 0.14473530578262023, test loss: 0.26459841101794324\n",
      "epoch 4852: train loss: 0.14472665677022467, test loss: 0.26459669538474495\n",
      "epoch 4853: train loss: 0.1447180103778004, test loss: 0.26459498082395405\n",
      "epoch 4854: train loss: 0.1447093666039719, test loss: 0.26459326733482436\n",
      "epoch 4855: train loss: 0.1447007254473646, test loss: 0.26459155491661057\n",
      "epoch 4856: train loss: 0.14469208690660498, test loss: 0.2645898435685679\n",
      "epoch 4857: train loss: 0.14468345098032048, test loss: 0.2645881332899524\n",
      "epoch 4858: train loss: 0.1446748176671396, test loss: 0.26458642408002075\n",
      "epoch 4859: train loss: 0.1446661869656919, test loss: 0.2645847159380304\n",
      "epoch 4860: train loss: 0.1446575588746078, test loss: 0.2645830088632395\n",
      "epoch 4861: train loss: 0.14464893339251883, test loss: 0.2645813028549067\n",
      "epoch 4862: train loss: 0.14464031051805745, test loss: 0.2645795979122917\n",
      "epoch 4863: train loss: 0.14463169024985723, test loss: 0.2645778940346546\n",
      "epoch 4864: train loss: 0.1446230725865526, test loss: 0.26457619122125636\n",
      "epoch 4865: train loss: 0.14461445752677915, test loss: 0.26457448947135853\n",
      "epoch 4866: train loss: 0.14460584506917332, test loss: 0.26457278878422347\n",
      "epoch 4867: train loss: 0.1445972352123726, test loss: 0.2645710891591142\n",
      "epoch 4868: train loss: 0.14458862795501556, test loss: 0.2645693905952944\n",
      "epoch 4869: train loss: 0.14458002329574168, test loss: 0.2645676930920284\n",
      "epoch 4870: train loss: 0.1445714212331914, test loss: 0.26456599664858127\n",
      "epoch 4871: train loss: 0.1445628217660062, test loss: 0.2645643012642189\n",
      "epoch 4872: train loss: 0.14455422489282868, test loss: 0.26456260693820777\n",
      "epoch 4873: train loss: 0.14454563061230222, test loss: 0.2645609136698149\n",
      "epoch 4874: train loss: 0.1445370389230713, test loss: 0.26455922145830835\n",
      "epoch 4875: train loss: 0.14452844982378132, test loss: 0.26455753030295637\n",
      "epoch 4876: train loss: 0.1445198633130788, test loss: 0.2645558402030285\n",
      "epoch 4877: train loss: 0.14451127938961122, test loss: 0.26455415115779446\n",
      "epoch 4878: train loss: 0.14450269805202687, test loss: 0.264552463166525\n",
      "epoch 4879: train loss: 0.1444941192989753, test loss: 0.26455077622849127\n",
      "epoch 4880: train loss: 0.1444855431291068, test loss: 0.26454909034296537\n",
      "epoch 4881: train loss: 0.14447696954107278, test loss: 0.2645474055092199\n",
      "epoch 4882: train loss: 0.14446839853352564, test loss: 0.2645457217265282\n",
      "epoch 4883: train loss: 0.14445983010511873, test loss: 0.26454403899416445\n",
      "epoch 4884: train loss: 0.14445126425450633, test loss: 0.26454235731140324\n",
      "epoch 4885: train loss: 0.1444427009803438, test loss: 0.2645406766775199\n",
      "epoch 4886: train loss: 0.14443414028128745, test loss: 0.26453899709179074\n",
      "epoch 4887: train loss: 0.14442558215599452, test loss: 0.26453731855349244\n",
      "epoch 4888: train loss: 0.14441702660312328, test loss: 0.26453564106190236\n",
      "epoch 4889: train loss: 0.14440847362133297, test loss: 0.26453396461629874\n",
      "epoch 4890: train loss: 0.1443999232092838, test loss: 0.2645322892159603\n",
      "epoch 4891: train loss: 0.14439137536563693, test loss: 0.26453061486016655\n",
      "epoch 4892: train loss: 0.14438283008905456, test loss: 0.2645289415481977\n",
      "epoch 4893: train loss: 0.1443742873781998, test loss: 0.26452726927933456\n",
      "epoch 4894: train loss: 0.14436574723173679, test loss: 0.2645255980528586\n",
      "epoch 4895: train loss: 0.14435720964833057, test loss: 0.26452392786805207\n",
      "epoch 4896: train loss: 0.14434867462664724, test loss: 0.2645222587241978\n",
      "epoch 4897: train loss: 0.1443401421653538, test loss: 0.2645205906205794\n",
      "epoch 4898: train loss: 0.14433161226311825, test loss: 0.2645189235564811\n",
      "epoch 4899: train loss: 0.14432308491860954, test loss: 0.2645172575311877\n",
      "epoch 4900: train loss: 0.14431456013049762, test loss: 0.26451559254398477\n",
      "epoch 4901: train loss: 0.14430603789745342, test loss: 0.2645139285941585\n",
      "epoch 4902: train loss: 0.14429751821814876, test loss: 0.26451226568099584\n",
      "epoch 4903: train loss: 0.14428900109125653, test loss: 0.2645106038037845\n",
      "epoch 4904: train loss: 0.14428048651545045, test loss: 0.26450894296181243\n",
      "epoch 4905: train loss: 0.14427197448940535, test loss: 0.2645072831543687\n",
      "epoch 4906: train loss: 0.1442634650117969, test loss: 0.26450562438074293\n",
      "epoch 4907: train loss: 0.14425495808130184, test loss: 0.26450396664022535\n",
      "epoch 4908: train loss: 0.14424645369659778, test loss: 0.2645023099321067\n",
      "epoch 4909: train loss: 0.14423795185636332, test loss: 0.2645006542556787\n",
      "epoch 4910: train loss: 0.14422945255927805, test loss: 0.2644989996102336\n",
      "epoch 4911: train loss: 0.1442209558040225, test loss: 0.26449734599506425\n",
      "epoch 4912: train loss: 0.14421246158927814, test loss: 0.26449569340946427\n",
      "epoch 4913: train loss: 0.14420396991372741, test loss: 0.26449404185272773\n",
      "epoch 4914: train loss: 0.1441954807760537, test loss: 0.26449239132414964\n",
      "epoch 4915: train loss: 0.14418699417494138, test loss: 0.2644907418230256\n",
      "epoch 4916: train loss: 0.14417851010907576, test loss: 0.2644890933486517\n",
      "epoch 4917: train loss: 0.14417002857714306, test loss: 0.26448744590032497\n",
      "epoch 4918: train loss: 0.1441615495778305, test loss: 0.26448579947734274\n",
      "epoch 4919: train loss: 0.14415307310982622, test loss: 0.2644841540790033\n",
      "epoch 4920: train loss: 0.14414459917181943, test loss: 0.2644825097046054\n",
      "epoch 4921: train loss: 0.1441361277625001, test loss: 0.2644808663534487\n",
      "epoch 4922: train loss: 0.14412765888055923, test loss: 0.2644792240248332\n",
      "epoch 4923: train loss: 0.14411919252468885, test loss: 0.2644775827180598\n",
      "epoch 4924: train loss: 0.1441107286935818, test loss: 0.26447594243243\n",
      "epoch 4925: train loss: 0.14410226738593196, test loss: 0.2644743031672458\n",
      "epoch 4926: train loss: 0.14409380860043414, test loss: 0.2644726649218101\n",
      "epoch 4927: train loss: 0.14408535233578404, test loss: 0.26447102769542624\n",
      "epoch 4928: train loss: 0.14407689859067838, test loss: 0.26446939148739834\n",
      "epoch 4929: train loss: 0.14406844736381474, test loss: 0.26446775629703106\n",
      "epoch 4930: train loss: 0.14405999865389177, test loss: 0.2644661221236299\n",
      "epoch 4931: train loss: 0.1440515524596089, test loss: 0.2644644889665008\n",
      "epoch 4932: train loss: 0.14404310877966664, test loss: 0.26446285682495035\n",
      "epoch 4933: train loss: 0.14403466761276632, test loss: 0.2644612256982861\n",
      "epoch 4934: train loss: 0.14402622895761033, test loss: 0.26445959558581583\n",
      "epoch 4935: train loss: 0.1440177928129019, test loss: 0.2644579664868483\n",
      "epoch 4936: train loss: 0.14400935917734523, test loss: 0.2644563384006926\n",
      "epoch 4937: train loss: 0.14400092804964543, test loss: 0.2644547113266588\n",
      "epoch 4938: train loss: 0.14399249942850864, test loss: 0.2644530852640574\n",
      "epoch 4939: train loss: 0.14398407331264182, test loss: 0.2644514602121996\n",
      "epoch 4940: train loss: 0.1439756497007529, test loss: 0.2644498361703973\n",
      "epoch 4941: train loss: 0.1439672285915508, test loss: 0.2644482131379629\n",
      "epoch 4942: train loss: 0.1439588099837453, test loss: 0.26444659111420954\n",
      "epoch 4943: train loss: 0.14395039387604708, test loss: 0.264444970098451\n",
      "epoch 4944: train loss: 0.1439419802671679, test loss: 0.2644433500900017\n",
      "epoch 4945: train loss: 0.14393356915582026, test loss: 0.2644417310881768\n",
      "epoch 4946: train loss: 0.14392516054071774, test loss: 0.2644401130922918\n",
      "epoch 4947: train loss: 0.1439167544205748, test loss: 0.2644384961016631\n",
      "epoch 4948: train loss: 0.14390835079410674, test loss: 0.2644368801156077\n",
      "epoch 4949: train loss: 0.14389994966002995, test loss: 0.2644352651334432\n",
      "epoch 4950: train loss: 0.14389155101706155, test loss: 0.2644336511544878\n",
      "epoch 4951: train loss: 0.14388315486391978, test loss: 0.2644320381780604\n",
      "epoch 4952: train loss: 0.1438747611993237, test loss: 0.26443042620348045\n",
      "epoch 4953: train loss: 0.1438663700219932, test loss: 0.26442881523006817\n",
      "epoch 4954: train loss: 0.1438579813306494, test loss: 0.26442720525714436\n",
      "epoch 4955: train loss: 0.1438495951240139, test loss: 0.26442559628403045\n",
      "epoch 4956: train loss: 0.14384121140080963, test loss: 0.2644239883100483\n",
      "epoch 4957: train loss: 0.14383283015976014, test loss: 0.26442238133452084\n",
      "epoch 4958: train loss: 0.14382445139959013, test loss: 0.26442077535677116\n",
      "epoch 4959: train loss: 0.14381607511902503, test loss: 0.26441917037612334\n",
      "epoch 4960: train loss: 0.14380770131679133, test loss: 0.2644175663919019\n",
      "epoch 4961: train loss: 0.14379932999161626, test loss: 0.26441596340343204\n",
      "epoch 4962: train loss: 0.1437909611422282, test loss: 0.26441436141003954\n",
      "epoch 4963: train loss: 0.14378259476735625, test loss: 0.26441276041105094\n",
      "epoch 4964: train loss: 0.1437742308657305, test loss: 0.26441116040579327\n",
      "epoch 4965: train loss: 0.1437658694360819, test loss: 0.26440956139359423\n",
      "epoch 4966: train loss: 0.14375751047714247, test loss: 0.26440796337378214\n",
      "epoch 4967: train loss: 0.14374915398764496, test loss: 0.26440636634568604\n",
      "epoch 4968: train loss: 0.14374079996632305, test loss: 0.26440477030863535\n",
      "epoch 4969: train loss: 0.14373244841191146, test loss: 0.26440317526196044\n",
      "epoch 4970: train loss: 0.14372409932314562, test loss: 0.2644015812049921\n",
      "epoch 4971: train loss: 0.14371575269876208, test loss: 0.26439998813706167\n",
      "epoch 4972: train loss: 0.14370740853749817, test loss: 0.2643983960575014\n",
      "epoch 4973: train loss: 0.14369906683809208, test loss: 0.2643968049656438\n",
      "epoch 4974: train loss: 0.14369072759928309, test loss: 0.2643952148608223\n",
      "epoch 4975: train loss: 0.14368239081981118, test loss: 0.26439362574237085\n",
      "epoch 4976: train loss: 0.14367405649841736, test loss: 0.2643920376096239\n",
      "epoch 4977: train loss: 0.14366572463384347, test loss: 0.26439045046191667\n",
      "epoch 4978: train loss: 0.14365739522483234, test loss: 0.264388864298585\n",
      "epoch 4979: train loss: 0.1436490682701276, test loss: 0.2643872791189652\n",
      "epoch 4980: train loss: 0.14364074376847383, test loss: 0.26438569492239433\n",
      "epoch 4981: train loss: 0.1436324217186165, test loss: 0.2643841117082101\n",
      "epoch 4982: train loss: 0.14362410211930204, test loss: 0.26438252947575064\n",
      "epoch 4983: train loss: 0.14361578496927763, test loss: 0.2643809482243549\n",
      "epoch 4984: train loss: 0.14360747026729145, test loss: 0.2643793679533623\n",
      "epoch 4985: train loss: 0.14359915801209266, test loss: 0.264377788662113\n",
      "epoch 4986: train loss: 0.1435908482024311, test loss: 0.2643762103499476\n",
      "epoch 4987: train loss: 0.14358254083705763, test loss: 0.2643746330162076\n",
      "epoch 4988: train loss: 0.14357423591472407, test loss: 0.26437305666023475\n",
      "epoch 4989: train loss: 0.14356593343418297, test loss: 0.2643714812813716\n",
      "epoch 4990: train loss: 0.14355763339418792, test loss: 0.2643699068789615\n",
      "epoch 4991: train loss: 0.1435493357934933, test loss: 0.26436833345234795\n",
      "epoch 4992: train loss: 0.14354104063085443, test loss: 0.26436676100087547\n",
      "epoch 4993: train loss: 0.1435327479050275, test loss: 0.26436518952388893\n",
      "epoch 4994: train loss: 0.14352445761476965, test loss: 0.264363619020734\n",
      "epoch 4995: train loss: 0.14351616975883874, test loss: 0.2643620494907568\n",
      "epoch 4996: train loss: 0.14350788433599374, test loss: 0.2643604809333042\n",
      "epoch 4997: train loss: 0.1434996013449943, test loss: 0.26435891334772355\n",
      "epoch 4998: train loss: 0.1434913207846011, test loss: 0.2643573467333629\n",
      "epoch 4999: train loss: 0.14348304265357567, test loss: 0.26435578108957075\n",
      "epoch 5000: train loss: 0.14347476695068037, test loss: 0.2643542164156964\n",
      "epoch 5001: train loss: 0.1434664936746785, test loss: 0.26435265271108965\n",
      "epoch 5002: train loss: 0.14345822282433424, test loss: 0.2643510899751009\n",
      "epoch 5003: train loss: 0.14344995439841257, test loss: 0.2643495282070812\n",
      "epoch 5004: train loss: 0.14344168839567945, test loss: 0.26434796740638206\n",
      "epoch 5005: train loss: 0.1434334248149017, test loss: 0.26434640757235583\n",
      "epoch 5006: train loss: 0.14342516365484698, test loss: 0.26434484870435526\n",
      "epoch 5007: train loss: 0.14341690491428388, test loss: 0.2643432908017337\n",
      "epoch 5008: train loss: 0.14340864859198177, test loss: 0.2643417338638453\n",
      "epoch 5009: train loss: 0.143400394686711, test loss: 0.2643401778900446\n",
      "epoch 5010: train loss: 0.1433921431972428, test loss: 0.2643386228796868\n",
      "epoch 5011: train loss: 0.1433838941223492, test loss: 0.2643370688321277\n",
      "epoch 5012: train loss: 0.1433756474608031, test loss: 0.26433551574672376\n",
      "epoch 5013: train loss: 0.14336740321137836, test loss: 0.2643339636228319\n",
      "epoch 5014: train loss: 0.14335916137284963, test loss: 0.2643324124598097\n",
      "epoch 5015: train loss: 0.14335092194399246, test loss: 0.26433086225701546\n",
      "epoch 5016: train loss: 0.1433426849235833, test loss: 0.2643293130138078\n",
      "epoch 5017: train loss: 0.14333445031039943, test loss: 0.26432776472954617\n",
      "epoch 5018: train loss: 0.14332621810321902, test loss: 0.2643262174035905\n",
      "epoch 5019: train loss: 0.1433179883008211, test loss: 0.2643246710353014\n",
      "epoch 5020: train loss: 0.14330976090198558, test loss: 0.26432312562404\n",
      "epoch 5021: train loss: 0.14330153590549322, test loss: 0.2643215811691679\n",
      "epoch 5022: train loss: 0.14329331331012563, test loss: 0.2643200376700476\n",
      "epoch 5023: train loss: 0.1432850931146653, test loss: 0.26431849512604183\n",
      "epoch 5024: train loss: 0.14327687531789565, test loss: 0.2643169535365143\n",
      "epoch 5025: train loss: 0.14326865991860085, test loss: 0.26431541290082883\n",
      "epoch 5026: train loss: 0.14326044691556603, test loss: 0.2643138732183503\n",
      "epoch 5027: train loss: 0.1432522363075771, test loss: 0.26431233448844393\n",
      "epoch 5028: train loss: 0.1432440280934209, test loss: 0.26431079671047547\n",
      "epoch 5029: train loss: 0.1432358222718851, test loss: 0.2643092598838115\n",
      "epoch 5030: train loss: 0.14322761884175822, test loss: 0.2643077240078188\n",
      "epoch 5031: train loss: 0.14321941780182967, test loss: 0.26430618908186526\n",
      "epoch 5032: train loss: 0.1432112191508897, test loss: 0.26430465510531875\n",
      "epoch 5033: train loss: 0.1432030228877294, test loss: 0.2643031220775483\n",
      "epoch 5034: train loss: 0.14319482901114072, test loss: 0.264301589997923\n",
      "epoch 5035: train loss: 0.1431866375199165, test loss: 0.264300058865813\n",
      "epoch 5036: train loss: 0.14317844841285043, test loss: 0.2642985286805887\n",
      "epoch 5037: train loss: 0.14317026168873703, test loss: 0.2642969994416211\n",
      "epoch 5038: train loss: 0.14316207734637165, test loss: 0.26429547114828206\n",
      "epoch 5039: train loss: 0.14315389538455053, test loss: 0.2642939437999436\n",
      "epoch 5040: train loss: 0.14314571580207086, test loss: 0.26429241739597864\n",
      "epoch 5041: train loss: 0.14313753859773043, test loss: 0.26429089193576066\n",
      "epoch 5042: train loss: 0.14312936377032817, test loss: 0.2642893674186635\n",
      "epoch 5043: train loss: 0.14312119131866358, test loss: 0.2642878438440618\n",
      "epoch 5044: train loss: 0.14311302124153727, test loss: 0.26428632121133055\n",
      "epoch 5045: train loss: 0.14310485353775046, test loss: 0.2642847995198456\n",
      "epoch 5046: train loss: 0.14309668820610547, test loss: 0.2642832787689832\n",
      "epoch 5047: train loss: 0.14308852524540522, test loss: 0.2642817589581201\n",
      "epoch 5048: train loss: 0.14308036465445365, test loss: 0.26428024008663387\n",
      "epoch 5049: train loss: 0.14307220643205545, test loss: 0.2642787221539023\n",
      "epoch 5050: train loss: 0.1430640505770162, test loss: 0.2642772051593041\n",
      "epoch 5051: train loss: 0.14305589708814231, test loss: 0.26427568910221844\n",
      "epoch 5052: train loss: 0.14304774596424102, test loss: 0.2642741739820249\n",
      "epoch 5053: train loss: 0.14303959720412046, test loss: 0.2642726597981038\n",
      "epoch 5054: train loss: 0.14303145080658955, test loss: 0.264271146549836\n",
      "epoch 5055: train loss: 0.14302330677045805, test loss: 0.264269634236603\n",
      "epoch 5056: train loss: 0.14301516509453654, test loss: 0.2642681228577866\n",
      "epoch 5057: train loss: 0.1430070257776366, test loss: 0.26426661241276944\n",
      "epoch 5058: train loss: 0.14299888881857042, test loss: 0.2642651029009347\n",
      "epoch 5059: train loss: 0.14299075421615115, test loss: 0.264263594321666\n",
      "epoch 5060: train loss: 0.1429826219691928, test loss: 0.2642620866743476\n",
      "epoch 5061: train loss: 0.14297449207651014, test loss: 0.2642605799583643\n",
      "epoch 5062: train loss: 0.14296636453691886, test loss: 0.26425907417310146\n",
      "epoch 5063: train loss: 0.14295823934923535, test loss: 0.2642575693179451\n",
      "epoch 5064: train loss: 0.142950116512277, test loss: 0.2642560653922816\n",
      "epoch 5065: train loss: 0.1429419960248619, test loss: 0.26425456239549816\n",
      "epoch 5066: train loss: 0.14293387788580908, test loss: 0.26425306032698237\n",
      "epoch 5067: train loss: 0.14292576209393829, test loss: 0.26425155918612236\n",
      "epoch 5068: train loss: 0.14291764864807022, test loss: 0.26425005897230697\n",
      "epoch 5069: train loss: 0.1429095375470263, test loss: 0.26424855968492544\n",
      "epoch 5070: train loss: 0.14290142878962883, test loss: 0.2642470613233677\n",
      "epoch 5071: train loss: 0.14289332237470095, test loss: 0.2642455638870242\n",
      "epoch 5072: train loss: 0.14288521830106662, test loss: 0.26424406737528583\n",
      "epoch 5073: train loss: 0.14287711656755062, test loss: 0.26424257178754423\n",
      "epoch 5074: train loss: 0.14286901717297854, test loss: 0.2642410771231915\n",
      "epoch 5075: train loss: 0.14286092011617682, test loss: 0.2642395833816203\n",
      "epoch 5076: train loss: 0.14285282539597274, test loss: 0.2642380905622238\n",
      "epoch 5077: train loss: 0.14284473301119432, test loss: 0.2642365986643958\n",
      "epoch 5078: train loss: 0.1428366429606706, test loss: 0.26423510768753067\n",
      "epoch 5079: train loss: 0.14282855524323118, test loss: 0.26423361763102327\n",
      "epoch 5080: train loss: 0.14282046985770666, test loss: 0.264232128494269\n",
      "epoch 5081: train loss: 0.14281238680292843, test loss: 0.2642306402766639\n",
      "epoch 5082: train loss: 0.14280430607772865, test loss: 0.2642291529776045\n",
      "epoch 5083: train loss: 0.14279622768094036, test loss: 0.26422766659648794\n",
      "epoch 5084: train loss: 0.14278815161139735, test loss: 0.26422618113271173\n",
      "epoch 5085: train loss: 0.14278007786793434, test loss: 0.26422469658567427\n",
      "epoch 5086: train loss: 0.14277200644938673, test loss: 0.26422321295477413\n",
      "epoch 5087: train loss: 0.1427639373545909, test loss: 0.2642217302394107\n",
      "epoch 5088: train loss: 0.1427558705823838, test loss: 0.26422024843898384\n",
      "epoch 5089: train loss: 0.14274780613160348, test loss: 0.26421876755289386\n",
      "epoch 5090: train loss: 0.14273974400108863, test loss: 0.2642172875805418\n",
      "epoch 5091: train loss: 0.1427316841896788, test loss: 0.26421580852132914\n",
      "epoch 5092: train loss: 0.14272362669621433, test loss: 0.2642143303746579\n",
      "epoch 5093: train loss: 0.14271557151953643, test loss: 0.2642128531399306\n",
      "epoch 5094: train loss: 0.14270751865848705, test loss: 0.2642113768165505\n",
      "epoch 5095: train loss: 0.14269946811190898, test loss: 0.26420990140392114\n",
      "epoch 5096: train loss: 0.14269141987864586, test loss: 0.26420842690144686\n",
      "epoch 5097: train loss: 0.14268337395754208, test loss: 0.26420695330853233\n",
      "epoch 5098: train loss: 0.14267533034744287, test loss: 0.264205480624583\n",
      "epoch 5099: train loss: 0.14266728904719428, test loss: 0.2642040088490045\n",
      "epoch 5100: train loss: 0.14265925005564312, test loss: 0.2642025379812034\n",
      "epoch 5101: train loss: 0.14265121337163705, test loss: 0.26420106802058657\n",
      "epoch 5102: train loss: 0.14264317899402454, test loss: 0.26419959896656153\n",
      "epoch 5103: train loss: 0.1426351469216548, test loss: 0.2641981308185362\n",
      "epoch 5104: train loss: 0.14262711715337795, test loss: 0.2641966635759192\n",
      "epoch 5105: train loss: 0.14261908968804485, test loss: 0.2641951972381197\n",
      "epoch 5106: train loss: 0.14261106452450711, test loss: 0.26419373180454714\n",
      "epoch 5107: train loss: 0.14260304166161727, test loss: 0.26419226727461187\n",
      "epoch 5108: train loss: 0.14259502109822858, test loss: 0.2641908036477245\n",
      "epoch 5109: train loss: 0.14258700283319514, test loss: 0.2641893409232964\n",
      "epoch 5110: train loss: 0.14257898686537174, test loss: 0.26418787910073915\n",
      "epoch 5111: train loss: 0.14257097319361417, test loss: 0.2641864181794653\n",
      "epoch 5112: train loss: 0.14256296181677885, test loss: 0.2641849581588875\n",
      "epoch 5113: train loss: 0.14255495273372304, test loss: 0.26418349903841926\n",
      "epoch 5114: train loss: 0.1425469459433048, test loss: 0.26418204081747443\n",
      "epoch 5115: train loss: 0.1425389414443831, test loss: 0.2641805834954675\n",
      "epoch 5116: train loss: 0.14253093923581744, test loss: 0.26417912707181357\n",
      "epoch 5117: train loss: 0.14252293931646842, test loss: 0.26417767154592803\n",
      "epoch 5118: train loss: 0.14251494168519724, test loss: 0.26417621691722687\n",
      "epoch 5119: train loss: 0.14250694634086591, test loss: 0.2641747631851268\n",
      "epoch 5120: train loss: 0.14249895328233736, test loss: 0.264173310349045\n",
      "epoch 5121: train loss: 0.14249096250847515, test loss: 0.2641718584083989\n",
      "epoch 5122: train loss: 0.14248297401814372, test loss: 0.2641704073626069\n",
      "epoch 5123: train loss: 0.14247498781020831, test loss: 0.2641689572110875\n",
      "epoch 5124: train loss: 0.1424670038835349, test loss: 0.2641675079532601\n",
      "epoch 5125: train loss: 0.14245902223699028, test loss: 0.2641660595885444\n",
      "epoch 5126: train loss: 0.14245104286944207, test loss: 0.26416461211636066\n",
      "epoch 5127: train loss: 0.14244306577975863, test loss: 0.26416316553612973\n",
      "epoch 5128: train loss: 0.14243509096680912, test loss: 0.26416171984727294\n",
      "epoch 5129: train loss: 0.14242711842946348, test loss: 0.2641602750492122\n",
      "epoch 5130: train loss: 0.14241914816659249, test loss: 0.26415883114136984\n",
      "epoch 5131: train loss: 0.1424111801770676, test loss: 0.2641573881231688\n",
      "epoch 5132: train loss: 0.1424032144597612, test loss: 0.2641559459940326\n",
      "epoch 5133: train loss: 0.1423952510135463, test loss: 0.26415450475338503\n",
      "epoch 5134: train loss: 0.14238728983729682, test loss: 0.2641530644006507\n",
      "epoch 5135: train loss: 0.14237933092988742, test loss: 0.2641516249352546\n",
      "epoch 5136: train loss: 0.14237137429019348, test loss: 0.2641501863566223\n",
      "epoch 5137: train loss: 0.14236341991709128, test loss: 0.2641487486641798\n",
      "epoch 5138: train loss: 0.14235546780945782, test loss: 0.2641473118573537\n",
      "epoch 5139: train loss: 0.1423475179661709, test loss: 0.2641458759355711\n",
      "epoch 5140: train loss: 0.142339570386109, test loss: 0.2641444408982596\n",
      "epoch 5141: train loss: 0.14233162506815153, test loss: 0.2641430067448473\n",
      "epoch 5142: train loss: 0.14232368201117854, test loss: 0.2641415734747629\n",
      "epoch 5143: train loss: 0.14231574121407103, test loss: 0.26414014108743555\n",
      "epoch 5144: train loss: 0.14230780267571055, test loss: 0.26413870958229496\n",
      "epoch 5145: train loss: 0.14229986639497968, test loss: 0.26413727895877137\n",
      "epoch 5146: train loss: 0.14229193237076154, test loss: 0.2641358492162954\n",
      "epoch 5147: train loss: 0.14228400060194016, test loss: 0.26413442035429835\n",
      "epoch 5148: train loss: 0.14227607108740029, test loss: 0.26413299237221194\n",
      "epoch 5149: train loss: 0.1422681438260275, test loss: 0.26413156526946846\n",
      "epoch 5150: train loss: 0.1422602188167081, test loss: 0.2641301390455007\n",
      "epoch 5151: train loss: 0.1422522960583292, test loss: 0.264128713699742\n",
      "epoch 5152: train loss: 0.14224437554977865, test loss: 0.26412728923162604\n",
      "epoch 5153: train loss: 0.14223645728994502, test loss: 0.2641258656405872\n",
      "epoch 5154: train loss: 0.14222854127771778, test loss: 0.2641244429260603\n",
      "epoch 5155: train loss: 0.1422206275119871, test loss: 0.2641230210874808\n",
      "epoch 5156: train loss: 0.14221271599164387, test loss: 0.26412160012428443\n",
      "epoch 5157: train loss: 0.14220480671557986, test loss: 0.26412018003590754\n",
      "epoch 5158: train loss: 0.14219689968268748, test loss: 0.2641187608217871\n",
      "epoch 5159: train loss: 0.14218899489186002, test loss: 0.26411734248136043\n",
      "epoch 5160: train loss: 0.14218109234199142, test loss: 0.2641159250140654\n",
      "epoch 5161: train loss: 0.14217319203197654, test loss: 0.2641145084193405\n",
      "epoch 5162: train loss: 0.14216529396071084, test loss: 0.2641130926966245\n",
      "epoch 5163: train loss: 0.14215739812709063, test loss: 0.26411167784535694\n",
      "epoch 5164: train loss: 0.14214950453001304, test loss: 0.2641102638649777\n",
      "epoch 5165: train loss: 0.14214161316837579, test loss: 0.26410885075492724\n",
      "epoch 5166: train loss: 0.14213372404107752, test loss: 0.2641074385146465\n",
      "epoch 5167: train loss: 0.14212583714701757, test loss: 0.2641060271435769\n",
      "epoch 5168: train loss: 0.14211795248509607, test loss: 0.26410461664116025\n",
      "epoch 5169: train loss: 0.14211007005421386, test loss: 0.2641032070068392\n",
      "epoch 5170: train loss: 0.14210218985327258, test loss: 0.2641017982400566\n",
      "epoch 5171: train loss: 0.14209431188117458, test loss: 0.2641003903402559\n",
      "epoch 5172: train loss: 0.14208643613682306, test loss: 0.2640989833068811\n",
      "epoch 5173: train loss: 0.14207856261912188, test loss: 0.2640975771393766\n",
      "epoch 5174: train loss: 0.14207069132697567, test loss: 0.2640961718371874\n",
      "epoch 5175: train loss: 0.1420628222592899, test loss: 0.26409476739975896\n",
      "epoch 5176: train loss: 0.14205495541497073, test loss: 0.26409336382653725\n",
      "epoch 5177: train loss: 0.14204709079292505, test loss: 0.2640919611169687\n",
      "epoch 5178: train loss: 0.14203922839206054, test loss: 0.2640905592705003\n",
      "epoch 5179: train loss: 0.14203136821128562, test loss: 0.2640891582865795\n",
      "epoch 5180: train loss: 0.14202351024950952, test loss: 0.2640877581646542\n",
      "epoch 5181: train loss: 0.1420156545056421, test loss: 0.26408635890417287\n",
      "epoch 5182: train loss: 0.1420078009785941, test loss: 0.2640849605045846\n",
      "epoch 5183: train loss: 0.14199994966727694, test loss: 0.2640835629653385\n",
      "epoch 5184: train loss: 0.1419921005706028, test loss: 0.26408216628588493\n",
      "epoch 5185: train loss: 0.1419842536874846, test loss: 0.2640807704656741\n",
      "epoch 5186: train loss: 0.14197640901683603, test loss: 0.26407937550415694\n",
      "epoch 5187: train loss: 0.1419685665575715, test loss: 0.264077981400785\n",
      "epoch 5188: train loss: 0.14196072630860623, test loss: 0.26407658815501\n",
      "epoch 5189: train loss: 0.14195288826885608, test loss: 0.2640751957662846\n",
      "epoch 5190: train loss: 0.1419450524372378, test loss: 0.26407380423406146\n",
      "epoch 5191: train loss: 0.14193721881266877, test loss: 0.2640724135577942\n",
      "epoch 5192: train loss: 0.1419293873940671, test loss: 0.2640710237369365\n",
      "epoch 5193: train loss: 0.1419215581803517, test loss: 0.2640696347709429\n",
      "epoch 5194: train loss: 0.14191373117044231, test loss: 0.2640682466592683\n",
      "epoch 5195: train loss: 0.14190590636325923, test loss: 0.2640668594013679\n",
      "epoch 5196: train loss: 0.1418980837577236, test loss: 0.26406547299669775\n",
      "epoch 5197: train loss: 0.1418902633527573, test loss: 0.264064087444714\n",
      "epoch 5198: train loss: 0.14188244514728296, test loss: 0.26406270274487365\n",
      "epoch 5199: train loss: 0.1418746291402239, test loss: 0.26406131889663387\n",
      "epoch 5200: train loss: 0.14186681533050424, test loss: 0.26405993589945254\n",
      "epoch 5201: train loss: 0.14185900371704882, test loss: 0.26405855375278803\n",
      "epoch 5202: train loss: 0.14185119429878315, test loss: 0.264057172456099\n",
      "epoch 5203: train loss: 0.1418433870746336, test loss: 0.26405579200884477\n",
      "epoch 5204: train loss: 0.14183558204352717, test loss: 0.26405441241048516\n",
      "epoch 5205: train loss: 0.14182777920439169, test loss: 0.2640530336604802\n",
      "epoch 5206: train loss: 0.1418199785561556, test loss: 0.26405165575829087\n",
      "epoch 5207: train loss: 0.1418121800977482, test loss: 0.2640502787033782\n",
      "epoch 5208: train loss: 0.14180438382809946, test loss: 0.26404890249520396\n",
      "epoch 5209: train loss: 0.1417965897461401, test loss: 0.2640475271332303\n",
      "epoch 5210: train loss: 0.14178879785080153, test loss: 0.2640461526169199\n",
      "epoch 5211: train loss: 0.14178100814101602, test loss: 0.2640447789457359\n",
      "epoch 5212: train loss: 0.1417732206157164, test loss: 0.2640434061191419\n",
      "epoch 5213: train loss: 0.14176543527383637, test loss: 0.264042034136602\n",
      "epoch 5214: train loss: 0.14175765211431027, test loss: 0.2640406629975807\n",
      "epoch 5215: train loss: 0.14174987113607324, test loss: 0.26403929270154325\n",
      "epoch 5216: train loss: 0.14174209233806107, test loss: 0.26403792324795505\n",
      "epoch 5217: train loss: 0.14173431571921033, test loss: 0.2640365546362822\n",
      "epoch 5218: train loss: 0.14172654127845832, test loss: 0.26403518686599114\n",
      "epoch 5219: train loss: 0.14171876901474312, test loss: 0.2640338199365489\n",
      "epoch 5220: train loss: 0.14171099892700337, test loss: 0.26403245384742285\n",
      "epoch 5221: train loss: 0.14170323101417862, test loss: 0.264031088598081\n",
      "epoch 5222: train loss: 0.14169546527520901, test loss: 0.26402972418799175\n",
      "epoch 5223: train loss: 0.14168770170903547, test loss: 0.26402836061662394\n",
      "epoch 5224: train loss: 0.14167994031459968, test loss: 0.26402699788344697\n",
      "epoch 5225: train loss: 0.14167218109084398, test loss: 0.26402563598793066\n",
      "epoch 5226: train loss: 0.14166442403671148, test loss: 0.2640242749295453\n",
      "epoch 5227: train loss: 0.14165666915114597, test loss: 0.2640229147077618\n",
      "epoch 5228: train loss: 0.141648916433092, test loss: 0.26402155532205124\n",
      "epoch 5229: train loss: 0.1416411658814948, test loss: 0.2640201967718854\n",
      "epoch 5230: train loss: 0.14163341749530037, test loss: 0.2640188390567365\n",
      "epoch 5231: train loss: 0.14162567127345543, test loss: 0.26401748217607723\n",
      "epoch 5232: train loss: 0.14161792721490735, test loss: 0.26401612612938075\n",
      "epoch 5233: train loss: 0.14161018531860428, test loss: 0.2640147709161207\n",
      "epoch 5234: train loss: 0.14160244558349505, test loss: 0.26401341653577104\n",
      "epoch 5235: train loss: 0.14159470800852925, test loss: 0.26401206298780644\n",
      "epoch 5236: train loss: 0.14158697259265715, test loss: 0.2640107102717019\n",
      "epoch 5237: train loss: 0.1415792393348298, test loss: 0.26400935838693296\n",
      "epoch 5238: train loss: 0.14157150823399886, test loss: 0.26400800733297547\n",
      "epoch 5239: train loss: 0.14156377928911676, test loss: 0.264006657109306\n",
      "epoch 5240: train loss: 0.14155605249913666, test loss: 0.2640053077154013\n",
      "epoch 5241: train loss: 0.14154832786301244, test loss: 0.2640039591507389\n",
      "epoch 5242: train loss: 0.14154060537969865, test loss: 0.26400261141479653\n",
      "epoch 5243: train loss: 0.14153288504815056, test loss: 0.2640012645070524\n",
      "epoch 5244: train loss: 0.1415251668673242, test loss: 0.26399991842698556\n",
      "epoch 5245: train loss: 0.14151745083617626, test loss: 0.26399857317407505\n",
      "epoch 5246: train loss: 0.1415097369536641, test loss: 0.2639972287478005\n",
      "epoch 5247: train loss: 0.14150202521874594, test loss: 0.2639958851476423\n",
      "epoch 5248: train loss: 0.14149431563038056, test loss: 0.2639945423730808\n",
      "epoch 5249: train loss: 0.14148660818752753, test loss: 0.26399320042359725\n",
      "epoch 5250: train loss: 0.14147890288914705, test loss: 0.2639918592986732\n",
      "epoch 5251: train loss: 0.14147119973420014, test loss: 0.2639905189977906\n",
      "epoch 5252: train loss: 0.14146349872164843, test loss: 0.263989179520432\n",
      "epoch 5253: train loss: 0.1414557998504543, test loss: 0.26398784086608024\n",
      "epoch 5254: train loss: 0.1414481031195808, test loss: 0.2639865030342188\n",
      "epoch 5255: train loss: 0.14144040852799178, test loss: 0.2639851660243315\n",
      "epoch 5256: train loss: 0.1414327160746517, test loss: 0.2639838298359027\n",
      "epoch 5257: train loss: 0.14142502575852572, test loss: 0.26398249446841704\n",
      "epoch 5258: train loss: 0.14141733757857974, test loss: 0.2639811599213599\n",
      "epoch 5259: train loss: 0.14140965153378038, test loss: 0.2639798261942169\n",
      "epoch 5260: train loss: 0.1414019676230949, test loss: 0.26397849328647416\n",
      "epoch 5261: train loss: 0.14139428584549132, test loss: 0.26397716119761844\n",
      "epoch 5262: train loss: 0.14138660619993834, test loss: 0.2639758299271366\n",
      "epoch 5263: train loss: 0.14137892868540536, test loss: 0.26397449947451634\n",
      "epoch 5264: train loss: 0.1413712533008625, test loss: 0.26397316983924546\n",
      "epoch 5265: train loss: 0.1413635800452805, test loss: 0.2639718410208126\n",
      "epoch 5266: train loss: 0.1413559089176309, test loss: 0.26397051301870644\n",
      "epoch 5267: train loss: 0.1413482399168859, test loss: 0.2639691858324165\n",
      "epoch 5268: train loss: 0.14134057304201833, test loss: 0.2639678594614324\n",
      "epoch 5269: train loss: 0.14133290829200182, test loss: 0.26396653390524455\n",
      "epoch 5270: train loss: 0.14132524566581067, test loss: 0.2639652091633436\n",
      "epoch 5271: train loss: 0.14131758516241985, test loss: 0.26396388523522063\n",
      "epoch 5272: train loss: 0.141309926780805, test loss: 0.26396256212036734\n",
      "epoch 5273: train loss: 0.1413022705199425, test loss: 0.26396123981827574\n",
      "epoch 5274: train loss: 0.14129461637880944, test loss: 0.26395991832843835\n",
      "epoch 5275: train loss: 0.1412869643563835, test loss: 0.26395859765034824\n",
      "epoch 5276: train loss: 0.1412793144516432, test loss: 0.2639572777834986\n",
      "epoch 5277: train loss: 0.14127166666356766, test loss: 0.26395595872738337\n",
      "epoch 5278: train loss: 0.1412640209911367, test loss: 0.26395464048149697\n",
      "epoch 5279: train loss: 0.14125637743333078, test loss: 0.26395332304533403\n",
      "epoch 5280: train loss: 0.14124873598913118, test loss: 0.2639520064183899\n",
      "epoch 5281: train loss: 0.1412410966575198, test loss: 0.2639506906001601\n",
      "epoch 5282: train loss: 0.14123345943747917, test loss: 0.26394937559014076\n",
      "epoch 5283: train loss: 0.14122582432799258, test loss: 0.26394806138782845\n",
      "epoch 5284: train loss: 0.14121819132804403, test loss: 0.26394674799272017\n",
      "epoch 5285: train loss: 0.14121056043661814, test loss: 0.26394543540431337\n",
      "epoch 5286: train loss: 0.14120293165270023, test loss: 0.2639441236221059\n",
      "epoch 5287: train loss: 0.14119530497527635, test loss: 0.2639428126455961\n",
      "epoch 5288: train loss: 0.14118768040333318, test loss: 0.26394150247428283\n",
      "epoch 5289: train loss: 0.1411800579358581, test loss: 0.26394019310766514\n",
      "epoch 5290: train loss: 0.14117243757183923, test loss: 0.2639388845452429\n",
      "epoch 5291: train loss: 0.14116481931026528, test loss: 0.2639375767865161\n",
      "epoch 5292: train loss: 0.1411572031501257, test loss: 0.2639362698309852\n",
      "epoch 5293: train loss: 0.14114958909041062, test loss: 0.2639349636781514\n",
      "epoch 5294: train loss: 0.1411419771301108, test loss: 0.26393365832751603\n",
      "epoch 5295: train loss: 0.14113436726821782, test loss: 0.263932353778581\n",
      "epoch 5296: train loss: 0.14112675950372378, test loss: 0.2639310500308485\n",
      "epoch 5297: train loss: 0.1411191538356215, test loss: 0.2639297470838214\n",
      "epoch 5298: train loss: 0.14111155026290456, test loss: 0.26392844493700296\n",
      "epoch 5299: train loss: 0.14110394878456714, test loss: 0.26392714358989666\n",
      "epoch 5300: train loss: 0.14109634939960408, test loss: 0.2639258430420067\n",
      "epoch 5301: train loss: 0.141088752107011, test loss: 0.2639245432928376\n",
      "epoch 5302: train loss: 0.14108115690578407, test loss: 0.2639232443418941\n",
      "epoch 5303: train loss: 0.14107356379492028, test loss: 0.26392194618868187\n",
      "epoch 5304: train loss: 0.14106597277341718, test loss: 0.26392064883270666\n",
      "epoch 5305: train loss: 0.141058383840273, test loss: 0.26391935227347463\n",
      "epoch 5306: train loss: 0.14105079699448667, test loss: 0.2639180565104926\n",
      "epoch 5307: train loss: 0.1410432122350579, test loss: 0.26391676154326765\n",
      "epoch 5308: train loss: 0.14103562956098684, test loss: 0.2639154673713073\n",
      "epoch 5309: train loss: 0.14102804897127455, test loss: 0.26391417399411965\n",
      "epoch 5310: train loss: 0.1410204704649226, test loss: 0.26391288141121305\n",
      "epoch 5311: train loss: 0.1410128940409333, test loss: 0.2639115896220965\n",
      "epoch 5312: train loss: 0.14100531969830968, test loss: 0.26391029862627924\n",
      "epoch 5313: train loss: 0.14099774743605528, test loss: 0.263909008423271\n",
      "epoch 5314: train loss: 0.1409901772531745, test loss: 0.2639077190125819\n",
      "epoch 5315: train loss: 0.14098260914867228, test loss: 0.2639064303937227\n",
      "epoch 5316: train loss: 0.14097504312155426, test loss: 0.2639051425662043\n",
      "epoch 5317: train loss: 0.1409674791708268, test loss: 0.2639038555295382\n",
      "epoch 5318: train loss: 0.14095991729549684, test loss: 0.26390256928323647\n",
      "epoch 5319: train loss: 0.14095235749457205, test loss: 0.2639012838268111\n",
      "epoch 5320: train loss: 0.14094479976706079, test loss: 0.26389999915977513\n",
      "epoch 5321: train loss: 0.140937244111972, test loss: 0.2638987152816417\n",
      "epoch 5322: train loss: 0.14092969052831536, test loss: 0.2638974321919244\n",
      "epoch 5323: train loss: 0.14092213901510112, test loss: 0.2638961498901373\n",
      "epoch 5324: train loss: 0.14091458957134034, test loss: 0.26389486837579473\n",
      "epoch 5325: train loss: 0.14090704219604464, test loss: 0.26389358764841186\n",
      "epoch 5326: train loss: 0.14089949688822628, test loss: 0.2638923077075038\n",
      "epoch 5327: train loss: 0.14089195364689833, test loss: 0.2638910285525864\n",
      "epoch 5328: train loss: 0.14088441247107433, test loss: 0.2638897501831759\n",
      "epoch 5329: train loss: 0.1408768733597686, test loss: 0.2638884725987888\n",
      "epoch 5330: train loss: 0.1408693363119961, test loss: 0.2638871957989422\n",
      "epoch 5331: train loss: 0.14086180132677245, test loss: 0.26388591978315357\n",
      "epoch 5332: train loss: 0.14085426840311394, test loss: 0.2638846445509407\n",
      "epoch 5333: train loss: 0.14084673754003746, test loss: 0.263883370101822\n",
      "epoch 5334: train loss: 0.1408392087365606, test loss: 0.2638820964353162\n",
      "epoch 5335: train loss: 0.1408316819917017, test loss: 0.2638808235509424\n",
      "epoch 5336: train loss: 0.14082415730447959, test loss: 0.26387955144822023\n",
      "epoch 5337: train loss: 0.1408166346739138, test loss: 0.2638782801266697\n",
      "epoch 5338: train loss: 0.14080911409902463, test loss: 0.26387700958581123\n",
      "epoch 5339: train loss: 0.14080159557883293, test loss: 0.26387573982516555\n",
      "epoch 5340: train loss: 0.1407940791123602, test loss: 0.26387447084425414\n",
      "epoch 5341: train loss: 0.14078656469862869, test loss: 0.26387320264259856\n",
      "epoch 5342: train loss: 0.1407790523366612, test loss: 0.26387193521972085\n",
      "epoch 5343: train loss: 0.14077154202548123, test loss: 0.2638706685751437\n",
      "epoch 5344: train loss: 0.1407640337641129, test loss: 0.26386940270838993\n",
      "epoch 5345: train loss: 0.14075652755158108, test loss: 0.263868137618983\n",
      "epoch 5346: train loss: 0.14074902338691114, test loss: 0.26386687330644665\n",
      "epoch 5347: train loss: 0.14074152126912925, test loss: 0.263865609770305\n",
      "epoch 5348: train loss: 0.14073402119726214, test loss: 0.2638643470100828\n",
      "epoch 5349: train loss: 0.1407265231703372, test loss: 0.2638630850253051\n",
      "epoch 5350: train loss: 0.14071902718738252, test loss: 0.2638618238154972\n",
      "epoch 5351: train loss: 0.14071153324742677, test loss: 0.2638605633801852\n",
      "epoch 5352: train loss: 0.14070404134949932, test loss: 0.2638593037188951\n",
      "epoch 5353: train loss: 0.14069655149263016, test loss: 0.2638580448311539\n",
      "epoch 5354: train loss: 0.14068906367584996, test loss: 0.2638567867164884\n",
      "epoch 5355: train loss: 0.14068157789818997, test loss: 0.2638555293744264\n",
      "epoch 5356: train loss: 0.14067409415868215, test loss: 0.26385427280449575\n",
      "epoch 5357: train loss: 0.1406666124563591, test loss: 0.2638530170062248\n",
      "epoch 5358: train loss: 0.1406591327902541, test loss: 0.26385176197914234\n",
      "epoch 5359: train loss: 0.14065165515940092, test loss: 0.26385050772277757\n",
      "epoch 5360: train loss: 0.14064417956283415, test loss: 0.26384925423665995\n",
      "epoch 5361: train loss: 0.14063670599958894, test loss: 0.26384800152031973\n",
      "epoch 5362: train loss: 0.1406292344687011, test loss: 0.2638467495732872\n",
      "epoch 5363: train loss: 0.14062176496920709, test loss: 0.2638454983950931\n",
      "epoch 5364: train loss: 0.14061429750014398, test loss: 0.2638442479852688\n",
      "epoch 5365: train loss: 0.14060683206054952, test loss: 0.26384299834334596\n",
      "epoch 5366: train loss: 0.1405993686494621, test loss: 0.2638417494688566\n",
      "epoch 5367: train loss: 0.14059190726592072, test loss: 0.26384050136133314\n",
      "epoch 5368: train loss: 0.14058444790896502, test loss: 0.26383925402030856\n",
      "epoch 5369: train loss: 0.1405769905776353, test loss: 0.2638380074453161\n",
      "epoch 5370: train loss: 0.14056953527097252, test loss: 0.2638367616358895\n",
      "epoch 5371: train loss: 0.14056208198801823, test loss: 0.26383551659156285\n",
      "epoch 5372: train loss: 0.14055463072781463, test loss: 0.2638342723118705\n",
      "epoch 5373: train loss: 0.14054718148940462, test loss: 0.2638330287963476\n",
      "epoch 5374: train loss: 0.14053973427183164, test loss: 0.26383178604452945\n",
      "epoch 5375: train loss: 0.14053228907413978, test loss: 0.2638305440559517\n",
      "epoch 5376: train loss: 0.1405248458953738, test loss: 0.2638293028301505\n",
      "epoch 5377: train loss: 0.1405174047345792, test loss: 0.2638280623666624\n",
      "epoch 5378: train loss: 0.14050996559080187, test loss: 0.26382682266502433\n",
      "epoch 5379: train loss: 0.14050252846308856, test loss: 0.2638255837247737\n",
      "epoch 5380: train loss: 0.1404950933504865, test loss: 0.2638243455454482\n",
      "epoch 5381: train loss: 0.14048766025204362, test loss: 0.2638231081265861\n",
      "epoch 5382: train loss: 0.14048022916680852, test loss: 0.26382187146772584\n",
      "epoch 5383: train loss: 0.14047280009383037, test loss: 0.2638206355684064\n",
      "epoch 5384: train loss: 0.14046537303215897, test loss: 0.2638194004281672\n",
      "epoch 5385: train loss: 0.1404579479808448, test loss: 0.26381816604654795\n",
      "epoch 5386: train loss: 0.14045052493893892, test loss: 0.263816932423089\n",
      "epoch 5387: train loss: 0.14044310390549306, test loss: 0.26381569955733064\n",
      "epoch 5388: train loss: 0.14043568487955954, test loss: 0.26381446744881404\n",
      "epoch 5389: train loss: 0.14042826786019133, test loss: 0.2638132360970804\n",
      "epoch 5390: train loss: 0.14042085284644204, test loss: 0.2638120055016717\n",
      "epoch 5391: train loss: 0.14041343983736587, test loss: 0.26381077566213007\n",
      "epoch 5392: train loss: 0.14040602883201772, test loss: 0.263809546577998\n",
      "epoch 5393: train loss: 0.14039861982945304, test loss: 0.26380831824881845\n",
      "epoch 5394: train loss: 0.1403912128287279, test loss: 0.2638070906741348\n",
      "epoch 5395: train loss: 0.1403838078288991, test loss: 0.2638058638534909\n",
      "epoch 5396: train loss: 0.1403764048290239, test loss: 0.2638046377864308\n",
      "epoch 5397: train loss: 0.14036900382816034, test loss: 0.2638034124724992\n",
      "epoch 5398: train loss: 0.14036160482536705, test loss: 0.26380218791124094\n",
      "epoch 5399: train loss: 0.14035420781970318, test loss: 0.26380096410220133\n",
      "epoch 5400: train loss: 0.14034681281022865, test loss: 0.26379974104492626\n",
      "epoch 5401: train loss: 0.14033941979600384, test loss: 0.2637985187389617\n",
      "epoch 5402: train loss: 0.14033202877608997, test loss: 0.26379729718385436\n",
      "epoch 5403: train loss: 0.14032463974954865, test loss: 0.2637960763791511\n",
      "epoch 5404: train loss: 0.14031725271544226, test loss: 0.2637948563243992\n",
      "epoch 5405: train loss: 0.14030986767283374, test loss: 0.2637936370191464\n",
      "epoch 5406: train loss: 0.14030248462078665, test loss: 0.2637924184629409\n",
      "epoch 5407: train loss: 0.14029510355836522, test loss: 0.26379120065533107\n",
      "epoch 5408: train loss: 0.14028772448463422, test loss: 0.2637899835958659\n",
      "epoch 5409: train loss: 0.14028034739865913, test loss: 0.2637887672840946\n",
      "epoch 5410: train loss: 0.14027297229950594, test loss: 0.2637875517195669\n",
      "epoch 5411: train loss: 0.14026559918624135, test loss: 0.26378633690183295\n",
      "epoch 5412: train loss: 0.14025822805793262, test loss: 0.26378512283044314\n",
      "epoch 5413: train loss: 0.14025085891364766, test loss: 0.26378390950494823\n",
      "epoch 5414: train loss: 0.140243491752455, test loss: 0.2637826969248996\n",
      "epoch 5415: train loss: 0.14023612657342377, test loss: 0.2637814850898488\n",
      "epoch 5416: train loss: 0.14022876337562365, test loss: 0.2637802739993479\n",
      "epoch 5417: train loss: 0.14022140215812506, test loss: 0.2637790636529493\n",
      "epoch 5418: train loss: 0.14021404291999898, test loss: 0.26377785405020576\n",
      "epoch 5419: train loss: 0.1402066856603169, test loss: 0.26377664519067046\n",
      "epoch 5420: train loss: 0.1401993303781511, test loss: 0.2637754370738971\n",
      "epoch 5421: train loss: 0.14019197707257441, test loss: 0.2637742296994395\n",
      "epoch 5422: train loss: 0.14018462574266013, test loss: 0.263773023066852\n",
      "epoch 5423: train loss: 0.1401772763874824, test loss: 0.2637718171756895\n",
      "epoch 5424: train loss: 0.1401699290061158, test loss: 0.26377061202550695\n",
      "epoch 5425: train loss: 0.14016258359763561, test loss: 0.2637694076158599\n",
      "epoch 5426: train loss: 0.14015524016111766, test loss: 0.26376820394630424\n",
      "epoch 5427: train loss: 0.14014789869563846, test loss: 0.2637670010163962\n",
      "epoch 5428: train loss: 0.14014055920027504, test loss: 0.26376579882569257\n",
      "epoch 5429: train loss: 0.1401332216741051, test loss: 0.2637645973737503\n",
      "epoch 5430: train loss: 0.14012588611620694, test loss: 0.2637633966601268\n",
      "epoch 5431: train loss: 0.14011855252565944, test loss: 0.26376219668437995\n",
      "epoch 5432: train loss: 0.14011122090154213, test loss: 0.26376099744606796\n",
      "epoch 5433: train loss: 0.14010389124293507, test loss: 0.2637597989447492\n",
      "epoch 5434: train loss: 0.14009656354891903, test loss: 0.26375860117998284\n",
      "epoch 5435: train loss: 0.1400892378185753, test loss: 0.2637574041513282\n",
      "epoch 5436: train loss: 0.14008191405098583, test loss: 0.26375620785834486\n",
      "epoch 5437: train loss: 0.14007459224523308, test loss: 0.2637550123005931\n",
      "epoch 5438: train loss: 0.1400672724004003, test loss: 0.26375381747763327\n",
      "epoch 5439: train loss: 0.1400599545155711, test loss: 0.2637526233890263\n",
      "epoch 5440: train loss: 0.1400526385898299, test loss: 0.2637514300343333\n",
      "epoch 5441: train loss: 0.14004532462226155, test loss: 0.263750237413116\n",
      "epoch 5442: train loss: 0.1400380126119517, test loss: 0.26374904552493644\n",
      "epoch 5443: train loss: 0.14003070255798644, test loss: 0.2637478543693569\n",
      "epoch 5444: train loss: 0.14002339445945247, test loss: 0.26374666394594015\n",
      "epoch 5445: train loss: 0.14001608831543713, test loss: 0.2637454742542494\n",
      "epoch 5446: train loss: 0.14000878412502846, test loss: 0.263744285293848\n",
      "epoch 5447: train loss: 0.14000148188731487, test loss: 0.26374309706430005\n",
      "epoch 5448: train loss: 0.13999418160138558, test loss: 0.26374190956516963\n",
      "epoch 5449: train loss: 0.13998688326633027, test loss: 0.2637407227960214\n",
      "epoch 5450: train loss: 0.13997958688123932, test loss: 0.2637395367564205\n",
      "epoch 5451: train loss: 0.13997229244520365, test loss: 0.26373835144593216\n",
      "epoch 5452: train loss: 0.1399649999573147, test loss: 0.26373716686412224\n",
      "epoch 5453: train loss: 0.13995770941666472, test loss: 0.26373598301055684\n",
      "epoch 5454: train loss: 0.13995042082234632, test loss: 0.26373479988480253\n",
      "epoch 5455: train loss: 0.13994313417345286, test loss: 0.2637336174864261\n",
      "epoch 5456: train loss: 0.13993584946907822, test loss: 0.2637324358149948\n",
      "epoch 5457: train loss: 0.1399285667083169, test loss: 0.2637312548700764\n",
      "epoch 5458: train loss: 0.139921285890264, test loss: 0.2637300746512388\n",
      "epoch 5459: train loss: 0.1399140070140152, test loss: 0.2637288951580504\n",
      "epoch 5460: train loss: 0.1399067300786668, test loss: 0.2637277163900799\n",
      "epoch 5461: train loss: 0.1398994550833156, test loss: 0.2637265383468966\n",
      "epoch 5462: train loss: 0.13989218202705914, test loss: 0.2637253610280697\n",
      "epoch 5463: train loss: 0.1398849109089954, test loss: 0.26372418443316925\n",
      "epoch 5464: train loss: 0.13987764172822306, test loss: 0.26372300856176545\n",
      "epoch 5465: train loss: 0.13987037448384132, test loss: 0.263721833413429\n",
      "epoch 5466: train loss: 0.13986310917495, test loss: 0.26372065898773067\n",
      "epoch 5467: train loss: 0.13985584580064953, test loss: 0.2637194852842419\n",
      "epoch 5468: train loss: 0.13984858436004088, test loss: 0.2637183123025344\n",
      "epoch 5469: train loss: 0.1398413248522257, test loss: 0.2637171400421802\n",
      "epoch 5470: train loss: 0.13983406727630604, test loss: 0.26371596850275186\n",
      "epoch 5471: train loss: 0.13982681163138477, test loss: 0.2637147976838221\n",
      "epoch 5472: train loss: 0.13981955791656517, test loss: 0.2637136275849641\n",
      "epoch 5473: train loss: 0.1398123061309512, test loss: 0.2637124582057514\n",
      "epoch 5474: train loss: 0.13980505627364737, test loss: 0.26371128954575795\n",
      "epoch 5475: train loss: 0.13979780834375874, test loss: 0.26371012160455803\n",
      "epoch 5476: train loss: 0.13979056234039103, test loss: 0.26370895438172626\n",
      "epoch 5477: train loss: 0.13978331826265053, test loss: 0.2637077878768377\n",
      "epoch 5478: train loss: 0.13977607610964402, test loss: 0.2637066220894676\n",
      "epoch 5479: train loss: 0.13976883588047898, test loss: 0.2637054570191918\n",
      "epoch 5480: train loss: 0.13976159757426346, test loss: 0.2637042926655864\n",
      "epoch 5481: train loss: 0.13975436119010604, test loss: 0.2637031290282279\n",
      "epoch 5482: train loss: 0.13974712672711584, test loss: 0.26370196610669294\n",
      "epoch 5483: train loss: 0.13973989418440266, test loss: 0.26370080390055894\n",
      "epoch 5484: train loss: 0.13973266356107686, test loss: 0.2636996424094034\n",
      "epoch 5485: train loss: 0.13972543485624936, test loss: 0.2636984816328042\n",
      "epoch 5486: train loss: 0.13971820806903165, test loss: 0.2636973215703396\n",
      "epoch 5487: train loss: 0.13971098319853578, test loss: 0.26369616222158826\n",
      "epoch 5488: train loss: 0.1397037602438745, test loss: 0.2636950035861292\n",
      "epoch 5489: train loss: 0.13969653920416092, test loss: 0.2636938456635417\n",
      "epoch 5490: train loss: 0.13968932007850893, test loss: 0.2636926884534057\n",
      "epoch 5491: train loss: 0.13968210286603294, test loss: 0.26369153195530104\n",
      "epoch 5492: train loss: 0.1396748875658479, test loss: 0.26369037616880836\n",
      "epoch 5493: train loss: 0.13966767417706932, test loss: 0.2636892210935083\n",
      "epoch 5494: train loss: 0.13966046269881333, test loss: 0.2636880667289821\n",
      "epoch 5495: train loss: 0.13965325313019666, test loss: 0.2636869130748113\n",
      "epoch 5496: train loss: 0.1396460454703366, test loss: 0.2636857601305777\n",
      "epoch 5497: train loss: 0.13963883971835092, test loss: 0.2636846078958637\n",
      "epoch 5498: train loss: 0.13963163587335806, test loss: 0.26368345637025165\n",
      "epoch 5499: train loss: 0.13962443393447707, test loss: 0.26368230555332467\n",
      "epoch 5500: train loss: 0.13961723390082745, test loss: 0.263681155444666\n",
      "epoch 5501: train loss: 0.13961003577152936, test loss: 0.2636800060438595\n",
      "epoch 5502: train loss: 0.13960283954570352, test loss: 0.26367885735048885\n",
      "epoch 5503: train loss: 0.13959564522247123, test loss: 0.2636777093641387\n",
      "epoch 5504: train loss: 0.13958845280095425, test loss: 0.26367656208439366\n",
      "epoch 5505: train loss: 0.1395812622802751, test loss: 0.26367541551083884\n",
      "epoch 5506: train loss: 0.13957407365955674, test loss: 0.2636742696430597\n",
      "epoch 5507: train loss: 0.1395668869379227, test loss: 0.26367312448064206\n",
      "epoch 5508: train loss: 0.13955970211449717, test loss: 0.26367198002317194\n",
      "epoch 5509: train loss: 0.13955251918840478, test loss: 0.2636708362702359\n",
      "epoch 5510: train loss: 0.13954533815877088, test loss: 0.2636696932214209\n",
      "epoch 5511: train loss: 0.13953815902472125, test loss: 0.263668550876314\n",
      "epoch 5512: train loss: 0.1395309817853823, test loss: 0.26366740923450294\n",
      "epoch 5513: train loss: 0.13952380643988102, test loss: 0.2636662682955755\n",
      "epoch 5514: train loss: 0.1395166329873449, test loss: 0.2636651280591199\n",
      "epoch 5515: train loss: 0.13950946142690207, test loss: 0.26366398852472483\n",
      "epoch 5516: train loss: 0.13950229175768122, test loss: 0.2636628496919793\n",
      "epoch 5517: train loss: 0.13949512397881153, test loss: 0.2636617115604725\n",
      "epoch 5518: train loss: 0.13948795808942285, test loss: 0.2636605741297942\n",
      "epoch 5519: train loss: 0.13948079408864553, test loss: 0.26365943739953446\n",
      "epoch 5520: train loss: 0.13947363197561047, test loss: 0.26365830136928353\n",
      "epoch 5521: train loss: 0.13946647174944912, test loss: 0.2636571660386322\n",
      "epoch 5522: train loss: 0.13945931340929363, test loss: 0.2636560314071715\n",
      "epoch 5523: train loss: 0.1394521569542765, test loss: 0.26365489747449283\n",
      "epoch 5524: train loss: 0.139445002383531, test loss: 0.26365376424018794\n",
      "epoch 5525: train loss: 0.1394378496961908, test loss: 0.263652631703849\n",
      "epoch 5526: train loss: 0.13943069889139026, test loss: 0.2636514998650684\n",
      "epoch 5527: train loss: 0.13942354996826412, test loss: 0.26365036872343905\n",
      "epoch 5528: train loss: 0.1394164029259479, test loss: 0.26364923827855397\n",
      "epoch 5529: train loss: 0.13940925776357754, test loss: 0.2636481085300067\n",
      "epoch 5530: train loss: 0.13940211448028955, test loss: 0.2636469794773912\n",
      "epoch 5531: train loss: 0.13939497307522106, test loss: 0.2636458511203014\n",
      "epoch 5532: train loss: 0.13938783354750972, test loss: 0.263644723458332\n",
      "epoch 5533: train loss: 0.13938069589629368, test loss: 0.2636435964910779\n",
      "epoch 5534: train loss: 0.13937356012071178, test loss: 0.2636424702181343\n",
      "epoch 5535: train loss: 0.13936642621990328, test loss: 0.2636413446390967\n",
      "epoch 5536: train loss: 0.13935929419300805, test loss: 0.2636402197535611\n",
      "epoch 5537: train loss: 0.1393521640391666, test loss: 0.2636390955611237\n",
      "epoch 5538: train loss: 0.13934503575751983, test loss: 0.26363797206138107\n",
      "epoch 5539: train loss: 0.13933790934720933, test loss: 0.26363684925393016\n",
      "epoch 5540: train loss: 0.1393307848073772, test loss: 0.26363572713836836\n",
      "epoch 5541: train loss: 0.13932366213716604, test loss: 0.26363460571429315\n",
      "epoch 5542: train loss: 0.1393165413357191, test loss: 0.2636334849813025\n",
      "epoch 5543: train loss: 0.13930942240218014, test loss: 0.26363236493899483\n",
      "epoch 5544: train loss: 0.13930230533569343, test loss: 0.2636312455869687\n",
      "epoch 5545: train loss: 0.13929519013540387, test loss: 0.2636301269248231\n",
      "epoch 5546: train loss: 0.13928807680045682, test loss: 0.2636290089521574\n",
      "epoch 5547: train loss: 0.1392809653299983, test loss: 0.26362789166857126\n",
      "epoch 5548: train loss: 0.1392738557231748, test loss: 0.26362677507366467\n",
      "epoch 5549: train loss: 0.13926674797913335, test loss: 0.26362565916703795\n",
      "epoch 5550: train loss: 0.13925964209702166, test loss: 0.2636245439482918\n",
      "epoch 5551: train loss: 0.13925253807598778, test loss: 0.26362342941702727\n",
      "epoch 5552: train loss: 0.1392454359151805, test loss: 0.2636223155728457\n",
      "epoch 5553: train loss: 0.13923833561374901, test loss: 0.26362120241534887\n",
      "epoch 5554: train loss: 0.13923123717084318, test loss: 0.2636200899441387\n",
      "epoch 5555: train loss: 0.1392241405856134, test loss: 0.2636189781588175\n",
      "epoch 5556: train loss: 0.13921704585721045, test loss: 0.2636178670589883\n",
      "epoch 5557: train loss: 0.13920995298478583, test loss: 0.26361675664425377\n",
      "epoch 5558: train loss: 0.13920286196749163, test loss: 0.2636156469142175\n",
      "epoch 5559: train loss: 0.13919577280448028, test loss: 0.2636145378684832\n",
      "epoch 5560: train loss: 0.13918868549490485, test loss: 0.2636134295066549\n",
      "epoch 5561: train loss: 0.13918160003791907, test loss: 0.26361232182833705\n",
      "epoch 5562: train loss: 0.13917451643267706, test loss: 0.26361121483313416\n",
      "epoch 5563: train loss: 0.13916743467833353, test loss: 0.2636101085206516\n",
      "epoch 5564: train loss: 0.13916035477404376, test loss: 0.2636090028904945\n",
      "epoch 5565: train loss: 0.13915327671896355, test loss: 0.2636078979422687\n",
      "epoch 5566: train loss: 0.13914620051224927, test loss: 0.2636067936755802\n",
      "epoch 5567: train loss: 0.13913912615305776, test loss: 0.26360569009003554\n",
      "epoch 5568: train loss: 0.13913205364054648, test loss: 0.26360458718524127\n",
      "epoch 5569: train loss: 0.13912498297387343, test loss: 0.2636034849608046\n",
      "epoch 5570: train loss: 0.1391179141521971, test loss: 0.2636023834163328\n",
      "epoch 5571: train loss: 0.13911084717467653, test loss: 0.26360128255143367\n",
      "epoch 5572: train loss: 0.1391037820404713, test loss: 0.2636001823657152\n",
      "epoch 5573: train loss: 0.1390967187487416, test loss: 0.26359908285878586\n",
      "epoch 5574: train loss: 0.13908965729864808, test loss: 0.26359798403025425\n",
      "epoch 5575: train loss: 0.13908259768935194, test loss: 0.2635968858797295\n",
      "epoch 5576: train loss: 0.13907553992001495, test loss: 0.263595788406821\n",
      "epoch 5577: train loss: 0.13906848398979935, test loss: 0.2635946916111384\n",
      "epoch 5578: train loss: 0.13906142989786802, test loss: 0.26359359549229167\n",
      "epoch 5579: train loss: 0.13905437764338424, test loss: 0.2635925000498914\n",
      "epoch 5580: train loss: 0.13904732722551202, test loss: 0.263591405283548\n",
      "epoch 5581: train loss: 0.13904027864341573, test loss: 0.2635903111928727\n",
      "epoch 5582: train loss: 0.13903323189626035, test loss: 0.2635892177774768\n",
      "epoch 5583: train loss: 0.13902618698321134, test loss: 0.2635881250369719\n",
      "epoch 5584: train loss: 0.1390191439034348, test loss: 0.2635870329709701\n",
      "epoch 5585: train loss: 0.1390121026560973, test loss: 0.26358594157908355\n",
      "epoch 5586: train loss: 0.13900506324036593, test loss: 0.26358485086092515\n",
      "epoch 5587: train loss: 0.1389980256554083, test loss: 0.26358376081610774\n",
      "epoch 5588: train loss: 0.13899098990039263, test loss: 0.2635826714442447\n",
      "epoch 5589: train loss: 0.13898395597448762, test loss: 0.2635815827449495\n",
      "epoch 5590: train loss: 0.13897692387686247, test loss: 0.26358049471783634\n",
      "epoch 5591: train loss: 0.138969893606687, test loss: 0.26357940736251934\n",
      "epoch 5592: train loss: 0.13896286516313144, test loss: 0.26357832067861314\n",
      "epoch 5593: train loss: 0.13895583854536667, test loss: 0.26357723466573274\n",
      "epoch 5594: train loss: 0.13894881375256407, test loss: 0.2635761493234933\n",
      "epoch 5595: train loss: 0.13894179078389549, test loss: 0.2635750646515105\n",
      "epoch 5596: train loss: 0.13893476963853335, test loss: 0.2635739806494002\n",
      "epoch 5597: train loss: 0.13892775031565063, test loss: 0.26357289731677847\n",
      "epoch 5598: train loss: 0.1389207328144208, test loss: 0.2635718146532622\n",
      "epoch 5599: train loss: 0.13891371713401784, test loss: 0.2635707326584679\n",
      "epoch 5600: train loss: 0.13890670327361632, test loss: 0.26356965133201304\n",
      "epoch 5601: train loss: 0.13889969123239126, test loss: 0.2635685706735151\n",
      "epoch 5602: train loss: 0.13889268100951832, test loss: 0.2635674906825917\n",
      "epoch 5603: train loss: 0.1388856726041735, test loss: 0.26356641135886116\n",
      "epoch 5604: train loss: 0.13887866601553353, test loss: 0.26356533270194193\n",
      "epoch 5605: train loss: 0.13887166124277556, test loss: 0.26356425471145284\n",
      "epoch 5606: train loss: 0.13886465828507724, test loss: 0.263563177387013\n",
      "epoch 5607: train loss: 0.13885765714161685, test loss: 0.2635621007282418\n",
      "epoch 5608: train loss: 0.13885065781157307, test loss: 0.2635610247347591\n",
      "epoch 5609: train loss: 0.13884366029412523, test loss: 0.263559949406185\n",
      "epoch 5610: train loss: 0.13883666458845303, test loss: 0.2635588747421396\n",
      "epoch 5611: train loss: 0.13882967069373686, test loss: 0.2635578007422439\n",
      "epoch 5612: train loss: 0.1388226786091575, test loss: 0.26355672740611885\n",
      "epoch 5613: train loss: 0.13881568833389632, test loss: 0.2635556547333859\n",
      "epoch 5614: train loss: 0.13880869986713523, test loss: 0.2635545827236665\n",
      "epoch 5615: train loss: 0.1388017132080566, test loss: 0.26355351137658284\n",
      "epoch 5616: train loss: 0.1387947283558433, test loss: 0.26355244069175704\n",
      "epoch 5617: train loss: 0.13878774530967886, test loss: 0.2635513706688119\n",
      "epoch 5618: train loss: 0.13878076406874718, test loss: 0.2635503013073702\n",
      "epoch 5619: train loss: 0.13877378463223275, test loss: 0.26354923260705526\n",
      "epoch 5620: train loss: 0.13876680699932056, test loss: 0.2635481645674907\n",
      "epoch 5621: train loss: 0.13875983116919616, test loss: 0.26354709718830027\n",
      "epoch 5622: train loss: 0.13875285714104554, test loss: 0.26354603046910824\n",
      "epoch 5623: train loss: 0.13874588491405532, test loss: 0.2635449644095392\n",
      "epoch 5624: train loss: 0.13873891448741252, test loss: 0.2635438990092178\n",
      "epoch 5625: train loss: 0.13873194586030468, test loss: 0.2635428342677693\n",
      "epoch 5626: train loss: 0.13872497903192002, test loss: 0.2635417701848191\n",
      "epoch 5627: train loss: 0.1387180140014471, test loss: 0.26354070675999297\n",
      "epoch 5628: train loss: 0.138711050768075, test loss: 0.263539643992917\n",
      "epoch 5629: train loss: 0.13870408933099346, test loss: 0.2635385818832176\n",
      "epoch 5630: train loss: 0.1386971296893926, test loss: 0.2635375204305214\n",
      "epoch 5631: train loss: 0.13869017184246313, test loss: 0.2635364596344555\n",
      "epoch 5632: train loss: 0.13868321578939624, test loss: 0.26353539949464716\n",
      "epoch 5633: train loss: 0.13867626152938362, test loss: 0.26353434001072407\n",
      "epoch 5634: train loss: 0.1386693090616175, test loss: 0.2635332811823141\n",
      "epoch 5635: train loss: 0.13866235838529062, test loss: 0.2635322230090456\n",
      "epoch 5636: train loss: 0.13865540949959623, test loss: 0.2635311654905471\n",
      "epoch 5637: train loss: 0.13864846240372808, test loss: 0.2635301086264475\n",
      "epoch 5638: train loss: 0.13864151709688044, test loss: 0.2635290524163759\n",
      "epoch 5639: train loss: 0.1386345735782481, test loss: 0.263527996859962\n",
      "epoch 5640: train loss: 0.13862763184702637, test loss: 0.2635269419568353\n",
      "epoch 5641: train loss: 0.13862069190241105, test loss: 0.26352588770662616\n",
      "epoch 5642: train loss: 0.1386137537435984, test loss: 0.263524834108965\n",
      "epoch 5643: train loss: 0.1386068173697853, test loss: 0.26352378116348235\n",
      "epoch 5644: train loss: 0.1385998827801691, test loss: 0.26352272886980943\n",
      "epoch 5645: train loss: 0.1385929499739476, test loss: 0.2635216772275776\n",
      "epoch 5646: train loss: 0.13858601895031913, test loss: 0.26352062623641836\n",
      "epoch 5647: train loss: 0.1385790897084826, test loss: 0.26351957589596375\n",
      "epoch 5648: train loss: 0.13857216224763735, test loss: 0.26351852620584615\n",
      "epoch 5649: train loss: 0.13856523656698325, test loss: 0.26351747716569796\n",
      "epoch 5650: train loss: 0.13855831266572072, test loss: 0.2635164287751522\n",
      "epoch 5651: train loss: 0.13855139054305057, test loss: 0.263515381033842\n",
      "epoch 5652: train loss: 0.13854447019817426, test loss: 0.2635143339414008\n",
      "epoch 5653: train loss: 0.13853755163029366, test loss: 0.2635132874974625\n",
      "epoch 5654: train loss: 0.13853063483861117, test loss: 0.2635122417016611\n",
      "epoch 5655: train loss: 0.1385237198223297, test loss: 0.2635111965536311\n",
      "epoch 5656: train loss: 0.13851680658065266, test loss: 0.2635101520530071\n",
      "epoch 5657: train loss: 0.13850989511278397, test loss: 0.2635091081994243\n",
      "epoch 5658: train loss: 0.13850298541792802, test loss: 0.2635080649925177\n",
      "epoch 5659: train loss: 0.13849607749528978, test loss: 0.2635070224319233\n",
      "epoch 5660: train loss: 0.13848917134407465, test loss: 0.26350598051727675\n",
      "epoch 5661: train loss: 0.13848226696348856, test loss: 0.26350493924821444\n",
      "epoch 5662: train loss: 0.1384753643527379, test loss: 0.26350389862437273\n",
      "epoch 5663: train loss: 0.13846846351102968, test loss: 0.26350285864538864\n",
      "epoch 5664: train loss: 0.13846156443757127, test loss: 0.26350181931089905\n",
      "epoch 5665: train loss: 0.13845466713157062, test loss: 0.2635007806205417\n",
      "epoch 5666: train loss: 0.13844777159223617, test loss: 0.26349974257395414\n",
      "epoch 5667: train loss: 0.13844087781877684, test loss: 0.2634987051707744\n",
      "epoch 5668: train loss: 0.13843398581040206, test loss: 0.2634976684106409\n",
      "epoch 5669: train loss: 0.13842709556632174, test loss: 0.2634966322931922\n",
      "epoch 5670: train loss: 0.13842020708574634, test loss: 0.2634955968180673\n",
      "epoch 5671: train loss: 0.13841332036788678, test loss: 0.26349456198490534\n",
      "epoch 5672: train loss: 0.1384064354119545, test loss: 0.26349352779334595\n",
      "epoch 5673: train loss: 0.13839955221716138, test loss: 0.2634924942430289\n",
      "epoch 5674: train loss: 0.13839267078271988, test loss: 0.2634914613335944\n",
      "epoch 5675: train loss: 0.13838579110784288, test loss: 0.2634904290646828\n",
      "epoch 5676: train loss: 0.13837891319174386, test loss: 0.2634893974359349\n",
      "epoch 5677: train loss: 0.13837203703363662, test loss: 0.26348836644699164\n",
      "epoch 5678: train loss: 0.13836516263273563, test loss: 0.26348733609749436\n",
      "epoch 5679: train loss: 0.1383582899882558, test loss: 0.26348630638708476\n",
      "epoch 5680: train loss: 0.1383514190994125, test loss: 0.26348527731540466\n",
      "epoch 5681: train loss: 0.1383445499654216, test loss: 0.2634842488820964\n",
      "epoch 5682: train loss: 0.1383376825854995, test loss: 0.26348322108680233\n",
      "epoch 5683: train loss: 0.1383308169588631, test loss: 0.2634821939291653\n",
      "epoch 5684: train loss: 0.1383239530847297, test loss: 0.2634811674088285\n",
      "epoch 5685: train loss: 0.13831709096231726, test loss: 0.26348014152543525\n",
      "epoch 5686: train loss: 0.13831023059084402, test loss: 0.2634791162786293\n",
      "epoch 5687: train loss: 0.1383033719695289, test loss: 0.2634780916680545\n",
      "epoch 5688: train loss: 0.1382965150975912, test loss: 0.26347706769335544\n",
      "epoch 5689: train loss: 0.13828965997425077, test loss: 0.2634760443541763\n",
      "epoch 5690: train loss: 0.1382828065987279, test loss: 0.2634750216501622\n",
      "epoch 5691: train loss: 0.13827595497024345, test loss: 0.2634739995809582\n",
      "epoch 5692: train loss: 0.13826910508801862, test loss: 0.2634729781462098\n",
      "epoch 5693: train loss: 0.1382622569512753, test loss: 0.2634719573455627\n",
      "epoch 5694: train loss: 0.13825541055923574, test loss: 0.2634709371786631\n",
      "epoch 5695: train loss: 0.13824856591112267, test loss: 0.2634699176451571\n",
      "epoch 5696: train loss: 0.13824172300615933, test loss: 0.2634688987446915\n",
      "epoch 5697: train loss: 0.13823488184356955, test loss: 0.26346788047691305\n",
      "epoch 5698: train loss: 0.13822804242257744, test loss: 0.26346686284146914\n",
      "epoch 5699: train loss: 0.13822120474240782, test loss: 0.26346584583800714\n",
      "epoch 5700: train loss: 0.13821436880228583, test loss: 0.26346482946617494\n",
      "epoch 5701: train loss: 0.1382075346014372, test loss: 0.26346381372562055\n",
      "epoch 5702: train loss: 0.13820070213908806, test loss: 0.2634627986159924\n",
      "epoch 5703: train loss: 0.1381938714144651, test loss: 0.2634617841369391\n",
      "epoch 5704: train loss: 0.13818704242679544, test loss: 0.2634607702881096\n",
      "epoch 5705: train loss: 0.13818021517530674, test loss: 0.2634597570691531\n",
      "epoch 5706: train loss: 0.1381733896592271, test loss: 0.2634587444797193\n",
      "epoch 5707: train loss: 0.13816656587778509, test loss: 0.2634577325194578\n",
      "epoch 5708: train loss: 0.13815974383020987, test loss: 0.2634567211880189\n",
      "epoch 5709: train loss: 0.1381529235157309, test loss: 0.2634557104850528\n",
      "epoch 5710: train loss: 0.13814610493357835, test loss: 0.2634547004102103\n",
      "epoch 5711: train loss: 0.13813928808298265, test loss: 0.2634536909631424\n",
      "epoch 5712: train loss: 0.13813247296317485, test loss: 0.2634526821435003\n",
      "epoch 5713: train loss: 0.13812565957338646, test loss: 0.26345167395093555\n",
      "epoch 5714: train loss: 0.13811884791284942, test loss: 0.26345066638510006\n",
      "epoch 5715: train loss: 0.13811203798079622, test loss: 0.2634496594456458\n",
      "epoch 5716: train loss: 0.1381052297764598, test loss: 0.2634486531322252\n",
      "epoch 5717: train loss: 0.13809842329907354, test loss: 0.26344764744449106\n",
      "epoch 5718: train loss: 0.13809161854787136, test loss: 0.26344664238209625\n",
      "epoch 5719: train loss: 0.13808481552208765, test loss: 0.26344563794469417\n",
      "epoch 5720: train loss: 0.13807801422095722, test loss: 0.2634446341319382\n",
      "epoch 5721: train loss: 0.13807121464371547, test loss: 0.2634436309434822\n",
      "epoch 5722: train loss: 0.1380644167895982, test loss: 0.2634426283789804\n",
      "epoch 5723: train loss: 0.13805762065784163, test loss: 0.2634416264380871\n",
      "epoch 5724: train loss: 0.1380508262476826, test loss: 0.2634406251204571\n",
      "epoch 5725: train loss: 0.13804403355835831, test loss: 0.2634396244257451\n",
      "epoch 5726: train loss: 0.13803724258910655, test loss: 0.26343862435360665\n",
      "epoch 5727: train loss: 0.13803045333916544, test loss: 0.26343762490369704\n",
      "epoch 5728: train loss: 0.1380236658077737, test loss: 0.2634366260756723\n",
      "epoch 5729: train loss: 0.13801687999417048, test loss: 0.2634356278691884\n",
      "epoch 5730: train loss: 0.1380100958975954, test loss: 0.2634346302839018\n",
      "epoch 5731: train loss: 0.1380033135172885, test loss: 0.26343363331946906\n",
      "epoch 5732: train loss: 0.13799653285249044, test loss: 0.2634326369755472\n",
      "epoch 5733: train loss: 0.13798975390244225, test loss: 0.26343164125179336\n",
      "epoch 5734: train loss: 0.13798297666638543, test loss: 0.2634306461478652\n",
      "epoch 5735: train loss: 0.13797620114356196, test loss: 0.2634296516634204\n",
      "epoch 5736: train loss: 0.13796942733321435, test loss: 0.26342865779811697\n",
      "epoch 5737: train loss: 0.1379626552345855, test loss: 0.2634276645516133\n",
      "epoch 5738: train loss: 0.13795588484691887, test loss: 0.26342667192356817\n",
      "epoch 5739: train loss: 0.13794911616945832, test loss: 0.2634256799136403\n",
      "epoch 5740: train loss: 0.1379423492014482, test loss: 0.2634246885214889\n",
      "epoch 5741: train loss: 0.13793558394213337, test loss: 0.2634236977467735\n",
      "epoch 5742: train loss: 0.1379288203907591, test loss: 0.2634227075891538\n",
      "epoch 5743: train loss: 0.13792205854657114, test loss: 0.26342171804828995\n",
      "epoch 5744: train loss: 0.13791529840881578, test loss: 0.26342072912384196\n",
      "epoch 5745: train loss: 0.13790853997673966, test loss: 0.2634197408154707\n",
      "epoch 5746: train loss: 0.13790178324959002, test loss: 0.26341875312283686\n",
      "epoch 5747: train loss: 0.13789502822661448, test loss: 0.2634177660456017\n",
      "epoch 5748: train loss: 0.13788827490706113, test loss: 0.2634167795834265\n",
      "epoch 5749: train loss: 0.1378815232901786, test loss: 0.26341579373597296\n",
      "epoch 5750: train loss: 0.13787477337521595, test loss: 0.26341480850290316\n",
      "epoch 5751: train loss: 0.1378680251614226, test loss: 0.2634138238838793\n",
      "epoch 5752: train loss: 0.13786127864804865, test loss: 0.2634128398785638\n",
      "epoch 5753: train loss: 0.1378545338343445, test loss: 0.26341185648661947\n",
      "epoch 5754: train loss: 0.1378477907195611, test loss: 0.2634108737077095\n",
      "epoch 5755: train loss: 0.13784104930294977, test loss: 0.2634098915414971\n",
      "epoch 5756: train loss: 0.13783430958376236, test loss: 0.263408909987646\n",
      "epoch 5757: train loss: 0.13782757156125128, test loss: 0.26340792904582\n",
      "epoch 5758: train loss: 0.1378208352346692, test loss: 0.2634069487156833\n",
      "epoch 5759: train loss: 0.13781410060326946, test loss: 0.26340596899690033\n",
      "epoch 5760: train loss: 0.13780736766630566, test loss: 0.26340498988913585\n",
      "epoch 5761: train loss: 0.13780063642303206, test loss: 0.26340401139205477\n",
      "epoch 5762: train loss: 0.13779390687270326, test loss: 0.2634030335053224\n",
      "epoch 5763: train loss: 0.13778717901457432, test loss: 0.2634020562286043\n",
      "epoch 5764: train loss: 0.13778045284790089, test loss: 0.2634010795615662\n",
      "epoch 5765: train loss: 0.1377737283719389, test loss: 0.2634001035038742\n",
      "epoch 5766: train loss: 0.1377670055859449, test loss: 0.26339912805519466\n",
      "epoch 5767: train loss: 0.13776028448917582, test loss: 0.2633981532151942\n",
      "epoch 5768: train loss: 0.13775356508088904, test loss: 0.26339717898353987\n",
      "epoch 5769: train loss: 0.13774684736034243, test loss: 0.26339620535989855\n",
      "epoch 5770: train loss: 0.13774013132679433, test loss: 0.26339523234393786\n",
      "epoch 5771: train loss: 0.13773341697950353, test loss: 0.2633942599353254\n",
      "epoch 5772: train loss: 0.13772670431772926, test loss: 0.2633932881337293\n",
      "epoch 5773: train loss: 0.1377199933407313, test loss: 0.2633923169388177\n",
      "epoch 5774: train loss: 0.13771328404776967, test loss: 0.26339134635025907\n",
      "epoch 5775: train loss: 0.13770657643810513, test loss: 0.26339037636772233\n",
      "epoch 5776: train loss: 0.1376998705109987, test loss: 0.26338940699087643\n",
      "epoch 5777: train loss: 0.1376931662657119, test loss: 0.26338843821939073\n",
      "epoch 5778: train loss: 0.13768646370150675, test loss: 0.26338747005293484\n",
      "epoch 5779: train loss: 0.13767976281764574, test loss: 0.26338650249117856\n",
      "epoch 5780: train loss: 0.13767306361339174, test loss: 0.2633855355337922\n",
      "epoch 5781: train loss: 0.1376663660880081, test loss: 0.26338456918044595\n",
      "epoch 5782: train loss: 0.13765967024075867, test loss: 0.2633836034308105\n",
      "epoch 5783: train loss: 0.13765297607090776, test loss: 0.26338263828455694\n",
      "epoch 5784: train loss: 0.13764628357772007, test loss: 0.2633816737413563\n",
      "epoch 5785: train loss: 0.13763959276046075, test loss: 0.26338070980088013\n",
      "epoch 5786: train loss: 0.13763290361839553, test loss: 0.26337974646280027\n",
      "epoch 5787: train loss: 0.13762621615079046, test loss: 0.26337878372678847\n",
      "epoch 5788: train loss: 0.13761953035691205, test loss: 0.2633778215925172\n",
      "epoch 5789: train loss: 0.13761284623602738, test loss: 0.26337686005965893\n",
      "epoch 5790: train loss: 0.13760616378740384, test loss: 0.26337589912788645\n",
      "epoch 5791: train loss: 0.1375994830103094, test loss: 0.2633749387968728\n",
      "epoch 5792: train loss: 0.1375928039040124, test loss: 0.2633739790662914\n",
      "epoch 5793: train loss: 0.13758612646778165, test loss: 0.26337301993581574\n",
      "epoch 5794: train loss: 0.13757945070088642, test loss: 0.26337206140511976\n",
      "epoch 5795: train loss: 0.13757277660259642, test loss: 0.26337110347387754\n",
      "epoch 5796: train loss: 0.13756610417218185, test loss: 0.2633701461417635\n",
      "epoch 5797: train loss: 0.13755943340891327, test loss: 0.2633691894084522\n",
      "epoch 5798: train loss: 0.13755276431206184, test loss: 0.26336823327361875\n",
      "epoch 5799: train loss: 0.137546096880899, test loss: 0.2633672777369381\n",
      "epoch 5800: train loss: 0.13753943111469674, test loss: 0.26336632279808575\n",
      "epoch 5801: train loss: 0.13753276701272749, test loss: 0.2633653684567376\n",
      "epoch 5802: train loss: 0.13752610457426415, test loss: 0.2633644147125694\n",
      "epoch 5803: train loss: 0.13751944379857997, test loss: 0.2633634615652574\n",
      "epoch 5804: train loss: 0.13751278468494876, test loss: 0.2633625090144781\n",
      "epoch 5805: train loss: 0.1375061272326447, test loss: 0.2633615570599084\n",
      "epoch 5806: train loss: 0.1374994714409425, test loss: 0.2633606057012251\n",
      "epoch 5807: train loss: 0.1374928173091172, test loss: 0.26335965493810565\n",
      "epoch 5808: train loss: 0.1374861648364444, test loss: 0.26335870477022744\n",
      "epoch 5809: train loss: 0.13747951402220007, test loss: 0.2633577551972684\n",
      "epoch 5810: train loss: 0.1374728648656607, test loss: 0.26335680621890645\n",
      "epoch 5811: train loss: 0.13746621736610315, test loss: 0.26335585783481996\n",
      "epoch 5812: train loss: 0.13745957152280475, test loss: 0.2633549100446876\n",
      "epoch 5813: train loss: 0.13745292733504333, test loss: 0.263353962848188\n",
      "epoch 5814: train loss: 0.13744628480209703, test loss: 0.26335301624500046\n",
      "epoch 5815: train loss: 0.1374396439232446, test loss: 0.26335207023480434\n",
      "epoch 5816: train loss: 0.1374330046977651, test loss: 0.26335112481727907\n",
      "epoch 5817: train loss: 0.13742636712493816, test loss: 0.26335017999210464\n",
      "epoch 5818: train loss: 0.13741973120404372, test loss: 0.26334923575896113\n",
      "epoch 5819: train loss: 0.1374130969343622, test loss: 0.2633482921175289\n",
      "epoch 5820: train loss: 0.13740646431517456, test loss: 0.2633473490674887\n",
      "epoch 5821: train loss: 0.1373998333457621, test loss: 0.26334640660852143\n",
      "epoch 5822: train loss: 0.13739320402540658, test loss: 0.2633454647403082\n",
      "epoch 5823: train loss: 0.13738657635339024, test loss: 0.2633445234625304\n",
      "epoch 5824: train loss: 0.13737995032899572, test loss: 0.26334358277486963\n",
      "epoch 5825: train loss: 0.13737332595150606, test loss: 0.2633426426770081\n",
      "epoch 5826: train loss: 0.13736670322020486, test loss: 0.2633417031686277\n",
      "epoch 5827: train loss: 0.13736008213437612, test loss: 0.263340764249411\n",
      "epoch 5828: train loss: 0.13735346269330417, test loss: 0.2633398259190407\n",
      "epoch 5829: train loss: 0.13734684489627394, test loss: 0.26333888817719975\n",
      "epoch 5830: train loss: 0.13734022874257068, test loss: 0.2633379510235714\n",
      "epoch 5831: train loss: 0.1373336142314801, test loss: 0.26333701445783897\n",
      "epoch 5832: train loss: 0.13732700136228845, test loss: 0.2633360784796863\n",
      "epoch 5833: train loss: 0.1373203901342823, test loss: 0.26333514308879735\n",
      "epoch 5834: train loss: 0.13731378054674864, test loss: 0.2633342082848563\n",
      "epoch 5835: train loss: 0.13730717259897507, test loss: 0.26333327406754775\n",
      "epoch 5836: train loss: 0.1373005662902494, test loss: 0.26333234043655623\n",
      "epoch 5837: train loss: 0.13729396161986002, test loss: 0.2633314073915669\n",
      "epoch 5838: train loss: 0.13728735858709576, test loss: 0.2633304749322649\n",
      "epoch 5839: train loss: 0.13728075719124583, test loss: 0.26332954305833584\n",
      "epoch 5840: train loss: 0.13727415743159985, test loss: 0.2633286117694653\n",
      "epoch 5841: train loss: 0.13726755930744802, test loss: 0.26332768106533944\n",
      "epoch 5842: train loss: 0.13726096281808078, test loss: 0.26332675094564445\n",
      "epoch 5843: train loss: 0.13725436796278914, test loss: 0.2633258214100668\n",
      "epoch 5844: train loss: 0.13724777474086453, test loss: 0.2633248924582932\n",
      "epoch 5845: train loss: 0.1372411831515987, test loss: 0.2633239640900108\n",
      "epoch 5846: train loss: 0.13723459319428402, test loss: 0.2633230363049068\n",
      "epoch 5847: train loss: 0.13722800486821318, test loss: 0.2633221091026686\n",
      "epoch 5848: train loss: 0.13722141817267927, test loss: 0.26332118248298414\n",
      "epoch 5849: train loss: 0.1372148331069759, test loss: 0.2633202564455413\n",
      "epoch 5850: train loss: 0.13720824967039705, test loss: 0.2633193309900283\n",
      "epoch 5851: train loss: 0.13720166786223717, test loss: 0.2633184061161338\n",
      "epoch 5852: train loss: 0.13719508768179114, test loss: 0.2633174818235465\n",
      "epoch 5853: train loss: 0.1371885091283542, test loss: 0.2633165581119553\n",
      "epoch 5854: train loss: 0.13718193220122216, test loss: 0.2633156349810496\n",
      "epoch 5855: train loss: 0.1371753568996911, test loss: 0.26331471243051885\n",
      "epoch 5856: train loss: 0.13716878322305767, test loss: 0.26331379046005277\n",
      "epoch 5857: train loss: 0.1371622111706189, test loss: 0.2633128690693414\n",
      "epoch 5858: train loss: 0.13715564074167216, test loss: 0.263311948258075\n",
      "epoch 5859: train loss: 0.1371490719355154, test loss: 0.2633110280259441\n",
      "epoch 5860: train loss: 0.1371425047514469, test loss: 0.2633101083726393\n",
      "epoch 5861: train loss: 0.13713593918876543, test loss: 0.2633091892978518\n",
      "epoch 5862: train loss: 0.13712937524677007, test loss: 0.2633082708012727\n",
      "epoch 5863: train loss: 0.1371228129247605, test loss: 0.2633073528825935\n",
      "epoch 5864: train loss: 0.13711625222203672, test loss: 0.26330643554150596\n",
      "epoch 5865: train loss: 0.13710969313789914, test loss: 0.2633055187777021\n",
      "epoch 5866: train loss: 0.13710313567164864, test loss: 0.263304602590874\n",
      "epoch 5867: train loss: 0.13709657982258658, test loss: 0.2633036869807144\n",
      "epoch 5868: train loss: 0.1370900255900146, test loss: 0.2633027719469157\n",
      "epoch 5869: train loss: 0.13708347297323492, test loss: 0.2633018574891712\n",
      "epoch 5870: train loss: 0.13707692197155008, test loss: 0.26330094360717377\n",
      "epoch 5871: train loss: 0.1370703725842631, test loss: 0.26330003030061705\n",
      "epoch 5872: train loss: 0.13706382481067741, test loss: 0.2632991175691948\n",
      "epoch 5873: train loss: 0.13705727865009687, test loss: 0.2632982054126008\n",
      "epoch 5874: train loss: 0.13705073410182572, test loss: 0.2632972938305293\n",
      "epoch 5875: train loss: 0.1370441911651687, test loss: 0.26329638282267476\n",
      "epoch 5876: train loss: 0.13703764983943093, test loss: 0.2632954723887319\n",
      "epoch 5877: train loss: 0.13703111012391792, test loss: 0.26329456252839556\n",
      "epoch 5878: train loss: 0.13702457201793566, test loss: 0.26329365324136084\n",
      "epoch 5879: train loss: 0.1370180355207906, test loss: 0.2632927445273233\n",
      "epoch 5880: train loss: 0.13701150063178946, test loss: 0.26329183638597853\n",
      "epoch 5881: train loss: 0.13700496735023956, test loss: 0.2632909288170224\n",
      "epoch 5882: train loss: 0.1369984356754485, test loss: 0.26329002182015093\n",
      "epoch 5883: train loss: 0.1369919056067244, test loss: 0.2632891153950607\n",
      "epoch 5884: train loss: 0.13698537714337575, test loss: 0.2632882095414482\n",
      "epoch 5885: train loss: 0.13697885028471146, test loss: 0.2632873042590104\n",
      "epoch 5886: train loss: 0.13697232503004086, test loss: 0.2632863995474443\n",
      "epoch 5887: train loss: 0.13696580137867373, test loss: 0.26328549540644725\n",
      "epoch 5888: train loss: 0.13695927932992027, test loss: 0.26328459183571684\n",
      "epoch 5889: train loss: 0.13695275888309105, test loss: 0.2632836888349509\n",
      "epoch 5890: train loss: 0.1369462400374971, test loss: 0.2632827864038475\n",
      "epoch 5891: train loss: 0.1369397227924499, test loss: 0.2632818845421049\n",
      "epoch 5892: train loss: 0.13693320714726123, test loss: 0.2632809832494217\n",
      "epoch 5893: train loss: 0.13692669310124342, test loss: 0.2632800825254967\n",
      "epoch 5894: train loss: 0.13692018065370917, test loss: 0.26327918237002873\n",
      "epoch 5895: train loss: 0.13691366980397154, test loss: 0.2632782827827173\n",
      "epoch 5896: train loss: 0.13690716055134408, test loss: 0.26327738376326176\n",
      "epoch 5897: train loss: 0.13690065289514075, test loss: 0.2632764853113619\n",
      "epoch 5898: train loss: 0.1368941468346759, test loss: 0.2632755874267177\n",
      "epoch 5899: train loss: 0.1368876423692643, test loss: 0.2632746901090293\n",
      "epoch 5900: train loss: 0.13688113949822114, test loss: 0.26327379335799733\n",
      "epoch 5901: train loss: 0.13687463822086204, test loss: 0.26327289717332225\n",
      "epoch 5902: train loss: 0.136868138536503, test loss: 0.2632720015547052\n",
      "epoch 5903: train loss: 0.13686164044446053, test loss: 0.26327110650184715\n",
      "epoch 5904: train loss: 0.13685514394405138, test loss: 0.2632702120144497\n",
      "epoch 5905: train loss: 0.13684864903459287, test loss: 0.2632693180922143\n",
      "epoch 5906: train loss: 0.13684215571540267, test loss: 0.26326842473484297\n",
      "epoch 5907: train loss: 0.1368356639857989, test loss: 0.2632675319420378\n",
      "epoch 5908: train loss: 0.13682917384510002, test loss: 0.263266639713501\n",
      "epoch 5909: train loss: 0.13682268529262498, test loss: 0.26326574804893527\n",
      "epoch 5910: train loss: 0.13681619832769312, test loss: 0.26326485694804347\n",
      "epoch 5911: train loss: 0.13680971294962416, test loss: 0.26326396641052857\n",
      "epoch 5912: train loss: 0.13680322915773824, test loss: 0.2632630764360939\n",
      "epoch 5913: train loss: 0.13679674695135596, test loss: 0.263262187024443\n",
      "epoch 5914: train loss: 0.13679026632979832, test loss: 0.26326129817527955\n",
      "epoch 5915: train loss: 0.13678378729238666, test loss: 0.26326040988830757\n",
      "epoch 5916: train loss: 0.13677730983844283, test loss: 0.2632595221632314\n",
      "epoch 5917: train loss: 0.13677083396728895, test loss: 0.2632586349997553\n",
      "epoch 5918: train loss: 0.13676435967824777, test loss: 0.2632577483975842\n",
      "epoch 5919: train loss: 0.1367578869706422, test loss: 0.2632568623564228\n",
      "epoch 5920: train loss: 0.13675141584379577, test loss: 0.2632559768759765\n",
      "epoch 5921: train loss: 0.1367449462970323, test loss: 0.2632550919559505\n",
      "epoch 5922: train loss: 0.13673847832967603, test loss: 0.2632542075960505\n",
      "epoch 5923: train loss: 0.1367320119410516, test loss: 0.2632533237959823\n",
      "epoch 5924: train loss: 0.13672554713048415, test loss: 0.2632524405554521\n",
      "epoch 5925: train loss: 0.13671908389729914, test loss: 0.2632515578741662\n",
      "epoch 5926: train loss: 0.13671262224082245, test loss: 0.26325067575183114\n",
      "epoch 5927: train loss: 0.13670616216038037, test loss: 0.2632497941881537\n",
      "epoch 5928: train loss: 0.13669970365529965, test loss: 0.2632489131828408\n",
      "epoch 5929: train loss: 0.13669324672490732, test loss: 0.26324803273559993\n",
      "epoch 5930: train loss: 0.13668679136853093, test loss: 0.26324715284613837\n",
      "epoch 5931: train loss: 0.13668033758549844, test loss: 0.26324627351416396\n",
      "epoch 5932: train loss: 0.13667388537513814, test loss: 0.2632453947393845\n",
      "epoch 5933: train loss: 0.13666743473677875, test loss: 0.2632445165215083\n",
      "epoch 5934: train loss: 0.1366609856697495, test loss: 0.2632436388602437\n",
      "epoch 5935: train loss: 0.13665453817337977, test loss: 0.26324276175529937\n",
      "epoch 5936: train loss: 0.13664809224699964, test loss: 0.26324188520638414\n",
      "epoch 5937: train loss: 0.1366416478899394, test loss: 0.26324100921320714\n",
      "epoch 5938: train loss: 0.13663520510152985, test loss: 0.26324013377547767\n",
      "epoch 5939: train loss: 0.13662876388110212, test loss: 0.2632392588929052\n",
      "epoch 5940: train loss: 0.13662232422798776, test loss: 0.26323838456519966\n",
      "epoch 5941: train loss: 0.13661588614151873, test loss: 0.263237510792071\n",
      "epoch 5942: train loss: 0.13660944962102747, test loss: 0.26323663757322935\n",
      "epoch 5943: train loss: 0.13660301466584662, test loss: 0.26323576490838535\n",
      "epoch 5944: train loss: 0.13659658127530946, test loss: 0.2632348927972496\n",
      "epoch 5945: train loss: 0.13659014944874953, test loss: 0.2632340212395331\n",
      "epoch 5946: train loss: 0.1365837191855008, test loss: 0.26323315023494687\n",
      "epoch 5947: train loss: 0.1365772904848976, test loss: 0.26323227978320235\n",
      "epoch 5948: train loss: 0.1365708633462748, test loss: 0.26323140988401117\n",
      "epoch 5949: train loss: 0.13656443776896748, test loss: 0.26323054053708506\n",
      "epoch 5950: train loss: 0.13655801375231127, test loss: 0.2632296717421362\n",
      "epoch 5951: train loss: 0.13655159129564215, test loss: 0.2632288034988768\n",
      "epoch 5952: train loss: 0.13654517039829644, test loss: 0.2632279358070193\n",
      "epoch 5953: train loss: 0.13653875105961097, test loss: 0.2632270686662766\n",
      "epoch 5954: train loss: 0.1365323332789229, test loss: 0.2632262020763616\n",
      "epoch 5955: train loss: 0.13652591705556977, test loss: 0.2632253360369874\n",
      "epoch 5956: train loss: 0.13651950238888955, test loss: 0.2632244705478675\n",
      "epoch 5957: train loss: 0.13651308927822062, test loss: 0.26322360560871544\n",
      "epoch 5958: train loss: 0.13650667772290176, test loss: 0.26322274121924516\n",
      "epoch 5959: train loss: 0.13650026772227214, test loss: 0.26322187737917074\n",
      "epoch 5960: train loss: 0.13649385927567131, test loss: 0.26322101408820653\n",
      "epoch 5961: train loss: 0.1364874523824392, test loss: 0.26322015134606697\n",
      "epoch 5962: train loss: 0.13648104704191613, test loss: 0.26321928915246673\n",
      "epoch 5963: train loss: 0.13647464325344294, test loss: 0.263218427507121\n",
      "epoch 5964: train loss: 0.1364682410163607, test loss: 0.26321756640974486\n",
      "epoch 5965: train loss: 0.13646184033001096, test loss: 0.2632167058600538\n",
      "epoch 5966: train loss: 0.13645544119373568, test loss: 0.26321584585776336\n",
      "epoch 5967: train loss: 0.13644904360687718, test loss: 0.2632149864025895\n",
      "epoch 5968: train loss: 0.13644264756877816, test loss: 0.2632141274942484\n",
      "epoch 5969: train loss: 0.13643625307878174, test loss: 0.26321326913245613\n",
      "epoch 5970: train loss: 0.13642986013623146, test loss: 0.2632124113169294\n",
      "epoch 5971: train loss: 0.1364234687404712, test loss: 0.263211554047385\n",
      "epoch 5972: train loss: 0.13641707889084528, test loss: 0.26321069732353974\n",
      "epoch 5973: train loss: 0.13641069058669836, test loss: 0.26320984114511103\n",
      "epoch 5974: train loss: 0.13640430382737553, test loss: 0.26320898551181615\n",
      "epoch 5975: train loss: 0.1363979186122223, test loss: 0.26320813042337277\n",
      "epoch 5976: train loss: 0.13639153494058448, test loss: 0.26320727587949877\n",
      "epoch 5977: train loss: 0.13638515281180838, test loss: 0.26320642187991233\n",
      "epoch 5978: train loss: 0.13637877222524064, test loss: 0.26320556842433157\n",
      "epoch 5979: train loss: 0.13637239318022826, test loss: 0.26320471551247515\n",
      "epoch 5980: train loss: 0.13636601567611872, test loss: 0.2632038631440617\n",
      "epoch 5981: train loss: 0.13635963971225984, test loss: 0.26320301131881035\n",
      "epoch 5982: train loss: 0.13635326528799982, test loss: 0.26320216003644015\n",
      "epoch 5983: train loss: 0.13634689240268727, test loss: 0.26320130929667057\n",
      "epoch 5984: train loss: 0.13634052105567118, test loss: 0.26320045909922124\n",
      "epoch 5985: train loss: 0.1363341512463009, test loss: 0.26319960944381193\n",
      "epoch 5986: train loss: 0.1363277829739263, test loss: 0.26319876033016276\n",
      "epoch 5987: train loss: 0.13632141623789745, test loss: 0.263197911757994\n",
      "epoch 5988: train loss: 0.13631505103756492, test loss: 0.26319706372702617\n",
      "epoch 5989: train loss: 0.13630868737227966, test loss: 0.2631962162369799\n",
      "epoch 5990: train loss: 0.136302325241393, test loss: 0.26319536928757625\n",
      "epoch 5991: train loss: 0.1362959646442566, test loss: 0.26319452287853623\n",
      "epoch 5992: train loss: 0.13628960558022268, test loss: 0.2631936770095813\n",
      "epoch 5993: train loss: 0.13628324804864358, test loss: 0.26319283168043306\n",
      "epoch 5994: train loss: 0.13627689204887228, test loss: 0.26319198689081325\n",
      "epoch 5995: train loss: 0.136270537580262, test loss: 0.26319114264044396\n",
      "epoch 5996: train loss: 0.13626418464216639, test loss: 0.26319029892904733\n",
      "epoch 5997: train loss: 0.1362578332339395, test loss: 0.2631894557563458\n",
      "epoch 5998: train loss: 0.13625148335493573, test loss: 0.26318861312206215\n",
      "epoch 5999: train loss: 0.13624513500450985, test loss: 0.2631877710259192\n",
      "epoch 6000: train loss: 0.13623878818201712, test loss: 0.2631869294676401\n",
      "epoch 6001: train loss: 0.13623244288681308, test loss: 0.2631860884469481\n",
      "epoch 6002: train loss: 0.13622609911825367, test loss: 0.2631852479635667\n",
      "epoch 6003: train loss: 0.13621975687569526, test loss: 0.2631844080172197\n",
      "epoch 6004: train loss: 0.13621341615849455, test loss: 0.2631835686076311\n",
      "epoch 6005: train loss: 0.1362070769660087, test loss: 0.2631827297345249\n",
      "epoch 6006: train loss: 0.1362007392975951, test loss: 0.26318189139762566\n",
      "epoch 6007: train loss: 0.13619440315261172, test loss: 0.26318105359665794\n",
      "epoch 6008: train loss: 0.1361880685304168, test loss: 0.2631802163313466\n",
      "epoch 6009: train loss: 0.13618173543036896, test loss: 0.26317937960141646\n",
      "epoch 6010: train loss: 0.13617540385182722, test loss: 0.2631785434065931\n",
      "epoch 6011: train loss: 0.13616907379415097, test loss: 0.2631777077466017\n",
      "epoch 6012: train loss: 0.13616274525670002, test loss: 0.263176872621168\n",
      "epoch 6013: train loss: 0.13615641823883456, test loss: 0.26317603803001793\n",
      "epoch 6014: train loss: 0.13615009273991507, test loss: 0.2631752039728775\n",
      "epoch 6015: train loss: 0.13614376875930256, test loss: 0.26317437044947306\n",
      "epoch 6016: train loss: 0.13613744629635827, test loss: 0.26317353745953115\n",
      "epoch 6017: train loss: 0.13613112535044392, test loss: 0.26317270500277856\n",
      "epoch 6018: train loss: 0.13612480592092155, test loss: 0.2631718730789421\n",
      "epoch 6019: train loss: 0.13611848800715365, test loss: 0.26317104168774896\n",
      "epoch 6020: train loss: 0.13611217160850303, test loss: 0.26317021082892655\n",
      "epoch 6021: train loss: 0.13610585672433287, test loss: 0.2631693805022024\n",
      "epoch 6022: train loss: 0.13609954335400676, test loss: 0.2631685507073044\n",
      "epoch 6023: train loss: 0.13609323149688868, test loss: 0.2631677214439604\n",
      "epoch 6024: train loss: 0.13608692115234297, test loss: 0.26316689271189864\n",
      "epoch 6025: train loss: 0.13608061231973434, test loss: 0.2631660645108477\n",
      "epoch 6026: train loss: 0.13607430499842788, test loss: 0.26316523684053594\n",
      "epoch 6027: train loss: 0.13606799918778908, test loss: 0.26316440970069244\n",
      "epoch 6028: train loss: 0.13606169488718375, test loss: 0.2631635830910461\n",
      "epoch 6029: train loss: 0.13605539209597817, test loss: 0.26316275701132624\n",
      "epoch 6030: train loss: 0.1360490908135389, test loss: 0.2631619314612623\n",
      "epoch 6031: train loss: 0.13604279103923292, test loss: 0.26316110644058405\n",
      "epoch 6032: train loss: 0.1360364927724276, test loss: 0.2631602819490213\n",
      "epoch 6033: train loss: 0.13603019601249064, test loss: 0.263159457986304\n",
      "epoch 6034: train loss: 0.1360239007587902, test loss: 0.26315863455216276\n",
      "epoch 6035: train loss: 0.13601760701069474, test loss: 0.2631578116463278\n",
      "epoch 6036: train loss: 0.13601131476757305, test loss: 0.26315698926853004\n",
      "epoch 6037: train loss: 0.1360050240287944, test loss: 0.26315616741850034\n",
      "epoch 6038: train loss: 0.13599873479372843, test loss: 0.26315534609596974\n",
      "epoch 6039: train loss: 0.13599244706174507, test loss: 0.2631545253006697\n",
      "epoch 6040: train loss: 0.13598616083221468, test loss: 0.2631537050323318\n",
      "epoch 6041: train loss: 0.135979876104508, test loss: 0.26315288529068764\n",
      "epoch 6042: train loss: 0.1359735928779961, test loss: 0.2631520660754694\n",
      "epoch 6043: train loss: 0.1359673111520504, test loss: 0.26315124738640905\n",
      "epoch 6044: train loss: 0.13596103092604286, test loss: 0.26315042922323906\n",
      "epoch 6045: train loss: 0.1359547521993456, test loss: 0.26314961158569194\n",
      "epoch 6046: train loss: 0.13594847497133122, test loss: 0.26314879447350065\n",
      "epoch 6047: train loss: 0.1359421992413727, test loss: 0.263147977886398\n",
      "epoch 6048: train loss: 0.13593592500884333, test loss: 0.2631471618241173\n",
      "epoch 6049: train loss: 0.13592965227311685, test loss: 0.26314634628639183\n",
      "epoch 6050: train loss: 0.13592338103356727, test loss: 0.2631455312729553\n",
      "epoch 6051: train loss: 0.13591711128956907, test loss: 0.2631447167835416\n",
      "epoch 6052: train loss: 0.13591084304049703, test loss: 0.2631439028178846\n",
      "epoch 6053: train loss: 0.13590457628572636, test loss: 0.26314308937571856\n",
      "epoch 6054: train loss: 0.13589831102463257, test loss: 0.263142276456778\n",
      "epoch 6055: train loss: 0.1358920472565916, test loss: 0.2631414640607975\n",
      "epoch 6056: train loss: 0.13588578498097972, test loss: 0.2631406521875118\n",
      "epoch 6057: train loss: 0.1358795241971736, test loss: 0.26313984083665615\n",
      "epoch 6058: train loss: 0.1358732649045503, test loss: 0.26313903000796557\n",
      "epoch 6059: train loss: 0.13586700710248711, test loss: 0.2631382197011757\n",
      "epoch 6060: train loss: 0.13586075079036186, test loss: 0.263137409916022\n",
      "epoch 6061: train loss: 0.13585449596755264, test loss: 0.26313660065224054\n",
      "epoch 6062: train loss: 0.135848242633438, test loss: 0.2631357919095672\n",
      "epoch 6063: train loss: 0.1358419907873967, test loss: 0.26313498368773836\n",
      "epoch 6064: train loss: 0.13583574042880808, test loss: 0.2631341759864904\n",
      "epoch 6065: train loss: 0.13582949155705165, test loss: 0.26313336880556\n",
      "epoch 6066: train loss: 0.1358232441715074, test loss: 0.2631325621446841\n",
      "epoch 6067: train loss: 0.13581699827155566, test loss: 0.2631317560035997\n",
      "epoch 6068: train loss: 0.1358107538565771, test loss: 0.2631309503820441\n",
      "epoch 6069: train loss: 0.1358045109259528, test loss: 0.26313014527975476\n",
      "epoch 6070: train loss: 0.13579826947906415, test loss: 0.26312934069646937\n",
      "epoch 6071: train loss: 0.1357920295152929, test loss: 0.26312853663192576\n",
      "epoch 6072: train loss: 0.13578579103402133, test loss: 0.263127733085862\n",
      "epoch 6073: train loss: 0.13577955403463182, test loss: 0.26312693005801646\n",
      "epoch 6074: train loss: 0.13577331851650734, test loss: 0.2631261275481276\n",
      "epoch 6075: train loss: 0.13576708447903107, test loss: 0.263125325555934\n",
      "epoch 6076: train loss: 0.13576085192158666, test loss: 0.26312452408117454\n",
      "epoch 6077: train loss: 0.13575462084355802, test loss: 0.26312372312358845\n",
      "epoch 6078: train loss: 0.13574839124432952, test loss: 0.2631229226829149\n",
      "epoch 6079: train loss: 0.13574216312328588, test loss: 0.2631221227588933\n",
      "epoch 6080: train loss: 0.13573593647981208, test loss: 0.2631213233512634\n",
      "epoch 6081: train loss: 0.13572971131329362, test loss: 0.26312052445976514\n",
      "epoch 6082: train loss: 0.13572348762311623, test loss: 0.2631197260841385\n",
      "epoch 6083: train loss: 0.13571726540866608, test loss: 0.2631189282241238\n",
      "epoch 6084: train loss: 0.13571104466932962, test loss: 0.2631181308794615\n",
      "epoch 6085: train loss: 0.13570482540449375, test loss: 0.26311733404989224\n",
      "epoch 6086: train loss: 0.13569860761354569, test loss: 0.2631165377351569\n",
      "epoch 6087: train loss: 0.13569239129587302, test loss: 0.26311574193499654\n",
      "epoch 6088: train loss: 0.1356861764508637, test loss: 0.26311494664915247\n",
      "epoch 6089: train loss: 0.135679963077906, test loss: 0.26311415187736614\n",
      "epoch 6090: train loss: 0.13567375117638864, test loss: 0.2631133576193792\n",
      "epoch 6091: train loss: 0.13566754074570056, test loss: 0.26311256387493337\n",
      "epoch 6092: train loss: 0.1356613317852312, test loss: 0.263111770643771\n",
      "epoch 6093: train loss: 0.13565512429437032, test loss: 0.26311097792563404\n",
      "epoch 6094: train loss: 0.13564891827250797, test loss: 0.26311018572026507\n",
      "epoch 6095: train loss: 0.13564271371903464, test loss: 0.2631093940274068\n",
      "epoch 6096: train loss: 0.1356365106333411, test loss: 0.26310860284680193\n",
      "epoch 6097: train loss: 0.13563030901481854, test loss: 0.26310781217819357\n",
      "epoch 6098: train loss: 0.13562410886285853, test loss: 0.2631070220213249\n",
      "epoch 6099: train loss: 0.1356179101768529, test loss: 0.2631062323759394\n",
      "epoch 6100: train loss: 0.13561171295619395, test loss: 0.26310544324178076\n",
      "epoch 6101: train loss: 0.13560551720027422, test loss: 0.2631046546185926\n",
      "epoch 6102: train loss: 0.13559932290848675, test loss: 0.26310386650611917\n",
      "epoch 6103: train loss: 0.13559313008022478, test loss: 0.2631030789041044\n",
      "epoch 6104: train loss: 0.135586938714882, test loss: 0.26310229181229294\n",
      "epoch 6105: train loss: 0.13558074881185242, test loss: 0.26310150523042924\n",
      "epoch 6106: train loss: 0.13557456037053045, test loss: 0.26310071915825817\n",
      "epoch 6107: train loss: 0.13556837339031083, test loss: 0.26309993359552464\n",
      "epoch 6108: train loss: 0.1355621878705886, test loss: 0.26309914854197397\n",
      "epoch 6109: train loss: 0.13555600381075922, test loss: 0.2630983639973513\n",
      "epoch 6110: train loss: 0.13554982121021855, test loss: 0.26309757996140243\n",
      "epoch 6111: train loss: 0.13554364006836264, test loss: 0.26309679643387296\n",
      "epoch 6112: train loss: 0.13553746038458808, test loss: 0.26309601341450894\n",
      "epoch 6113: train loss: 0.13553128215829172, test loss: 0.2630952309030565\n",
      "epoch 6114: train loss: 0.1355251053888707, test loss: 0.2630944488992618\n",
      "epoch 6115: train loss: 0.13551893007572266, test loss: 0.26309366740287166\n",
      "epoch 6116: train loss: 0.1355127562182455, test loss: 0.26309288641363265\n",
      "epoch 6117: train loss: 0.13550658381583747, test loss: 0.26309210593129173\n",
      "epoch 6118: train loss: 0.13550041286789719, test loss: 0.26309132595559587\n",
      "epoch 6119: train loss: 0.13549424337382365, test loss: 0.26309054648629265\n",
      "epoch 6120: train loss: 0.1354880753330162, test loss: 0.2630897675231293\n",
      "epoch 6121: train loss: 0.13548190874487448, test loss: 0.26308898906585354\n",
      "epoch 6122: train loss: 0.13547574360879852, test loss: 0.26308821111421343\n",
      "epoch 6123: train loss: 0.1354695799241887, test loss: 0.26308743366795684\n",
      "epoch 6124: train loss: 0.13546341769044573, test loss: 0.26308665672683207\n",
      "epoch 6125: train loss: 0.13545725690697075, test loss: 0.2630858802905876\n",
      "epoch 6126: train loss: 0.13545109757316517, test loss: 0.26308510435897203\n",
      "epoch 6127: train loss: 0.13544493968843072, test loss: 0.2630843289317342\n",
      "epoch 6128: train loss: 0.1354387832521696, test loss: 0.26308355400862327\n",
      "epoch 6129: train loss: 0.13543262826378422, test loss: 0.2630827795893882\n",
      "epoch 6130: train loss: 0.13542647472267738, test loss: 0.26308200567377854\n",
      "epoch 6131: train loss: 0.13542032262825238, test loss: 0.26308123226154384\n",
      "epoch 6132: train loss: 0.13541417197991268, test loss: 0.2630804593524339\n",
      "epoch 6133: train loss: 0.1354080227770621, test loss: 0.26307968694619865\n",
      "epoch 6134: train loss: 0.13540187501910492, test loss: 0.2630789150425883\n",
      "epoch 6135: train loss: 0.13539572870544567, test loss: 0.263078143641353\n",
      "epoch 6136: train loss: 0.1353895838354893, test loss: 0.26307737274224363\n",
      "epoch 6137: train loss: 0.13538344040864106, test loss: 0.26307660234501073\n",
      "epoch 6138: train loss: 0.13537729842430657, test loss: 0.2630758324494052\n",
      "epoch 6139: train loss: 0.13537115788189175, test loss: 0.26307506305517797\n",
      "epoch 6140: train loss: 0.13536501878080293, test loss: 0.26307429416208067\n",
      "epoch 6141: train loss: 0.13535888112044675, test loss: 0.2630735257698646\n",
      "epoch 6142: train loss: 0.13535274490023017, test loss: 0.2630727578782813\n",
      "epoch 6143: train loss: 0.13534661011956056, test loss: 0.26307199048708285\n",
      "epoch 6144: train loss: 0.13534047677784564, test loss: 0.26307122359602114\n",
      "epoch 6145: train loss: 0.1353343448744934, test loss: 0.26307045720484845\n",
      "epoch 6146: train loss: 0.13532821440891218, test loss: 0.26306969131331714\n",
      "epoch 6147: train loss: 0.13532208538051077, test loss: 0.2630689259211799\n",
      "epoch 6148: train loss: 0.1353159577886982, test loss: 0.2630681610281895\n",
      "epoch 6149: train loss: 0.13530983163288388, test loss: 0.2630673966340987\n",
      "epoch 6150: train loss: 0.13530370691247753, test loss: 0.263066632738661\n",
      "epoch 6151: train loss: 0.13529758362688926, test loss: 0.2630658693416295\n",
      "epoch 6152: train loss: 0.13529146177552953, test loss: 0.2630651064427578\n",
      "epoch 6153: train loss: 0.13528534135780915, test loss: 0.2630643440417996\n",
      "epoch 6154: train loss: 0.13527922237313914, test loss: 0.26306358213850883\n",
      "epoch 6155: train loss: 0.13527310482093105, test loss: 0.2630628207326396\n",
      "epoch 6156: train loss: 0.13526698870059664, test loss: 0.2630620598239461\n",
      "epoch 6157: train loss: 0.1352608740115481, test loss: 0.26306129941218276\n",
      "epoch 6158: train loss: 0.1352547607531979, test loss: 0.26306053949710434\n",
      "epoch 6159: train loss: 0.1352486489249589, test loss: 0.26305978007846553\n",
      "epoch 6160: train loss: 0.13524253852624418, test loss: 0.26305902115602153\n",
      "epoch 6161: train loss: 0.13523642955646736, test loss: 0.2630582627295273\n",
      "epoch 6162: train loss: 0.13523032201504226, test loss: 0.26305750479873846\n",
      "epoch 6163: train loss: 0.1352242159013831, test loss: 0.26305674736341034\n",
      "epoch 6164: train loss: 0.13521811121490437, test loss: 0.2630559904232988\n",
      "epoch 6165: train loss: 0.13521200795502092, test loss: 0.26305523397815983\n",
      "epoch 6166: train loss: 0.13520590612114808, test loss: 0.2630544780277494\n",
      "epoch 6167: train loss: 0.13519980571270132, test loss: 0.2630537225718239\n",
      "epoch 6168: train loss: 0.13519370672909656, test loss: 0.2630529676101398\n",
      "epoch 6169: train loss: 0.13518760916974998, test loss: 0.26305221314245375\n",
      "epoch 6170: train loss: 0.13518151303407824, test loss: 0.2630514591685225\n",
      "epoch 6171: train loss: 0.13517541832149818, test loss: 0.26305070568810324\n",
      "epoch 6172: train loss: 0.1351693250314271, test loss: 0.26304995270095305\n",
      "epoch 6173: train loss: 0.13516323316328255, test loss: 0.2630492002068294\n",
      "epoch 6174: train loss: 0.13515714271648246, test loss: 0.26304844820548984\n",
      "epoch 6175: train loss: 0.1351510536904451, test loss: 0.26304769669669203\n",
      "epoch 6176: train loss: 0.13514496608458906, test loss: 0.26304694568019416\n",
      "epoch 6177: train loss: 0.1351388798983333, test loss: 0.26304619515575406\n",
      "epoch 6178: train loss: 0.13513279513109705, test loss: 0.26304544512313016\n",
      "epoch 6179: train loss: 0.13512671178229996, test loss: 0.26304469558208093\n",
      "epoch 6180: train loss: 0.13512062985136195, test loss: 0.263043946532365\n",
      "epoch 6181: train loss: 0.13511454933770328, test loss: 0.26304319797374126\n",
      "epoch 6182: train loss: 0.13510847024074463, test loss: 0.2630424499059687\n",
      "epoch 6183: train loss: 0.1351023925599069, test loss: 0.2630417023288065\n",
      "epoch 6184: train loss: 0.13509631629461138, test loss: 0.2630409552420141\n",
      "epoch 6185: train loss: 0.13509024144427972, test loss: 0.2630402086453511\n",
      "epoch 6186: train loss: 0.13508416800833384, test loss: 0.263039462538577\n",
      "epoch 6187: train loss: 0.13507809598619608, test loss: 0.26303871692145203\n",
      "epoch 6188: train loss: 0.13507202537728902, test loss: 0.2630379717937361\n",
      "epoch 6189: train loss: 0.13506595618103562, test loss: 0.26303722715518957\n",
      "epoch 6190: train loss: 0.1350598883968592, test loss: 0.26303648300557286\n",
      "epoch 6191: train loss: 0.1350538220241834, test loss: 0.2630357393446466\n",
      "epoch 6192: train loss: 0.13504775706243208, test loss: 0.26303499617217163\n",
      "epoch 6193: train loss: 0.13504169351102968, test loss: 0.26303425348790893\n",
      "epoch 6194: train loss: 0.13503563136940072, test loss: 0.2630335112916197\n",
      "epoch 6195: train loss: 0.1350295706369702, test loss: 0.2630327695830653\n",
      "epoch 6196: train loss: 0.13502351131316337, test loss: 0.26303202836200723\n",
      "epoch 6197: train loss: 0.13501745339740587, test loss: 0.2630312876282072\n",
      "epoch 6198: train loss: 0.13501139688912367, test loss: 0.2630305473814271\n",
      "epoch 6199: train loss: 0.13500534178774307, test loss: 0.263029807621429\n",
      "epoch 6200: train loss: 0.1349992880926906, test loss: 0.2630290683479752\n",
      "epoch 6201: train loss: 0.1349932358033933, test loss: 0.26302832956082794\n",
      "epoch 6202: train loss: 0.1349871849192784, test loss: 0.26302759125975\n",
      "epoch 6203: train loss: 0.1349811354397736, test loss: 0.26302685344450416\n",
      "epoch 6204: train loss: 0.1349750873643067, test loss: 0.26302611611485316\n",
      "epoch 6205: train loss: 0.13496904069230603, test loss: 0.2630253792705603\n",
      "epoch 6206: train loss: 0.13496299542320017, test loss: 0.2630246429113889\n",
      "epoch 6207: train loss: 0.13495695155641807, test loss: 0.2630239070371023\n",
      "epoch 6208: train loss: 0.13495090909138902, test loss: 0.26302317164746425\n",
      "epoch 6209: train loss: 0.13494486802754252, test loss: 0.2630224367422386\n",
      "epoch 6210: train loss: 0.13493882836430854, test loss: 0.26302170232118927\n",
      "epoch 6211: train loss: 0.1349327901011173, test loss: 0.2630209683840805\n",
      "epoch 6212: train loss: 0.13492675323739942, test loss: 0.2630202349306767\n",
      "epoch 6213: train loss: 0.13492071777258577, test loss: 0.2630195019607423\n",
      "epoch 6214: train loss: 0.1349146837061075, test loss: 0.263018769474042\n",
      "epoch 6215: train loss: 0.13490865103739633, test loss: 0.2630180374703408\n",
      "epoch 6216: train loss: 0.134902619765884, test loss: 0.26301730594940365\n",
      "epoch 6217: train loss: 0.13489658989100273, test loss: 0.2630165749109959\n",
      "epoch 6218: train loss: 0.13489056141218514, test loss: 0.26301584435488273\n",
      "epoch 6219: train loss: 0.13488453432886405, test loss: 0.26301511428083\n",
      "epoch 6220: train loss: 0.1348785086404726, test loss: 0.2630143846886033\n",
      "epoch 6221: train loss: 0.13487248434644436, test loss: 0.26301365557796846\n",
      "epoch 6222: train loss: 0.13486646144621317, test loss: 0.2630129269486918\n",
      "epoch 6223: train loss: 0.13486043993921318, test loss: 0.2630121988005395\n",
      "epoch 6224: train loss: 0.1348544198248789, test loss: 0.263011471133278\n",
      "epoch 6225: train loss: 0.1348484011026451, test loss: 0.26301074394667395\n",
      "epoch 6226: train loss: 0.134842383771947, test loss: 0.2630100172404941\n",
      "epoch 6227: train loss: 0.13483636783222, test loss: 0.2630092910145055\n",
      "epoch 6228: train loss: 0.1348303532828999, test loss: 0.26300856526847516\n",
      "epoch 6229: train loss: 0.13482434012342284, test loss: 0.26300784000217053\n",
      "epoch 6230: train loss: 0.13481832835322524, test loss: 0.263007115215359\n",
      "epoch 6231: train loss: 0.13481231797174387, test loss: 0.2630063909078083\n",
      "epoch 6232: train loss: 0.13480630897841586, test loss: 0.2630056670792862\n",
      "epoch 6233: train loss: 0.13480030137267854, test loss: 0.2630049437295607\n",
      "epoch 6234: train loss: 0.13479429515396965, test loss: 0.2630042208584\n",
      "epoch 6235: train loss: 0.1347882903217273, test loss: 0.2630034984655724\n",
      "epoch 6236: train loss: 0.13478228687538985, test loss: 0.2630027765508465\n",
      "epoch 6237: train loss: 0.13477628481439596, test loss: 0.263002055113991\n",
      "epoch 6238: train loss: 0.1347702841381847, test loss: 0.2630013341547746\n",
      "epoch 6239: train loss: 0.1347642848461954, test loss: 0.2630006136729665\n",
      "epoch 6240: train loss: 0.1347582869378677, test loss: 0.26299989366833587\n",
      "epoch 6241: train loss: 0.13475229041264164, test loss: 0.26299917414065194\n",
      "epoch 6242: train loss: 0.13474629526995746, test loss: 0.26299845508968445\n",
      "epoch 6243: train loss: 0.13474030150925587, test loss: 0.2629977365152029\n",
      "epoch 6244: train loss: 0.13473430912997772, test loss: 0.2629970184169773\n",
      "epoch 6245: train loss: 0.13472831813156438, test loss: 0.26299630079477776\n",
      "epoch 6246: train loss: 0.1347223285134574, test loss: 0.26299558364837444\n",
      "epoch 6247: train loss: 0.13471634027509866, test loss: 0.26299486697753766\n",
      "epoch 6248: train loss: 0.1347103534159304, test loss: 0.262994150782038\n",
      "epoch 6249: train loss: 0.1347043679353952, test loss: 0.26299343506164635\n",
      "epoch 6250: train loss: 0.1346983838329359, test loss: 0.26299271981613337\n",
      "epoch 6251: train loss: 0.13469240110799569, test loss: 0.2629920050452703\n",
      "epoch 6252: train loss: 0.13468641976001813, test loss: 0.2629912907488283\n",
      "epoch 6253: train loss: 0.13468043978844693, test loss: 0.26299057692657885\n",
      "epoch 6254: train loss: 0.13467446119272636, test loss: 0.2629898635782934\n",
      "epoch 6255: train loss: 0.1346684839723008, test loss: 0.2629891507037438\n",
      "epoch 6256: train loss: 0.13466250812661504, test loss: 0.26298843830270185\n",
      "epoch 6257: train loss: 0.1346565336551142, test loss: 0.2629877263749398\n",
      "epoch 6258: train loss: 0.13465056055724367, test loss: 0.2629870149202297\n",
      "epoch 6259: train loss: 0.1346445888324492, test loss: 0.2629863039383441\n",
      "epoch 6260: train loss: 0.1346386184801768, test loss: 0.26298559342905553\n",
      "epoch 6261: train loss: 0.13463264949987291, test loss: 0.2629848833921367\n",
      "epoch 6262: train loss: 0.13462668189098412, test loss: 0.2629841738273606\n",
      "epoch 6263: train loss: 0.13462071565295747, test loss: 0.2629834647345003\n",
      "epoch 6264: train loss: 0.13461475078524032, test loss: 0.26298275611332905\n",
      "epoch 6265: train loss: 0.1346087872872802, test loss: 0.2629820479636202\n",
      "epoch 6266: train loss: 0.13460282515852512, test loss: 0.26298134028514736\n",
      "epoch 6267: train loss: 0.13459686439842333, test loss: 0.2629806330776844\n",
      "epoch 6268: train loss: 0.1345909050064234, test loss: 0.26297992634100503\n",
      "epoch 6269: train loss: 0.13458494698197424, test loss: 0.26297922007488345\n",
      "epoch 6270: train loss: 0.134578990324525, test loss: 0.2629785142790939\n",
      "epoch 6271: train loss: 0.13457303503352522, test loss: 0.26297780895341083\n",
      "epoch 6272: train loss: 0.13456708110842475, test loss: 0.26297710409760877\n",
      "epoch 6273: train loss: 0.13456112854867372, test loss: 0.2629763997114623\n",
      "epoch 6274: train loss: 0.1345551773537226, test loss: 0.26297569579474667\n",
      "epoch 6275: train loss: 0.13454922752302217, test loss: 0.26297499234723676\n",
      "epoch 6276: train loss: 0.1345432790560235, test loss: 0.2629742893687077\n",
      "epoch 6277: train loss: 0.13453733195217799, test loss: 0.2629735868589351\n",
      "epoch 6278: train loss: 0.13453138621093735, test loss: 0.2629728848176945\n",
      "epoch 6279: train loss: 0.13452544183175363, test loss: 0.26297218324476157\n",
      "epoch 6280: train loss: 0.13451949881407912, test loss: 0.2629714821399121\n",
      "epoch 6281: train loss: 0.13451355715736654, test loss: 0.26297078150292236\n",
      "epoch 6282: train loss: 0.13450761686106877, test loss: 0.26297008133356853\n",
      "epoch 6283: train loss: 0.1345016779246391, test loss: 0.26296938163162686\n",
      "epoch 6284: train loss: 0.13449574034753117, test loss: 0.26296868239687404\n",
      "epoch 6285: train loss: 0.1344898041291988, test loss: 0.26296798362908674\n",
      "epoch 6286: train loss: 0.13448386926909628, test loss: 0.26296728532804187\n",
      "epoch 6287: train loss: 0.13447793576667802, test loss: 0.2629665874935164\n",
      "epoch 6288: train loss: 0.13447200362139897, test loss: 0.26296589012528765\n",
      "epoch 6289: train loss: 0.13446607283271414, test loss: 0.26296519322313283\n",
      "epoch 6290: train loss: 0.13446014340007909, test loss: 0.2629644967868296\n",
      "epoch 6291: train loss: 0.1344542153229495, test loss: 0.2629638008161556\n",
      "epoch 6292: train loss: 0.13444828860078142, test loss: 0.26296310531088873\n",
      "epoch 6293: train loss: 0.13444236323303133, test loss: 0.26296241027080697\n",
      "epoch 6294: train loss: 0.13443643921915582, test loss: 0.26296171569568855\n",
      "epoch 6295: train loss: 0.1344305165586119, test loss: 0.26296102158531176\n",
      "epoch 6296: train loss: 0.13442459525085693, test loss: 0.2629603279394551\n",
      "epoch 6297: train loss: 0.13441867529534846, test loss: 0.26295963475789724\n",
      "epoch 6298: train loss: 0.1344127566915444, test loss: 0.26295894204041714\n",
      "epoch 6299: train loss: 0.13440683943890303, test loss: 0.26295824978679355\n",
      "epoch 6300: train loss: 0.13440092353688285, test loss: 0.26295755799680587\n",
      "epoch 6301: train loss: 0.13439500898494272, test loss: 0.26295686667023316\n",
      "epoch 6302: train loss: 0.13438909578254174, test loss: 0.26295617580685515\n",
      "epoch 6303: train loss: 0.13438318392913942, test loss: 0.2629554854064513\n",
      "epoch 6304: train loss: 0.13437727342419556, test loss: 0.26295479546880146\n",
      "epoch 6305: train loss: 0.1343713642671701, test loss: 0.2629541059936855\n",
      "epoch 6306: train loss: 0.13436545645752354, test loss: 0.26295341698088376\n",
      "epoch 6307: train loss: 0.1343595499947165, test loss: 0.26295272843017625\n",
      "epoch 6308: train loss: 0.13435364487820997, test loss: 0.26295204034134356\n",
      "epoch 6309: train loss: 0.13434774110746528, test loss: 0.26295135271416625\n",
      "epoch 6310: train loss: 0.13434183868194402, test loss: 0.26295066554842506\n",
      "epoch 6311: train loss: 0.13433593760110804, test loss: 0.262949978843901\n",
      "epoch 6312: train loss: 0.1343300378644196, test loss: 0.2629492926003749\n",
      "epoch 6313: train loss: 0.13432413947134123, test loss: 0.2629486068176283\n",
      "epoch 6314: train loss: 0.13431824242133572, test loss: 0.26294792149544244\n",
      "epoch 6315: train loss: 0.13431234671386622, test loss: 0.2629472366335988\n",
      "epoch 6316: train loss: 0.13430645234839608, test loss: 0.26294655223187924\n",
      "epoch 6317: train loss: 0.13430055932438914, test loss: 0.2629458682900655\n",
      "epoch 6318: train loss: 0.13429466764130937, test loss: 0.2629451848079397\n",
      "epoch 6319: train loss: 0.13428877729862113, test loss: 0.262944501785284\n",
      "epoch 6320: train loss: 0.13428288829578905, test loss: 0.2629438192218807\n",
      "epoch 6321: train loss: 0.13427700063227807, test loss: 0.2629431371175124\n",
      "epoch 6322: train loss: 0.13427111430755345, test loss: 0.26294245547196154\n",
      "epoch 6323: train loss: 0.1342652293210808, test loss: 0.2629417742850112\n",
      "epoch 6324: train loss: 0.13425934567232586, test loss: 0.2629410935564443\n",
      "epoch 6325: train loss: 0.13425346336075486, test loss: 0.26294041328604384\n",
      "epoch 6326: train loss: 0.13424758238583423, test loss: 0.26293973347359323\n",
      "epoch 6327: train loss: 0.13424170274703073, test loss: 0.26293905411887586\n",
      "epoch 6328: train loss: 0.13423582444381144, test loss: 0.26293837522167535\n",
      "epoch 6329: train loss: 0.13422994747564373, test loss: 0.2629376967817754\n",
      "epoch 6330: train loss: 0.13422407184199522, test loss: 0.26293701879896003\n",
      "epoch 6331: train loss: 0.13421819754233388, test loss: 0.2629363412730133\n",
      "epoch 6332: train loss: 0.13421232457612803, test loss: 0.26293566420371944\n",
      "epoch 6333: train loss: 0.1342064529428462, test loss: 0.2629349875908627\n",
      "epoch 6334: train loss: 0.13420058264195725, test loss: 0.26293431143422785\n",
      "epoch 6335: train loss: 0.13419471367293032, test loss: 0.26293363573359935\n",
      "epoch 6336: train loss: 0.13418884603523495, test loss: 0.26293296048876225\n",
      "epoch 6337: train loss: 0.1341829797283408, test loss: 0.26293228569950144\n",
      "epoch 6338: train loss: 0.13417711475171806, test loss: 0.26293161136560217\n",
      "epoch 6339: train loss: 0.13417125110483702, test loss: 0.26293093748684965\n",
      "epoch 6340: train loss: 0.13416538878716833, test loss: 0.26293026406302944\n",
      "epoch 6341: train loss: 0.13415952779818294, test loss: 0.26292959109392716\n",
      "epoch 6342: train loss: 0.1341536681373522, test loss: 0.26292891857932865\n",
      "epoch 6343: train loss: 0.1341478098041476, test loss: 0.2629282465190197\n",
      "epoch 6344: train loss: 0.13414195279804095, test loss: 0.2629275749127865\n",
      "epoch 6345: train loss: 0.1341360971185045, test loss: 0.26292690376041533\n",
      "epoch 6346: train loss: 0.13413024276501065, test loss: 0.26292623306169266\n",
      "epoch 6347: train loss: 0.13412438973703217, test loss: 0.2629255628164048\n",
      "epoch 6348: train loss: 0.13411853803404208, test loss: 0.2629248930243387\n",
      "epoch 6349: train loss: 0.13411268765551376, test loss: 0.26292422368528107\n",
      "epoch 6350: train loss: 0.1341068386009208, test loss: 0.262923554799019\n",
      "epoch 6351: train loss: 0.13410099086973717, test loss: 0.2629228863653397\n",
      "epoch 6352: train loss: 0.1340951444614371, test loss: 0.26292221838403046\n",
      "epoch 6353: train loss: 0.13408929937549513, test loss: 0.2629215508548788\n",
      "epoch 6354: train loss: 0.13408345561138604, test loss: 0.26292088377767225\n",
      "epoch 6355: train loss: 0.13407761316858502, test loss: 0.26292021715219865\n",
      "epoch 6356: train loss: 0.1340717720465674, test loss: 0.26291955097824593\n",
      "epoch 6357: train loss: 0.13406593224480895, test loss: 0.26291888525560225\n",
      "epoch 6358: train loss: 0.1340600937627857, test loss: 0.2629182199840558\n",
      "epoch 6359: train loss: 0.13405425659997386, test loss: 0.26291755516339493\n",
      "epoch 6360: train loss: 0.1340484207558501, test loss: 0.2629168907934083\n",
      "epoch 6361: train loss: 0.1340425862298913, test loss: 0.26291622687388455\n",
      "epoch 6362: train loss: 0.13403675302157458, test loss: 0.26291556340461253\n",
      "epoch 6363: train loss: 0.13403092113037754, test loss: 0.26291490038538123\n",
      "epoch 6364: train loss: 0.13402509055577785, test loss: 0.2629142378159799\n",
      "epoch 6365: train loss: 0.13401926129725364, test loss: 0.2629135756961977\n",
      "epoch 6366: train loss: 0.1340134333542832, test loss: 0.26291291402582423\n",
      "epoch 6367: train loss: 0.13400760672634524, test loss: 0.26291225280464897\n",
      "epoch 6368: train loss: 0.13400178141291869, test loss: 0.26291159203246184\n",
      "epoch 6369: train loss: 0.13399595741348277, test loss: 0.2629109317090526\n",
      "epoch 6370: train loss: 0.133990134727517, test loss: 0.26291027183421134\n",
      "epoch 6371: train loss: 0.13398431335450126, test loss: 0.2629096124077284\n",
      "epoch 6372: train loss: 0.13397849329391562, test loss: 0.262908953429394\n",
      "epoch 6373: train loss: 0.1339726745452405, test loss: 0.2629082948989988\n",
      "epoch 6374: train loss: 0.13396685710795658, test loss: 0.2629076368163334\n",
      "epoch 6375: train loss: 0.13396104098154488, test loss: 0.2629069791811886\n",
      "epoch 6376: train loss: 0.13395522616548666, test loss: 0.26290632199335534\n",
      "epoch 6377: train loss: 0.1339494126592635, test loss: 0.2629056652526247\n",
      "epoch 6378: train loss: 0.13394360046235726, test loss: 0.2629050089587881\n",
      "epoch 6379: train loss: 0.13393778957425007, test loss: 0.26290435311163685\n",
      "epoch 6380: train loss: 0.1339319799944244, test loss: 0.2629036977109625\n",
      "epoch 6381: train loss: 0.13392617172236299, test loss: 0.26290304275655685\n",
      "epoch 6382: train loss: 0.13392036475754884, test loss: 0.2629023882482117\n",
      "epoch 6383: train loss: 0.1339145590994653, test loss: 0.26290173418571905\n",
      "epoch 6384: train loss: 0.13390875474759598, test loss: 0.2629010805688711\n",
      "epoch 6385: train loss: 0.1339029517014247, test loss: 0.2629004273974602\n",
      "epoch 6386: train loss: 0.1338971499604357, test loss: 0.2628997746712787\n",
      "epoch 6387: train loss: 0.1338913495241134, test loss: 0.2628991223901193\n",
      "epoch 6388: train loss: 0.1338855503919426, test loss: 0.2628984705537747\n",
      "epoch 6389: train loss: 0.13387975256340842, test loss: 0.2628978191620379\n",
      "epoch 6390: train loss: 0.13387395603799604, test loss: 0.2628971682147019\n",
      "epoch 6391: train loss: 0.1338681608151912, test loss: 0.26289651771155975\n",
      "epoch 6392: train loss: 0.13386236689447975, test loss: 0.262895867652405\n",
      "epoch 6393: train loss: 0.13385657427534794, test loss: 0.2628952180370311\n",
      "epoch 6394: train loss: 0.13385078295728223, test loss: 0.2628945688652317\n",
      "epoch 6395: train loss: 0.1338449929397694, test loss: 0.2628939201368004\n",
      "epoch 6396: train loss: 0.1338392042222965, test loss: 0.26289327185153144\n",
      "epoch 6397: train loss: 0.1338334168043509, test loss: 0.26289262400921876\n",
      "epoch 6398: train loss: 0.1338276306854202, test loss: 0.2628919766096565\n",
      "epoch 6399: train loss: 0.13382184586499235, test loss: 0.2628913296526392\n",
      "epoch 6400: train loss: 0.13381606234255558, test loss: 0.2628906831379614\n",
      "epoch 6401: train loss: 0.13381028011759835, test loss: 0.2628900370654175\n",
      "epoch 6402: train loss: 0.13380449918960943, test loss: 0.26288939143480267\n",
      "epoch 6403: train loss: 0.13379871955807793, test loss: 0.2628887462459116\n",
      "epoch 6404: train loss: 0.13379294122249316, test loss: 0.2628881014985396\n",
      "epoch 6405: train loss: 0.13378716418234476, test loss: 0.26288745719248174\n",
      "epoch 6406: train loss: 0.13378138843712267, test loss: 0.2628868133275337\n",
      "epoch 6407: train loss: 0.1337756139863171, test loss: 0.2628861699034907\n",
      "epoch 6408: train loss: 0.13376984082941853, test loss: 0.26288552692014866\n",
      "epoch 6409: train loss: 0.13376406896591775, test loss: 0.2628848843773033\n",
      "epoch 6410: train loss: 0.13375829839530576, test loss: 0.2628842422747507\n",
      "epoch 6411: train loss: 0.133752529117074, test loss: 0.2628836006122869\n",
      "epoch 6412: train loss: 0.133746761130714, test loss: 0.2628829593897082\n",
      "epoch 6413: train loss: 0.13374099443571774, test loss: 0.262882318606811\n",
      "epoch 6414: train loss: 0.1337352290315774, test loss: 0.262881678263392\n",
      "epoch 6415: train loss: 0.13372946491778542, test loss: 0.2628810383592477\n",
      "epoch 6416: train loss: 0.1337237020938346, test loss: 0.26288039889417514\n",
      "epoch 6417: train loss: 0.13371794055921796, test loss: 0.2628797598679712\n",
      "epoch 6418: train loss: 0.13371218031342885, test loss: 0.262879121280433\n",
      "epoch 6419: train loss: 0.13370642135596086, test loss: 0.262878483131358\n",
      "epoch 6420: train loss: 0.1337006636863079, test loss: 0.2628778454205435\n",
      "epoch 6421: train loss: 0.1336949073039641, test loss: 0.26287720814778703\n",
      "epoch 6422: train loss: 0.13368915220842395, test loss: 0.2628765713128864\n",
      "epoch 6423: train loss: 0.1336833983991822, test loss: 0.2628759349156394\n",
      "epoch 6424: train loss: 0.13367764587573383, test loss: 0.26287529895584405\n",
      "epoch 6425: train loss: 0.13367189463757415, test loss: 0.26287466343329857\n",
      "epoch 6426: train loss: 0.13366614468419874, test loss: 0.2628740283478012\n",
      "epoch 6427: train loss: 0.1336603960151035, test loss: 0.26287339369915036\n",
      "epoch 6428: train loss: 0.13365464862978446, test loss: 0.26287275948714467\n",
      "epoch 6429: train loss: 0.13364890252773814, test loss: 0.2628721257115828\n",
      "epoch 6430: train loss: 0.13364315770846125, test loss: 0.26287149237226354\n",
      "epoch 6431: train loss: 0.13363741417145072, test loss: 0.2628708594689861\n",
      "epoch 6432: train loss: 0.13363167191620381, test loss: 0.26287022700154944\n",
      "epoch 6433: train loss: 0.13362593094221806, test loss: 0.262869594969753\n",
      "epoch 6434: train loss: 0.13362019124899133, test loss: 0.26286896337339605\n",
      "epoch 6435: train loss: 0.1336144528360217, test loss: 0.2628683322122783\n",
      "epoch 6436: train loss: 0.13360871570280752, test loss: 0.26286770148619937\n",
      "epoch 6437: train loss: 0.1336029798488475, test loss: 0.26286707119495917\n",
      "epoch 6438: train loss: 0.13359724527364053, test loss: 0.2628664413383576\n",
      "epoch 6439: train loss: 0.13359151197668584, test loss: 0.26286581191619507\n",
      "epoch 6440: train loss: 0.13358577995748291, test loss: 0.26286518292827155\n",
      "epoch 6441: train loss: 0.13358004921553154, test loss: 0.2628645543743876\n",
      "epoch 6442: train loss: 0.13357431975033174, test loss: 0.26286392625434374\n",
      "epoch 6443: train loss: 0.13356859156138387, test loss: 0.26286329856794066\n",
      "epoch 6444: train loss: 0.13356286464818848, test loss: 0.2628626713149793\n",
      "epoch 6445: train loss: 0.1335571390102465, test loss: 0.2628620444952607\n",
      "epoch 6446: train loss: 0.13355141464705908, test loss: 0.26286141810858576\n",
      "epoch 6447: train loss: 0.13354569155812762, test loss: 0.26286079215475583\n",
      "epoch 6448: train loss: 0.13353996974295387, test loss: 0.26286016663357237\n",
      "epoch 6449: train loss: 0.13353424920103982, test loss: 0.2628595415448369\n",
      "epoch 6450: train loss: 0.13352852993188769, test loss: 0.2628589168883511\n",
      "epoch 6451: train loss: 0.13352281193500004, test loss: 0.26285829266391686\n",
      "epoch 6452: train loss: 0.1335170952098797, test loss: 0.26285766887133605\n",
      "epoch 6453: train loss: 0.13351137975602975, test loss: 0.2628570455104108\n",
      "epoch 6454: train loss: 0.1335056655729535, test loss: 0.2628564225809434\n",
      "epoch 6455: train loss: 0.13349995266015469, test loss: 0.2628558000827362\n",
      "epoch 6456: train loss: 0.13349424101713717, test loss: 0.26285517801559166\n",
      "epoch 6457: train loss: 0.13348853064340516, test loss: 0.26285455637931254\n",
      "epoch 6458: train loss: 0.1334828215384631, test loss: 0.26285393517370165\n",
      "epoch 6459: train loss: 0.13347711370181575, test loss: 0.26285331439856185\n",
      "epoch 6460: train loss: 0.13347140713296812, test loss: 0.26285269405369627\n",
      "epoch 6461: train loss: 0.1334657018314255, test loss: 0.26285207413890804\n",
      "epoch 6462: train loss: 0.1334599977966934, test loss: 0.2628514546540006\n",
      "epoch 6463: train loss: 0.13345429502827777, test loss: 0.26285083559877753\n",
      "epoch 6464: train loss: 0.13344859352568458, test loss: 0.2628502169730423\n",
      "epoch 6465: train loss: 0.13344289328842032, test loss: 0.26284959877659875\n",
      "epoch 6466: train loss: 0.1334371943159916, test loss: 0.26284898100925075\n",
      "epoch 6467: train loss: 0.13343149660790535, test loss: 0.2628483636708024\n",
      "epoch 6468: train loss: 0.13342580016366878, test loss: 0.2628477467610578\n",
      "epoch 6469: train loss: 0.13342010498278933, test loss: 0.2628471302798214\n",
      "epoch 6470: train loss: 0.13341441106477483, test loss: 0.26284651422689753\n",
      "epoch 6471: train loss: 0.13340871840913318, test loss: 0.2628458986020909\n",
      "epoch 6472: train loss: 0.13340302701537277, test loss: 0.2628452834052061\n",
      "epoch 6473: train loss: 0.13339733688300207, test loss: 0.26284466863604805\n",
      "epoch 6474: train loss: 0.13339164801153, test loss: 0.26284405429442187\n",
      "epoch 6475: train loss: 0.13338596040046563, test loss: 0.26284344038013263\n",
      "epoch 6476: train loss: 0.13338027404931832, test loss: 0.26284282689298555\n",
      "epoch 6477: train loss: 0.13337458895759774, test loss: 0.26284221383278605\n",
      "epoch 6478: train loss: 0.13336890512481378, test loss: 0.2628416011993398\n",
      "epoch 6479: train loss: 0.13336322255047664, test loss: 0.26284098899245234\n",
      "epoch 6480: train loss: 0.13335754123409677, test loss: 0.26284037721192965\n",
      "epoch 6481: train loss: 0.13335186117518494, test loss: 0.26283976585757757\n",
      "epoch 6482: train loss: 0.13334618237325205, test loss: 0.2628391549292022\n",
      "epoch 6483: train loss: 0.1333405048278095, test loss: 0.26283854442660987\n",
      "epoch 6484: train loss: 0.13333482853836873, test loss: 0.2628379343496068\n",
      "epoch 6485: train loss: 0.13332915350444158, test loss: 0.2628373246979996\n",
      "epoch 6486: train loss: 0.13332347972554012, test loss: 0.2628367154715949\n",
      "epoch 6487: train loss: 0.1333178072011767, test loss: 0.2628361066701994\n",
      "epoch 6488: train loss: 0.1333121359308639, test loss: 0.2628354982936201\n",
      "epoch 6489: train loss: 0.13330646591411466, test loss: 0.26283489034166396\n",
      "epoch 6490: train loss: 0.1333007971504421, test loss: 0.2628342828141383\n",
      "epoch 6491: train loss: 0.13329512963935966, test loss: 0.26283367571085026\n",
      "epoch 6492: train loss: 0.13328946338038097, test loss: 0.26283306903160736\n",
      "epoch 6493: train loss: 0.13328379837302, test loss: 0.26283246277621714\n",
      "epoch 6494: train loss: 0.13327813461679106, test loss: 0.26283185694448746\n",
      "epoch 6495: train loss: 0.13327247211120855, test loss: 0.26283125153622605\n",
      "epoch 6496: train loss: 0.13326681085578726, test loss: 0.26283064655124094\n",
      "epoch 6497: train loss: 0.1332611508500422, test loss: 0.2628300419893401\n",
      "epoch 6498: train loss: 0.13325549209348864, test loss: 0.262829437850332\n",
      "epoch 6499: train loss: 0.1332498345856422, test loss: 0.26282883413402497\n",
      "epoch 6500: train loss: 0.13324417832601868, test loss: 0.26282823084022744\n",
      "epoch 6501: train loss: 0.13323852331413413, test loss: 0.26282762796874803\n",
      "epoch 6502: train loss: 0.13323286954950497, test loss: 0.26282702551939574\n",
      "epoch 6503: train loss: 0.13322721703164775, test loss: 0.26282642349197927\n",
      "epoch 6504: train loss: 0.13322156576007943, test loss: 0.2628258218863078\n",
      "epoch 6505: train loss: 0.13321591573431715, test loss: 0.26282522070219044\n",
      "epoch 6506: train loss: 0.13321026695387828, test loss: 0.2628246199394366\n",
      "epoch 6507: train loss: 0.1332046194182806, test loss: 0.2628240195978557\n",
      "epoch 6508: train loss: 0.13319897312704196, test loss: 0.2628234196772572\n",
      "epoch 6509: train loss: 0.1331933280796806, test loss: 0.26282282017745096\n",
      "epoch 6510: train loss: 0.13318768427571506, test loss: 0.2628222210982468\n",
      "epoch 6511: train loss: 0.13318204171466402, test loss: 0.26282162243945467\n",
      "epoch 6512: train loss: 0.13317640039604653, test loss: 0.2628210242008847\n",
      "epoch 6513: train loss: 0.13317076031938183, test loss: 0.26282042638234715\n",
      "epoch 6514: train loss: 0.13316512148418944, test loss: 0.26281982898365236\n",
      "epoch 6515: train loss: 0.13315948388998922, test loss: 0.2628192320046108\n",
      "epoch 6516: train loss: 0.1331538475363012, test loss: 0.2628186354450333\n",
      "epoch 6517: train loss: 0.13314821242264574, test loss: 0.2628180393047304\n",
      "epoch 6518: train loss: 0.13314257854854336, test loss: 0.26281744358351306\n",
      "epoch 6519: train loss: 0.13313694591351496, test loss: 0.26281684828119245\n",
      "epoch 6520: train loss: 0.13313131451708166, test loss: 0.2628162533975796\n",
      "epoch 6521: train loss: 0.13312568435876485, test loss: 0.2628156589324858\n",
      "epoch 6522: train loss: 0.13312005543808614, test loss: 0.2628150648857226\n",
      "epoch 6523: train loss: 0.13311442775456744, test loss: 0.26281447125710145\n",
      "epoch 6524: train loss: 0.13310880130773095, test loss: 0.2628138780464341\n",
      "epoch 6525: train loss: 0.13310317609709904, test loss: 0.2628132852535323\n",
      "epoch 6526: train loss: 0.13309755212219443, test loss: 0.262812692878208\n",
      "epoch 6527: train loss: 0.13309192938254008, test loss: 0.26281210092027335\n",
      "epoch 6528: train loss: 0.13308630787765918, test loss: 0.2628115093795405\n",
      "epoch 6529: train loss: 0.13308068760707525, test loss: 0.26281091825582187\n",
      "epoch 6530: train loss: 0.13307506857031196, test loss: 0.26281032754892986\n",
      "epoch 6531: train loss: 0.13306945076689336, test loss: 0.26280973725867707\n",
      "epoch 6532: train loss: 0.13306383419634366, test loss: 0.26280914738487626\n",
      "epoch 6533: train loss: 0.13305821885818736, test loss: 0.26280855792734026\n",
      "epoch 6534: train loss: 0.13305260475194933, test loss: 0.26280796888588204\n",
      "epoch 6535: train loss: 0.13304699187715455, test loss: 0.2628073802603148\n",
      "epoch 6536: train loss: 0.1330413802333283, test loss: 0.26280679205045165\n",
      "epoch 6537: train loss: 0.13303576981999612, test loss: 0.2628062042561061\n",
      "epoch 6538: train loss: 0.1330301606366839, test loss: 0.26280561687709164\n",
      "epoch 6539: train loss: 0.13302455268291768, test loss: 0.26280502991322185\n",
      "epoch 6540: train loss: 0.13301894595822378, test loss: 0.2628044433643105\n",
      "epoch 6541: train loss: 0.1330133404621288, test loss: 0.26280385723017147\n",
      "epoch 6542: train loss: 0.13300773619415956, test loss: 0.2628032715106189\n",
      "epoch 6543: train loss: 0.13300213315384324, test loss: 0.2628026862054667\n",
      "epoch 6544: train loss: 0.13299653134070716, test loss: 0.2628021013145293\n",
      "epoch 6545: train loss: 0.13299093075427895, test loss: 0.2628015168376212\n",
      "epoch 6546: train loss: 0.1329853313940865, test loss: 0.2628009327745567\n",
      "epoch 6547: train loss: 0.13297973325965795, test loss: 0.26280034912515066\n",
      "epoch 6548: train loss: 0.13297413635052174, test loss: 0.2627997658892178\n",
      "epoch 6549: train loss: 0.13296854066620645, test loss: 0.26279918306657285\n",
      "epoch 6550: train loss: 0.13296294620624105, test loss: 0.2627986006570312\n",
      "epoch 6551: train loss: 0.1329573529701547, test loss: 0.2627980186604078\n",
      "epoch 6552: train loss: 0.13295176095747685, test loss: 0.262797437076518\n",
      "epoch 6553: train loss: 0.13294617016773713, test loss: 0.2627968559051772\n",
      "epoch 6554: train loss: 0.13294058060046557, test loss: 0.26279627514620096\n",
      "epoch 6555: train loss: 0.13293499225519229, test loss: 0.262795694799405\n",
      "epoch 6556: train loss: 0.13292940513144777, test loss: 0.262795114864605\n",
      "epoch 6557: train loss: 0.13292381922876273, test loss: 0.2627945353416171\n",
      "epoch 6558: train loss: 0.13291823454666812, test loss: 0.2627939562302572\n",
      "epoch 6559: train loss: 0.13291265108469522, test loss: 0.26279337753034154\n",
      "epoch 6560: train loss: 0.1329070688423754, test loss: 0.2627927992416864\n",
      "epoch 6561: train loss: 0.1329014878192405, test loss: 0.2627922213641083\n",
      "epoch 6562: train loss: 0.13289590801482246, test loss: 0.2627916438974237\n",
      "epoch 6563: train loss: 0.1328903294286535, test loss: 0.26279106684144926\n",
      "epoch 6564: train loss: 0.13288475206026618, test loss: 0.262790490196002\n",
      "epoch 6565: train loss: 0.13287917590919324, test loss: 0.2627899139608986\n",
      "epoch 6566: train loss: 0.13287360097496767, test loss: 0.26278933813595634\n",
      "epoch 6567: train loss: 0.1328680272571227, test loss: 0.2627887627209924\n",
      "epoch 6568: train loss: 0.1328624547551919, test loss: 0.26278818771582396\n",
      "epoch 6569: train loss: 0.132856883468709, test loss: 0.2627876131202685\n",
      "epoch 6570: train loss: 0.13285131339720807, test loss: 0.2627870389341436\n",
      "epoch 6571: train loss: 0.13284574454022333, test loss: 0.2627864651572671\n",
      "epoch 6572: train loss: 0.13284017689728939, test loss: 0.26278589178945666\n",
      "epoch 6573: train loss: 0.13283461046794096, test loss: 0.2627853188305302\n",
      "epoch 6574: train loss: 0.13282904525171307, test loss: 0.26278474628030585\n",
      "epoch 6575: train loss: 0.13282348124814108, test loss: 0.26278417413860183\n",
      "epoch 6576: train loss: 0.1328179184567605, test loss: 0.26278360240523646\n",
      "epoch 6577: train loss: 0.1328123568771071, test loss: 0.2627830310800281\n",
      "epoch 6578: train loss: 0.13280679650871696, test loss: 0.26278246016279544\n",
      "epoch 6579: train loss: 0.13280123735112637, test loss: 0.262781889653357\n",
      "epoch 6580: train loss: 0.13279567940387188, test loss: 0.2627813195515318\n",
      "epoch 6581: train loss: 0.1327901226664903, test loss: 0.26278074985713856\n",
      "epoch 6582: train loss: 0.13278456713851863, test loss: 0.2627801805699965\n",
      "epoch 6583: train loss: 0.13277901281949428, test loss: 0.2627796116899248\n",
      "epoch 6584: train loss: 0.1327734597089547, test loss: 0.2627790432167428\n",
      "epoch 6585: train loss: 0.1327679078064378, test loss: 0.26277847515026975\n",
      "epoch 6586: train loss: 0.13276235711148157, test loss: 0.2627779074903253\n",
      "epoch 6587: train loss: 0.13275680762362435, test loss: 0.2627773402367292\n",
      "epoch 6588: train loss: 0.13275125934240464, test loss: 0.2627767733893013\n",
      "epoch 6589: train loss: 0.13274571226736134, test loss: 0.2627762069478613\n",
      "epoch 6590: train loss: 0.13274016639803352, test loss: 0.26277564091222944\n",
      "epoch 6591: train loss: 0.13273462173396036, test loss: 0.26277507528222577\n",
      "epoch 6592: train loss: 0.13272907827468156, test loss: 0.2627745100576707\n",
      "epoch 6593: train loss: 0.13272353601973685, test loss: 0.2627739452383846\n",
      "epoch 6594: train loss: 0.13271799496866635, test loss: 0.262773380824188\n",
      "epoch 6595: train loss: 0.13271245512101032, test loss: 0.2627728168149015\n",
      "epoch 6596: train loss: 0.13270691647630933, test loss: 0.262772253210346\n",
      "epoch 6597: train loss: 0.13270137903410417, test loss: 0.2627716900103424\n",
      "epoch 6598: train loss: 0.13269584279393598, test loss: 0.2627711272147116\n",
      "epoch 6599: train loss: 0.13269030775534596, test loss: 0.26277056482327493\n",
      "epoch 6600: train loss: 0.13268477391787573, test loss: 0.2627700028358535\n",
      "epoch 6601: train loss: 0.1326792412810671, test loss: 0.26276944125226875\n",
      "epoch 6602: train loss: 0.13267370984446203, test loss: 0.2627688800723423\n",
      "epoch 6603: train loss: 0.1326681796076029, test loss: 0.2627683192958956\n",
      "epoch 6604: train loss: 0.13266265057003226, test loss: 0.2627677589227506\n",
      "epoch 6605: train loss: 0.13265712273129288, test loss: 0.2627671989527291\n",
      "epoch 6606: train loss: 0.13265159609092783, test loss: 0.262766639385653\n",
      "epoch 6607: train loss: 0.13264607064848033, test loss: 0.2627660802213446\n",
      "epoch 6608: train loss: 0.13264054640349396, test loss: 0.26276552145962606\n",
      "epoch 6609: train loss: 0.13263502335551253, test loss: 0.2627649631003196\n",
      "epoch 6610: train loss: 0.13262950150407998, test loss: 0.262764405143248\n",
      "epoch 6611: train loss: 0.1326239808487407, test loss: 0.2627638475882337\n",
      "epoch 6612: train loss: 0.1326184613890391, test loss: 0.2627632904350994\n",
      "epoch 6613: train loss: 0.13261294312452007, test loss: 0.2627627336836679\n",
      "epoch 6614: train loss: 0.1326074260547285, test loss: 0.26276217733376234\n",
      "epoch 6615: train loss: 0.13260191017920975, test loss: 0.2627616213852057\n",
      "epoch 6616: train loss: 0.13259639549750926, test loss: 0.26276106583782116\n",
      "epoch 6617: train loss: 0.13259088200917282, test loss: 0.2627605106914321\n",
      "epoch 6618: train loss: 0.1325853697137464, test loss: 0.26275995594586193\n",
      "epoch 6619: train loss: 0.13257985861077629, test loss: 0.26275940160093425\n",
      "epoch 6620: train loss: 0.1325743486998089, test loss: 0.2627588476564728\n",
      "epoch 6621: train loss: 0.132568839980391, test loss: 0.2627582941123012\n",
      "epoch 6622: train loss: 0.1325633324520696, test loss: 0.2627577409682435\n",
      "epoch 6623: train loss: 0.1325578261143919, test loss: 0.2627571882241238\n",
      "epoch 6624: train loss: 0.13255232096690533, test loss: 0.26275663587976616\n",
      "epoch 6625: train loss: 0.13254681700915763, test loss: 0.2627560839349949\n",
      "epoch 6626: train loss: 0.13254131424069676, test loss: 0.26275553238963434\n",
      "epoch 6627: train loss: 0.1325358126610709, test loss: 0.26275498124350916\n",
      "epoch 6628: train loss: 0.13253031226982853, test loss: 0.26275443049644387\n",
      "epoch 6629: train loss: 0.13252481306651828, test loss: 0.2627538801482633\n",
      "epoch 6630: train loss: 0.1325193150506891, test loss: 0.26275333019879227\n",
      "epoch 6631: train loss: 0.13251381822189018, test loss: 0.2627527806478558\n",
      "epoch 6632: train loss: 0.1325083225796709, test loss: 0.26275223149527904\n",
      "epoch 6633: train loss: 0.13250282812358094, test loss: 0.26275168274088706\n",
      "epoch 6634: train loss: 0.1324973348531702, test loss: 0.26275113438450537\n",
      "epoch 6635: train loss: 0.1324918427679888, test loss: 0.26275058642595944\n",
      "epoch 6636: train loss: 0.13248635186758717, test loss: 0.26275003886507475\n",
      "epoch 6637: train loss: 0.1324808621515159, test loss: 0.26274949170167716\n",
      "epoch 6638: train loss: 0.13247537361932585, test loss: 0.26274894493559225\n",
      "epoch 6639: train loss: 0.13246988627056816, test loss: 0.2627483985666462\n",
      "epoch 6640: train loss: 0.13246440010479418, test loss: 0.26274785259466493\n",
      "epoch 6641: train loss: 0.1324589151215555, test loss: 0.2627473070194746\n",
      "epoch 6642: train loss: 0.1324534313204039, test loss: 0.2627467618409015\n",
      "epoch 6643: train loss: 0.13244794870089152, test loss: 0.2627462170587721\n",
      "epoch 6644: train loss: 0.1324424672625707, test loss: 0.26274567267291293\n",
      "epoch 6645: train loss: 0.13243698700499393, test loss: 0.2627451286831506\n",
      "epoch 6646: train loss: 0.13243150792771402, test loss: 0.2627445850893118\n",
      "epoch 6647: train loss: 0.13242603003028408, test loss: 0.2627440418912235\n",
      "epoch 6648: train loss: 0.1324205533122573, test loss: 0.26274349908871264\n",
      "epoch 6649: train loss: 0.13241507777318723, test loss: 0.26274295668160635\n",
      "epoch 6650: train loss: 0.13240960341262764, test loss: 0.26274241466973186\n",
      "epoch 6651: train loss: 0.13240413023013253, test loss: 0.2627418730529164\n",
      "epoch 6652: train loss: 0.13239865822525612, test loss: 0.2627413318309876\n",
      "epoch 6653: train loss: 0.13239318739755293, test loss: 0.2627407910037729\n",
      "epoch 6654: train loss: 0.13238771774657762, test loss: 0.26274025057110006\n",
      "epoch 6655: train loss: 0.1323822492718852, test loss: 0.2627397105327968\n",
      "epoch 6656: train loss: 0.13237678197303082, test loss: 0.2627391708886912\n",
      "epoch 6657: train loss: 0.13237131584956996, test loss: 0.2627386316386111\n",
      "epoch 6658: train loss: 0.13236585090105824, test loss: 0.26273809278238475\n",
      "epoch 6659: train loss: 0.13236038712705164, test loss: 0.2627375543198404\n",
      "epoch 6660: train loss: 0.13235492452710623, test loss: 0.2627370162508064\n",
      "epoch 6661: train loss: 0.13234946310077844, test loss: 0.26273647857511134\n",
      "epoch 6662: train loss: 0.13234400284762493, test loss: 0.26273594129258376\n",
      "epoch 6663: train loss: 0.13233854376720253, test loss: 0.2627354044030524\n",
      "epoch 6664: train loss: 0.13233308585906833, test loss: 0.2627348679063461\n",
      "epoch 6665: train loss: 0.13232762912277965, test loss: 0.26273433180229383\n",
      "epoch 6666: train loss: 0.13232217355789416, test loss: 0.26273379609072456\n",
      "epoch 6667: train loss: 0.13231671916396956, test loss: 0.26273326077146764\n",
      "epoch 6668: train loss: 0.13231126594056397, test loss: 0.26273272584435214\n",
      "epoch 6669: train loss: 0.13230581388723567, test loss: 0.2627321913092077\n",
      "epoch 6670: train loss: 0.13230036300354317, test loss: 0.26273165716586383\n",
      "epoch 6671: train loss: 0.13229491328904525, test loss: 0.26273112341415\n",
      "epoch 6672: train loss: 0.13228946474330086, test loss: 0.2627305900538961\n",
      "epoch 6673: train loss: 0.1322840173658693, test loss: 0.262730057084932\n",
      "epoch 6674: train loss: 0.13227857115630998, test loss: 0.2627295245070877\n",
      "epoch 6675: train loss: 0.13227312611418265, test loss: 0.2627289923201932\n",
      "epoch 6676: train loss: 0.1322676822390472, test loss: 0.2627284605240787\n",
      "epoch 6677: train loss: 0.13226223953046387, test loss: 0.2627279291185747\n",
      "epoch 6678: train loss: 0.13225679798799306, test loss: 0.2627273981035115\n",
      "epoch 6679: train loss: 0.13225135761119536, test loss: 0.26272686747871965\n",
      "epoch 6680: train loss: 0.1322459183996317, test loss: 0.2627263372440299\n",
      "epoch 6681: train loss: 0.1322404803528632, test loss: 0.26272580739927287\n",
      "epoch 6682: train loss: 0.13223504347045117, test loss: 0.2627252779442796\n",
      "epoch 6683: train loss: 0.13222960775195727, test loss: 0.26272474887888103\n",
      "epoch 6684: train loss: 0.13222417319694327, test loss: 0.2627242202029083\n",
      "epoch 6685: train loss: 0.13221873980497117, test loss: 0.2627236919161927\n",
      "epoch 6686: train loss: 0.1322133075756034, test loss: 0.26272316401856544\n",
      "epoch 6687: train loss: 0.13220787650840238, test loss: 0.26272263650985805\n",
      "epoch 6688: train loss: 0.13220244660293087, test loss: 0.26272210938990204\n",
      "epoch 6689: train loss: 0.1321970178587519, test loss: 0.2627215826585292\n",
      "epoch 6690: train loss: 0.13219159027542868, test loss: 0.2627210563155712\n",
      "epoch 6691: train loss: 0.13218616385252463, test loss: 0.26272053036086\n",
      "epoch 6692: train loss: 0.13218073858960352, test loss: 0.26272000479422764\n",
      "epoch 6693: train loss: 0.13217531448622918, test loss: 0.26271947961550624\n",
      "epoch 6694: train loss: 0.13216989154196587, test loss: 0.262718954824528\n",
      "epoch 6695: train loss: 0.13216446975637786, test loss: 0.2627184304211253\n",
      "epoch 6696: train loss: 0.1321590491290299, test loss: 0.2627179064051306\n",
      "epoch 6697: train loss: 0.1321536296594867, test loss: 0.26271738277637646\n",
      "epoch 6698: train loss: 0.13214821134731347, test loss: 0.2627168595346956\n",
      "epoch 6699: train loss: 0.13214279419207547, test loss: 0.2627163366799207\n",
      "epoch 6700: train loss: 0.13213737819333826, test loss: 0.26271581421188483\n",
      "epoch 6701: train loss: 0.13213196335066762, test loss: 0.262715292130421\n",
      "epoch 6702: train loss: 0.13212654966362958, test loss: 0.2627147704353622\n",
      "epoch 6703: train loss: 0.13212113713179036, test loss: 0.26271424912654173\n",
      "epoch 6704: train loss: 0.13211572575471642, test loss: 0.26271372820379296\n",
      "epoch 6705: train loss: 0.13211031553197453, test loss: 0.2627132076669494\n",
      "epoch 6706: train loss: 0.13210490646313156, test loss: 0.2627126875158446\n",
      "epoch 6707: train loss: 0.1320994985477547, test loss: 0.2627121677503122\n",
      "epoch 6708: train loss: 0.13209409178541137, test loss: 0.26271164837018596\n",
      "epoch 6709: train loss: 0.13208868617566918, test loss: 0.26271112937529983\n",
      "epoch 6710: train loss: 0.13208328171809597, test loss: 0.2627106107654879\n",
      "epoch 6711: train loss: 0.13207787841225987, test loss: 0.26271009254058425\n",
      "epoch 6712: train loss: 0.1320724762577292, test loss: 0.26270957470042305\n",
      "epoch 6713: train loss: 0.13206707525407246, test loss: 0.2627090572448387\n",
      "epoch 6714: train loss: 0.13206167540085847, test loss: 0.2627085401736656\n",
      "epoch 6715: train loss: 0.1320562766976562, test loss: 0.2627080234867384\n",
      "epoch 6716: train loss: 0.13205087914403493, test loss: 0.26270750718389174\n",
      "epoch 6717: train loss: 0.13204548273956415, test loss: 0.26270699126496033\n",
      "epoch 6718: train loss: 0.1320400874838135, test loss: 0.26270647572977923\n",
      "epoch 6719: train loss: 0.13203469337635287, test loss: 0.26270596057818324\n",
      "epoch 6720: train loss: 0.1320293004167525, test loss: 0.2627054458100076\n",
      "epoch 6721: train loss: 0.13202390860458277, test loss: 0.2627049314250875\n",
      "epoch 6722: train loss: 0.13201851793941421, test loss: 0.2627044174232583\n",
      "epoch 6723: train loss: 0.13201312842081772, test loss: 0.26270390380435527\n",
      "epoch 6724: train loss: 0.13200774004836435, test loss: 0.2627033905682143\n",
      "epoch 6725: train loss: 0.13200235282162537, test loss: 0.2627028777146707\n",
      "epoch 6726: train loss: 0.1319969667401724, test loss: 0.2627023652435604\n",
      "epoch 6727: train loss: 0.13199158180357703, test loss: 0.2627018531547193\n",
      "epoch 6728: train loss: 0.13198619801141137, test loss: 0.2627013414479833\n",
      "epoch 6729: train loss: 0.13198081536324757, test loss: 0.2627008301231885\n",
      "epoch 6730: train loss: 0.13197543385865806, test loss: 0.26270031918017106\n",
      "epoch 6731: train loss: 0.13197005349721555, test loss: 0.26269980861876746\n",
      "epoch 6732: train loss: 0.13196467427849282, test loss: 0.26269929843881384\n",
      "epoch 6733: train loss: 0.13195929620206306, test loss: 0.26269878864014706\n",
      "epoch 6734: train loss: 0.1319539192674996, test loss: 0.26269827922260347\n",
      "epoch 6735: train loss: 0.13194854347437604, test loss: 0.26269777018601986\n",
      "epoch 6736: train loss: 0.1319431688222661, test loss: 0.26269726153023326\n",
      "epoch 6737: train loss: 0.1319377953107438, test loss: 0.2626967532550804\n",
      "epoch 6738: train loss: 0.13193242293938343, test loss: 0.26269624536039843\n",
      "epoch 6739: train loss: 0.13192705170775945, test loss: 0.26269573784602446\n",
      "epoch 6740: train loss: 0.1319216816154465, test loss: 0.2626952307117959\n",
      "epoch 6741: train loss: 0.13191631266201956, test loss: 0.26269472395754995\n",
      "epoch 6742: train loss: 0.13191094484705376, test loss: 0.26269421758312433\n",
      "epoch 6743: train loss: 0.13190557817012447, test loss: 0.2626937115883564\n",
      "epoch 6744: train loss: 0.13190021263080726, test loss: 0.26269320597308404\n",
      "epoch 6745: train loss: 0.131894848228678, test loss: 0.26269270073714496\n",
      "epoch 6746: train loss: 0.13188948496331268, test loss: 0.26269219588037707\n",
      "epoch 6747: train loss: 0.1318841228342876, test loss: 0.26269169140261844\n",
      "epoch 6748: train loss: 0.13187876184117928, test loss: 0.26269118730370716\n",
      "epoch 6749: train loss: 0.13187340198356437, test loss: 0.2626906835834816\n",
      "epoch 6750: train loss: 0.13186804326101983, test loss: 0.26269018024177987\n",
      "epoch 6751: train loss: 0.13186268567312287, test loss: 0.2626896772784406\n",
      "epoch 6752: train loss: 0.13185732921945081, test loss: 0.26268917469330233\n",
      "epoch 6753: train loss: 0.13185197389958134, test loss: 0.2626886724862036\n",
      "epoch 6754: train loss: 0.13184661971309222, test loss: 0.2626881706569833\n",
      "epoch 6755: train loss: 0.13184126665956158, test loss: 0.26268766920548026\n",
      "epoch 6756: train loss: 0.13183591473856765, test loss: 0.2626871681315334\n",
      "epoch 6757: train loss: 0.13183056394968898, test loss: 0.2626866674349819\n",
      "epoch 6758: train loss: 0.13182521429250427, test loss: 0.26268616711566495\n",
      "epoch 6759: train loss: 0.13181986576659246, test loss: 0.26268566717342173\n",
      "epoch 6760: train loss: 0.13181451837153277, test loss: 0.2626851676080918\n",
      "epoch 6761: train loss: 0.1318091721069045, test loss: 0.2626846684195145\n",
      "epoch 6762: train loss: 0.13180382697228732, test loss: 0.2626841696075296\n",
      "epoch 6763: train loss: 0.13179848296726115, test loss: 0.2626836711719766\n",
      "epoch 6764: train loss: 0.13179314009140594, test loss: 0.26268317311269557\n",
      "epoch 6765: train loss: 0.13178779834430204, test loss: 0.2626826754295263\n",
      "epoch 6766: train loss: 0.1317824577255299, test loss: 0.2626821781223088\n",
      "epoch 6767: train loss: 0.13177711823467034, test loss: 0.2626816811908833\n",
      "epoch 6768: train loss: 0.13177177987130417, test loss: 0.26268118463509\n",
      "epoch 6769: train loss: 0.1317664426350127, test loss: 0.26268068845476916\n",
      "epoch 6770: train loss: 0.13176110652537723, test loss: 0.26268019264976133\n",
      "epoch 6771: train loss: 0.1317557715419794, test loss: 0.262679697219907\n",
      "epoch 6772: train loss: 0.13175043768440103, test loss: 0.262679202165047\n",
      "epoch 6773: train loss: 0.13174510495222416, test loss: 0.2626787074850218\n",
      "epoch 6774: train loss: 0.13173977334503112, test loss: 0.2626782131796725\n",
      "epoch 6775: train loss: 0.13173444286240435, test loss: 0.2626777192488399\n",
      "epoch 6776: train loss: 0.1317291135039266, test loss: 0.26267722569236523\n",
      "epoch 6777: train loss: 0.13172378526918077, test loss: 0.2626767325100896\n",
      "epoch 6778: train loss: 0.13171845815775002, test loss: 0.2626762397018542\n",
      "epoch 6779: train loss: 0.1317131321692177, test loss: 0.26267574726750065\n",
      "epoch 6780: train loss: 0.1317078073031675, test loss: 0.2626752552068702\n",
      "epoch 6781: train loss: 0.1317024835591831, test loss: 0.2626747635198045\n",
      "epoch 6782: train loss: 0.13169716093684863, test loss: 0.2626742722061453\n",
      "epoch 6783: train loss: 0.1316918394357483, test loss: 0.2626737812657344\n",
      "epoch 6784: train loss: 0.13168651905546655, test loss: 0.26267329069841366\n",
      "epoch 6785: train loss: 0.1316811997955881, test loss: 0.26267280050402503\n",
      "epoch 6786: train loss: 0.13167588165569785, test loss: 0.2626723106824108\n",
      "epoch 6787: train loss: 0.13167056463538096, test loss: 0.262671821233413\n",
      "epoch 6788: train loss: 0.13166524873422272, test loss: 0.262671332156874\n",
      "epoch 6789: train loss: 0.13165993395180872, test loss: 0.26267084345263625\n",
      "epoch 6790: train loss: 0.13165462028772473, test loss: 0.26267035512054215\n",
      "epoch 6791: train loss: 0.13164930774155675, test loss: 0.2626698671604345\n",
      "epoch 6792: train loss: 0.13164399631289098, test loss: 0.2626693795721559\n",
      "epoch 6793: train loss: 0.13163868600131387, test loss: 0.2626688923555492\n",
      "epoch 6794: train loss: 0.13163337680641204, test loss: 0.2626684055104573\n",
      "epoch 6795: train loss: 0.1316280687277724, test loss: 0.26266791903672326\n",
      "epoch 6796: train loss: 0.13162276176498203, test loss: 0.2626674329341903\n",
      "epoch 6797: train loss: 0.1316174559176282, test loss: 0.2626669472027015\n",
      "epoch 6798: train loss: 0.13161215118529845, test loss: 0.2626664618421003\n",
      "epoch 6799: train loss: 0.1316068475675805, test loss: 0.26266597685223003\n",
      "epoch 6800: train loss: 0.1316015450640623, test loss: 0.26266549223293434\n",
      "epoch 6801: train loss: 0.13159624367433198, test loss: 0.2626650079840568\n",
      "epoch 6802: train loss: 0.13159094339797806, test loss: 0.26266452410544117\n",
      "epoch 6803: train loss: 0.13158564423458902, test loss: 0.2626640405969313\n",
      "epoch 6804: train loss: 0.13158034618375367, test loss: 0.2626635574583711\n",
      "epoch 6805: train loss: 0.1315750492450611, test loss: 0.2626630746896047\n",
      "epoch 6806: train loss: 0.13156975341810054, test loss: 0.26266259229047617\n",
      "epoch 6807: train loss: 0.13156445870246145, test loss: 0.26266211026082975\n",
      "epoch 6808: train loss: 0.1315591650977335, test loss: 0.26266162860050984\n",
      "epoch 6809: train loss: 0.13155387260350657, test loss: 0.26266114730936096\n",
      "epoch 6810: train loss: 0.13154858121937077, test loss: 0.26266066638722746\n",
      "epoch 6811: train loss: 0.13154329094491646, test loss: 0.2626601858339541\n",
      "epoch 6812: train loss: 0.13153800177973415, test loss: 0.2626597056493857\n",
      "epoch 6813: train loss: 0.13153271372341463, test loss: 0.262659225833367\n",
      "epoch 6814: train loss: 0.1315274267755488, test loss: 0.26265874638574294\n",
      "epoch 6815: train loss: 0.13152214093572792, test loss: 0.2626582673063586\n",
      "epoch 6816: train loss: 0.1315168562035433, test loss: 0.2626577885950593\n",
      "epoch 6817: train loss: 0.13151157257858664, test loss: 0.2626573102516901\n",
      "epoch 6818: train loss: 0.13150629006044967, test loss: 0.2626568322760964\n",
      "epoch 6819: train loss: 0.1315010086487245, test loss: 0.26265635466812354\n",
      "epoch 6820: train loss: 0.13149572834300335, test loss: 0.26265587742761726\n",
      "epoch 6821: train loss: 0.1314904491428787, test loss: 0.26265540055442316\n",
      "epoch 6822: train loss: 0.13148517104794322, test loss: 0.262654924048387\n",
      "epoch 6823: train loss: 0.13147989405778981, test loss: 0.26265444790935455\n",
      "epoch 6824: train loss: 0.13147461817201156, test loss: 0.26265397213717184\n",
      "epoch 6825: train loss: 0.13146934339020178, test loss: 0.2626534967316848\n",
      "epoch 6826: train loss: 0.13146406971195404, test loss: 0.2626530216927398\n",
      "epoch 6827: train loss: 0.13145879713686204, test loss: 0.262652547020183\n",
      "epoch 6828: train loss: 0.13145352566451976, test loss: 0.2626520727138606\n",
      "epoch 6829: train loss: 0.13144825529452134, test loss: 0.26265159877361915\n",
      "epoch 6830: train loss: 0.13144298602646118, test loss: 0.2626511251993052\n",
      "epoch 6831: train loss: 0.1314377178599339, test loss: 0.2626506519907655\n",
      "epoch 6832: train loss: 0.13143245079453425, test loss: 0.26265017914784666\n",
      "epoch 6833: train loss: 0.13142718482985727, test loss: 0.26264970667039544\n",
      "epoch 6834: train loss: 0.1314219199654982, test loss: 0.26264923455825906\n",
      "epoch 6835: train loss: 0.13141665620105245, test loss: 0.26264876281128435\n",
      "epoch 6836: train loss: 0.13141139353611572, test loss: 0.26264829142931845\n",
      "epoch 6837: train loss: 0.1314061319702838, test loss: 0.26264782041220863\n",
      "epoch 6838: train loss: 0.1314008715031528, test loss: 0.2626473497598023\n",
      "epoch 6839: train loss: 0.13139561213431902, test loss: 0.2626468794719468\n",
      "epoch 6840: train loss: 0.13139035386337894, test loss: 0.2626464095484897\n",
      "epoch 6841: train loss: 0.13138509668992923, test loss: 0.26264593998927865\n",
      "epoch 6842: train loss: 0.13137984061356686, test loss: 0.26264547079416134\n",
      "epoch 6843: train loss: 0.13137458563388893, test loss: 0.26264500196298557\n",
      "epoch 6844: train loss: 0.13136933175049278, test loss: 0.2626445334955993\n",
      "epoch 6845: train loss: 0.13136407896297594, test loss: 0.2626440653918506\n",
      "epoch 6846: train loss: 0.13135882727093617, test loss: 0.26264359765158746\n",
      "epoch 6847: train loss: 0.13135357667397143, test loss: 0.2626431302746582\n",
      "epoch 6848: train loss: 0.13134832717167996, test loss: 0.2626426632609111\n",
      "epoch 6849: train loss: 0.13134307876366008, test loss: 0.2626421966101946\n",
      "epoch 6850: train loss: 0.1313378314495104, test loss: 0.26264173032235716\n",
      "epoch 6851: train loss: 0.13133258522882968, test loss: 0.2626412643972475\n",
      "epoch 6852: train loss: 0.131327340101217, test loss: 0.2626407988347141\n",
      "epoch 6853: train loss: 0.13132209606627154, test loss: 0.2626403336346059\n",
      "epoch 6854: train loss: 0.13131685312359273, test loss: 0.2626398687967719\n",
      "epoch 6855: train loss: 0.13131161127278024, test loss: 0.2626394043210609\n",
      "epoch 6856: train loss: 0.1313063705134339, test loss: 0.2626389402073221\n",
      "epoch 6857: train loss: 0.13130113084515377, test loss: 0.2626384764554046\n",
      "epoch 6858: train loss: 0.1312958922675401, test loss: 0.2626380130651577\n",
      "epoch 6859: train loss: 0.13129065478019336, test loss: 0.26263755003643097\n",
      "epoch 6860: train loss: 0.13128541838271424, test loss: 0.2626370873690736\n",
      "epoch 6861: train loss: 0.13128018307470363, test loss: 0.2626366250629353\n",
      "epoch 6862: train loss: 0.13127494885576263, test loss: 0.2626361631178657\n",
      "epoch 6863: train loss: 0.13126971572549254, test loss: 0.2626357015337147\n",
      "epoch 6864: train loss: 0.13126448368349483, test loss: 0.262635240310332\n",
      "epoch 6865: train loss: 0.13125925272937128, test loss: 0.2626347794475676\n",
      "epoch 6866: train loss: 0.13125402286272378, test loss: 0.26263431894527156\n",
      "epoch 6867: train loss: 0.13124879408315449, test loss: 0.26263385880329404\n",
      "epoch 6868: train loss: 0.1312435663902657, test loss: 0.2626333990214853\n",
      "epoch 6869: train loss: 0.13123833978365998, test loss: 0.2626329395996956\n",
      "epoch 6870: train loss: 0.13123311426294007, test loss: 0.26263248053777544\n",
      "epoch 6871: train loss: 0.13122788982770894, test loss: 0.2626320218355754\n",
      "epoch 6872: train loss: 0.1312226664775698, test loss: 0.2626315634929459\n",
      "epoch 6873: train loss: 0.13121744421212594, test loss: 0.26263110550973795\n",
      "epoch 6874: train loss: 0.13121222303098098, test loss: 0.26263064788580215\n",
      "epoch 6875: train loss: 0.1312070029337387, test loss: 0.2626301906209895\n",
      "epoch 6876: train loss: 0.1312017839200031, test loss: 0.262629733715151\n",
      "epoch 6877: train loss: 0.1311965659893783, test loss: 0.26262927716813766\n",
      "epoch 6878: train loss: 0.1311913491414688, test loss: 0.26262882097980084\n",
      "epoch 6879: train loss: 0.13118613337587914, test loss: 0.2626283651499917\n",
      "epoch 6880: train loss: 0.13118091869221415, test loss: 0.2626279096785617\n",
      "epoch 6881: train loss: 0.13117570509007886, test loss: 0.2626274545653623\n",
      "epoch 6882: train loss: 0.13117049256907848, test loss: 0.262626999810245\n",
      "epoch 6883: train loss: 0.13116528112881842, test loss: 0.2626265454130616\n",
      "epoch 6884: train loss: 0.1311600707689043, test loss: 0.2626260913736638\n",
      "epoch 6885: train loss: 0.13115486148894198, test loss: 0.2626256376919034\n",
      "epoch 6886: train loss: 0.1311496532885375, test loss: 0.2626251843676324\n",
      "epoch 6887: train loss: 0.13114444616729706, test loss: 0.2626247314007029\n",
      "epoch 6888: train loss: 0.13113924012482717, test loss: 0.262624278790967\n",
      "epoch 6889: train loss: 0.13113403516073444, test loss: 0.26262382653827687\n",
      "epoch 6890: train loss: 0.1311288312746257, test loss: 0.2626233746424849\n",
      "epoch 6891: train loss: 0.13112362846610806, test loss: 0.2626229231034436\n",
      "epoch 6892: train loss: 0.1311184267347888, test loss: 0.2626224719210053\n",
      "epoch 6893: train loss: 0.13111322608027529, test loss: 0.2626220210950228\n",
      "epoch 6894: train loss: 0.13110802650217526, test loss: 0.26262157062534863\n",
      "epoch 6895: train loss: 0.1311028280000966, test loss: 0.2626211205118356\n",
      "epoch 6896: train loss: 0.13109763057364734, test loss: 0.26262067075433676\n",
      "epoch 6897: train loss: 0.13109243422243577, test loss: 0.26262022135270496\n",
      "epoch 6898: train loss: 0.13108723894607038, test loss: 0.26261977230679334\n",
      "epoch 6899: train loss: 0.13108204474415985, test loss: 0.26261932361645496\n",
      "epoch 6900: train loss: 0.13107685161631305, test loss: 0.26261887528154326\n",
      "epoch 6901: train loss: 0.1310716595621391, test loss: 0.26261842730191143\n",
      "epoch 6902: train loss: 0.13106646858124726, test loss: 0.26261797967741296\n",
      "epoch 6903: train loss: 0.131061278673247, test loss: 0.2626175324079015\n",
      "epoch 6904: train loss: 0.13105608983774808, test loss: 0.26261708549323054\n",
      "epoch 6905: train loss: 0.13105090207436032, test loss: 0.2626166389332539\n",
      "epoch 6906: train loss: 0.13104571538269386, test loss: 0.2626161927278253\n",
      "epoch 6907: train loss: 0.131040529762359, test loss: 0.26261574687679873\n",
      "epoch 6908: train loss: 0.13103534521296623, test loss: 0.2626153013800282\n",
      "epoch 6909: train loss: 0.13103016173412624, test loss: 0.2626148562373678\n",
      "epoch 6910: train loss: 0.13102497932545, test loss: 0.26261441144867176\n",
      "epoch 6911: train loss: 0.13101979798654848, test loss: 0.2626139670137942\n",
      "epoch 6912: train loss: 0.1310146177170331, test loss: 0.2626135229325896\n",
      "epoch 6913: train loss: 0.13100943851651528, test loss: 0.2626130792049125\n",
      "epoch 6914: train loss: 0.13100426038460683, test loss: 0.26261263583061734\n",
      "epoch 6915: train loss: 0.13099908332091956, test loss: 0.2626121928095588\n",
      "epoch 6916: train loss: 0.1309939073250656, test loss: 0.26261175014159155\n",
      "epoch 6917: train loss: 0.1309887323966573, test loss: 0.2626113078265706\n",
      "epoch 6918: train loss: 0.13098355853530708, test loss: 0.2626108658643508\n",
      "epoch 6919: train loss: 0.13097838574062773, test loss: 0.262610424254787\n",
      "epoch 6920: train loss: 0.1309732140122321, test loss: 0.2626099829977345\n",
      "epoch 6921: train loss: 0.13096804334973336, test loss: 0.2626095420930485\n",
      "epoch 6922: train loss: 0.13096287375274474, test loss: 0.26260910154058414\n",
      "epoch 6923: train loss: 0.1309577052208798, test loss: 0.26260866134019695\n",
      "epoch 6924: train loss: 0.1309525377537522, test loss: 0.2626082214917423\n",
      "epoch 6925: train loss: 0.13094737135097584, test loss: 0.2626077819950759\n",
      "epoch 6926: train loss: 0.13094220601216489, test loss: 0.26260734285005316\n",
      "epoch 6927: train loss: 0.13093704173693357, test loss: 0.26260690405653003\n",
      "epoch 6928: train loss: 0.1309318785248964, test loss: 0.26260646561436224\n",
      "epoch 6929: train loss: 0.13092671637566813, test loss: 0.2626060275234058\n",
      "epoch 6930: train loss: 0.13092155528886362, test loss: 0.2626055897835166\n",
      "epoch 6931: train loss: 0.13091639526409796, test loss: 0.26260515239455084\n",
      "epoch 6932: train loss: 0.1309112363009864, test loss: 0.2626047153563648\n",
      "epoch 6933: train loss: 0.1309060783991445, test loss: 0.26260427866881453\n",
      "epoch 6934: train loss: 0.13090092155818792, test loss: 0.26260384233175665\n",
      "epoch 6935: train loss: 0.13089576577773254, test loss: 0.26260340634504753\n",
      "epoch 6936: train loss: 0.13089061105739447, test loss: 0.26260297070854366\n",
      "epoch 6937: train loss: 0.13088545739678997, test loss: 0.2626025354221017\n",
      "epoch 6938: train loss: 0.13088030479553553, test loss: 0.2626021004855786\n",
      "epoch 6939: train loss: 0.1308751532532478, test loss: 0.26260166589883094\n",
      "epoch 6940: train loss: 0.1308700027695437, test loss: 0.26260123166171573\n",
      "epoch 6941: train loss: 0.13086485334404024, test loss: 0.26260079777408996\n",
      "epoch 6942: train loss: 0.13085970497635474, test loss: 0.2626003642358108\n",
      "epoch 6943: train loss: 0.13085455766610463, test loss: 0.26259993104673535\n",
      "epoch 6944: train loss: 0.1308494114129076, test loss: 0.2625994982067209\n",
      "epoch 6945: train loss: 0.13084426621638146, test loss: 0.2625990657156249\n",
      "epoch 6946: train loss: 0.13083912207614434, test loss: 0.26259863357330465\n",
      "epoch 6947: train loss: 0.13083397899181443, test loss: 0.2625982017796178\n",
      "epoch 6948: train loss: 0.13082883696301015, test loss: 0.262597770334422\n",
      "epoch 6949: train loss: 0.1308236959893502, test loss: 0.262597339237575\n",
      "epoch 6950: train loss: 0.13081855607045342, test loss: 0.26259690848893447\n",
      "epoch 6951: train loss: 0.1308134172059388, test loss: 0.2625964780883585\n",
      "epoch 6952: train loss: 0.1308082793954256, test loss: 0.2625960480357048\n",
      "epoch 6953: train loss: 0.13080314263853324, test loss: 0.2625956183308318\n",
      "epoch 6954: train loss: 0.13079800693488133, test loss: 0.26259518897359746\n",
      "epoch 6955: train loss: 0.1307928722840897, test loss: 0.26259475996386006\n",
      "epoch 6956: train loss: 0.1307877386857783, test loss: 0.26259433130147797\n",
      "epoch 6957: train loss: 0.13078260613956744, test loss: 0.26259390298630964\n",
      "epoch 6958: train loss: 0.13077747464507744, test loss: 0.26259347501821345\n",
      "epoch 6959: train loss: 0.1307723442019289, test loss: 0.26259304739704825\n",
      "epoch 6960: train loss: 0.13076721480974265, test loss: 0.2625926201226726\n",
      "epoch 6961: train loss: 0.13076208646813964, test loss: 0.2625921931949452\n",
      "epoch 6962: train loss: 0.13075695917674107, test loss: 0.26259176661372513\n",
      "epoch 6963: train loss: 0.13075183293516826, test loss: 0.2625913403788712\n",
      "epoch 6964: train loss: 0.13074670774304284, test loss: 0.26259091449024263\n",
      "epoch 6965: train loss: 0.13074158359998653, test loss: 0.2625904889476983\n",
      "epoch 6966: train loss: 0.13073646050562132, test loss: 0.26259006375109767\n",
      "epoch 6967: train loss: 0.1307313384595693, test loss: 0.2625896389002999\n",
      "epoch 6968: train loss: 0.13072621746145285, test loss: 0.2625892143951646\n",
      "epoch 6969: train loss: 0.1307210975108945, test loss: 0.262588790235551\n",
      "epoch 6970: train loss: 0.13071597860751696, test loss: 0.2625883664213188\n",
      "epoch 6971: train loss: 0.13071086075094318, test loss: 0.2625879429523276\n",
      "epoch 6972: train loss: 0.13070574394079626, test loss: 0.26258751982843737\n",
      "epoch 6973: train loss: 0.13070062817669947, test loss: 0.2625870970495077\n",
      "epoch 6974: train loss: 0.13069551345827635, test loss: 0.26258667461539864\n",
      "epoch 6975: train loss: 0.1306903997851506, test loss: 0.26258625252597023\n",
      "epoch 6976: train loss: 0.13068528715694605, test loss: 0.2625858307810825\n",
      "epoch 6977: train loss: 0.13068017557328684, test loss: 0.26258540938059555\n",
      "epoch 6978: train loss: 0.13067506503379722, test loss: 0.26258498832436994\n",
      "epoch 6979: train loss: 0.13066995553810162, test loss: 0.26258456761226573\n",
      "epoch 6980: train loss: 0.1306648470858247, test loss: 0.26258414724414353\n",
      "epoch 6981: train loss: 0.13065973967659136, test loss: 0.2625837272198639\n",
      "epoch 6982: train loss: 0.13065463331002655, test loss: 0.2625833075392873\n",
      "epoch 6983: train loss: 0.13064952798575555, test loss: 0.2625828882022746\n",
      "epoch 6984: train loss: 0.13064442370340376, test loss: 0.26258246920868655\n",
      "epoch 6985: train loss: 0.13063932046259683, test loss: 0.262582050558384\n",
      "epoch 6986: train loss: 0.1306342182629605, test loss: 0.262581632251228\n",
      "epoch 6987: train loss: 0.13062911710412084, test loss: 0.2625812142870794\n",
      "epoch 6988: train loss: 0.13062401698570394, test loss: 0.26258079666579964\n",
      "epoch 6989: train loss: 0.13061891790733623, test loss: 0.26258037938724976\n",
      "epoch 6990: train loss: 0.1306138198686443, test loss: 0.2625799624512911\n",
      "epoch 6991: train loss: 0.13060872286925482, test loss: 0.26257954585778503\n",
      "epoch 6992: train loss: 0.13060362690879482, test loss: 0.26257912960659313\n",
      "epoch 6993: train loss: 0.13059853198689142, test loss: 0.26257871369757685\n",
      "epoch 6994: train loss: 0.13059343810317192, test loss: 0.2625782981305979\n",
      "epoch 6995: train loss: 0.13058834525726384, test loss: 0.26257788290551815\n",
      "epoch 6996: train loss: 0.1305832534487949, test loss: 0.2625774680221992\n",
      "epoch 6997: train loss: 0.13057816267739303, test loss: 0.26257705348050314\n",
      "epoch 6998: train loss: 0.13057307294268622, test loss: 0.2625766392802918\n",
      "epoch 6999: train loss: 0.13056798424430283, test loss: 0.2625762254214274\n",
      "epoch 7000: train loss: 0.13056289658187134, test loss: 0.26257581190377216\n",
      "epoch 7001: train loss: 0.1305578099550203, test loss: 0.26257539872718816\n",
      "epoch 7002: train loss: 0.13055272436337867, test loss: 0.2625749858915379\n",
      "epoch 7003: train loss: 0.1305476398065754, test loss: 0.2625745733966837\n",
      "epoch 7004: train loss: 0.1305425562842398, test loss: 0.262574161242488\n",
      "epoch 7005: train loss: 0.13053747379600117, test loss: 0.2625737494288136\n",
      "epoch 7006: train loss: 0.13053239234148922, test loss: 0.262573337955523\n",
      "epoch 7007: train loss: 0.13052731192033365, test loss: 0.2625729268224791\n",
      "epoch 7008: train loss: 0.1305222325321645, test loss: 0.26257251602954473\n",
      "epoch 7009: train loss: 0.13051715417661186, test loss: 0.2625721055765827\n",
      "epoch 7010: train loss: 0.13051207685330618, test loss: 0.2625716954634562\n",
      "epoch 7011: train loss: 0.13050700056187792, test loss: 0.2625712856900282\n",
      "epoch 7012: train loss: 0.13050192530195784, test loss: 0.26257087625616193\n",
      "epoch 7013: train loss: 0.13049685107317688, test loss: 0.2625704671617206\n",
      "epoch 7014: train loss: 0.1304917778751661, test loss: 0.2625700584065677\n",
      "epoch 7015: train loss: 0.13048670570755683, test loss: 0.26256964999056664\n",
      "epoch 7016: train loss: 0.13048163456998055, test loss: 0.26256924191358083\n",
      "epoch 7017: train loss: 0.1304765644620689, test loss: 0.262568834175474\n",
      "epoch 7018: train loss: 0.1304714953834538, test loss: 0.2625684267761097\n",
      "epoch 7019: train loss: 0.13046642733376715, test loss: 0.2625680197153519\n",
      "epoch 7020: train loss: 0.13046136031264133, test loss: 0.2625676129930643\n",
      "epoch 7021: train loss: 0.13045629431970865, test loss: 0.2625672066091109\n",
      "epoch 7022: train loss: 0.13045122935460182, test loss: 0.2625668005633558\n",
      "epoch 7023: train loss: 0.1304461654169535, test loss: 0.26256639485566297\n",
      "epoch 7024: train loss: 0.13044110250639676, test loss: 0.2625659894858967\n",
      "epoch 7025: train loss: 0.13043604062256475, test loss: 0.2625655844539212\n",
      "epoch 7026: train loss: 0.1304309797650908, test loss: 0.2625651797596009\n",
      "epoch 7027: train loss: 0.13042591993360844, test loss: 0.26256477540280027\n",
      "epoch 7028: train loss: 0.13042086112775142, test loss: 0.2625643713833838\n",
      "epoch 7029: train loss: 0.13041580334715355, test loss: 0.262563967701216\n",
      "epoch 7030: train loss: 0.13041074659144905, test loss: 0.2625635643561617\n",
      "epoch 7031: train loss: 0.13040569086027212, test loss: 0.2625631613480856\n",
      "epoch 7032: train loss: 0.13040063615325728, test loss: 0.26256275867685264\n",
      "epoch 7033: train loss: 0.13039558247003913, test loss: 0.26256235634232766\n",
      "epoch 7034: train loss: 0.13039052981025248, test loss: 0.26256195434437574\n",
      "epoch 7035: train loss: 0.1303854781735324, test loss: 0.262561552682862\n",
      "epoch 7036: train loss: 0.13038042755951412, test loss: 0.2625611513576516\n",
      "epoch 7037: train loss: 0.13037537796783297, test loss: 0.26256075036860993\n",
      "epoch 7038: train loss: 0.13037032939812454, test loss: 0.2625603497156021\n",
      "epoch 7039: train loss: 0.13036528185002458, test loss: 0.2625599493984938\n",
      "epoch 7040: train loss: 0.13036023532316907, test loss: 0.2625595494171505\n",
      "epoch 7041: train loss: 0.1303551898171941, test loss: 0.2625591497714377\n",
      "epoch 7042: train loss: 0.130350145331736, test loss: 0.26255875046122107\n",
      "epoch 7043: train loss: 0.13034510186643122, test loss: 0.2625583514863665\n",
      "epoch 7044: train loss: 0.1303400594209165, test loss: 0.2625579528467398\n",
      "epoch 7045: train loss: 0.13033501799482872, test loss: 0.26255755454220686\n",
      "epoch 7046: train loss: 0.13032997758780485, test loss: 0.2625571565726338\n",
      "epoch 7047: train loss: 0.13032493819948215, test loss: 0.2625567589378866\n",
      "epoch 7048: train loss: 0.13031989982949807, test loss: 0.2625563616378315\n",
      "epoch 7049: train loss: 0.1303148624774902, test loss: 0.2625559646723347\n",
      "epoch 7050: train loss: 0.1303098261430963, test loss: 0.2625555680412626\n",
      "epoch 7051: train loss: 0.13030479082595434, test loss: 0.2625551717444816\n",
      "epoch 7052: train loss: 0.13029975652570247, test loss: 0.26255477578185826\n",
      "epoch 7053: train loss: 0.130294723241979, test loss: 0.26255438015325905\n",
      "epoch 7054: train loss: 0.13028969097442247, test loss: 0.2625539848585508\n",
      "epoch 7055: train loss: 0.1302846597226716, test loss: 0.26255358989760014\n",
      "epoch 7056: train loss: 0.13027962948636523, test loss: 0.2625531952702739\n",
      "epoch 7057: train loss: 0.1302746002651424, test loss: 0.26255280097643896\n",
      "epoch 7058: train loss: 0.13026957205864245, test loss: 0.26255240701596244\n",
      "epoch 7059: train loss: 0.1302645448665047, test loss: 0.2625520133887113\n",
      "epoch 7060: train loss: 0.13025951868836885, test loss: 0.26255162009455274\n",
      "epoch 7061: train loss: 0.1302544935238746, test loss: 0.26255122713335405\n",
      "epoch 7062: train loss: 0.13024946937266196, test loss: 0.26255083450498246\n",
      "epoch 7063: train loss: 0.13024444623437112, test loss: 0.26255044220930546\n",
      "epoch 7064: train loss: 0.13023942410864237, test loss: 0.2625500502461904\n",
      "epoch 7065: train loss: 0.13023440299511624, test loss: 0.2625496586155049\n",
      "epoch 7066: train loss: 0.13022938289343347, test loss: 0.26254926731711664\n",
      "epoch 7067: train loss: 0.13022436380323488, test loss: 0.26254887635089325\n",
      "epoch 7068: train loss: 0.13021934572416152, test loss: 0.26254848571670264\n",
      "epoch 7069: train loss: 0.13021432865585472, test loss: 0.26254809541441265\n",
      "epoch 7070: train loss: 0.13020931259795585, test loss: 0.26254770544389117\n",
      "epoch 7071: train loss: 0.13020429755010648, test loss: 0.2625473158050064\n",
      "epoch 7072: train loss: 0.13019928351194845, test loss: 0.26254692649762623\n",
      "epoch 7073: train loss: 0.13019427048312374, test loss: 0.26254653752161905\n",
      "epoch 7074: train loss: 0.13018925846327442, test loss: 0.2625461488768531\n",
      "epoch 7075: train loss: 0.13018424745204288, test loss: 0.2625457605631968\n",
      "epoch 7076: train loss: 0.13017923744907162, test loss: 0.2625453725805184\n",
      "epoch 7077: train loss: 0.13017422845400328, test loss: 0.2625449849286866\n",
      "epoch 7078: train loss: 0.1301692204664808, test loss: 0.26254459760756993\n",
      "epoch 7079: train loss: 0.13016421348614718, test loss: 0.2625442106170371\n",
      "epoch 7080: train loss: 0.13015920751264567, test loss: 0.26254382395695686\n",
      "epoch 7081: train loss: 0.13015420254561963, test loss: 0.26254343762719806\n",
      "epoch 7082: train loss: 0.1301491985847127, test loss: 0.2625430516276295\n",
      "epoch 7083: train loss: 0.1301441956295686, test loss: 0.2625426659581204\n",
      "epoch 7084: train loss: 0.13013919367983134, test loss: 0.2625422806185396\n",
      "epoch 7085: train loss: 0.13013419273514498, test loss: 0.2625418956087565\n",
      "epoch 7086: train loss: 0.13012919279515386, test loss: 0.2625415109286402\n",
      "epoch 7087: train loss: 0.13012419385950247, test loss: 0.26254112657805995\n",
      "epoch 7088: train loss: 0.13011919592783538, test loss: 0.2625407425568853\n",
      "epoch 7089: train loss: 0.13011419899979756, test loss: 0.26254035886498567\n",
      "epoch 7090: train loss: 0.13010920307503399, test loss: 0.2625399755022306\n",
      "epoch 7091: train loss: 0.1301042081531898, test loss: 0.26253959246848974\n",
      "epoch 7092: train loss: 0.1300992142339104, test loss: 0.2625392097636327\n",
      "epoch 7093: train loss: 0.13009422131684137, test loss: 0.2625388273875293\n",
      "epoch 7094: train loss: 0.13008922940162845, test loss: 0.26253844534004955\n",
      "epoch 7095: train loss: 0.13008423848791753, test loss: 0.2625380636210633\n",
      "epoch 7096: train loss: 0.1300792485753547, test loss: 0.2625376822304406\n",
      "epoch 7097: train loss: 0.13007425966358618, test loss: 0.2625373011680515\n",
      "epoch 7098: train loss: 0.13006927175225846, test loss: 0.2625369204337662\n",
      "epoch 7099: train loss: 0.13006428484101817, test loss: 0.26253654002745497\n",
      "epoch 7100: train loss: 0.13005929892951207, test loss: 0.2625361599489882\n",
      "epoch 7101: train loss: 0.13005431401738718, test loss: 0.26253578019823626\n",
      "epoch 7102: train loss: 0.13004933010429062, test loss: 0.2625354007750696\n",
      "epoch 7103: train loss: 0.13004434718986974, test loss: 0.26253502167935894\n",
      "epoch 7104: train loss: 0.13003936527377205, test loss: 0.2625346429109748\n",
      "epoch 7105: train loss: 0.1300343843556452, test loss: 0.2625342644697879\n",
      "epoch 7106: train loss: 0.13002940443513708, test loss: 0.2625338863556691\n",
      "epoch 7107: train loss: 0.13002442551189575, test loss: 0.26253350856848934\n",
      "epoch 7108: train loss: 0.13001944758556935, test loss: 0.26253313110811943\n",
      "epoch 7109: train loss: 0.13001447065580635, test loss: 0.26253275397443054\n",
      "epoch 7110: train loss: 0.1300094947222553, test loss: 0.26253237716729383\n",
      "epoch 7111: train loss: 0.13000451978456484, test loss: 0.26253200068658034\n",
      "epoch 7112: train loss: 0.12999954584238405, test loss: 0.2625316245321614\n",
      "epoch 7113: train loss: 0.12999457289536193, test loss: 0.2625312487039085\n",
      "epoch 7114: train loss: 0.12998960094314774, test loss: 0.26253087320169277\n",
      "epoch 7115: train loss: 0.12998462998539098, test loss: 0.262530498025386\n",
      "epoch 7116: train loss: 0.12997966002174124, test loss: 0.2625301231748597\n",
      "epoch 7117: train loss: 0.12997469105184833, test loss: 0.2625297486499854\n",
      "epoch 7118: train loss: 0.12996972307536225, test loss: 0.262529374450635\n",
      "epoch 7119: train loss: 0.1299647560919331, test loss: 0.26252900057668027\n",
      "epoch 7120: train loss: 0.1299597901012112, test loss: 0.26252862702799307\n",
      "epoch 7121: train loss: 0.1299548251028471, test loss: 0.26252825380444533\n",
      "epoch 7122: train loss: 0.1299498610964914, test loss: 0.2625278809059092\n",
      "epoch 7123: train loss: 0.12994489808179505, test loss: 0.26252750833225674\n",
      "epoch 7124: train loss: 0.12993993605840898, test loss: 0.26252713608336015\n",
      "epoch 7125: train loss: 0.12993497502598442, test loss: 0.26252676415909165\n",
      "epoch 7126: train loss: 0.12993001498417275, test loss: 0.26252639255932375\n",
      "epoch 7127: train loss: 0.12992505593262554, test loss: 0.2625260212839287\n",
      "epoch 7128: train loss: 0.12992009787099446, test loss: 0.2625256503327791\n",
      "epoch 7129: train loss: 0.1299151407989314, test loss: 0.2625252797057475\n",
      "epoch 7130: train loss: 0.12991018471608848, test loss: 0.2625249094027067\n",
      "epoch 7131: train loss: 0.12990522962211795, test loss: 0.26252453942352916\n",
      "epoch 7132: train loss: 0.1299002755166722, test loss: 0.2625241697680879\n",
      "epoch 7133: train loss: 0.12989532239940377, test loss: 0.2625238004362557\n",
      "epoch 7134: train loss: 0.12989037026996553, test loss: 0.26252343142790563\n",
      "epoch 7135: train loss: 0.1298854191280103, test loss: 0.2625230627429106\n",
      "epoch 7136: train loss: 0.1298804689731913, test loss: 0.26252269438114384\n",
      "epoch 7137: train loss: 0.1298755198051618, test loss: 0.26252232634247846\n",
      "epoch 7138: train loss: 0.12987057162357515, test loss: 0.26252195862678773\n",
      "epoch 7139: train loss: 0.12986562442808508, test loss: 0.26252159123394503\n",
      "epoch 7140: train loss: 0.12986067821834538, test loss: 0.26252122416382373\n",
      "epoch 7141: train loss: 0.12985573299401001, test loss: 0.26252085741629744\n",
      "epoch 7142: train loss: 0.12985078875473313, test loss: 0.26252049099123953\n",
      "epoch 7143: train loss: 0.12984584550016906, test loss: 0.26252012488852383\n",
      "epoch 7144: train loss: 0.1298409032299723, test loss: 0.26251975910802394\n",
      "epoch 7145: train loss: 0.12983596194379746, test loss: 0.26251939364961374\n",
      "epoch 7146: train loss: 0.12983102164129945, test loss: 0.26251902851316705\n",
      "epoch 7147: train loss: 0.12982608232213327, test loss: 0.2625186636985578\n",
      "epoch 7148: train loss: 0.1298211439859541, test loss: 0.26251829920566\n",
      "epoch 7149: train loss: 0.12981620663241727, test loss: 0.26251793503434784\n",
      "epoch 7150: train loss: 0.12981127026117834, test loss: 0.2625175711844955\n",
      "epoch 7151: train loss: 0.12980633487189294, test loss: 0.26251720765597697\n",
      "epoch 7152: train loss: 0.12980140046421704, test loss: 0.2625168444486669\n",
      "epoch 7153: train loss: 0.12979646703780662, test loss: 0.2625164815624395\n",
      "epoch 7154: train loss: 0.1297915345923179, test loss: 0.26251611899716926\n",
      "epoch 7155: train loss: 0.1297866031274073, test loss: 0.2625157567527308\n",
      "epoch 7156: train loss: 0.12978167264273133, test loss: 0.26251539482899866\n",
      "epoch 7157: train loss: 0.12977674313794674, test loss: 0.2625150332258475\n",
      "epoch 7158: train loss: 0.12977181461271037, test loss: 0.2625146719431522\n",
      "epoch 7159: train loss: 0.12976688706667935, test loss: 0.26251431098078754\n",
      "epoch 7160: train loss: 0.12976196049951094, test loss: 0.2625139503386284\n",
      "epoch 7161: train loss: 0.1297570349108625, test loss: 0.2625135900165499\n",
      "epoch 7162: train loss: 0.12975211030039163, test loss: 0.26251323001442695\n",
      "epoch 7163: train loss: 0.1297471866677561, test loss: 0.26251287033213483\n",
      "epoch 7164: train loss: 0.12974226401261377, test loss: 0.2625125109695487\n",
      "epoch 7165: train loss: 0.12973734233462278, test loss: 0.26251215192654376\n",
      "epoch 7166: train loss: 0.1297324216334414, test loss: 0.26251179320299556\n",
      "epoch 7167: train loss: 0.12972750190872798, test loss: 0.2625114347987793\n",
      "epoch 7168: train loss: 0.12972258316014124, test loss: 0.26251107671377066\n",
      "epoch 7169: train loss: 0.12971766538733986, test loss: 0.26251071894784517\n",
      "epoch 7170: train loss: 0.12971274858998283, test loss: 0.2625103615008786\n",
      "epoch 7171: train loss: 0.12970783276772924, test loss: 0.26251000437274646\n",
      "epoch 7172: train loss: 0.12970291792023836, test loss: 0.2625096475633248\n",
      "epoch 7173: train loss: 0.12969800404716963, test loss: 0.2625092910724893\n",
      "epoch 7174: train loss: 0.12969309114818273, test loss: 0.262508934900116\n",
      "epoch 7175: train loss: 0.12968817922293738, test loss: 0.2625085790460809\n",
      "epoch 7176: train loss: 0.12968326827109355, test loss: 0.26250822351026015\n",
      "epoch 7177: train loss: 0.1296783582923114, test loss: 0.26250786829252987\n",
      "epoch 7178: train loss: 0.12967344928625113, test loss: 0.26250751339276635\n",
      "epoch 7179: train loss: 0.12966854125257332, test loss: 0.26250715881084585\n",
      "epoch 7180: train loss: 0.12966363419093854, test loss: 0.26250680454664477\n",
      "epoch 7181: train loss: 0.1296587281010076, test loss: 0.26250645060003974\n",
      "epoch 7182: train loss: 0.12965382298244144, test loss: 0.26250609697090704\n",
      "epoch 7183: train loss: 0.12964891883490123, test loss: 0.2625057436591234\n",
      "epoch 7184: train loss: 0.12964401565804828, test loss: 0.2625053906645655\n",
      "epoch 7185: train loss: 0.129639113451544, test loss: 0.26250503798711017\n",
      "epoch 7186: train loss: 0.12963421221505012, test loss: 0.2625046856266342\n",
      "epoch 7187: train loss: 0.12962931194822835, test loss: 0.26250433358301445\n",
      "epoch 7188: train loss: 0.12962441265074076, test loss: 0.26250398185612783\n",
      "epoch 7189: train loss: 0.12961951432224944, test loss: 0.26250363044585157\n",
      "epoch 7190: train loss: 0.12961461696241666, test loss: 0.26250327935206275\n",
      "epoch 7191: train loss: 0.129609720570905, test loss: 0.2625029285746383\n",
      "epoch 7192: train loss: 0.129604825147377, test loss: 0.26250257811345584\n",
      "epoch 7193: train loss: 0.12959993069149556, test loss: 0.26250222796839257\n",
      "epoch 7194: train loss: 0.1295950372029236, test loss: 0.26250187813932585\n",
      "epoch 7195: train loss: 0.12959014468132427, test loss: 0.26250152862613324\n",
      "epoch 7196: train loss: 0.1295852531263609, test loss: 0.2625011794286923\n",
      "epoch 7197: train loss: 0.12958036253769695, test loss: 0.26250083054688056\n",
      "epoch 7198: train loss: 0.1295754729149961, test loss: 0.2625004819805758\n",
      "epoch 7199: train loss: 0.12957058425792212, test loss: 0.26250013372965575\n",
      "epoch 7200: train loss: 0.12956569656613903, test loss: 0.2624997857939983\n",
      "epoch 7201: train loss: 0.12956080983931095, test loss: 0.2624994381734813\n",
      "epoch 7202: train loss: 0.1295559240771022, test loss: 0.26249909086798284\n",
      "epoch 7203: train loss: 0.12955103927917722, test loss: 0.2624987438773809\n",
      "epoch 7204: train loss: 0.1295461554452007, test loss: 0.26249839720155366\n",
      "epoch 7205: train loss: 0.12954127257483744, test loss: 0.26249805084037925\n",
      "epoch 7206: train loss: 0.1295363906677524, test loss: 0.26249770479373596\n",
      "epoch 7207: train loss: 0.12953150972361077, test loss: 0.26249735906150223\n",
      "epoch 7208: train loss: 0.12952662974207774, test loss: 0.2624970136435563\n",
      "epoch 7209: train loss: 0.1295217507228189, test loss: 0.2624966685397769\n",
      "epoch 7210: train loss: 0.12951687266549983, test loss: 0.26249632375004234\n",
      "epoch 7211: train loss: 0.12951199556978632, test loss: 0.2624959792742314\n",
      "epoch 7212: train loss: 0.1295071194353444, test loss: 0.26249563511222285\n",
      "epoch 7213: train loss: 0.12950224426184012, test loss: 0.2624952912638952\n",
      "epoch 7214: train loss: 0.12949737004893985, test loss: 0.2624949477291276\n",
      "epoch 7215: train loss: 0.12949249679631003, test loss: 0.26249460450779877\n",
      "epoch 7216: train loss: 0.12948762450361723, test loss: 0.2624942615997877\n",
      "epoch 7217: train loss: 0.1294827531705283, test loss: 0.2624939190049735\n",
      "epoch 7218: train loss: 0.1294778827967102, test loss: 0.26249357672323537\n",
      "epoch 7219: train loss: 0.12947301338182998, test loss: 0.2624932347544524\n",
      "epoch 7220: train loss: 0.12946814492555503, test loss: 0.26249289309850393\n",
      "epoch 7221: train loss: 0.1294632774275527, test loss: 0.26249255175526925\n",
      "epoch 7222: train loss: 0.12945841088749066, test loss: 0.2624922107246278\n",
      "epoch 7223: train loss: 0.12945354530503667, test loss: 0.262491870006459\n",
      "epoch 7224: train loss: 0.12944868067985868, test loss: 0.2624915296006425\n",
      "epoch 7225: train loss: 0.12944381701162475, test loss: 0.26249118950705785\n",
      "epoch 7226: train loss: 0.12943895430000324, test loss: 0.26249084972558484\n",
      "epoch 7227: train loss: 0.12943409254466245, test loss: 0.26249051025610304\n",
      "epoch 7228: train loss: 0.1294292317452711, test loss: 0.2624901710984925\n",
      "epoch 7229: train loss: 0.12942437190149786, test loss: 0.262489832252633\n",
      "epoch 7230: train loss: 0.1294195130130117, test loss: 0.2624894937184046\n",
      "epoch 7231: train loss: 0.1294146550794817, test loss: 0.26248915549568724\n",
      "epoch 7232: train loss: 0.12940979810057704, test loss: 0.26248881758436116\n",
      "epoch 7233: train loss: 0.12940494207596723, test loss: 0.26248847998430636\n",
      "epoch 7234: train loss: 0.1294000870053218, test loss: 0.26248814269540327\n",
      "epoch 7235: train loss: 0.12939523288831048, test loss: 0.26248780571753216\n",
      "epoch 7236: train loss: 0.12939037972460316, test loss: 0.26248746905057346\n",
      "epoch 7237: train loss: 0.1293855275138699, test loss: 0.26248713269440754\n",
      "epoch 7238: train loss: 0.12938067625578095, test loss: 0.262486796648915\n",
      "epoch 7239: train loss: 0.12937582595000666, test loss: 0.26248646091397637\n",
      "epoch 7240: train loss: 0.12937097659621763, test loss: 0.2624861254894725\n",
      "epoch 7241: train loss: 0.1293661281940845, test loss: 0.26248579037528397\n",
      "epoch 7242: train loss: 0.12936128074327818, test loss: 0.26248545557129155\n",
      "epoch 7243: train loss: 0.12935643424346974, test loss: 0.2624851210773763\n",
      "epoch 7244: train loss: 0.12935158869433028, test loss: 0.2624847868934191\n",
      "epoch 7245: train loss: 0.12934674409553124, test loss: 0.2624844530193009\n",
      "epoch 7246: train loss: 0.1293419004467441, test loss: 0.2624841194549029\n",
      "epoch 7247: train loss: 0.12933705774764057, test loss: 0.2624837862001062\n",
      "epoch 7248: train loss: 0.12933221599789246, test loss: 0.262483453254792\n",
      "epoch 7249: train loss: 0.12932737519717177, test loss: 0.26248312061884166\n",
      "epoch 7250: train loss: 0.12932253534515067, test loss: 0.26248278829213645\n",
      "epoch 7251: train loss: 0.1293176964415015, test loss: 0.26248245627455785\n",
      "epoch 7252: train loss: 0.12931285848589677, test loss: 0.26248212456598746\n",
      "epoch 7253: train loss: 0.12930802147800907, test loss: 0.26248179316630676\n",
      "epoch 7254: train loss: 0.1293031854175112, test loss: 0.26248146207539746\n",
      "epoch 7255: train loss: 0.12929835030407621, test loss: 0.26248113129314116\n",
      "epoch 7256: train loss: 0.12929351613737716, test loss: 0.26248080081941966\n",
      "epoch 7257: train loss: 0.12928868291708734, test loss: 0.2624804706541149\n",
      "epoch 7258: train loss: 0.1292838506428802, test loss: 0.2624801407971088\n",
      "epoch 7259: train loss: 0.12927901931442942, test loss: 0.2624798112482832\n",
      "epoch 7260: train loss: 0.12927418893140868, test loss: 0.26247948200752025\n",
      "epoch 7261: train loss: 0.12926935949349194, test loss: 0.2624791530747021\n",
      "epoch 7262: train loss: 0.1292645310003533, test loss: 0.2624788244497109\n",
      "epoch 7263: train loss: 0.129259703451667, test loss: 0.2624784961324289\n",
      "epoch 7264: train loss: 0.12925487684710743, test loss: 0.2624781681227384\n",
      "epoch 7265: train loss: 0.12925005118634922, test loss: 0.26247784042052197\n",
      "epoch 7266: train loss: 0.12924522646906703, test loss: 0.2624775130256618\n",
      "epoch 7267: train loss: 0.12924040269493578, test loss: 0.2624771859380406\n",
      "epoch 7268: train loss: 0.12923557986363052, test loss: 0.26247685915754093\n",
      "epoch 7269: train loss: 0.12923075797482642, test loss: 0.26247653268404547\n",
      "epoch 7270: train loss: 0.1292259370281989, test loss: 0.2624762065174369\n",
      "epoch 7271: train loss: 0.12922111702342345, test loss: 0.2624758806575981\n",
      "epoch 7272: train loss: 0.12921629796017578, test loss: 0.2624755551044119\n",
      "epoch 7273: train loss: 0.12921147983813172, test loss: 0.2624752298577613\n",
      "epoch 7274: train loss: 0.12920666265696723, test loss: 0.26247490491752923\n",
      "epoch 7275: train loss: 0.12920184641635854, test loss: 0.2624745802835987\n",
      "epoch 7276: train loss: 0.12919703111598188, test loss: 0.262474255955853\n",
      "epoch 7277: train loss: 0.12919221675551382, test loss: 0.26247393193417523\n",
      "epoch 7278: train loss: 0.12918740333463094, test loss: 0.26247360821844873\n",
      "epoch 7279: train loss: 0.12918259085301007, test loss: 0.2624732848085568\n",
      "epoch 7280: train loss: 0.1291777793103281, test loss: 0.26247296170438283\n",
      "epoch 7281: train loss: 0.12917296870626216, test loss: 0.26247263890581046\n",
      "epoch 7282: train loss: 0.1291681590404896, test loss: 0.26247231641272306\n",
      "epoch 7283: train loss: 0.12916335031268775, test loss: 0.2624719942250043\n",
      "epoch 7284: train loss: 0.12915854252253423, test loss: 0.26247167234253793\n",
      "epoch 7285: train loss: 0.12915373566970673, test loss: 0.26247135076520756\n",
      "epoch 7286: train loss: 0.1291489297538832, test loss: 0.262471029492897\n",
      "epoch 7287: train loss: 0.12914412477474171, test loss: 0.2624707085254903\n",
      "epoch 7288: train loss: 0.1291393207319604, test loss: 0.26247038786287136\n",
      "epoch 7289: train loss: 0.12913451762521774, test loss: 0.262470067504924\n",
      "epoch 7290: train loss: 0.1291297154541922, test loss: 0.26246974745153256\n",
      "epoch 7291: train loss: 0.12912491421856245, test loss: 0.262469427702581\n",
      "epoch 7292: train loss: 0.12912011391800732, test loss: 0.26246910825795366\n",
      "epoch 7293: train loss: 0.12911531455220587, test loss: 0.2624687891175348\n",
      "epoch 7294: train loss: 0.1291105161208372, test loss: 0.2624684702812086\n",
      "epoch 7295: train loss: 0.12910571862358064, test loss: 0.26246815174885973\n",
      "epoch 7296: train loss: 0.1291009220601157, test loss: 0.2624678335203725\n",
      "epoch 7297: train loss: 0.12909612643012192, test loss: 0.2624675155956316\n",
      "epoch 7298: train loss: 0.1290913317332791, test loss: 0.26246719797452145\n",
      "epoch 7299: train loss: 0.12908653796926725, test loss: 0.26246688065692686\n",
      "epoch 7300: train loss: 0.12908174513776635, test loss: 0.2624665636427326\n",
      "epoch 7301: train loss: 0.12907695323845675, test loss: 0.2624662469318234\n",
      "epoch 7302: train loss: 0.12907216227101884, test loss: 0.2624659305240842\n",
      "epoch 7303: train loss: 0.1290673722351331, test loss: 0.26246561441939986\n",
      "epoch 7304: train loss: 0.1290625831304803, test loss: 0.2624652986176555\n",
      "epoch 7305: train loss: 0.12905779495674133, test loss: 0.26246498311873606\n",
      "epoch 7306: train loss: 0.1290530077135972, test loss: 0.2624646679225268\n",
      "epoch 7307: train loss: 0.12904822140072908, test loss: 0.262464353028913\n",
      "epoch 7308: train loss: 0.12904343601781834, test loss: 0.2624640384377797\n",
      "epoch 7309: train loss: 0.1290386515645464, test loss: 0.2624637241490124\n",
      "epoch 7310: train loss: 0.12903386804059505, test loss: 0.2624634101624965\n",
      "epoch 7311: train loss: 0.12902908544564595, test loss: 0.26246309647811744\n",
      "epoch 7312: train loss: 0.12902430377938112, test loss: 0.26246278309576077\n",
      "epoch 7313: train loss: 0.12901952304148268, test loss: 0.2624624700153121\n",
      "epoch 7314: train loss: 0.12901474323163284, test loss: 0.26246215723665706\n",
      "epoch 7315: train loss: 0.12900996434951412, test loss: 0.26246184475968143\n",
      "epoch 7316: train loss: 0.129005186394809, test loss: 0.2624615325842709\n",
      "epoch 7317: train loss: 0.1290004093672003, test loss: 0.2624612207103115\n",
      "epoch 7318: train loss: 0.12899563326637084, test loss: 0.2624609091376891\n",
      "epoch 7319: train loss: 0.1289908580920037, test loss: 0.26246059786628967\n",
      "epoch 7320: train loss: 0.12898608384378205, test loss: 0.2624602868959992\n",
      "epoch 7321: train loss: 0.1289813105213892, test loss: 0.2624599762267039\n",
      "epoch 7322: train loss: 0.12897653812450877, test loss: 0.26245966585829\n",
      "epoch 7323: train loss: 0.1289717666528243, test loss: 0.26245935579064367\n",
      "epoch 7324: train loss: 0.12896699610601964, test loss: 0.2624590460236513\n",
      "epoch 7325: train loss: 0.12896222648377878, test loss: 0.2624587365571992\n",
      "epoch 7326: train loss: 0.12895745778578582, test loss: 0.26245842739117387\n",
      "epoch 7327: train loss: 0.12895269001172502, test loss: 0.2624581185254618\n",
      "epoch 7328: train loss: 0.12894792316128081, test loss: 0.26245780995994966\n",
      "epoch 7329: train loss: 0.12894315723413777, test loss: 0.2624575016945239\n",
      "epoch 7330: train loss: 0.12893839222998066, test loss: 0.26245719372907145\n",
      "epoch 7331: train loss: 0.1289336281484943, test loss: 0.2624568860634789\n",
      "epoch 7332: train loss: 0.12892886498936376, test loss: 0.2624565786976333\n",
      "epoch 7333: train loss: 0.12892410275227426, test loss: 0.26245627163142127\n",
      "epoch 7334: train loss: 0.12891934143691114, test loss: 0.26245596486472994\n",
      "epoch 7335: train loss: 0.12891458104295983, test loss: 0.26245565839744633\n",
      "epoch 7336: train loss: 0.128909821570106, test loss: 0.2624553522294576\n",
      "epoch 7337: train loss: 0.12890506301803553, test loss: 0.26245504636065076\n",
      "epoch 7338: train loss: 0.12890030538643427, test loss: 0.2624547407909132\n",
      "epoch 7339: train loss: 0.1288955486749884, test loss: 0.262454435520132\n",
      "epoch 7340: train loss: 0.12889079288338412, test loss: 0.2624541305481947\n",
      "epoch 7341: train loss: 0.12888603801130788, test loss: 0.26245382587498867\n",
      "epoch 7342: train loss: 0.12888128405844623, test loss: 0.2624535215004014\n",
      "epoch 7343: train loss: 0.12887653102448587, test loss: 0.2624532174243203\n",
      "epoch 7344: train loss: 0.1288717789091137, test loss: 0.26245291364663315\n",
      "epoch 7345: train loss: 0.1288670277120167, test loss: 0.2624526101672276\n",
      "epoch 7346: train loss: 0.12886227743288206, test loss: 0.2624523069859912\n",
      "epoch 7347: train loss: 0.12885752807139708, test loss: 0.262452004102812\n",
      "epoch 7348: train loss: 0.12885277962724928, test loss: 0.26245170151757774\n",
      "epoch 7349: train loss: 0.12884803210012624, test loss: 0.2624513992301763\n",
      "epoch 7350: train loss: 0.12884328548971574, test loss: 0.26245109724049576\n",
      "epoch 7351: train loss: 0.12883853979570575, test loss: 0.2624507955484242\n",
      "epoch 7352: train loss: 0.12883379501778428, test loss: 0.26245049415384963\n",
      "epoch 7353: train loss: 0.12882905115563958, test loss: 0.2624501930566603\n",
      "epoch 7354: train loss: 0.12882430820896004, test loss: 0.26244989225674437\n",
      "epoch 7355: train loss: 0.1288195661774342, test loss: 0.26244959175399024\n",
      "epoch 7356: train loss: 0.12881482506075076, test loss: 0.2624492915482863\n",
      "epoch 7357: train loss: 0.1288100848585985, test loss: 0.2624489916395209\n",
      "epoch 7358: train loss: 0.1288053455706664, test loss: 0.2624486920275826\n",
      "epoch 7359: train loss: 0.12880060719664363, test loss: 0.26244839271235987\n",
      "epoch 7360: train loss: 0.12879586973621948, test loss: 0.26244809369374145\n",
      "epoch 7361: train loss: 0.12879113318908333, test loss: 0.26244779497161597\n",
      "epoch 7362: train loss: 0.12878639755492483, test loss: 0.2624474965458722\n",
      "epoch 7363: train loss: 0.12878166283343367, test loss: 0.2624471984163989\n",
      "epoch 7364: train loss: 0.12877692902429974, test loss: 0.262446900583085\n",
      "epoch 7365: train loss: 0.12877219612721305, test loss: 0.2624466030458194\n",
      "epoch 7366: train loss: 0.12876746414186382, test loss: 0.26244630580449113\n",
      "epoch 7367: train loss: 0.12876273306794236, test loss: 0.2624460088589892\n",
      "epoch 7368: train loss: 0.1287580029051392, test loss: 0.2624457122092029\n",
      "epoch 7369: train loss: 0.1287532736531449, test loss: 0.2624454158550212\n",
      "epoch 7370: train loss: 0.12874854531165028, test loss: 0.26244511979633345\n",
      "epoch 7371: train loss: 0.12874381788034628, test loss: 0.262444824033029\n",
      "epoch 7372: train loss: 0.12873909135892397, test loss: 0.2624445285649971\n",
      "epoch 7373: train loss: 0.12873436574707456, test loss: 0.26244423339212736\n",
      "epoch 7374: train loss: 0.12872964104448945, test loss: 0.2624439385143092\n",
      "epoch 7375: train loss: 0.12872491725086016, test loss: 0.26244364393143216\n",
      "epoch 7376: train loss: 0.12872019436587834, test loss: 0.2624433496433859\n",
      "epoch 7377: train loss: 0.12871547238923586, test loss: 0.2624430556500601\n",
      "epoch 7378: train loss: 0.12871075132062468, test loss: 0.26244276195134447\n",
      "epoch 7379: train loss: 0.1287060311597369, test loss: 0.26244246854712894\n",
      "epoch 7380: train loss: 0.1287013119062648, test loss: 0.2624421754373032\n",
      "epoch 7381: train loss: 0.12869659355990082, test loss: 0.2624418826217574\n",
      "epoch 7382: train loss: 0.12869187612033747, test loss: 0.2624415901003813\n",
      "epoch 7383: train loss: 0.12868715958726756, test loss: 0.26244129787306514\n",
      "epoch 7384: train loss: 0.12868244396038386, test loss: 0.262441005939699\n",
      "epoch 7385: train loss: 0.12867772923937942, test loss: 0.262440714300173\n",
      "epoch 7386: train loss: 0.1286730154239474, test loss: 0.26244042295437753\n",
      "epoch 7387: train loss: 0.12866830251378114, test loss: 0.2624401319022027\n",
      "epoch 7388: train loss: 0.12866359050857404, test loss: 0.2624398411435391\n",
      "epoch 7389: train loss: 0.12865887940801968, test loss: 0.2624395506782769\n",
      "epoch 7390: train loss: 0.1286541692118119, test loss: 0.2624392605063069\n",
      "epoch 7391: train loss: 0.1286494599196445, test loss: 0.26243897062751936\n",
      "epoch 7392: train loss: 0.12864475153121163, test loss: 0.2624386810418051\n",
      "epoch 7393: train loss: 0.12864004404620735, test loss: 0.2624383917490547\n",
      "epoch 7394: train loss: 0.12863533746432615, test loss: 0.2624381027491589\n",
      "epoch 7395: train loss: 0.1286306317852624, test loss: 0.2624378140420085\n",
      "epoch 7396: train loss: 0.12862592700871076, test loss: 0.26243752562749445\n",
      "epoch 7397: train loss: 0.12862122313436608, test loss: 0.26243723750550757\n",
      "epoch 7398: train loss: 0.1286165201619232, test loss: 0.26243694967593884\n",
      "epoch 7399: train loss: 0.1286118180910772, test loss: 0.26243666213867933\n",
      "epoch 7400: train loss: 0.12860711692152335, test loss: 0.2624363748936202\n",
      "epoch 7401: train loss: 0.128602416652957, test loss: 0.26243608794065254\n",
      "epoch 7402: train loss: 0.12859771728507363, test loss: 0.2624358012796676\n",
      "epoch 7403: train loss: 0.12859301881756893, test loss: 0.26243551491055667\n",
      "epoch 7404: train loss: 0.12858832125013872, test loss: 0.26243522883321113\n",
      "epoch 7405: train loss: 0.12858362458247888, test loss: 0.26243494304752235\n",
      "epoch 7406: train loss: 0.12857892881428562, test loss: 0.2624346575533818\n",
      "epoch 7407: train loss: 0.1285742339452551, test loss: 0.26243437235068096\n",
      "epoch 7408: train loss: 0.12856953997508375, test loss: 0.2624340874393116\n",
      "epoch 7409: train loss: 0.12856484690346806, test loss: 0.26243380281916523\n",
      "epoch 7410: train loss: 0.12856015473010476, test loss: 0.2624335184901336\n",
      "epoch 7411: train loss: 0.12855546345469068, test loss: 0.26243323445210837\n",
      "epoch 7412: train loss: 0.12855077307692275, test loss: 0.26243295070498157\n",
      "epoch 7413: train loss: 0.12854608359649813, test loss: 0.262432667248645\n",
      "epoch 7414: train loss: 0.12854139501311404, test loss: 0.2624323840829906\n",
      "epoch 7415: train loss: 0.1285367073264679, test loss: 0.26243210120791033\n",
      "epoch 7416: train loss: 0.1285320205362573, test loss: 0.2624318186232964\n",
      "epoch 7417: train loss: 0.12852733464217994, test loss: 0.26243153632904087\n",
      "epoch 7418: train loss: 0.12852264964393362, test loss: 0.26243125432503595\n",
      "epoch 7419: train loss: 0.12851796554121636, test loss: 0.26243097261117393\n",
      "epoch 7420: train loss: 0.12851328233372628, test loss: 0.262430691187347\n",
      "epoch 7421: train loss: 0.12850860002116166, test loss: 0.2624304100534477\n",
      "epoch 7422: train loss: 0.1285039186032209, test loss: 0.26243012920936837\n",
      "epoch 7423: train loss: 0.12849923807960265, test loss: 0.2624298486550016\n",
      "epoch 7424: train loss: 0.12849455845000554, test loss: 0.26242956839023973\n",
      "epoch 7425: train loss: 0.12848987971412845, test loss: 0.2624292884149756\n",
      "epoch 7426: train loss: 0.1284852018716704, test loss: 0.26242900872910174\n",
      "epoch 7427: train loss: 0.12848052492233047, test loss: 0.262428729332511\n",
      "epoch 7428: train loss: 0.1284758488658081, test loss: 0.2624284502250962\n",
      "epoch 7429: train loss: 0.12847117370180253, test loss: 0.26242817140675\n",
      "epoch 7430: train loss: 0.12846649943001345, test loss: 0.2624278928773655\n",
      "epoch 7431: train loss: 0.12846182605014062, test loss: 0.26242761463683556\n",
      "epoch 7432: train loss: 0.12845715356188378, test loss: 0.2624273366850533\n",
      "epoch 7433: train loss: 0.128452481964943, test loss: 0.26242705902191177\n",
      "epoch 7434: train loss: 0.1284478112590185, test loss: 0.26242678164730415\n",
      "epoch 7435: train loss: 0.12844314144381047, test loss: 0.26242650456112354\n",
      "epoch 7436: train loss: 0.1284384725190194, test loss: 0.26242622776326335\n",
      "epoch 7437: train loss: 0.12843380448434585, test loss: 0.2624259512536169\n",
      "epoch 7438: train loss: 0.1284291373394906, test loss: 0.2624256750320775\n",
      "epoch 7439: train loss: 0.12842447108415445, test loss: 0.2624253990985387\n",
      "epoch 7440: train loss: 0.12841980571803846, test loss: 0.2624251234528938\n",
      "epoch 7441: train loss: 0.12841514124084374, test loss: 0.26242484809503663\n",
      "epoch 7442: train loss: 0.12841047765227162, test loss: 0.2624245730248606\n",
      "epoch 7443: train loss: 0.12840581495202352, test loss: 0.2624242982422596\n",
      "epoch 7444: train loss: 0.12840115313980102, test loss: 0.2624240237471272\n",
      "epoch 7445: train loss: 0.1283964922153059, test loss: 0.2624237495393573\n",
      "epoch 7446: train loss: 0.12839183217824002, test loss: 0.26242347561884377\n",
      "epoch 7447: train loss: 0.12838717302830532, test loss: 0.2624232019854804\n",
      "epoch 7448: train loss: 0.128382514765204, test loss: 0.2624229286391613\n",
      "epoch 7449: train loss: 0.12837785738863833, test loss: 0.2624226555797805\n",
      "epoch 7450: train loss: 0.12837320089831078, test loss: 0.262422382807232\n",
      "epoch 7451: train loss: 0.1283685452939239, test loss: 0.26242211032141005\n",
      "epoch 7452: train loss: 0.12836389057518044, test loss: 0.2624218381222089\n",
      "epoch 7453: train loss: 0.12835923674178326, test loss: 0.26242156620952267\n",
      "epoch 7454: train loss: 0.12835458379343534, test loss: 0.26242129458324576\n",
      "epoch 7455: train loss: 0.12834993172983986, test loss: 0.26242102324327266\n",
      "epoch 7456: train loss: 0.12834528055070005, test loss: 0.2624207521894977\n",
      "epoch 7457: train loss: 0.1283406302557194, test loss: 0.2624204814218154\n",
      "epoch 7458: train loss: 0.12833598084460146, test loss: 0.26242021094012036\n",
      "epoch 7459: train loss: 0.12833133231704996, test loss: 0.2624199407443072\n",
      "epoch 7460: train loss: 0.1283266846727687, test loss: 0.26241967083427054\n",
      "epoch 7461: train loss: 0.12832203791146174, test loss: 0.2624194012099052\n",
      "epoch 7462: train loss: 0.1283173920328332, test loss: 0.262419131871106\n",
      "epoch 7463: train loss: 0.1283127470365873, test loss: 0.26241886281776766\n",
      "epoch 7464: train loss: 0.12830810292242856, test loss: 0.2624185940497851\n",
      "epoch 7465: train loss: 0.12830345969006143, test loss: 0.2624183255670535\n",
      "epoch 7466: train loss: 0.12829881733919068, test loss: 0.2624180573694677\n",
      "epoch 7467: train loss: 0.12829417586952116, test loss: 0.26241778945692285\n",
      "epoch 7468: train loss: 0.1282895352807578, test loss: 0.26241752182931405\n",
      "epoch 7469: train loss: 0.12828489557260575, test loss: 0.2624172544865365\n",
      "epoch 7470: train loss: 0.12828025674477028, test loss: 0.2624169874284856\n",
      "epoch 7471: train loss: 0.12827561879695676, test loss: 0.26241672065505645\n",
      "epoch 7472: train loss: 0.12827098172887078, test loss: 0.2624164541661447\n",
      "epoch 7473: train loss: 0.12826634554021796, test loss: 0.26241618796164545\n",
      "epoch 7474: train loss: 0.12826171023070418, test loss: 0.2624159220414544\n",
      "epoch 7475: train loss: 0.12825707580003537, test loss: 0.2624156564054671\n",
      "epoch 7476: train loss: 0.12825244224791768, test loss: 0.26241539105357914\n",
      "epoch 7477: train loss: 0.1282478095740573, test loss: 0.26241512598568606\n",
      "epoch 7478: train loss: 0.1282431777781606, test loss: 0.2624148612016837\n",
      "epoch 7479: train loss: 0.12823854685993413, test loss: 0.2624145967014679\n",
      "epoch 7480: train loss: 0.12823391681908458, test loss: 0.26241433248493434\n",
      "epoch 7481: train loss: 0.1282292876553187, test loss: 0.2624140685519791\n",
      "epoch 7482: train loss: 0.12822465936834346, test loss: 0.2624138049024978\n",
      "epoch 7483: train loss: 0.12822003195786594, test loss: 0.2624135415363868\n",
      "epoch 7484: train loss: 0.12821540542359333, test loss: 0.262413278453542\n",
      "epoch 7485: train loss: 0.12821077976523304, test loss: 0.26241301565385955\n",
      "epoch 7486: train loss: 0.1282061549824925, test loss: 0.2624127531372356\n",
      "epoch 7487: train loss: 0.12820153107507942, test loss: 0.2624124909035664\n",
      "epoch 7488: train loss: 0.12819690804270154, test loss: 0.26241222895274824\n",
      "epoch 7489: train loss: 0.1281922858850667, test loss: 0.2624119672846776\n",
      "epoch 7490: train loss: 0.12818766460188308, test loss: 0.2624117058992506\n",
      "epoch 7491: train loss: 0.1281830441928588, test loss: 0.2624114447963639\n",
      "epoch 7492: train loss: 0.12817842465770218, test loss: 0.262411183975914\n",
      "epoch 7493: train loss: 0.12817380599612174, test loss: 0.2624109234377975\n",
      "epoch 7494: train loss: 0.12816918820782605, test loss: 0.2624106631819109\n",
      "epoch 7495: train loss: 0.12816457129252382, test loss: 0.262410403208151\n",
      "epoch 7496: train loss: 0.12815995524992402, test loss: 0.26241014351641456\n",
      "epoch 7497: train loss: 0.12815534007973559, test loss: 0.2624098841065983\n",
      "epoch 7498: train loss: 0.12815072578166775, test loss: 0.2624096249785992\n",
      "epoch 7499: train loss: 0.12814611235542972, test loss: 0.262409366132314\n",
      "epoch 7500: train loss: 0.12814149980073103, test loss: 0.26240910756763985\n",
      "epoch 7501: train loss: 0.12813688811728116, test loss: 0.26240884928447367\n",
      "epoch 7502: train loss: 0.12813227730478988, test loss: 0.26240859128271254\n",
      "epoch 7503: train loss: 0.12812766736296705, test loss: 0.2624083335622537\n",
      "epoch 7504: train loss: 0.1281230582915226, test loss: 0.2624080761229942\n",
      "epoch 7505: train loss: 0.12811845009016667, test loss: 0.2624078189648314\n",
      "epoch 7506: train loss: 0.12811384275860951, test loss: 0.2624075620876626\n",
      "epoch 7507: train loss: 0.12810923629656157, test loss: 0.26240730549138513\n",
      "epoch 7508: train loss: 0.12810463070373332, test loss: 0.2624070491758964\n",
      "epoch 7509: train loss: 0.12810002597983547, test loss: 0.26240679314109394\n",
      "epoch 7510: train loss: 0.12809542212457883, test loss: 0.2624065373868752\n",
      "epoch 7511: train loss: 0.1280908191376743, test loss: 0.2624062819131378\n",
      "epoch 7512: train loss: 0.12808621701883305, test loss: 0.2624060267197795\n",
      "epoch 7513: train loss: 0.12808161576776617, test loss: 0.26240577180669783\n",
      "epoch 7514: train loss: 0.12807701538418514, test loss: 0.2624055171737906\n",
      "epoch 7515: train loss: 0.12807241586780138, test loss: 0.26240526282095566\n",
      "epoch 7516: train loss: 0.12806781721832652, test loss: 0.2624050087480909\n",
      "epoch 7517: train loss: 0.12806321943547236, test loss: 0.2624047549550942\n",
      "epoch 7518: train loss: 0.12805862251895078, test loss: 0.2624045014418635\n",
      "epoch 7519: train loss: 0.1280540264684738, test loss: 0.2624042482082969\n",
      "epoch 7520: train loss: 0.12804943128375365, test loss: 0.2624039952542924\n",
      "epoch 7521: train loss: 0.12804483696450258, test loss: 0.2624037425797483\n",
      "epoch 7522: train loss: 0.12804024351043305, test loss: 0.26240349018456266\n",
      "epoch 7523: train loss: 0.12803565092125768, test loss: 0.26240323806863375\n",
      "epoch 7524: train loss: 0.12803105919668914, test loss: 0.2624029862318599\n",
      "epoch 7525: train loss: 0.12802646833644027, test loss: 0.2624027346741396\n",
      "epoch 7526: train loss: 0.12802187834022413, test loss: 0.2624024833953711\n",
      "epoch 7527: train loss: 0.1280172892077538, test loss: 0.26240223239545285\n",
      "epoch 7528: train loss: 0.1280127009387425, test loss: 0.2624019816742835\n",
      "epoch 7529: train loss: 0.1280081135329037, test loss: 0.26240173123176175\n",
      "epoch 7530: train loss: 0.1280035269899509, test loss: 0.262401481067786\n",
      "epoch 7531: train loss: 0.12799894130959774, test loss: 0.26240123118225506\n",
      "epoch 7532: train loss: 0.12799435649155808, test loss: 0.26240098157506764\n",
      "epoch 7533: train loss: 0.12798977253554578, test loss: 0.26240073224612254\n",
      "epoch 7534: train loss: 0.127985189441275, test loss: 0.2624004831953187\n",
      "epoch 7535: train loss: 0.12798060720845986, test loss: 0.262400234422555\n",
      "epoch 7536: train loss: 0.12797602583681472, test loss: 0.26239998592773034\n",
      "epoch 7537: train loss: 0.1279714453260541, test loss: 0.2623997377107439\n",
      "epoch 7538: train loss: 0.12796686567589258, test loss: 0.2623994897714946\n",
      "epoch 7539: train loss: 0.12796228688604486, test loss: 0.2623992421098817\n",
      "epoch 7540: train loss: 0.12795770895622594, test loss: 0.26239899472580425\n",
      "epoch 7541: train loss: 0.12795313188615073, test loss: 0.2623987476191616\n",
      "epoch 7542: train loss: 0.1279485556755344, test loss: 0.2623985007898531\n",
      "epoch 7543: train loss: 0.12794398032409224, test loss: 0.26239825423777796\n",
      "epoch 7544: train loss: 0.12793940583153965, test loss: 0.26239800796283574\n",
      "epoch 7545: train loss: 0.12793483219759222, test loss: 0.26239776196492576\n",
      "epoch 7546: train loss: 0.1279302594219656, test loss: 0.26239751624394764\n",
      "epoch 7547: train loss: 0.1279256875043756, test loss: 0.2623972707998009\n",
      "epoch 7548: train loss: 0.1279211164445382, test loss: 0.26239702563238515\n",
      "epoch 7549: train loss: 0.12791654624216953, test loss: 0.2623967807416001\n",
      "epoch 7550: train loss: 0.1279119768969857, test loss: 0.2623965361273455\n",
      "epoch 7551: train loss: 0.12790740840870313, test loss: 0.2623962917895211\n",
      "epoch 7552: train loss: 0.12790284077703828, test loss: 0.26239604772802677\n",
      "epoch 7553: train loss: 0.12789827400170783, test loss: 0.2623958039427624\n",
      "epoch 7554: train loss: 0.12789370808242842, test loss: 0.26239556043362794\n",
      "epoch 7555: train loss: 0.12788914301891707, test loss: 0.26239531720052345\n",
      "epoch 7556: train loss: 0.12788457881089074, test loss: 0.2623950742433488\n",
      "epoch 7557: train loss: 0.12788001545806651, test loss: 0.2623948315620044\n",
      "epoch 7558: train loss: 0.1278754529601618, test loss: 0.26239458915639013\n",
      "epoch 7559: train loss: 0.12787089131689391, test loss: 0.2623943470264063\n",
      "epoch 7560: train loss: 0.1278663305279805, test loss: 0.26239410517195333\n",
      "epoch 7561: train loss: 0.1278617705931392, test loss: 0.2623938635929314\n",
      "epoch 7562: train loss: 0.1278572115120878, test loss: 0.26239362228924085\n",
      "epoch 7563: train loss: 0.12785265328454426, test loss: 0.26239338126078215\n",
      "epoch 7564: train loss: 0.12784809591022672, test loss: 0.26239314050745594\n",
      "epoch 7565: train loss: 0.12784353938885334, test loss: 0.26239290002916266\n",
      "epoch 7566: train loss: 0.1278389837201425, test loss: 0.2623926598258028\n",
      "epoch 7567: train loss: 0.12783442890381264, test loss: 0.2623924198972772\n",
      "epoch 7568: train loss: 0.1278298749395824, test loss: 0.2623921802434864\n",
      "epoch 7569: train loss: 0.12782532182717052, test loss: 0.2623919408643312\n",
      "epoch 7570: train loss: 0.12782076956629587, test loss: 0.26239170175971244\n",
      "epoch 7571: train loss: 0.12781621815667749, test loss: 0.262391462929531\n",
      "epoch 7572: train loss: 0.12781166759803447, test loss: 0.2623912243736877\n",
      "epoch 7573: train loss: 0.1278071178900861, test loss: 0.26239098609208367\n",
      "epoch 7574: train loss: 0.1278025690325518, test loss: 0.2623907480846199\n",
      "epoch 7575: train loss: 0.12779802102515112, test loss: 0.2623905103511972\n",
      "epoch 7576: train loss: 0.12779347386760365, test loss: 0.262390272891717\n",
      "epoch 7577: train loss: 0.12778892755962926, test loss: 0.26239003570608044\n",
      "epoch 7578: train loss: 0.12778438210094786, test loss: 0.2623897987941886\n",
      "epoch 7579: train loss: 0.12777983749127952, test loss: 0.262389562155943\n",
      "epoch 7580: train loss: 0.1277752937303444, test loss: 0.26238932579124474\n",
      "epoch 7581: train loss: 0.12777075081786285, test loss: 0.26238908969999536\n",
      "epoch 7582: train loss: 0.12776620875355538, test loss: 0.26238885388209626\n",
      "epoch 7583: train loss: 0.12776166753714244, test loss: 0.262388618337449\n",
      "epoch 7584: train loss: 0.12775712716834486, test loss: 0.26238838306595497\n",
      "epoch 7585: train loss: 0.12775258764688344, test loss: 0.2623881480675159\n",
      "epoch 7586: train loss: 0.12774804897247916, test loss: 0.2623879133420335\n",
      "epoch 7587: train loss: 0.12774351114485313, test loss: 0.26238767888940934\n",
      "epoch 7588: train loss: 0.12773897416372665, test loss: 0.2623874447095453\n",
      "epoch 7589: train loss: 0.12773443802882098, test loss: 0.2623872108023431\n",
      "epoch 7590: train loss: 0.12772990273985768, test loss: 0.2623869771677047\n",
      "epoch 7591: train loss: 0.12772536829655842, test loss: 0.262386743805532\n",
      "epoch 7592: train loss: 0.1277208346986449, test loss: 0.26238651071572694\n",
      "epoch 7593: train loss: 0.12771630194583902, test loss: 0.26238627789819147\n",
      "epoch 7594: train loss: 0.12771177003786283, test loss: 0.26238604535282783\n",
      "epoch 7595: train loss: 0.12770723897443845, test loss: 0.262385813079538\n",
      "epoch 7596: train loss: 0.1277027087552882, test loss: 0.2623855810782243\n",
      "epoch 7597: train loss: 0.12769817938013442, test loss: 0.26238534934878893\n",
      "epoch 7598: train loss: 0.12769365084869974, test loss: 0.2623851178911341\n",
      "epoch 7599: train loss: 0.1276891231607068, test loss: 0.2623848867051622\n",
      "epoch 7600: train loss: 0.12768459631587833, test loss: 0.2623846557907756\n",
      "epoch 7601: train loss: 0.1276800703139374, test loss: 0.26238442514787674\n",
      "epoch 7602: train loss: 0.12767554515460694, test loss: 0.2623841947763681\n",
      "epoch 7603: train loss: 0.12767102083761023, test loss: 0.26238396467615227\n",
      "epoch 7604: train loss: 0.12766649736267052, test loss: 0.26238373484713196\n",
      "epoch 7605: train loss: 0.1276619747295113, test loss: 0.2623835052892096\n",
      "epoch 7606: train loss: 0.12765745293785613, test loss: 0.262383276002288\n",
      "epoch 7607: train loss: 0.12765293198742875, test loss: 0.26238304698626996\n",
      "epoch 7608: train loss: 0.12764841187795292, test loss: 0.26238281824105814\n",
      "epoch 7609: train loss: 0.12764389260915268, test loss: 0.2623825897665555\n",
      "epoch 7610: train loss: 0.1276393741807521, test loss: 0.26238236156266503\n",
      "epoch 7611: train loss: 0.12763485659247537, test loss: 0.26238213362928947\n",
      "epoch 7612: train loss: 0.1276303398440469, test loss: 0.26238190596633204\n",
      "epoch 7613: train loss: 0.12762582393519112, test loss: 0.26238167857369565\n",
      "epoch 7614: train loss: 0.12762130886563267, test loss: 0.26238145145128366\n",
      "epoch 7615: train loss: 0.12761679463509623, test loss: 0.262381224598999\n",
      "epoch 7616: train loss: 0.12761228124330676, test loss: 0.26238099801674497\n",
      "epoch 7617: train loss: 0.12760776868998916, test loss: 0.26238077170442475\n",
      "epoch 7618: train loss: 0.12760325697486863, test loss: 0.26238054566194186\n",
      "epoch 7619: train loss: 0.12759874609767038, test loss: 0.2623803198891995\n",
      "epoch 7620: train loss: 0.12759423605811976, test loss: 0.2623800943861012\n",
      "epoch 7621: train loss: 0.12758972685594233, test loss: 0.2623798691525504\n",
      "epoch 7622: train loss: 0.1275852184908637, test loss: 0.2623796441884506\n",
      "epoch 7623: train loss: 0.12758071096260962, test loss: 0.2623794194937053\n",
      "epoch 7624: train loss: 0.127576204270906, test loss: 0.2623791950682183\n",
      "epoch 7625: train loss: 0.12757169841547886, test loss: 0.2623789709118933\n",
      "epoch 7626: train loss: 0.1275671933960543, test loss: 0.2623787470246338\n",
      "epoch 7627: train loss: 0.12756268921235864, test loss: 0.26237852340634377\n",
      "epoch 7628: train loss: 0.1275581858641183, test loss: 0.262378300056927\n",
      "epoch 7629: train loss: 0.12755368335105974, test loss: 0.26237807697628746\n",
      "epoch 7630: train loss: 0.12754918167290966, test loss: 0.2623778541643289\n",
      "epoch 7631: train loss: 0.12754468082939482, test loss: 0.2623776316209554\n",
      "epoch 7632: train loss: 0.12754018082024218, test loss: 0.262377409346071\n",
      "epoch 7633: train loss: 0.1275356816451787, test loss: 0.26237718733957993\n",
      "epoch 7634: train loss: 0.12753118330393157, test loss: 0.262376965601386\n",
      "epoch 7635: train loss: 0.1275266857962281, test loss: 0.2623767441313937\n",
      "epoch 7636: train loss: 0.12752218912179572, test loss: 0.2623765229295072\n",
      "epoch 7637: train loss: 0.12751769328036194, test loss: 0.2623763019956307\n",
      "epoch 7638: train loss: 0.12751319827165442, test loss: 0.2623760813296686\n",
      "epoch 7639: train loss: 0.12750870409540102, test loss: 0.2623758609315253\n",
      "epoch 7640: train loss: 0.12750421075132962, test loss: 0.26237564080110526\n",
      "epoch 7641: train loss: 0.12749971823916822, test loss: 0.2623754209383129\n",
      "epoch 7642: train loss: 0.12749522655864512, test loss: 0.26237520134305276\n",
      "epoch 7643: train loss: 0.12749073570948852, test loss: 0.2623749820152295\n",
      "epoch 7644: train loss: 0.12748624569142689, test loss: 0.2623747629547479\n",
      "epoch 7645: train loss: 0.1274817565041888, test loss: 0.26237454416151246\n",
      "epoch 7646: train loss: 0.12747726814750288, test loss: 0.2623743256354279\n",
      "epoch 7647: train loss: 0.127472780621098, test loss: 0.26237410737639905\n",
      "epoch 7648: train loss: 0.12746829392470305, test loss: 0.26237388938433087\n",
      "epoch 7649: train loss: 0.12746380805804713, test loss: 0.26237367165912817\n",
      "epoch 7650: train loss: 0.12745932302085933, test loss: 0.2623734542006959\n",
      "epoch 7651: train loss: 0.1274548388128691, test loss: 0.26237323700893905\n",
      "epoch 7652: train loss: 0.12745035543380578, test loss: 0.26237302008376273\n",
      "epoch 7653: train loss: 0.12744587288339898, test loss: 0.2623728034250719\n",
      "epoch 7654: train loss: 0.12744139116137834, test loss: 0.26237258703277183\n",
      "epoch 7655: train loss: 0.12743691026747375, test loss: 0.26237237090676774\n",
      "epoch 7656: train loss: 0.12743243020141506, test loss: 0.26237215504696476\n",
      "epoch 7657: train loss: 0.12742795096293238, test loss: 0.2623719394532682\n",
      "epoch 7658: train loss: 0.1274234725517559, test loss: 0.26237172412558357\n",
      "epoch 7659: train loss: 0.1274189949676159, test loss: 0.26237150906381607\n",
      "epoch 7660: train loss: 0.1274145182102429, test loss: 0.2623712942678712\n",
      "epoch 7661: train loss: 0.12741004227936742, test loss: 0.26237107973765456\n",
      "epoch 7662: train loss: 0.1274055671747201, test loss: 0.2623708654730716\n",
      "epoch 7663: train loss: 0.12740109289603183, test loss: 0.2623706514740278\n",
      "epoch 7664: train loss: 0.1273966194430335, test loss: 0.26237043774042906\n",
      "epoch 7665: train loss: 0.1273921468154562, test loss: 0.26237022427218093\n",
      "epoch 7666: train loss: 0.12738767501303108, test loss: 0.2623700110691891\n",
      "epoch 7667: train loss: 0.1273832040354895, test loss: 0.2623697981313595\n",
      "epoch 7668: train loss: 0.1273787338825629, test loss: 0.2623695854585979\n",
      "epoch 7669: train loss: 0.1273742645539828, test loss: 0.26236937305081026\n",
      "epoch 7670: train loss: 0.1273697960494809, test loss: 0.26236916090790235\n",
      "epoch 7671: train loss: 0.12736532836878905, test loss: 0.2623689490297803\n",
      "epoch 7672: train loss: 0.1273608615116391, test loss: 0.26236873741635014\n",
      "epoch 7673: train loss: 0.1273563954777632, test loss: 0.262368526067518\n",
      "epoch 7674: train loss: 0.12735193026689348, test loss: 0.26236831498318985\n",
      "epoch 7675: train loss: 0.12734746587876228, test loss: 0.262368104163272\n",
      "epoch 7676: train loss: 0.127343002313102, test loss: 0.26236789360767077\n",
      "epoch 7677: train loss: 0.1273385395696452, test loss: 0.2623676833162923\n",
      "epoch 7678: train loss: 0.12733407764812457, test loss: 0.2623674732890429\n",
      "epoch 7679: train loss: 0.12732961654827288, test loss: 0.2623672635258291\n",
      "epoch 7680: train loss: 0.1273251562698231, test loss: 0.2623670540265573\n",
      "epoch 7681: train loss: 0.12732069681250827, test loss: 0.26236684479113387\n",
      "epoch 7682: train loss: 0.12731623817606152, test loss: 0.2623666358194655\n",
      "epoch 7683: train loss: 0.12731178036021618, test loss: 0.26236642711145863\n",
      "epoch 7684: train loss: 0.12730732336470568, test loss: 0.26236621866702003\n",
      "epoch 7685: train loss: 0.12730286718926354, test loss: 0.2623660104860561\n",
      "epoch 7686: train loss: 0.12729841183362342, test loss: 0.26236580256847397\n",
      "epoch 7687: train loss: 0.12729395729751913, test loss: 0.26236559491418016\n",
      "epoch 7688: train loss: 0.12728950358068458, test loss: 0.2623653875230815\n",
      "epoch 7689: train loss: 0.12728505068285376, test loss: 0.26236518039508483\n",
      "epoch 7690: train loss: 0.12728059860376084, test loss: 0.2623649735300973\n",
      "epoch 7691: train loss: 0.1272761473431402, test loss: 0.2623647669280257\n",
      "epoch 7692: train loss: 0.1272716969007261, test loss: 0.26236456058877694\n",
      "epoch 7693: train loss: 0.12726724727625313, test loss: 0.2623643545122583\n",
      "epoch 7694: train loss: 0.12726279846945596, test loss: 0.2623641486983768\n",
      "epoch 7695: train loss: 0.1272583504800693, test loss: 0.2623639431470397\n",
      "epoch 7696: train loss: 0.1272539033078281, test loss: 0.2623637378581541\n",
      "epoch 7697: train loss: 0.12724945695246734, test loss: 0.2623635328316273\n",
      "epoch 7698: train loss: 0.1272450114137222, test loss: 0.26236332806736656\n",
      "epoch 7699: train loss: 0.12724056669132786, test loss: 0.2623631235652793\n",
      "epoch 7700: train loss: 0.12723612278501975, test loss: 0.26236291932527295\n",
      "epoch 7701: train loss: 0.12723167969453336, test loss: 0.26236271534725486\n",
      "epoch 7702: train loss: 0.12722723741960437, test loss: 0.2623625116311326\n",
      "epoch 7703: train loss: 0.12722279595996844, test loss: 0.2623623081768137\n",
      "epoch 7704: train loss: 0.1272183553153615, test loss: 0.26236210498420587\n",
      "epoch 7705: train loss: 0.12721391548551952, test loss: 0.2623619020532166\n",
      "epoch 7706: train loss: 0.12720947647017858, test loss: 0.2623616993837536\n",
      "epoch 7707: train loss: 0.127205038269075, test loss: 0.2623614969757247\n",
      "epoch 7708: train loss: 0.12720060088194504, test loss: 0.26236129482903764\n",
      "epoch 7709: train loss: 0.1271961643085252, test loss: 0.26236109294360027\n",
      "epoch 7710: train loss: 0.12719172854855215, test loss: 0.2623608913193204\n",
      "epoch 7711: train loss: 0.1271872936017625, test loss: 0.2623606899561062\n",
      "epoch 7712: train loss: 0.12718285946789318, test loss: 0.2623604888538653\n",
      "epoch 7713: train loss: 0.12717842614668112, test loss: 0.26236028801250605\n",
      "epoch 7714: train loss: 0.12717399363786336, test loss: 0.26236008743193634\n",
      "epoch 7715: train loss: 0.12716956194117718, test loss: 0.26235988711206437\n",
      "epoch 7716: train loss: 0.12716513105635985, test loss: 0.26235968705279833\n",
      "epoch 7717: train loss: 0.1271607009831488, test loss: 0.26235948725404634\n",
      "epoch 7718: train loss: 0.12715627172128166, test loss: 0.26235928771571676\n",
      "epoch 7719: train loss: 0.12715184327049608, test loss: 0.26235908843771794\n",
      "epoch 7720: train loss: 0.12714741563052992, test loss: 0.2623588894199581\n",
      "epoch 7721: train loss: 0.127142988801121, test loss: 0.2623586906623458\n",
      "epoch 7722: train loss: 0.12713856278200747, test loss: 0.26235849216478935\n",
      "epoch 7723: train loss: 0.1271341375729274, test loss: 0.26235829392719745\n",
      "epoch 7724: train loss: 0.1271297131736192, test loss: 0.26235809594947845\n",
      "epoch 7725: train loss: 0.1271252895838212, test loss: 0.2623578982315411\n",
      "epoch 7726: train loss: 0.12712086680327192, test loss: 0.2623577007732939\n",
      "epoch 7727: train loss: 0.12711644483171006, test loss: 0.26235750357464577\n",
      "epoch 7728: train loss: 0.12711202366887434, test loss: 0.26235730663550527\n",
      "epoch 7729: train loss: 0.1271076033145037, test loss: 0.26235710995578126\n",
      "epoch 7730: train loss: 0.12710318376833715, test loss: 0.2623569135353825\n",
      "epoch 7731: train loss: 0.12709876503011375, test loss: 0.26235671737421795\n",
      "epoch 7732: train loss: 0.12709434709957285, test loss: 0.26235652147219646\n",
      "epoch 7733: train loss: 0.12708992997645374, test loss: 0.2623563258292272\n",
      "epoch 7734: train loss: 0.12708551366049595, test loss: 0.26235613044521905\n",
      "epoch 7735: train loss: 0.12708109815143903, test loss: 0.26235593532008106\n",
      "epoch 7736: train loss: 0.1270766834490228, test loss: 0.26235574045372234\n",
      "epoch 7737: train loss: 0.12707226955298703, test loss: 0.26235554584605214\n",
      "epoch 7738: train loss: 0.12706785646307173, test loss: 0.26235535149697964\n",
      "epoch 7739: train loss: 0.12706344417901697, test loss: 0.26235515740641413\n",
      "epoch 7740: train loss: 0.12705903270056296, test loss: 0.2623549635742649\n",
      "epoch 7741: train loss: 0.12705462202745002, test loss: 0.2623547700004412\n",
      "epoch 7742: train loss: 0.1270502121594186, test loss: 0.2623545766848526\n",
      "epoch 7743: train loss: 0.12704580309620928, test loss: 0.2623543836274085\n",
      "epoch 7744: train loss: 0.12704139483756266, test loss: 0.26235419082801825\n",
      "epoch 7745: train loss: 0.12703698738321964, test loss: 0.26235399828659156\n",
      "epoch 7746: train loss: 0.12703258073292112, test loss: 0.2623538060030379\n",
      "epoch 7747: train loss: 0.12702817488640808, test loss: 0.26235361397726714\n",
      "epoch 7748: train loss: 0.12702376984342173, test loss: 0.26235342220918867\n",
      "epoch 7749: train loss: 0.12701936560370336, test loss: 0.2623532306987122\n",
      "epoch 7750: train loss: 0.12701496216699432, test loss: 0.26235303944574784\n",
      "epoch 7751: train loss: 0.1270105595330361, test loss: 0.26235284845020507\n",
      "epoch 7752: train loss: 0.12700615770157037, test loss: 0.26235265771199395\n",
      "epoch 7753: train loss: 0.1270017566723389, test loss: 0.2623524672310243\n",
      "epoch 7754: train loss: 0.12699735644508353, test loss: 0.26235227700720615\n",
      "epoch 7755: train loss: 0.12699295701954624, test loss: 0.26235208704044943\n",
      "epoch 7756: train loss: 0.12698855839546916, test loss: 0.2623518973306642\n",
      "epoch 7757: train loss: 0.12698416057259446, test loss: 0.2623517078777607\n",
      "epoch 7758: train loss: 0.1269797635506645, test loss: 0.26235151868164885\n",
      "epoch 7759: train loss: 0.12697536732942177, test loss: 0.26235132974223896\n",
      "epoch 7760: train loss: 0.12697097190860884, test loss: 0.2623511410594413\n",
      "epoch 7761: train loss: 0.12696657728796837, test loss: 0.26235095263316605\n",
      "epoch 7762: train loss: 0.1269621834672432, test loss: 0.2623507644633236\n",
      "epoch 7763: train loss: 0.12695779044617622, test loss: 0.26235057654982435\n",
      "epoch 7764: train loss: 0.1269533982245105, test loss: 0.2623503888925787\n",
      "epoch 7765: train loss: 0.1269490068019892, test loss: 0.26235020149149707\n",
      "epoch 7766: train loss: 0.12694461617835562, test loss: 0.26235001434649\n",
      "epoch 7767: train loss: 0.12694022635335311, test loss: 0.2623498274574681\n",
      "epoch 7768: train loss: 0.12693583732672523, test loss: 0.262349640824342\n",
      "epoch 7769: train loss: 0.12693144909821558, test loss: 0.2623494544470221\n",
      "epoch 7770: train loss: 0.12692706166756795, test loss: 0.2623492683254194\n",
      "epoch 7771: train loss: 0.12692267503452617, test loss: 0.2623490824594445\n",
      "epoch 7772: train loss: 0.12691828919883424, test loss: 0.2623488968490082\n",
      "epoch 7773: train loss: 0.12691390416023626, test loss: 0.26234871149402134\n",
      "epoch 7774: train loss: 0.12690951991847643, test loss: 0.26234852639439477\n",
      "epoch 7775: train loss: 0.12690513647329907, test loss: 0.26234834155003944\n",
      "epoch 7776: train loss: 0.1269007538244487, test loss: 0.26234815696086633\n",
      "epoch 7777: train loss: 0.12689637197166984, test loss: 0.2623479726267865\n",
      "epoch 7778: train loss: 0.12689199091470713, test loss: 0.26234778854771085\n",
      "epoch 7779: train loss: 0.12688761065330545, test loss: 0.2623476047235506\n",
      "epoch 7780: train loss: 0.12688323118720965, test loss: 0.26234742115421694\n",
      "epoch 7781: train loss: 0.12687885251616482, test loss: 0.26234723783962105\n",
      "epoch 7782: train loss: 0.1268744746399161, test loss: 0.262347054779674\n",
      "epoch 7783: train loss: 0.12687009755820872, test loss: 0.2623468719742874\n",
      "epoch 7784: train loss: 0.12686572127078807, test loss: 0.26234668942337236\n",
      "epoch 7785: train loss: 0.12686134577739963, test loss: 0.2623465071268402\n",
      "epoch 7786: train loss: 0.1268569710777891, test loss: 0.26234632508460254\n",
      "epoch 7787: train loss: 0.12685259717170214, test loss: 0.2623461432965707\n",
      "epoch 7788: train loss: 0.12684822405888457, test loss: 0.26234596176265623\n",
      "epoch 7789: train loss: 0.12684385173908239, test loss: 0.2623457804827708\n",
      "epoch 7790: train loss: 0.1268394802120417, test loss: 0.2623455994568258\n",
      "epoch 7791: train loss: 0.12683510947750865, test loss: 0.2623454186847331\n",
      "epoch 7792: train loss: 0.12683073953522958, test loss: 0.26234523816640426\n",
      "epoch 7793: train loss: 0.12682637038495087, test loss: 0.262345057901751\n",
      "epoch 7794: train loss: 0.12682200202641908, test loss: 0.2623448778906852\n",
      "epoch 7795: train loss: 0.12681763445938088, test loss: 0.2623446981331186\n",
      "epoch 7796: train loss: 0.12681326768358303, test loss: 0.26234451862896324\n",
      "epoch 7797: train loss: 0.12680890169877243, test loss: 0.2623443393781308\n",
      "epoch 7798: train loss: 0.12680453650469603, test loss: 0.2623441603805333\n",
      "epoch 7799: train loss: 0.12680017210110103, test loss: 0.262343981636083\n",
      "epoch 7800: train loss: 0.12679580848773456, test loss: 0.26234380314469163\n",
      "epoch 7801: train loss: 0.12679144566434405, test loss: 0.2623436249062715\n",
      "epoch 7802: train loss: 0.12678708363067692, test loss: 0.2623434469207346\n",
      "epoch 7803: train loss: 0.12678272238648072, test loss: 0.2623432691879933\n",
      "epoch 7804: train loss: 0.1267783619315032, test loss: 0.26234309170795955\n",
      "epoch 7805: train loss: 0.12677400226549213, test loss: 0.2623429144805459\n",
      "epoch 7806: train loss: 0.12676964338819544, test loss: 0.26234273750566456\n",
      "epoch 7807: train loss: 0.12676528529936115, test loss: 0.26234256078322793\n",
      "epoch 7808: train loss: 0.12676092799873742, test loss: 0.2623423843131484\n",
      "epoch 7809: train loss: 0.1267565714860725, test loss: 0.26234220809533837\n",
      "epoch 7810: train loss: 0.12675221576111478, test loss: 0.2623420321297104\n",
      "epoch 7811: train loss: 0.12674786082361275, test loss: 0.262341856416177\n",
      "epoch 7812: train loss: 0.12674350667331502, test loss: 0.2623416809546509\n",
      "epoch 7813: train loss: 0.12673915330997026, test loss: 0.2623415057450445\n",
      "epoch 7814: train loss: 0.12673480073332738, test loss: 0.2623413307872706\n",
      "epoch 7815: train loss: 0.12673044894313532, test loss: 0.262341156081242\n",
      "epoch 7816: train loss: 0.12672609793914308, test loss: 0.2623409816268713\n",
      "epoch 7817: train loss: 0.12672174772109987, test loss: 0.2623408074240714\n",
      "epoch 7818: train loss: 0.12671739828875497, test loss: 0.26234063347275516\n",
      "epoch 7819: train loss: 0.12671304964185784, test loss: 0.2623404597728354\n",
      "epoch 7820: train loss: 0.1267087017801579, test loss: 0.2623402863242251\n",
      "epoch 7821: train loss: 0.12670435470340485, test loss: 0.26234011312683736\n",
      "epoch 7822: train loss: 0.12670000841134843, test loss: 0.2623399401805851\n",
      "epoch 7823: train loss: 0.1266956629037385, test loss: 0.26233976748538135\n",
      "epoch 7824: train loss: 0.12669131818032497, test loss: 0.26233959504113935\n",
      "epoch 7825: train loss: 0.12668697424085798, test loss: 0.2623394228477721\n",
      "epoch 7826: train loss: 0.12668263108508773, test loss: 0.26233925090519306\n",
      "epoch 7827: train loss: 0.12667828871276451, test loss: 0.2623390792133151\n",
      "epoch 7828: train loss: 0.12667394712363877, test loss: 0.2623389077720519\n",
      "epoch 7829: train loss: 0.12666960631746102, test loss: 0.26233873658131657\n",
      "epoch 7830: train loss: 0.12666526629398192, test loss: 0.2623385656410226\n",
      "epoch 7831: train loss: 0.12666092705295223, test loss: 0.2623383949510833\n",
      "epoch 7832: train loss: 0.12665658859412285, test loss: 0.2623382245114123\n",
      "epoch 7833: train loss: 0.12665225091724472, test loss: 0.2623380543219229\n",
      "epoch 7834: train loss: 0.12664791402206899, test loss: 0.26233788438252886\n",
      "epoch 7835: train loss: 0.12664357790834685, test loss: 0.2623377146931437\n",
      "epoch 7836: train loss: 0.12663924257582962, test loss: 0.26233754525368097\n",
      "epoch 7837: train loss: 0.12663490802426877, test loss: 0.26233737606405455\n",
      "epoch 7838: train loss: 0.12663057425341584, test loss: 0.262337207124178\n",
      "epoch 7839: train loss: 0.12662624126302247, test loss: 0.26233703843396516\n",
      "epoch 7840: train loss: 0.1266219090528405, test loss: 0.26233686999332984\n",
      "epoch 7841: train loss: 0.12661757762262174, test loss: 0.26233670180218593\n",
      "epoch 7842: train loss: 0.1266132469721182, test loss: 0.26233653386044725\n",
      "epoch 7843: train loss: 0.12660891710108207, test loss: 0.2623363661680279\n",
      "epoch 7844: train loss: 0.12660458800926552, test loss: 0.2623361987248417\n",
      "epoch 7845: train loss: 0.12660025969642089, test loss: 0.2623360315308028\n",
      "epoch 7846: train loss: 0.1265959321623006, test loss: 0.26233586458582514\n",
      "epoch 7847: train loss: 0.1265916054066573, test loss: 0.262335697889823\n",
      "epoch 7848: train loss: 0.12658727942924358, test loss: 0.26233553144271055\n",
      "epoch 7849: train loss: 0.12658295422981225, test loss: 0.2623353652444018\n",
      "epoch 7850: train loss: 0.1265786298081162, test loss: 0.26233519929481125\n",
      "epoch 7851: train loss: 0.12657430616390852, test loss: 0.262335033593853\n",
      "epoch 7852: train loss: 0.1265699832969422, test loss: 0.2623348681414414\n",
      "epoch 7853: train loss: 0.12656566120697055, test loss: 0.26233470293749095\n",
      "epoch 7854: train loss: 0.12656133989374688, test loss: 0.262334537981916\n",
      "epoch 7855: train loss: 0.12655701935702468, test loss: 0.26233437327463105\n",
      "epoch 7856: train loss: 0.12655269959655752, test loss: 0.26233420881555053\n",
      "epoch 7857: train loss: 0.126548380612099, test loss: 0.2623340446045891\n",
      "epoch 7858: train loss: 0.126544062403403, test loss: 0.2623338806416612\n",
      "epoch 7859: train loss: 0.12653974497022336, test loss: 0.26233371692668156\n",
      "epoch 7860: train loss: 0.12653542831231412, test loss: 0.26233355345956494\n",
      "epoch 7861: train loss: 0.1265311124294294, test loss: 0.2623333902402259\n",
      "epoch 7862: train loss: 0.12652679732132344, test loss: 0.2623332272685793\n",
      "epoch 7863: train loss: 0.12652248298775057, test loss: 0.26233306454454003\n",
      "epoch 7864: train loss: 0.12651816942846525, test loss: 0.26233290206802273\n",
      "epoch 7865: train loss: 0.126513856643222, test loss: 0.26233273983894245\n",
      "epoch 7866: train loss: 0.12650954463177558, test loss: 0.2623325778572141\n",
      "epoch 7867: train loss: 0.12650523339388073, test loss: 0.2623324161227526\n",
      "epoch 7868: train loss: 0.12650092292929233, test loss: 0.262332254635473\n",
      "epoch 7869: train loss: 0.12649661323776543, test loss: 0.2623320933952904\n",
      "epoch 7870: train loss: 0.1264923043190551, test loss: 0.2623319324021199\n",
      "epoch 7871: train loss: 0.1264879961729166, test loss: 0.2623317716558766\n",
      "epoch 7872: train loss: 0.12648368879910524, test loss: 0.26233161115647574\n",
      "epoch 7873: train loss: 0.1264793821973765, test loss: 0.2623314509038325\n",
      "epoch 7874: train loss: 0.12647507636748592, test loss: 0.26233129089786217\n",
      "epoch 7875: train loss: 0.12647077130918918, test loss: 0.2623311311384802\n",
      "epoch 7876: train loss: 0.12646646702224207, test loss: 0.2623309716256017\n",
      "epoch 7877: train loss: 0.12646216350640047, test loss: 0.2623308123591423\n",
      "epoch 7878: train loss: 0.12645786076142038, test loss: 0.2623306533390173\n",
      "epoch 7879: train loss: 0.12645355878705783, test loss: 0.26233049456514224\n",
      "epoch 7880: train loss: 0.12644925758306919, test loss: 0.26233033603743267\n",
      "epoch 7881: train loss: 0.12644495714921067, test loss: 0.26233017775580414\n",
      "epoch 7882: train loss: 0.12644065748523872, test loss: 0.2623300197201722\n",
      "epoch 7883: train loss: 0.12643635859090996, test loss: 0.26232986193045266\n",
      "epoch 7884: train loss: 0.12643206046598096, test loss: 0.26232970438656106\n",
      "epoch 7885: train loss: 0.12642776311020854, test loss: 0.2623295470884133\n",
      "epoch 7886: train loss: 0.12642346652334957, test loss: 0.262329390035925\n",
      "epoch 7887: train loss: 0.126419170705161, test loss: 0.262329233229012\n",
      "epoch 7888: train loss: 0.12641487565539997, test loss: 0.2623290766675903\n",
      "epoch 7889: train loss: 0.12641058137382363, test loss: 0.2623289203515757\n",
      "epoch 7890: train loss: 0.12640628786018937, test loss: 0.2623287642808842\n",
      "epoch 7891: train loss: 0.12640199511425454, test loss: 0.26232860845543177\n",
      "epoch 7892: train loss: 0.1263977031357767, test loss: 0.26232845287513434\n",
      "epoch 7893: train loss: 0.12639341192451348, test loss: 0.26232829753990816\n",
      "epoch 7894: train loss: 0.12638912148022263, test loss: 0.2623281424496693\n",
      "epoch 7895: train loss: 0.12638483180266202, test loss: 0.26232798760433385\n",
      "epoch 7896: train loss: 0.12638054289158962, test loss: 0.26232783300381807\n",
      "epoch 7897: train loss: 0.12637625474676345, test loss: 0.26232767864803813\n",
      "epoch 7898: train loss: 0.12637196736794176, test loss: 0.26232752453691044\n",
      "epoch 7899: train loss: 0.1263676807548828, test loss: 0.26232737067035117\n",
      "epoch 7900: train loss: 0.126363394907345, test loss: 0.26232721704827683\n",
      "epoch 7901: train loss: 0.12635910982508686, test loss: 0.26232706367060366\n",
      "epoch 7902: train loss: 0.12635482550786697, test loss: 0.2623269105372483\n",
      "epoch 7903: train loss: 0.1263505419554441, test loss: 0.2623267576481272\n",
      "epoch 7904: train loss: 0.12634625916757705, test loss: 0.2623266050031568\n",
      "epoch 7905: train loss: 0.12634197714402476, test loss: 0.26232645260225373\n",
      "epoch 7906: train loss: 0.12633769588454627, test loss: 0.26232630044533456\n",
      "epoch 7907: train loss: 0.12633341538890083, test loss: 0.262326148532316\n",
      "epoch 7908: train loss: 0.12632913565684756, test loss: 0.26232599686311475\n",
      "epoch 7909: train loss: 0.12632485668814597, test loss: 0.26232584543764753\n",
      "epoch 7910: train loss: 0.12632057848255546, test loss: 0.2623256942558311\n",
      "epoch 7911: train loss: 0.1263163010398356, test loss: 0.2623255433175823\n",
      "epoch 7912: train loss: 0.12631202435974614, test loss: 0.26232539262281795\n",
      "epoch 7913: train loss: 0.1263077484420469, test loss: 0.2623252421714552\n",
      "epoch 7914: train loss: 0.12630347328649777, test loss: 0.2623250919634106\n",
      "epoch 7915: train loss: 0.12629919889285873, test loss: 0.2623249419986014\n",
      "epoch 7916: train loss: 0.12629492526088995, test loss: 0.26232479227694455\n",
      "epoch 7917: train loss: 0.12629065239035167, test loss: 0.2623246427983571\n",
      "epoch 7918: train loss: 0.12628638028100422, test loss: 0.2623244935627562\n",
      "epoch 7919: train loss: 0.12628210893260802, test loss: 0.26232434457005904\n",
      "epoch 7920: train loss: 0.12627783834492368, test loss: 0.26232419582018274\n",
      "epoch 7921: train loss: 0.12627356851771182, test loss: 0.26232404731304454\n",
      "epoch 7922: train loss: 0.12626929945073326, test loss: 0.2623238990485618\n",
      "epoch 7923: train loss: 0.12626503114374882, test loss: 0.2623237510266517\n",
      "epoch 7924: train loss: 0.12626076359651953, test loss: 0.2623236032472317\n",
      "epoch 7925: train loss: 0.12625649680880646, test loss: 0.26232345571021903\n",
      "epoch 7926: train loss: 0.12625223078037082, test loss: 0.2623233084155314\n",
      "epoch 7927: train loss: 0.1262479655109739, test loss: 0.2623231613630861\n",
      "epoch 7928: train loss: 0.1262437010003771, test loss: 0.2623230145528007\n",
      "epoch 7929: train loss: 0.126239437248342, test loss: 0.2623228679845927\n",
      "epoch 7930: train loss: 0.12623517425463013, test loss: 0.2623227216583798\n",
      "epoch 7931: train loss: 0.12623091201900336, test loss: 0.26232257557407956\n",
      "epoch 7932: train loss: 0.1262266505412234, test loss: 0.26232242973160974\n",
      "epoch 7933: train loss: 0.12622238982105224, test loss: 0.262322284130888\n",
      "epoch 7934: train loss: 0.12621812985825195, test loss: 0.262322138771832\n",
      "epoch 7935: train loss: 0.12621387065258466, test loss: 0.2623219936543598\n",
      "epoch 7936: train loss: 0.12620961220381266, test loss: 0.26232184877838904\n",
      "epoch 7937: train loss: 0.1262053545116983, test loss: 0.26232170414383765\n",
      "epoch 7938: train loss: 0.12620109757600406, test loss: 0.2623215597506236\n",
      "epoch 7939: train loss: 0.12619684139649254, test loss: 0.26232141559866484\n",
      "epoch 7940: train loss: 0.12619258597292637, test loss: 0.2623212716878794\n",
      "epoch 7941: train loss: 0.12618833130506843, test loss: 0.2623211280181852\n",
      "epoch 7942: train loss: 0.12618407739268162, test loss: 0.2623209845895004\n",
      "epoch 7943: train loss: 0.12617982423552881, test loss: 0.2623208414017432\n",
      "epoch 7944: train loss: 0.12617557183337327, test loss: 0.2623206984548317\n",
      "epoch 7945: train loss: 0.12617132018597813, test loss: 0.2623205557486841\n",
      "epoch 7946: train loss: 0.12616706929310675, test loss: 0.26232041328321865\n",
      "epoch 7947: train loss: 0.12616281915452254, test loss: 0.2623202710583537\n",
      "epoch 7948: train loss: 0.12615856976998907, test loss: 0.2623201290740075\n",
      "epoch 7949: train loss: 0.12615432113926991, test loss: 0.2623199873300984\n",
      "epoch 7950: train loss: 0.12615007326212888, test loss: 0.2623198458265449\n",
      "epoch 7951: train loss: 0.12614582613832975, test loss: 0.26231970456326537\n",
      "epoch 7952: train loss: 0.12614157976763654, test loss: 0.2623195635401783\n",
      "epoch 7953: train loss: 0.12613733414981332, test loss: 0.2623194227572022\n",
      "epoch 7954: train loss: 0.12613308928462422, test loss: 0.2623192822142558\n",
      "epoch 7955: train loss: 0.1261288451718335, test loss: 0.26231914191125755\n",
      "epoch 7956: train loss: 0.12612460181120558, test loss: 0.262319001848126\n",
      "epoch 7957: train loss: 0.1261203592025049, test loss: 0.26231886202478016\n",
      "epoch 7958: train loss: 0.12611611734549608, test loss: 0.26231872244113846\n",
      "epoch 7959: train loss: 0.12611187623994377, test loss: 0.26231858309711986\n",
      "epoch 7960: train loss: 0.1261076358856128, test loss: 0.26231844399264304\n",
      "epoch 7961: train loss: 0.12610339628226808, test loss: 0.2623183051276268\n",
      "epoch 7962: train loss: 0.12609915742967456, test loss: 0.26231816650199025\n",
      "epoch 7963: train loss: 0.12609491932759742, test loss: 0.26231802811565214\n",
      "epoch 7964: train loss: 0.1260906819758018, test loss: 0.26231788996853145\n",
      "epoch 7965: train loss: 0.12608644537405309, test loss: 0.2623177520605473\n",
      "epoch 7966: train loss: 0.12608220952211668, test loss: 0.2623176143916185\n",
      "epoch 7967: train loss: 0.1260779744197581, test loss: 0.26231747696166435\n",
      "epoch 7968: train loss: 0.12607374006674296, test loss: 0.26231733977060395\n",
      "epoch 7969: train loss: 0.12606950646283704, test loss: 0.2623172028183564\n",
      "epoch 7970: train loss: 0.12606527360780614, test loss: 0.2623170661048409\n",
      "epoch 7971: train loss: 0.12606104150141617, test loss: 0.26231692962997677\n",
      "epoch 7972: train loss: 0.1260568101434333, test loss: 0.2623167933936832\n",
      "epoch 7973: train loss: 0.12605257953362362, test loss: 0.2623166573958795\n",
      "epoch 7974: train loss: 0.12604834967175335, test loss: 0.26231652163648517\n",
      "epoch 7975: train loss: 0.12604412055758885, test loss: 0.2623163861154195\n",
      "epoch 7976: train loss: 0.12603989219089665, test loss: 0.2623162508326019\n",
      "epoch 7977: train loss: 0.12603566457144327, test loss: 0.26231611578795194\n",
      "epoch 7978: train loss: 0.12603143769899539, test loss: 0.2623159809813892\n",
      "epoch 7979: train loss: 0.12602721157331978, test loss: 0.262315846412833\n",
      "epoch 7980: train loss: 0.1260229861941833, test loss: 0.26231571208220317\n",
      "epoch 7981: train loss: 0.126018761561353, test loss: 0.26231557798941924\n",
      "epoch 7982: train loss: 0.1260145376745959, test loss: 0.26231544413440083\n",
      "epoch 7983: train loss: 0.1260103145336792, test loss: 0.2623153105170678\n",
      "epoch 7984: train loss: 0.1260060921383702, test loss: 0.26231517713733976\n",
      "epoch 7985: train loss: 0.1260018704884363, test loss: 0.26231504399513667\n",
      "epoch 7986: train loss: 0.12599764958364504, test loss: 0.26231491109037813\n",
      "epoch 7987: train loss: 0.12599342942376393, test loss: 0.2623147784229842\n",
      "epoch 7988: train loss: 0.12598921000856073, test loss: 0.2623146459928748\n",
      "epoch 7989: train loss: 0.12598499133780325, test loss: 0.2623145137999697\n",
      "epoch 7990: train loss: 0.1259807734112594, test loss: 0.26231438184418904\n",
      "epoch 7991: train loss: 0.1259765562286972, test loss: 0.2623142501254529\n",
      "epoch 7992: train loss: 0.12597233978988473, test loss: 0.26231411864368115\n",
      "epoch 7993: train loss: 0.1259681240945902, test loss: 0.26231398739879397\n",
      "epoch 7994: train loss: 0.12596390914258204, test loss: 0.2623138563907116\n",
      "epoch 7995: train loss: 0.12595969493362852, test loss: 0.2623137256193541\n",
      "epoch 7996: train loss: 0.1259554814674983, test loss: 0.2623135950846418\n",
      "epoch 7997: train loss: 0.12595126874395995, test loss: 0.26231346478649487\n",
      "epoch 7998: train loss: 0.1259470567627822, test loss: 0.26231333472483365\n",
      "epoch 7999: train loss: 0.12594284552373392, test loss: 0.26231320489957854\n",
      "epoch 8000: train loss: 0.12593863502658398, test loss: 0.26231307531064973\n",
      "epoch 8001: train loss: 0.12593442527110152, test loss: 0.2623129459579678\n",
      "epoch 8002: train loss: 0.12593021625705558, test loss: 0.26231281684145313\n",
      "epoch 8003: train loss: 0.12592600798421547, test loss: 0.26231268796102625\n",
      "epoch 8004: train loss: 0.1259218004523505, test loss: 0.2623125593166077\n",
      "epoch 8005: train loss: 0.12591759366123015, test loss: 0.26231243090811796\n",
      "epoch 8006: train loss: 0.12591338761062398, test loss: 0.26231230273547773\n",
      "epoch 8007: train loss: 0.1259091823003016, test loss: 0.2623121747986077\n",
      "epoch 8008: train loss: 0.12590497773003276, test loss: 0.2623120470974284\n",
      "epoch 8009: train loss: 0.12590077389958737, test loss: 0.2623119196318607\n",
      "epoch 8010: train loss: 0.12589657080873531, test loss: 0.2623117924018253\n",
      "epoch 8011: train loss: 0.12589236845724674, test loss: 0.2623116654072429\n",
      "epoch 8012: train loss: 0.12588816684489174, test loss: 0.26231153864803447\n",
      "epoch 8013: train loss: 0.12588396597144064, test loss: 0.2623114121241209\n",
      "epoch 8014: train loss: 0.12587976583666374, test loss: 0.262311285835423\n",
      "epoch 8015: train loss: 0.12587556644033154, test loss: 0.2623111597818618\n",
      "epoch 8016: train loss: 0.1258713677822146, test loss: 0.2623110339633583\n",
      "epoch 8017: train loss: 0.12586716986208357, test loss: 0.2623109083798335\n",
      "epoch 8018: train loss: 0.12586297267970928, test loss: 0.26231078303120847\n",
      "epoch 8019: train loss: 0.12585877623486255, test loss: 0.26231065791740427\n",
      "epoch 8020: train loss: 0.12585458052731435, test loss: 0.26231053303834206\n",
      "epoch 8021: train loss: 0.1258503855568358, test loss: 0.2623104083939431\n",
      "epoch 8022: train loss: 0.12584619132319802, test loss: 0.26231028398412853\n",
      "epoch 8023: train loss: 0.12584199782617234, test loss: 0.26231015980881967\n",
      "epoch 8024: train loss: 0.12583780506553008, test loss: 0.2623100358679378\n",
      "epoch 8025: train loss: 0.12583361304104276, test loss: 0.26230991216140415\n",
      "epoch 8026: train loss: 0.12582942175248196, test loss: 0.26230978868914023\n",
      "epoch 8027: train loss: 0.12582523119961933, test loss: 0.2623096654510674\n",
      "epoch 8028: train loss: 0.1258210413822267, test loss: 0.26230954244710714\n",
      "epoch 8029: train loss: 0.12581685230007586, test loss: 0.26230941967718086\n",
      "epoch 8030: train loss: 0.12581266395293889, test loss: 0.26230929714121015\n",
      "epoch 8031: train loss: 0.12580847634058784, test loss: 0.2623091748391165\n",
      "epoch 8032: train loss: 0.12580428946279487, test loss: 0.26230905277082156\n",
      "epoch 8033: train loss: 0.1258001033193323, test loss: 0.262308930936247\n",
      "epoch 8034: train loss: 0.1257959179099725, test loss: 0.2623088093353145\n",
      "epoch 8035: train loss: 0.1257917332344879, test loss: 0.2623086879679456\n",
      "epoch 8036: train loss: 0.12578754929265115, test loss: 0.2623085668340623\n",
      "epoch 8037: train loss: 0.125783366084235, test loss: 0.2623084459335862\n",
      "epoch 8038: train loss: 0.12577918360901205, test loss: 0.2623083252664392\n",
      "epoch 8039: train loss: 0.12577500186675533, test loss: 0.26230820483254313\n",
      "epoch 8040: train loss: 0.1257708208572378, test loss: 0.26230808463182004\n",
      "epoch 8041: train loss: 0.1257666405802325, test loss: 0.26230796466419165\n",
      "epoch 8042: train loss: 0.12576246103551267, test loss: 0.26230784492958004\n",
      "epoch 8043: train loss: 0.12575828222285157, test loss: 0.26230772542790726\n",
      "epoch 8044: train loss: 0.12575410414202257, test loss: 0.26230760615909526\n",
      "epoch 8045: train loss: 0.12574992679279917, test loss: 0.26230748712306623\n",
      "epoch 8046: train loss: 0.12574575017495496, test loss: 0.2623073683197422\n",
      "epoch 8047: train loss: 0.12574157428826363, test loss: 0.26230724974904546\n",
      "epoch 8048: train loss: 0.12573739913249893, test loss: 0.26230713141089806\n",
      "epoch 8049: train loss: 0.12573322470743478, test loss: 0.2623070133052223\n",
      "epoch 8050: train loss: 0.12572905101284515, test loss: 0.2623068954319406\n",
      "epoch 8051: train loss: 0.12572487804850413, test loss: 0.262306777790975\n",
      "epoch 8052: train loss: 0.12572070581418587, test loss: 0.26230666038224804\n",
      "epoch 8053: train loss: 0.12571653430966467, test loss: 0.2623065432056821\n",
      "epoch 8054: train loss: 0.12571236353471493, test loss: 0.2623064262611995\n",
      "epoch 8055: train loss: 0.1257081934891111, test loss: 0.2623063095487228\n",
      "epoch 8056: train loss: 0.1257040241726278, test loss: 0.2623061930681744\n",
      "epoch 8057: train loss: 0.12569985558503965, test loss: 0.262306076819477\n",
      "epoch 8058: train loss: 0.12569568772612147, test loss: 0.26230596080255303\n",
      "epoch 8059: train loss: 0.12569152059564814, test loss: 0.26230584501732507\n",
      "epoch 8060: train loss: 0.1256873541933946, test loss: 0.2623057294637159\n",
      "epoch 8061: train loss: 0.12568318851913593, test loss: 0.26230561414164816\n",
      "epoch 8062: train loss: 0.12567902357264732, test loss: 0.26230549905104455\n",
      "epoch 8063: train loss: 0.12567485935370404, test loss: 0.26230538419182775\n",
      "epoch 8064: train loss: 0.12567069586208146, test loss: 0.26230526956392075\n",
      "epoch 8065: train loss: 0.12566653309755502, test loss: 0.26230515516724623\n",
      "epoch 8066: train loss: 0.1256623710599003, test loss: 0.2623050410017271\n",
      "epoch 8067: train loss: 0.125658209748893, test loss: 0.2623049270672862\n",
      "epoch 8068: train loss: 0.12565404916430886, test loss: 0.2623048133638466\n",
      "epoch 8069: train loss: 0.12564988930592372, test loss: 0.2623046998913312\n",
      "epoch 8070: train loss: 0.12564573017351358, test loss: 0.26230458664966305\n",
      "epoch 8071: train loss: 0.12564157176685448, test loss: 0.26230447363876513\n",
      "epoch 8072: train loss: 0.12563741408572254, test loss: 0.2623043608585606\n",
      "epoch 8073: train loss: 0.1256332571298941, test loss: 0.26230424830897264\n",
      "epoch 8074: train loss: 0.12562910089914547, test loss: 0.2623041359899243\n",
      "epoch 8075: train loss: 0.12562494539325308, test loss: 0.2623040239013388\n",
      "epoch 8076: train loss: 0.1256207906119935, test loss: 0.26230391204313935\n",
      "epoch 8077: train loss: 0.12561663655514338, test loss: 0.26230380041524937\n",
      "epoch 8078: train loss: 0.12561248322247948, test loss: 0.26230368901759205\n",
      "epoch 8079: train loss: 0.1256083306137786, test loss: 0.2623035778500908\n",
      "epoch 8080: train loss: 0.1256041787288177, test loss: 0.26230346691266887\n",
      "epoch 8081: train loss: 0.12560002756737387, test loss: 0.2623033562052498\n",
      "epoch 8082: train loss: 0.12559587712922415, test loss: 0.2623032457277571\n",
      "epoch 8083: train loss: 0.1255917274141459, test loss: 0.2623031354801141\n",
      "epoch 8084: train loss: 0.1255875784219163, test loss: 0.26230302546224443\n",
      "epoch 8085: train loss: 0.1255834301523129, test loss: 0.26230291567407166\n",
      "epoch 8086: train loss: 0.1255792826051132, test loss: 0.2623028061155193\n",
      "epoch 8087: train loss: 0.1255751357800948, test loss: 0.26230269678651114\n",
      "epoch 8088: train loss: 0.12557098967703542, test loss: 0.2623025876869707\n",
      "epoch 8089: train loss: 0.12556684429571288, test loss: 0.26230247881682184\n",
      "epoch 8090: train loss: 0.12556269963590516, test loss: 0.2623023701759882\n",
      "epoch 8091: train loss: 0.1255585556973902, test loss: 0.26230226176439353\n",
      "epoch 8092: train loss: 0.1255544124799461, test loss: 0.26230215358196174\n",
      "epoch 8093: train loss: 0.12555026998335111, test loss: 0.26230204562861664\n",
      "epoch 8094: train loss: 0.1255461282073836, test loss: 0.2623019379042822\n",
      "epoch 8095: train loss: 0.12554198715182183, test loss: 0.26230183040888216\n",
      "epoch 8096: train loss: 0.12553784681644437, test loss: 0.26230172314234074\n",
      "epoch 8097: train loss: 0.12553370720102983, test loss: 0.26230161610458175\n",
      "epoch 8098: train loss: 0.12552956830535686, test loss: 0.2623015092955293\n",
      "epoch 8099: train loss: 0.1255254301292043, test loss: 0.26230140271510743\n",
      "epoch 8100: train loss: 0.12552129267235101, test loss: 0.2623012963632403\n",
      "epoch 8101: train loss: 0.12551715593457596, test loss: 0.26230119023985204\n",
      "epoch 8102: train loss: 0.12551301991565825, test loss: 0.2623010843448668\n",
      "epoch 8103: train loss: 0.12550888461537701, test loss: 0.2623009786782088\n",
      "epoch 8104: train loss: 0.1255047500335116, test loss: 0.2623008732398022\n",
      "epoch 8105: train loss: 0.12550061616984134, test loss: 0.2623007680295715\n",
      "epoch 8106: train loss: 0.12549648302414565, test loss: 0.2623006630474409\n",
      "epoch 8107: train loss: 0.12549235059620417, test loss: 0.26230055829333476\n",
      "epoch 8108: train loss: 0.1254882188857965, test loss: 0.26230045376717753\n",
      "epoch 8109: train loss: 0.12548408789270243, test loss: 0.2623003494688935\n",
      "epoch 8110: train loss: 0.1254799576167018, test loss: 0.26230024539840735\n",
      "epoch 8111: train loss: 0.12547582805757454, test loss: 0.26230014155564335\n",
      "epoch 8112: train loss: 0.1254716992151007, test loss: 0.26230003794052625\n",
      "epoch 8113: train loss: 0.12546757108906043, test loss: 0.26229993455298056\n",
      "epoch 8114: train loss: 0.12546344367923395, test loss: 0.26229983139293084\n",
      "epoch 8115: train loss: 0.1254593169854016, test loss: 0.2622997284603017\n",
      "epoch 8116: train loss: 0.1254551910073438, test loss: 0.262299625755018\n",
      "epoch 8117: train loss: 0.12545106574484108, test loss: 0.26229952327700423\n",
      "epoch 8118: train loss: 0.12544694119767408, test loss: 0.2622994210261853\n",
      "epoch 8119: train loss: 0.1254428173656234, test loss: 0.26229931900248593\n",
      "epoch 8120: train loss: 0.12543869424846998, test loss: 0.26229921720583094\n",
      "epoch 8121: train loss: 0.12543457184599469, test loss: 0.2622991156361453\n",
      "epoch 8122: train loss: 0.1254304501579785, test loss: 0.2622990142933538\n",
      "epoch 8123: train loss: 0.1254263291842025, test loss: 0.2622989131773813\n",
      "epoch 8124: train loss: 0.12542220892444791, test loss: 0.262298812288153\n",
      "epoch 8125: train loss: 0.12541808937849602, test loss: 0.26229871162559365\n",
      "epoch 8126: train loss: 0.12541397054612818, test loss: 0.26229861118962844\n",
      "epoch 8127: train loss: 0.12540985242712588, test loss: 0.2622985109801823\n",
      "epoch 8128: train loss: 0.1254057350212707, test loss: 0.2622984109971806\n",
      "epoch 8129: train loss: 0.12540161832834432, test loss: 0.2622983112405482\n",
      "epoch 8130: train loss: 0.12539750234812846, test loss: 0.26229821171021045\n",
      "epoch 8131: train loss: 0.125393387080405, test loss: 0.26229811240609247\n",
      "epoch 8132: train loss: 0.1253892725249559, test loss: 0.26229801332811953\n",
      "epoch 8133: train loss: 0.12538515868156316, test loss: 0.2622979144762169\n",
      "epoch 8134: train loss: 0.125381045550009, test loss: 0.26229781585031003\n",
      "epoch 8135: train loss: 0.1253769331300756, test loss: 0.26229771745032415\n",
      "epoch 8136: train loss: 0.12537282142154532, test loss: 0.2622976192761846\n",
      "epoch 8137: train loss: 0.12536871042420056, test loss: 0.262297521327817\n",
      "epoch 8138: train loss: 0.12536460013782386, test loss: 0.2622974236051466\n",
      "epoch 8139: train loss: 0.1253604905621978, test loss: 0.2622973261080989\n",
      "epoch 8140: train loss: 0.12535638169710514, test loss: 0.2622972288365996\n",
      "epoch 8141: train loss: 0.1253522735423286, test loss: 0.2622971317905742\n",
      "epoch 8142: train loss: 0.1253481660976512, test loss: 0.2622970349699482\n",
      "epoch 8143: train loss: 0.12534405936285586, test loss: 0.26229693837464735\n",
      "epoch 8144: train loss: 0.12533995333772568, test loss: 0.26229684200459735\n",
      "epoch 8145: train loss: 0.1253358480220438, test loss: 0.26229674585972373\n",
      "epoch 8146: train loss: 0.12533174341559358, test loss: 0.2622966499399524\n",
      "epoch 8147: train loss: 0.1253276395181583, test loss: 0.2622965542452089\n",
      "epoch 8148: train loss: 0.1253235363295215, test loss: 0.2622964587754194\n",
      "epoch 8149: train loss: 0.12531943384946667, test loss: 0.2622963635305095\n",
      "epoch 8150: train loss: 0.12531533207777754, test loss: 0.26229626851040505\n",
      "epoch 8151: train loss: 0.1253112310142378, test loss: 0.2622961737150321\n",
      "epoch 8152: train loss: 0.12530713065863128, test loss: 0.2622960791443165\n",
      "epoch 8153: train loss: 0.12530303101074194, test loss: 0.26229598479818433\n",
      "epoch 8154: train loss: 0.1252989320703538, test loss: 0.2622958906765615\n",
      "epoch 8155: train loss: 0.12529483383725099, test loss: 0.2622957967793741\n",
      "epoch 8156: train loss: 0.12529073631121773, test loss: 0.26229570310654826\n",
      "epoch 8157: train loss: 0.12528663949203828, test loss: 0.2622956096580101\n",
      "epoch 8158: train loss: 0.1252825433794971, test loss: 0.26229551643368565\n",
      "epoch 8159: train loss: 0.12527844797337867, test loss: 0.26229542343350126\n",
      "epoch 8160: train loss: 0.12527435327346756, test loss: 0.2622953306573831\n",
      "epoch 8161: train loss: 0.12527025927954843, test loss: 0.2622952381052574\n",
      "epoch 8162: train loss: 0.12526616599140614, test loss: 0.2622951457770505\n",
      "epoch 8163: train loss: 0.12526207340882548, test loss: 0.2622950536726887\n",
      "epoch 8164: train loss: 0.12525798153159146, test loss: 0.26229496179209827\n",
      "epoch 8165: train loss: 0.12525389035948908, test loss: 0.2622948701352058\n",
      "epoch 8166: train loss: 0.12524979989230356, test loss: 0.2622947787019376\n",
      "epoch 8167: train loss: 0.12524571012982008, test loss: 0.2622946874922202\n",
      "epoch 8168: train loss: 0.12524162107182402, test loss: 0.2622945965059801\n",
      "epoch 8169: train loss: 0.12523753271810079, test loss: 0.2622945057431437\n",
      "epoch 8170: train loss: 0.12523344506843592, test loss: 0.2622944152036377\n",
      "epoch 8171: train loss: 0.125229358122615, test loss: 0.26229432488738863\n",
      "epoch 8172: train loss: 0.12522527188042376, test loss: 0.2622942347943233\n",
      "epoch 8173: train loss: 0.12522118634164794, test loss: 0.26229414492436814\n",
      "epoch 8174: train loss: 0.1252171015060735, test loss: 0.26229405527745003\n",
      "epoch 8175: train loss: 0.12521301737348645, test loss: 0.2622939658534956\n",
      "epoch 8176: train loss: 0.12520893394367277, test loss: 0.2622938766524317\n",
      "epoch 8177: train loss: 0.1252048512164187, test loss: 0.26229378767418515\n",
      "epoch 8178: train loss: 0.1252007691915105, test loss: 0.26229369891868276\n",
      "epoch 8179: train loss: 0.12519668786873447, test loss: 0.2622936103858513\n",
      "epoch 8180: train loss: 0.12519260724787715, test loss: 0.2622935220756179\n",
      "epoch 8181: train loss: 0.12518852732872499, test loss: 0.26229343398790933\n",
      "epoch 8182: train loss: 0.12518444811106463, test loss: 0.26229334612265265\n",
      "epoch 8183: train loss: 0.12518036959468287, test loss: 0.2622932584797749\n",
      "epoch 8184: train loss: 0.12517629177936646, test loss: 0.2622931710592031\n",
      "epoch 8185: train loss: 0.12517221466490233, test loss: 0.26229308386086425\n",
      "epoch 8186: train loss: 0.12516813825107748, test loss: 0.26229299688468555\n",
      "epoch 8187: train loss: 0.125164062537679, test loss: 0.26229291013059414\n",
      "epoch 8188: train loss: 0.1251599875244941, test loss: 0.2622928235985172\n",
      "epoch 8189: train loss: 0.12515591321131, test loss: 0.2622927372883819\n",
      "epoch 8190: train loss: 0.12515183959791412, test loss: 0.2622926512001156\n",
      "epoch 8191: train loss: 0.12514776668409386, test loss: 0.2622925653336455\n",
      "epoch 8192: train loss: 0.12514369446963683, test loss: 0.26229247968889896\n",
      "epoch 8193: train loss: 0.1251396229543307, test loss: 0.26229239426580325\n",
      "epoch 8194: train loss: 0.12513555213796315, test loss: 0.26229230906428586\n",
      "epoch 8195: train loss: 0.125131482020322, test loss: 0.26229222408427416\n",
      "epoch 8196: train loss: 0.1251274126011952, test loss: 0.26229213932569556\n",
      "epoch 8197: train loss: 0.12512334388037077, test loss: 0.2622920547884776\n",
      "epoch 8198: train loss: 0.12511927585763677, test loss: 0.2622919704725479\n",
      "epoch 8199: train loss: 0.1251152085327814, test loss: 0.2622918863778338\n",
      "epoch 8200: train loss: 0.125111141905593, test loss: 0.2622918025042631\n",
      "epoch 8201: train loss: 0.1251070759758599, test loss: 0.2622917188517633\n",
      "epoch 8202: train loss: 0.12510301074337055, test loss: 0.26229163542026207\n",
      "epoch 8203: train loss: 0.12509894620791356, test loss: 0.2622915522096871\n",
      "epoch 8204: train loss: 0.12509488236927754, test loss: 0.2622914692199661\n",
      "epoch 8205: train loss: 0.12509081922725127, test loss: 0.26229138645102684\n",
      "epoch 8206: train loss: 0.1250867567816235, test loss: 0.2622913039027972\n",
      "epoch 8207: train loss: 0.12508269503218328, test loss: 0.2622912215752048\n",
      "epoch 8208: train loss: 0.12507863397871952, test loss: 0.26229113946817767\n",
      "epoch 8209: train loss: 0.12507457362102137, test loss: 0.2622910575816437\n",
      "epoch 8210: train loss: 0.12507051395887803, test loss: 0.2622909759155307\n",
      "epoch 8211: train loss: 0.12506645499207875, test loss: 0.2622908944697668\n",
      "epoch 8212: train loss: 0.12506239672041294, test loss: 0.2622908132442797\n",
      "epoch 8213: train loss: 0.12505833914367007, test loss: 0.26229073223899774\n",
      "epoch 8214: train loss: 0.12505428226163967, test loss: 0.2622906514538488\n",
      "epoch 8215: train loss: 0.12505022607411143, test loss: 0.26229057088876095\n",
      "epoch 8216: train loss: 0.12504617058087505, test loss: 0.26229049054366244\n",
      "epoch 8217: train loss: 0.1250421157817204, test loss: 0.26229041041848133\n",
      "epoch 8218: train loss: 0.12503806167643736, test loss: 0.2622903305131459\n",
      "epoch 8219: train loss: 0.12503400826481598, test loss: 0.2622902508275843\n",
      "epoch 8220: train loss: 0.12502995554664634, test loss: 0.2622901713617247\n",
      "epoch 8221: train loss: 0.12502590352171863, test loss: 0.26229009211549553\n",
      "epoch 8222: train loss: 0.12502185218982315, test loss: 0.2622900130888251\n",
      "epoch 8223: train loss: 0.12501780155075026, test loss: 0.26228993428164166\n",
      "epoch 8224: train loss: 0.12501375160429043, test loss: 0.26228985569387375\n",
      "epoch 8225: train loss: 0.1250097023502342, test loss: 0.26228977732544967\n",
      "epoch 8226: train loss: 0.12500565378837222, test loss: 0.26228969917629796\n",
      "epoch 8227: train loss: 0.12500160591849524, test loss: 0.262289621246347\n",
      "epoch 8228: train loss: 0.12499755874039409, test loss: 0.26228954353552547\n",
      "epoch 8229: train loss: 0.12499351225385961, test loss: 0.2622894660437618\n",
      "epoch 8230: train loss: 0.1249894664586829, test loss: 0.2622893887709846\n",
      "epoch 8231: train loss: 0.12498542135465499, test loss: 0.2622893117171225\n",
      "epoch 8232: train loss: 0.1249813769415671, test loss: 0.26228923488210415\n",
      "epoch 8233: train loss: 0.12497733321921047, test loss: 0.2622891582658582\n",
      "epoch 8234: train loss: 0.12497329018737649, test loss: 0.26228908186831346\n",
      "epoch 8235: train loss: 0.12496924784585658, test loss: 0.26228900568939856\n",
      "epoch 8236: train loss: 0.12496520619444229, test loss: 0.2622889297290424\n",
      "epoch 8237: train loss: 0.1249611652329253, test loss: 0.26228885398717366\n",
      "epoch 8238: train loss: 0.12495712496109727, test loss: 0.26228877846372134\n",
      "epoch 8239: train loss: 0.12495308537875001, test loss: 0.2622887031586143\n",
      "epoch 8240: train loss: 0.12494904648567547, test loss: 0.2622886280717813\n",
      "epoch 8241: train loss: 0.12494500828166558, test loss: 0.26228855320315153\n",
      "epoch 8242: train loss: 0.12494097076651248, test loss: 0.2622884785526537\n",
      "epoch 8243: train loss: 0.1249369339400083, test loss: 0.26228840412021703\n",
      "epoch 8244: train loss: 0.12493289780194525, test loss: 0.26228832990577045\n",
      "epoch 8245: train loss: 0.12492886235211578, test loss: 0.26228825590924315\n",
      "epoch 8246: train loss: 0.12492482759031225, test loss: 0.26228818213056415\n",
      "epoch 8247: train loss: 0.12492079351632718, test loss: 0.2622881085696626\n",
      "epoch 8248: train loss: 0.12491676012995322, test loss: 0.2622880352264677\n",
      "epoch 8249: train loss: 0.12491272743098307, test loss: 0.26228796210090866\n",
      "epoch 8250: train loss: 0.12490869541920947, test loss: 0.26228788919291474\n",
      "epoch 8251: train loss: 0.12490466409442538, test loss: 0.26228781650241517\n",
      "epoch 8252: train loss: 0.12490063345642369, test loss: 0.26228774402933924\n",
      "epoch 8253: train loss: 0.12489660350499748, test loss: 0.2622876717736163\n",
      "epoch 8254: train loss: 0.12489257423993994, test loss: 0.26228759973517574\n",
      "epoch 8255: train loss: 0.12488854566104425, test loss: 0.26228752791394705\n",
      "epoch 8256: train loss: 0.12488451776810376, test loss: 0.26228745630985945\n",
      "epoch 8257: train loss: 0.12488049056091186, test loss: 0.2622873849228426\n",
      "epoch 8258: train loss: 0.12487646403926204, test loss: 0.26228731375282593\n",
      "epoch 8259: train loss: 0.12487243820294794, test loss: 0.262287242799739\n",
      "epoch 8260: train loss: 0.12486841305176317, test loss: 0.26228717206351126\n",
      "epoch 8261: train loss: 0.12486438858550156, test loss: 0.26228710154407253\n",
      "epoch 8262: train loss: 0.1248603648039569, test loss: 0.2622870312413522\n",
      "epoch 8263: train loss: 0.12485634170692318, test loss: 0.2622869611552801\n",
      "epoch 8264: train loss: 0.12485231929419444, test loss: 0.26228689128578586\n",
      "epoch 8265: train loss: 0.12484829756556474, test loss: 0.2622868216327992\n",
      "epoch 8266: train loss: 0.12484427652082829, test loss: 0.2622867521962498\n",
      "epoch 8267: train loss: 0.12484025615977945, test loss: 0.26228668297606766\n",
      "epoch 8268: train loss: 0.12483623648221254, test loss: 0.2622866139721824\n",
      "epoch 8269: train loss: 0.12483221748792206, test loss: 0.262286545184524\n",
      "epoch 8270: train loss: 0.12482819917670254, test loss: 0.26228647661302223\n",
      "epoch 8271: train loss: 0.12482418154834862, test loss: 0.2622864082576071\n",
      "epoch 8272: train loss: 0.12482016460265509, test loss: 0.26228634011820856\n",
      "epoch 8273: train loss: 0.1248161483394167, test loss: 0.26228627219475653\n",
      "epoch 8274: train loss: 0.12481213275842841, test loss: 0.2622862044871811\n",
      "epoch 8275: train loss: 0.12480811785948522, test loss: 0.26228613699541226\n",
      "epoch 8276: train loss: 0.12480410364238217, test loss: 0.2622860697193801\n",
      "epoch 8277: train loss: 0.12480009010691447, test loss: 0.26228600265901475\n",
      "epoch 8278: train loss: 0.12479607725287735, test loss: 0.26228593581424625\n",
      "epoch 8279: train loss: 0.12479206508006616, test loss: 0.26228586918500496\n",
      "epoch 8280: train loss: 0.12478805358827635, test loss: 0.26228580277122093\n",
      "epoch 8281: train loss: 0.12478404277730341, test loss: 0.26228573657282445\n",
      "epoch 8282: train loss: 0.12478003264694301, test loss: 0.26228567058974583\n",
      "epoch 8283: train loss: 0.12477602319699077, test loss: 0.2622856048219153\n",
      "epoch 8284: train loss: 0.12477201442724255, test loss: 0.26228553926926323\n",
      "epoch 8285: train loss: 0.12476800633749413, test loss: 0.26228547393172\n",
      "epoch 8286: train loss: 0.12476399892754156, test loss: 0.26228540880921597\n",
      "epoch 8287: train loss: 0.1247599921971808, test loss: 0.26228534390168157\n",
      "epoch 8288: train loss: 0.12475598614620803, test loss: 0.26228527920904726\n",
      "epoch 8289: train loss: 0.12475198077441946, test loss: 0.2622852147312436\n",
      "epoch 8290: train loss: 0.1247479760816114, test loss: 0.262285150468201\n",
      "epoch 8291: train loss: 0.12474397206758024, test loss: 0.2622850864198501\n",
      "epoch 8292: train loss: 0.12473996873212245, test loss: 0.26228502258612146\n",
      "epoch 8293: train loss: 0.1247359660750346, test loss: 0.26228495896694565\n",
      "epoch 8294: train loss: 0.12473196409611333, test loss: 0.2622848955622533\n",
      "epoch 8295: train loss: 0.1247279627951554, test loss: 0.26228483237197525\n",
      "epoch 8296: train loss: 0.12472396217195762, test loss: 0.2622847693960421\n",
      "epoch 8297: train loss: 0.12471996222631691, test loss: 0.26228470663438475\n",
      "epoch 8298: train loss: 0.12471596295803027, test loss: 0.26228464408693364\n",
      "epoch 8299: train loss: 0.12471196436689476, test loss: 0.2622845817536198\n",
      "epoch 8300: train loss: 0.1247079664527076, test loss: 0.262284519634374\n",
      "epoch 8301: train loss: 0.12470396921526598, test loss: 0.2622844577291273\n",
      "epoch 8302: train loss: 0.12469997265436733, test loss: 0.26228439603781034\n",
      "epoch 8303: train loss: 0.12469597676980902, test loss: 0.26228433456035416\n",
      "epoch 8304: train loss: 0.12469198156138857, test loss: 0.26228427329668974\n",
      "epoch 8305: train loss: 0.12468798702890359, test loss: 0.2622842122467481\n",
      "epoch 8306: train loss: 0.1246839931721518, test loss: 0.26228415141046013\n",
      "epoch 8307: train loss: 0.12467999999093092, test loss: 0.262284090787757\n",
      "epoch 8308: train loss: 0.12467600748503888, test loss: 0.26228403037856984\n",
      "epoch 8309: train loss: 0.12467201565427354, test loss: 0.2622839701828296\n",
      "epoch 8310: train loss: 0.124668024498433, test loss: 0.26228391020046765\n",
      "epoch 8311: train loss: 0.12466403401731536, test loss: 0.26228385043141506\n",
      "epoch 8312: train loss: 0.12466004421071883, test loss: 0.262283790875603\n",
      "epoch 8313: train loss: 0.12465605507844169, test loss: 0.26228373153296275\n",
      "epoch 8314: train loss: 0.12465206662028233, test loss: 0.26228367240342565\n",
      "epoch 8315: train loss: 0.1246480788360392, test loss: 0.26228361348692286\n",
      "epoch 8316: train loss: 0.12464409172551084, test loss: 0.26228355478338583\n",
      "epoch 8317: train loss: 0.12464010528849592, test loss: 0.26228349629274594\n",
      "epoch 8318: train loss: 0.12463611952479312, test loss: 0.26228343801493453\n",
      "epoch 8319: train loss: 0.12463213443420126, test loss: 0.26228337994988316\n",
      "epoch 8320: train loss: 0.12462815001651924, test loss: 0.2622833220975231\n",
      "epoch 8321: train loss: 0.12462416627154603, test loss: 0.26228326445778594\n",
      "epoch 8322: train loss: 0.12462018319908069, test loss: 0.2622832070306032\n",
      "epoch 8323: train loss: 0.12461620079892233, test loss: 0.26228314981590634\n",
      "epoch 8324: train loss: 0.12461221907087024, test loss: 0.2622830928136272\n",
      "epoch 8325: train loss: 0.12460823801472372, test loss: 0.2622830360236971\n",
      "epoch 8326: train loss: 0.12460425763028216, test loss: 0.26228297944604784\n",
      "epoch 8327: train loss: 0.12460027791734504, test loss: 0.2622829230806112\n",
      "epoch 8328: train loss: 0.12459629887571197, test loss: 0.2622828669273186\n",
      "epoch 8329: train loss: 0.12459232050518257, test loss: 0.2622828109861021\n",
      "epoch 8330: train loss: 0.1245883428055566, test loss: 0.2622827552568932\n",
      "epoch 8331: train loss: 0.12458436577663386, test loss: 0.26228269973962387\n",
      "epoch 8332: train loss: 0.12458038941821431, test loss: 0.26228264443422594\n",
      "epoch 8333: train loss: 0.12457641373009791, test loss: 0.26228258934063114\n",
      "epoch 8334: train loss: 0.12457243871208476, test loss: 0.2622825344587715\n",
      "epoch 8335: train loss: 0.12456846436397502, test loss: 0.2622824797885789\n",
      "epoch 8336: train loss: 0.12456449068556895, test loss: 0.2622824253299852\n",
      "epoch 8337: train loss: 0.1245605176766669, test loss: 0.26228237108292257\n",
      "epoch 8338: train loss: 0.12455654533706922, test loss: 0.26228231704732297\n",
      "epoch 8339: train loss: 0.12455257366657652, test loss: 0.2622822632231183\n",
      "epoch 8340: train loss: 0.1245486026649893, test loss: 0.26228220961024074\n",
      "epoch 8341: train loss: 0.12454463233210827, test loss: 0.2622821562086225\n",
      "epoch 8342: train loss: 0.12454066266773423, test loss: 0.2622821030181955\n",
      "epoch 8343: train loss: 0.12453669367166795, test loss: 0.2622820500388921\n",
      "epoch 8344: train loss: 0.1245327253437104, test loss: 0.26228199727064444\n",
      "epoch 8345: train loss: 0.12452875768366259, test loss: 0.2622819447133848\n",
      "epoch 8346: train loss: 0.12452479069132562, test loss: 0.26228189236704524\n",
      "epoch 8347: train loss: 0.12452082436650067, test loss: 0.26228184023155826\n",
      "epoch 8348: train loss: 0.12451685870898896, test loss: 0.26228178830685606\n",
      "epoch 8349: train loss: 0.1245128937185919, test loss: 0.26228173659287113\n",
      "epoch 8350: train loss: 0.12450892939511089, test loss: 0.26228168508953575\n",
      "epoch 8351: train loss: 0.12450496573834748, test loss: 0.2622816337967824\n",
      "epoch 8352: train loss: 0.12450100274810323, test loss: 0.2622815827145433\n",
      "epoch 8353: train loss: 0.12449704042417985, test loss: 0.26228153184275127\n",
      "epoch 8354: train loss: 0.1244930787663791, test loss: 0.2622814811813386\n",
      "epoch 8355: train loss: 0.12448911777450283, test loss: 0.2622814307302378\n",
      "epoch 8356: train loss: 0.12448515744835298, test loss: 0.26228138048938154\n",
      "epoch 8357: train loss: 0.12448119778773158, test loss: 0.26228133045870244\n",
      "epoch 8358: train loss: 0.12447723879244073, test loss: 0.26228128063813294\n",
      "epoch 8359: train loss: 0.12447328046228265, test loss: 0.2622812310276059\n",
      "epoch 8360: train loss: 0.12446932279705951, test loss: 0.2622811816270538\n",
      "epoch 8361: train loss: 0.12446536579657376, test loss: 0.2622811324364096\n",
      "epoch 8362: train loss: 0.12446140946062784, test loss: 0.26228108345560586\n",
      "epoch 8363: train loss: 0.12445745378902419, test loss: 0.2622810346845755\n",
      "epoch 8364: train loss: 0.12445349878156552, test loss: 0.2622809861232511\n",
      "epoch 8365: train loss: 0.12444954443805444, test loss: 0.26228093777156575\n",
      "epoch 8366: train loss: 0.12444559075829374, test loss: 0.2622808896294522\n",
      "epoch 8367: train loss: 0.12444163774208629, test loss: 0.26228084169684324\n",
      "epoch 8368: train loss: 0.12443768538923504, test loss: 0.262280793973672\n",
      "epoch 8369: train loss: 0.124433733699543, test loss: 0.26228074645987126\n",
      "epoch 8370: train loss: 0.12442978267281324, test loss: 0.2622806991553741\n",
      "epoch 8371: train loss: 0.124425832308849, test loss: 0.2622806520601136\n",
      "epoch 8372: train loss: 0.12442188260745354, test loss: 0.2622806051740227\n",
      "epoch 8373: train loss: 0.1244179335684302, test loss: 0.2622805584970345\n",
      "epoch 8374: train loss: 0.12441398519158241, test loss: 0.2622805120290821\n",
      "epoch 8375: train loss: 0.1244100374767137, test loss: 0.26228046577009856\n",
      "epoch 8376: train loss: 0.12440609042362771, test loss: 0.2622804197200172\n",
      "epoch 8377: train loss: 0.12440214403212808, test loss: 0.2622803738787712\n",
      "epoch 8378: train loss: 0.12439819830201859, test loss: 0.26228032824629366\n",
      "epoch 8379: train loss: 0.12439425323310312, test loss: 0.26228028282251786\n",
      "epoch 8380: train loss: 0.12439030882518554, test loss: 0.26228023760737723\n",
      "epoch 8381: train loss: 0.1243863650780699, test loss: 0.26228019260080493\n",
      "epoch 8382: train loss: 0.12438242199156033, test loss: 0.2622801478027344\n",
      "epoch 8383: train loss: 0.12437847956546101, test loss: 0.2622801032130989\n",
      "epoch 8384: train loss: 0.12437453779957615, test loss: 0.26228005883183203\n",
      "epoch 8385: train loss: 0.12437059669371013, test loss: 0.26228001465886697\n",
      "epoch 8386: train loss: 0.12436665624766739, test loss: 0.26227997069413733\n",
      "epoch 8387: train loss: 0.12436271646125244, test loss: 0.2622799269375766\n",
      "epoch 8388: train loss: 0.12435877733426987, test loss: 0.26227988338911834\n",
      "epoch 8389: train loss: 0.12435483886652435, test loss: 0.262279840048696\n",
      "epoch 8390: train loss: 0.12435090105782061, test loss: 0.26227979691624326\n",
      "epoch 8391: train loss: 0.12434696390796357, test loss: 0.2622797539916936\n",
      "epoch 8392: train loss: 0.12434302741675811, test loss: 0.2622797112749808\n",
      "epoch 8393: train loss: 0.12433909158400919, test loss: 0.2622796687660384\n",
      "epoch 8394: train loss: 0.12433515640952197, test loss: 0.26227962646480024\n",
      "epoch 8395: train loss: 0.1243312218931016, test loss: 0.2622795843712\n",
      "epoch 8396: train loss: 0.1243272880345533, test loss: 0.2622795424851714\n",
      "epoch 8397: train loss: 0.12432335483368245, test loss: 0.2622795008066482\n",
      "epoch 8398: train loss: 0.12431942229029441, test loss: 0.2622794593355643\n",
      "epoch 8399: train loss: 0.12431549040419475, test loss: 0.2622794180718535\n",
      "epoch 8400: train loss: 0.124311559175189, test loss: 0.2622793770154497\n",
      "epoch 8401: train loss: 0.12430762860308281, test loss: 0.2622793361662868\n",
      "epoch 8402: train loss: 0.12430369868768197, test loss: 0.2622792955242988\n",
      "epoch 8403: train loss: 0.12429976942879226, test loss: 0.26227925508941946\n",
      "epoch 8404: train loss: 0.12429584082621963, test loss: 0.26227921486158295\n",
      "epoch 8405: train loss: 0.12429191287977001, test loss: 0.26227917484072333\n",
      "epoch 8406: train loss: 0.1242879855892495, test loss: 0.26227913502677447\n",
      "epoch 8407: train loss: 0.12428405895446427, test loss: 0.2622790954196706\n",
      "epoch 8408: train loss: 0.12428013297522053, test loss: 0.2622790560193458\n",
      "epoch 8409: train loss: 0.12427620765132459, test loss: 0.2622790168257342\n",
      "epoch 8410: train loss: 0.12427228298258287, test loss: 0.2622789778387699\n",
      "epoch 8411: train loss: 0.12426835896880181, test loss: 0.26227893905838723\n",
      "epoch 8412: train loss: 0.124264435609788, test loss: 0.2622789004845203\n",
      "epoch 8413: train loss: 0.12426051290534808, test loss: 0.2622788621171035\n",
      "epoch 8414: train loss: 0.12425659085528873, test loss: 0.26227882395607094\n",
      "epoch 8415: train loss: 0.12425266945941679, test loss: 0.2622787860013571\n",
      "epoch 8416: train loss: 0.1242487487175391, test loss: 0.26227874825289627\n",
      "epoch 8417: train loss: 0.1242448286294627, test loss: 0.26227871071062275\n",
      "epoch 8418: train loss: 0.12424090919499456, test loss: 0.262278673374471\n",
      "epoch 8419: train loss: 0.12423699041394182, test loss: 0.26227863624437553\n",
      "epoch 8420: train loss: 0.12423307228611172, test loss: 0.26227859932027064\n",
      "epoch 8421: train loss: 0.12422915481131153, test loss: 0.2622785626020909\n",
      "epoch 8422: train loss: 0.12422523798934858, test loss: 0.26227852608977087\n",
      "epoch 8423: train loss: 0.12422132182003036, test loss: 0.2622784897832451\n",
      "epoch 8424: train loss: 0.12421740630316443, test loss: 0.262278453682448\n",
      "epoch 8425: train loss: 0.12421349143855831, test loss: 0.2622784177873145\n",
      "epoch 8426: train loss: 0.1242095772260198, test loss: 0.2622783820977789\n",
      "epoch 8427: train loss: 0.12420566366535656, test loss: 0.26227834661377597\n",
      "epoch 8428: train loss: 0.12420175075637652, test loss: 0.26227831133524043\n",
      "epoch 8429: train loss: 0.12419783849888757, test loss: 0.262278276262107\n",
      "epoch 8430: train loss: 0.12419392689269777, test loss: 0.26227824139431044\n",
      "epoch 8431: train loss: 0.12419001593761517, test loss: 0.26227820673178553\n",
      "epoch 8432: train loss: 0.12418610563344795, test loss: 0.2622781722744671\n",
      "epoch 8433: train loss: 0.1241821959800044, test loss: 0.26227813802228994\n",
      "epoch 8434: train loss: 0.12417828697709281, test loss: 0.2622781039751889\n",
      "epoch 8435: train loss: 0.1241743786245216, test loss: 0.2622780701330989\n",
      "epoch 8436: train loss: 0.1241704709220993, test loss: 0.2622780364959549\n",
      "epoch 8437: train loss: 0.12416656386963446, test loss: 0.2622780030636918\n",
      "epoch 8438: train loss: 0.12416265746693572, test loss: 0.2622779698362445\n",
      "epoch 8439: train loss: 0.12415875171381188, test loss: 0.26227793681354816\n",
      "epoch 8440: train loss: 0.12415484661007166, test loss: 0.26227790399553785\n",
      "epoch 8441: train loss: 0.12415094215552402, test loss: 0.2622778713821484\n",
      "epoch 8442: train loss: 0.12414703834997792, test loss: 0.26227783897331514\n",
      "epoch 8443: train loss: 0.12414313519324244, test loss: 0.26227780676897305\n",
      "epoch 8444: train loss: 0.12413923268512667, test loss: 0.26227777476905734\n",
      "epoch 8445: train loss: 0.12413533082543986, test loss: 0.2622777429735032\n",
      "epoch 8446: train loss: 0.12413142961399125, test loss: 0.2622777113822458\n",
      "epoch 8447: train loss: 0.12412752905059028, test loss: 0.2622776799952204\n",
      "epoch 8448: train loss: 0.12412362913504639, test loss: 0.2622776488123623\n",
      "epoch 8449: train loss: 0.12411972986716907, test loss: 0.26227761783360665\n",
      "epoch 8450: train loss: 0.12411583124676802, test loss: 0.26227758705888893\n",
      "epoch 8451: train loss: 0.12411193327365286, test loss: 0.2622775564881445\n",
      "epoch 8452: train loss: 0.12410803594763337, test loss: 0.2622775261213086\n",
      "epoch 8453: train loss: 0.12410413926851942, test loss: 0.26227749595831673\n",
      "epoch 8454: train loss: 0.12410024323612094, test loss: 0.26227746599910434\n",
      "epoch 8455: train loss: 0.12409634785024792, test loss: 0.26227743624360683\n",
      "epoch 8456: train loss: 0.1240924531107105, test loss: 0.26227740669175975\n",
      "epoch 8457: train loss: 0.12408855901731881, test loss: 0.2622773773434986\n",
      "epoch 8458: train loss: 0.12408466556988311, test loss: 0.2622773481987589\n",
      "epoch 8459: train loss: 0.12408077276821369, test loss: 0.2622773192574763\n",
      "epoch 8460: train loss: 0.12407688061212103, test loss: 0.26227729051958637\n",
      "epoch 8461: train loss: 0.12407298910141559, test loss: 0.26227726198502466\n",
      "epoch 8462: train loss: 0.1240690982359079, test loss: 0.26227723365372696\n",
      "epoch 8463: train loss: 0.12406520801540864, test loss: 0.26227720552562894\n",
      "epoch 8464: train loss: 0.12406131843972851, test loss: 0.26227717760066627\n",
      "epoch 8465: train loss: 0.12405742950867832, test loss: 0.2622771498787746\n",
      "epoch 8466: train loss: 0.12405354122206898, test loss: 0.26227712235989\n",
      "epoch 8467: train loss: 0.12404965357971143, test loss: 0.26227709504394797\n",
      "epoch 8468: train loss: 0.12404576658141668, test loss: 0.26227706793088446\n",
      "epoch 8469: train loss: 0.12404188022699589, test loss: 0.2622770410206354\n",
      "epoch 8470: train loss: 0.12403799451626024, test loss: 0.2622770143131366\n",
      "epoch 8471: train loss: 0.12403410944902102, test loss: 0.2622769878083239\n",
      "epoch 8472: train loss: 0.12403022502508958, test loss: 0.2622769615061335\n",
      "epoch 8473: train loss: 0.1240263412442773, test loss: 0.26227693540650104\n",
      "epoch 8474: train loss: 0.12402245810639577, test loss: 0.2622769095093627\n",
      "epoch 8475: train loss: 0.12401857561125652, test loss: 0.26227688381465464\n",
      "epoch 8476: train loss: 0.12401469375867126, test loss: 0.2622768583223126\n",
      "epoch 8477: train loss: 0.12401081254845174, test loss: 0.2622768330322729\n",
      "epoch 8478: train loss: 0.12400693198040974, test loss: 0.26227680794447156\n",
      "epoch 8479: train loss: 0.12400305205435723, test loss: 0.26227678305884483\n",
      "epoch 8480: train loss: 0.12399917277010611, test loss: 0.26227675837532866\n",
      "epoch 8481: train loss: 0.12399529412746847, test loss: 0.26227673389385947\n",
      "epoch 8482: train loss: 0.1239914161262565, test loss: 0.2622767096143733\n",
      "epoch 8483: train loss: 0.12398753876628238, test loss: 0.26227668553680655\n",
      "epoch 8484: train loss: 0.12398366204735836, test loss: 0.2622766616610954\n",
      "epoch 8485: train loss: 0.1239797859692969, test loss: 0.26227663798717626\n",
      "epoch 8486: train loss: 0.1239759105319104, test loss: 0.26227661451498535\n",
      "epoch 8487: train loss: 0.12397203573501138, test loss: 0.2622765912444591\n",
      "epoch 8488: train loss: 0.12396816157841246, test loss: 0.2622765681755339\n",
      "epoch 8489: train loss: 0.12396428806192634, test loss: 0.2622765453081461\n",
      "epoch 8490: train loss: 0.1239604151853658, test loss: 0.2622765226422322\n",
      "epoch 8491: train loss: 0.1239565429485436, test loss: 0.26227650017772874\n",
      "epoch 8492: train loss: 0.12395267135127273, test loss: 0.2622764779145721\n",
      "epoch 8493: train loss: 0.12394880039336618, test loss: 0.2622764558526988\n",
      "epoch 8494: train loss: 0.12394493007463701, test loss: 0.2622764339920456\n",
      "epoch 8495: train loss: 0.12394106039489836, test loss: 0.2622764123325488\n",
      "epoch 8496: train loss: 0.12393719135396349, test loss: 0.26227639087414517\n",
      "epoch 8497: train loss: 0.12393332295164566, test loss: 0.26227636961677137\n",
      "epoch 8498: train loss: 0.12392945518775832, test loss: 0.26227634856036397\n",
      "epoch 8499: train loss: 0.12392558806211489, test loss: 0.26227632770485987\n",
      "epoch 8500: train loss: 0.12392172157452891, test loss: 0.2622763070501955\n",
      "epoch 8501: train loss: 0.12391785572481398, test loss: 0.2622762865963078\n",
      "epoch 8502: train loss: 0.12391399051278387, test loss: 0.2622762663431335\n",
      "epoch 8503: train loss: 0.12391012593825228, test loss: 0.2622762462906094\n",
      "epoch 8504: train loss: 0.12390626200103305, test loss: 0.2622762264386724\n",
      "epoch 8505: train loss: 0.12390239870094018, test loss: 0.2622762067872592\n",
      "epoch 8506: train loss: 0.12389853603778761, test loss: 0.26227618733630687\n",
      "epoch 8507: train loss: 0.12389467401138944, test loss: 0.2622761680857522\n",
      "epoch 8508: train loss: 0.12389081262155983, test loss: 0.26227614903553215\n",
      "epoch 8509: train loss: 0.12388695186811302, test loss: 0.2622761301855837\n",
      "epoch 8510: train loss: 0.1238830917508633, test loss: 0.26227611153584396\n",
      "epoch 8511: train loss: 0.1238792322696251, test loss: 0.2622760930862498\n",
      "epoch 8512: train loss: 0.12387537342421284, test loss: 0.2622760748367382\n",
      "epoch 8513: train loss: 0.1238715152144411, test loss: 0.2622760567872465\n",
      "epoch 8514: train loss: 0.1238676576401245, test loss: 0.2622760389377117\n",
      "epoch 8515: train loss: 0.12386380070107769, test loss: 0.26227602128807076\n",
      "epoch 8516: train loss: 0.1238599443971155, test loss: 0.26227600383826105\n",
      "epoch 8517: train loss: 0.12385608872805279, test loss: 0.26227598658821966\n",
      "epoch 8518: train loss: 0.1238522336937044, test loss: 0.26227596953788385\n",
      "epoch 8519: train loss: 0.12384837929388542, test loss: 0.26227595268719095\n",
      "epoch 8520: train loss: 0.12384452552841087, test loss: 0.26227593603607796\n",
      "epoch 8521: train loss: 0.12384067239709597, test loss: 0.2622759195844824\n",
      "epoch 8522: train loss: 0.12383681989975592, test loss: 0.2622759033323415\n",
      "epoch 8523: train loss: 0.12383296803620598, test loss: 0.2622758872795927\n",
      "epoch 8524: train loss: 0.12382911680626166, test loss: 0.26227587142617337\n",
      "epoch 8525: train loss: 0.12382526620973831, test loss: 0.26227585577202084\n",
      "epoch 8526: train loss: 0.12382141624645153, test loss: 0.2622758403170725\n",
      "epoch 8527: train loss: 0.12381756691621688, test loss: 0.26227582506126595\n",
      "epoch 8528: train loss: 0.12381371821885015, test loss: 0.2622758100045386\n",
      "epoch 8529: train loss: 0.123809870154167, test loss: 0.262275795146828\n",
      "epoch 8530: train loss: 0.12380602272198335, test loss: 0.2622757804880716\n",
      "epoch 8531: train loss: 0.12380217592211508, test loss: 0.262275766028207\n",
      "epoch 8532: train loss: 0.12379832975437821, test loss: 0.26227575176717194\n",
      "epoch 8533: train loss: 0.12379448421858882, test loss: 0.26227573770490387\n",
      "epoch 8534: train loss: 0.12379063931456305, test loss: 0.2622757238413405\n",
      "epoch 8535: train loss: 0.1237867950421171, test loss: 0.2622757101764195\n",
      "epoch 8536: train loss: 0.12378295140106729, test loss: 0.2622756967100785\n",
      "epoch 8537: train loss: 0.12377910839123, test loss: 0.2622756834422554\n",
      "epoch 8538: train loss: 0.1237752660124217, test loss: 0.26227567037288785\n",
      "epoch 8539: train loss: 0.12377142426445889, test loss: 0.2622756575019135\n",
      "epoch 8540: train loss: 0.1237675831471582, test loss: 0.2622756448292705\n",
      "epoch 8541: train loss: 0.1237637426603363, test loss: 0.2622756323548963\n",
      "epoch 8542: train loss: 0.12375990280380993, test loss: 0.2622756200787291\n",
      "epoch 8543: train loss: 0.12375606357739595, test loss: 0.26227560800070665\n",
      "epoch 8544: train loss: 0.12375222498091126, test loss: 0.26227559612076673\n",
      "epoch 8545: train loss: 0.12374838701417286, test loss: 0.2622755844388475\n",
      "epoch 8546: train loss: 0.12374454967699774, test loss: 0.26227557295488685\n",
      "epoch 8547: train loss: 0.12374071296920314, test loss: 0.26227556166882277\n",
      "epoch 8548: train loss: 0.12373687689060618, test loss: 0.2622755505805932\n",
      "epoch 8549: train loss: 0.1237330414410242, test loss: 0.2622755396901364\n",
      "epoch 8550: train loss: 0.12372920662027452, test loss: 0.26227552899739026\n",
      "epoch 8551: train loss: 0.12372537242817462, test loss: 0.2622755185022929\n",
      "epoch 8552: train loss: 0.12372153886454199, test loss: 0.26227550820478257\n",
      "epoch 8553: train loss: 0.1237177059291942, test loss: 0.2622754981047974\n",
      "epoch 8554: train loss: 0.12371387362194895, test loss: 0.2622754882022754\n",
      "epoch 8555: train loss: 0.12371004194262396, test loss: 0.262275478497155\n",
      "epoch 8556: train loss: 0.12370621089103702, test loss: 0.2622754689893743\n",
      "epoch 8557: train loss: 0.12370238046700606, test loss: 0.26227545967887167\n",
      "epoch 8558: train loss: 0.12369855067034902, test loss: 0.2622754505655853\n",
      "epoch 8559: train loss: 0.12369472150088395, test loss: 0.26227544164945354\n",
      "epoch 8560: train loss: 0.12369089295842894, test loss: 0.26227543293041483\n",
      "epoch 8561: train loss: 0.1236870650428022, test loss: 0.2622754244084074\n",
      "epoch 8562: train loss: 0.12368323775382198, test loss: 0.26227541608336974\n",
      "epoch 8563: train loss: 0.12367941109130666, test loss: 0.2622754079552401\n",
      "epoch 8564: train loss: 0.12367558505507457, test loss: 0.2622754000239572\n",
      "epoch 8565: train loss: 0.12367175964494428, test loss: 0.26227539228945934\n",
      "epoch 8566: train loss: 0.1236679348607343, test loss: 0.26227538475168505\n",
      "epoch 8567: train loss: 0.1236641107022633, test loss: 0.2622753774105729\n",
      "epoch 8568: train loss: 0.12366028716934997, test loss: 0.26227537026606135\n",
      "epoch 8569: train loss: 0.1236564642618131, test loss: 0.2622753633180891\n",
      "epoch 8570: train loss: 0.12365264197947157, test loss: 0.2622753565665947\n",
      "epoch 8571: train loss: 0.12364882032214428, test loss: 0.26227535001151675\n",
      "epoch 8572: train loss: 0.12364499928965028, test loss: 0.26227534365279404\n",
      "epoch 8573: train loss: 0.12364117888180866, test loss: 0.2622753374903651\n",
      "epoch 8574: train loss: 0.12363735909843852, test loss: 0.26227533152416876\n",
      "epoch 8575: train loss: 0.12363353993935913, test loss: 0.26227532575414364\n",
      "epoch 8576: train loss: 0.12362972140438984, test loss: 0.26227532018022864\n",
      "epoch 8577: train loss: 0.12362590349334995, test loss: 0.2622753148023624\n",
      "epoch 8578: train loss: 0.12362208620605898, test loss: 0.2622753096204839\n",
      "epoch 8579: train loss: 0.12361826954233644, test loss: 0.26227530463453186\n",
      "epoch 8580: train loss: 0.12361445350200194, test loss: 0.26227529984444525\n",
      "epoch 8581: train loss: 0.12361063808487514, test loss: 0.26227529525016285\n",
      "epoch 8582: train loss: 0.12360682329077584, test loss: 0.26227529085162365\n",
      "epoch 8583: train loss: 0.12360300911952381, test loss: 0.2622752866487667\n",
      "epoch 8584: train loss: 0.12359919557093897, test loss: 0.26227528264153077\n",
      "epoch 8585: train loss: 0.12359538264484135, test loss: 0.262275278829855\n",
      "epoch 8586: train loss: 0.12359157034105092, test loss: 0.2622752752136785\n",
      "epoch 8587: train loss: 0.12358775865938786, test loss: 0.26227527179294\n",
      "epoch 8588: train loss: 0.12358394759967231, test loss: 0.2622752685675789\n",
      "epoch 8589: train loss: 0.12358013716172461, test loss: 0.26227526553753416\n",
      "epoch 8590: train loss: 0.12357632734536508, test loss: 0.2622752627027449\n",
      "epoch 8591: train loss: 0.12357251815041412, test loss: 0.2622752600631504\n",
      "epoch 8592: train loss: 0.12356870957669225, test loss: 0.26227525761868975\n",
      "epoch 8593: train loss: 0.12356490162402001, test loss: 0.2622752553693022\n",
      "epoch 8594: train loss: 0.12356109429221807, test loss: 0.26227525331492685\n",
      "epoch 8595: train loss: 0.12355728758110711, test loss: 0.2622752514555031\n",
      "epoch 8596: train loss: 0.12355348149050792, test loss: 0.2622752497909703\n",
      "epoch 8597: train loss: 0.12354967602024139, test loss: 0.26227524832126753\n",
      "epoch 8598: train loss: 0.12354587117012848, test loss: 0.26227524704633437\n",
      "epoch 8599: train loss: 0.1235420669399901, test loss: 0.26227524596610996\n",
      "epoch 8600: train loss: 0.12353826332964743, test loss: 0.262275245080534\n",
      "epoch 8601: train loss: 0.12353446033892158, test loss: 0.2622752443895456\n",
      "epoch 8602: train loss: 0.12353065796763378, test loss: 0.26227524389308426\n",
      "epoch 8603: train loss: 0.12352685621560536, test loss: 0.2622752435910896\n",
      "epoch 8604: train loss: 0.12352305508265765, test loss: 0.26227524348350106\n",
      "epoch 8605: train loss: 0.1235192545686121, test loss: 0.2622752435702581\n",
      "epoch 8606: train loss: 0.12351545467329028, test loss: 0.2622752438513003\n",
      "epoch 8607: train loss: 0.12351165539651374, test loss: 0.2622752443265671\n",
      "epoch 8608: train loss: 0.12350785673810422, test loss: 0.2622752449959984\n",
      "epoch 8609: train loss: 0.12350405869788332, test loss: 0.2622752458595336\n",
      "epoch 8610: train loss: 0.123500261275673, test loss: 0.2622752469171123\n",
      "epoch 8611: train loss: 0.12349646447129507, test loss: 0.26227524816867437\n",
      "epoch 8612: train loss: 0.12349266828457148, test loss: 0.2622752496141594\n",
      "epoch 8613: train loss: 0.12348887271532433, test loss: 0.26227525125350704\n",
      "epoch 8614: train loss: 0.12348507776337567, test loss: 0.2622752530866571\n",
      "epoch 8615: train loss: 0.12348128342854768, test loss: 0.2622752551135495\n",
      "epoch 8616: train loss: 0.12347748971066264, test loss: 0.2622752573341238\n",
      "epoch 8617: train loss: 0.12347369660954284, test loss: 0.26227525974832\n",
      "epoch 8618: train loss: 0.1234699041250107, test loss: 0.26227526235607795\n",
      "epoch 8619: train loss: 0.12346611225688868, test loss: 0.26227526515733746\n",
      "epoch 8620: train loss: 0.12346232100499935, test loss: 0.26227526815203844\n",
      "epoch 8621: train loss: 0.12345853036916526, test loss: 0.2622752713401208\n",
      "epoch 8622: train loss: 0.12345474034920918, test loss: 0.26227527472152457\n",
      "epoch 8623: train loss: 0.12345095094495381, test loss: 0.2622752782961897\n",
      "epoch 8624: train loss: 0.12344716215622203, test loss: 0.2622752820640562\n",
      "epoch 8625: train loss: 0.12344337398283667, test loss: 0.2622752860250641\n",
      "epoch 8626: train loss: 0.12343958642462077, test loss: 0.26227529017915335\n",
      "epoch 8627: train loss: 0.12343579948139739, test loss: 0.2622752945262643\n",
      "epoch 8628: train loss: 0.12343201315298961, test loss: 0.2622752990663368\n",
      "epoch 8629: train loss: 0.12342822743922063, test loss: 0.262275303799311\n",
      "epoch 8630: train loss: 0.12342444233991373, test loss: 0.2622753087251272\n",
      "epoch 8631: train loss: 0.12342065785489226, test loss: 0.26227531384372554\n",
      "epoch 8632: train loss: 0.1234168739839796, test loss: 0.2622753191550462\n",
      "epoch 8633: train loss: 0.12341309072699926, test loss: 0.2622753246590294\n",
      "epoch 8634: train loss: 0.12340930808377477, test loss: 0.2622753303556154\n",
      "epoch 8635: train loss: 0.12340552605412979, test loss: 0.2622753362447446\n",
      "epoch 8636: train loss: 0.12340174463788799, test loss: 0.2622753423263572\n",
      "epoch 8637: train loss: 0.12339796383487314, test loss: 0.26227534860039353\n",
      "epoch 8638: train loss: 0.12339418364490914, test loss: 0.26227535506679395\n",
      "epoch 8639: train loss: 0.12339040406781983, test loss: 0.262275361725499\n",
      "epoch 8640: train loss: 0.12338662510342926, test loss: 0.2622753685764488\n",
      "epoch 8641: train loss: 0.12338284675156143, test loss: 0.26227537561958403\n",
      "epoch 8642: train loss: 0.12337906901204052, test loss: 0.2622753828548451\n",
      "epoch 8643: train loss: 0.1233752918846907, test loss: 0.26227539028217245\n",
      "epoch 8644: train loss: 0.12337151536933626, test loss: 0.26227539790150656\n",
      "epoch 8645: train loss: 0.12336773946580155, test loss: 0.2622754057127882\n",
      "epoch 8646: train loss: 0.12336396417391096, test loss: 0.2622754137159576\n",
      "epoch 8647: train loss: 0.12336018949348901, test loss: 0.2622754219109556\n",
      "epoch 8648: train loss: 0.12335641542436028, test loss: 0.26227543029772266\n",
      "epoch 8649: train loss: 0.12335264196634936, test loss: 0.2622754388761994\n",
      "epoch 8650: train loss: 0.12334886911928095, test loss: 0.26227544764632676\n",
      "epoch 8651: train loss: 0.12334509688297986, test loss: 0.2622754566080451\n",
      "epoch 8652: train loss: 0.12334132525727093, test loss: 0.26227546576129535\n",
      "epoch 8653: train loss: 0.12333755424197906, test loss: 0.2622754751060181\n",
      "epoch 8654: train loss: 0.12333378383692924, test loss: 0.26227548464215417\n",
      "epoch 8655: train loss: 0.12333001404194655, test loss: 0.26227549436964437\n",
      "epoch 8656: train loss: 0.12332624485685612, test loss: 0.26227550428842955\n",
      "epoch 8657: train loss: 0.12332247628148314, test loss: 0.2622755143984504\n",
      "epoch 8658: train loss: 0.12331870831565288, test loss: 0.26227552469964793\n",
      "epoch 8659: train loss: 0.12331494095919072, test loss: 0.262275535191963\n",
      "epoch 8660: train loss: 0.12331117421192203, test loss: 0.2622755458753365\n",
      "epoch 8661: train loss: 0.12330740807367235, test loss: 0.26227555674970937\n",
      "epoch 8662: train loss: 0.12330364254426719, test loss: 0.2622755678150226\n",
      "epoch 8663: train loss: 0.12329987762353221, test loss: 0.2622755790712171\n",
      "epoch 8664: train loss: 0.1232961133112931, test loss: 0.26227559051823396\n",
      "epoch 8665: train loss: 0.12329234960737566, test loss: 0.2622756021560142\n",
      "epoch 8666: train loss: 0.1232885865116057, test loss: 0.26227561398449883\n",
      "epoch 8667: train loss: 0.1232848240238091, test loss: 0.262275626003629\n",
      "epoch 8668: train loss: 0.12328106214381193, test loss: 0.2622756382133458\n",
      "epoch 8669: train loss: 0.12327730087144019, test loss: 0.2622756506135904\n",
      "epoch 8670: train loss: 0.12327354020652004, test loss: 0.26227566320430395\n",
      "epoch 8671: train loss: 0.12326978014887764, test loss: 0.2622756759854276\n",
      "epoch 8672: train loss: 0.12326602069833928, test loss: 0.2622756889569026\n",
      "epoch 8673: train loss: 0.12326226185473128, test loss: 0.26227570211867013\n",
      "epoch 8674: train loss: 0.12325850361788007, test loss: 0.2622757154706715\n",
      "epoch 8675: train loss: 0.12325474598761213, test loss: 0.26227572901284807\n",
      "epoch 8676: train loss: 0.12325098896375401, test loss: 0.262275742745141\n",
      "epoch 8677: train loss: 0.1232472325461323, test loss: 0.2622757566674916\n",
      "epoch 8678: train loss: 0.12324347673457375, test loss: 0.2622757707798414\n",
      "epoch 8679: train loss: 0.12323972152890503, test loss: 0.26227578508213173\n",
      "epoch 8680: train loss: 0.12323596692895307, test loss: 0.2622757995743039\n",
      "epoch 8681: train loss: 0.12323221293454471, test loss: 0.26227581425629937\n",
      "epoch 8682: train loss: 0.12322845954550696, test loss: 0.2622758291280597\n",
      "epoch 8683: train loss: 0.12322470676166683, test loss: 0.2622758441895263\n",
      "epoch 8684: train loss: 0.12322095458285144, test loss: 0.26227585944064075\n",
      "epoch 8685: train loss: 0.12321720300888801, test loss: 0.2622758748813444\n",
      "epoch 8686: train loss: 0.12321345203960372, test loss: 0.262275890511579\n",
      "epoch 8687: train loss: 0.12320970167482599, test loss: 0.26227590633128606\n",
      "epoch 8688: train loss: 0.12320595191438212, test loss: 0.26227592234040714\n",
      "epoch 8689: train loss: 0.12320220275809964, test loss: 0.2622759385388839\n",
      "epoch 8690: train loss: 0.12319845420580605, test loss: 0.262275954926658\n",
      "epoch 8691: train loss: 0.123194706257329, test loss: 0.26227597150367116\n",
      "epoch 8692: train loss: 0.12319095891249607, test loss: 0.2622759882698651\n",
      "epoch 8693: train loss: 0.1231872121711351, test loss: 0.26227600522518135\n",
      "epoch 8694: train loss: 0.12318346603307383, test loss: 0.26227602236956193\n",
      "epoch 8695: train loss: 0.1231797204981402, test loss: 0.2622760397029484\n",
      "epoch 8696: train loss: 0.12317597556616214, test loss: 0.2622760572252826\n",
      "epoch 8697: train loss: 0.12317223123696765, test loss: 0.2622760749365065\n",
      "epoch 8698: train loss: 0.12316848751038485, test loss: 0.2622760928365617\n",
      "epoch 8699: train loss: 0.12316474438624192, test loss: 0.2622761109253904\n",
      "epoch 8700: train loss: 0.12316100186436703, test loss: 0.2622761292029343\n",
      "epoch 8701: train loss: 0.12315725994458855, test loss: 0.2622761476691352\n",
      "epoch 8702: train loss: 0.12315351862673482, test loss: 0.26227616632393536\n",
      "epoch 8703: train loss: 0.12314977791063429, test loss: 0.2622761851672764\n",
      "epoch 8704: train loss: 0.12314603779611545, test loss: 0.2622762041991006\n",
      "epoch 8705: train loss: 0.12314229828300689, test loss: 0.2622762234193498\n",
      "epoch 8706: train loss: 0.12313855937113727, test loss: 0.26227624282796624\n",
      "epoch 8707: train loss: 0.1231348210603353, test loss: 0.26227626242489177\n",
      "epoch 8708: train loss: 0.12313108335042974, test loss: 0.26227628221006855\n",
      "epoch 8709: train loss: 0.12312734624124949, test loss: 0.2622763021834387\n",
      "epoch 8710: train loss: 0.12312360973262343, test loss: 0.2622763223449444\n",
      "epoch 8711: train loss: 0.12311987382438064, test loss: 0.2622763426945279\n",
      "epoch 8712: train loss: 0.12311613851635012, test loss: 0.26227636323213105\n",
      "epoch 8713: train loss: 0.12311240380836097, test loss: 0.2622763839576964\n",
      "epoch 8714: train loss: 0.12310866970024247, test loss: 0.26227640487116616\n",
      "epoch 8715: train loss: 0.12310493619182385, test loss: 0.26227642597248246\n",
      "epoch 8716: train loss: 0.12310120328293443, test loss: 0.26227644726158755\n",
      "epoch 8717: train loss: 0.1230974709734037, test loss: 0.2622764687384238\n",
      "epoch 8718: train loss: 0.12309373926306107, test loss: 0.2622764904029337\n",
      "epoch 8719: train loss: 0.1230900081517361, test loss: 0.2622765122550594\n",
      "epoch 8720: train loss: 0.12308627763925842, test loss: 0.2622765342947433\n",
      "epoch 8721: train loss: 0.1230825477254577, test loss: 0.2622765565219279\n",
      "epoch 8722: train loss: 0.1230788184101637, test loss: 0.26227657893655554\n",
      "epoch 8723: train loss: 0.12307508969320625, test loss: 0.26227660153856874\n",
      "epoch 8724: train loss: 0.12307136157441527, test loss: 0.26227662432790994\n",
      "epoch 8725: train loss: 0.12306763405362066, test loss: 0.2622766473045216\n",
      "epoch 8726: train loss: 0.12306390713065248, test loss: 0.26227667046834635\n",
      "epoch 8727: train loss: 0.12306018080534084, test loss: 0.2622766938193266\n",
      "epoch 8728: train loss: 0.1230564550775159, test loss: 0.2622767173574051\n",
      "epoch 8729: train loss: 0.1230527299470079, test loss: 0.2622767410825243\n",
      "epoch 8730: train loss: 0.1230490054136471, test loss: 0.2622767649946268\n",
      "epoch 8731: train loss: 0.12304528147726394, test loss: 0.2622767890936554\n",
      "epoch 8732: train loss: 0.12304155813768883, test loss: 0.2622768133795527\n",
      "epoch 8733: train loss: 0.12303783539475227, test loss: 0.2622768378522613\n",
      "epoch 8734: train loss: 0.12303411324828485, test loss: 0.262276862511724\n",
      "epoch 8735: train loss: 0.12303039169811722, test loss: 0.26227688735788346\n",
      "epoch 8736: train loss: 0.12302667074408007, test loss: 0.2622769123906826\n",
      "epoch 8737: train loss: 0.12302295038600423, test loss: 0.26227693761006404\n",
      "epoch 8738: train loss: 0.12301923062372051, test loss: 0.2622769630159707\n",
      "epoch 8739: train loss: 0.12301551145705983, test loss: 0.26227698860834536\n",
      "epoch 8740: train loss: 0.1230117928858532, test loss: 0.26227701438713086\n",
      "epoch 8741: train loss: 0.12300807490993167, test loss: 0.2622770403522702\n",
      "epoch 8742: train loss: 0.12300435752912638, test loss: 0.2622770665037061\n",
      "epoch 8743: train loss: 0.12300064074326848, test loss: 0.2622770928413816\n",
      "epoch 8744: train loss: 0.12299692455218922, test loss: 0.2622771193652396\n",
      "epoch 8745: train loss: 0.12299320895572002, test loss: 0.26227714607522323\n",
      "epoch 8746: train loss: 0.12298949395369219, test loss: 0.2622771729712753\n",
      "epoch 8747: train loss: 0.12298577954593722, test loss: 0.26227720005333893\n",
      "epoch 8748: train loss: 0.12298206573228664, test loss: 0.26227722732135705\n",
      "epoch 8749: train loss: 0.12297835251257207, test loss: 0.2622772547752729\n",
      "epoch 8750: train loss: 0.12297463988662515, test loss: 0.2622772824150295\n",
      "epoch 8751: train loss: 0.12297092785427761, test loss: 0.26227731024056994\n",
      "epoch 8752: train loss: 0.12296721641536128, test loss: 0.2622773382518374\n",
      "epoch 8753: train loss: 0.12296350556970802, test loss: 0.26227736644877503\n",
      "epoch 8754: train loss: 0.12295979531714979, test loss: 0.262277394831326\n",
      "epoch 8755: train loss: 0.12295608565751856, test loss: 0.2622774233994335\n",
      "epoch 8756: train loss: 0.12295237659064641, test loss: 0.2622774521530409\n",
      "epoch 8757: train loss: 0.12294866811636553, test loss: 0.26227748109209126\n",
      "epoch 8758: train loss: 0.12294496023450804, test loss: 0.262277510216528\n",
      "epoch 8759: train loss: 0.1229412529449063, test loss: 0.26227753952629435\n",
      "epoch 8760: train loss: 0.12293754624739262, test loss: 0.2622775690213337\n",
      "epoch 8761: train loss: 0.12293384014179938, test loss: 0.26227759870158945\n",
      "epoch 8762: train loss: 0.12293013462795914, test loss: 0.2622776285670049\n",
      "epoch 8763: train loss: 0.12292642970570437, test loss: 0.26227765861752333\n",
      "epoch 8764: train loss: 0.12292272537486772, test loss: 0.26227768885308833\n",
      "epoch 8765: train loss: 0.12291902163528184, test loss: 0.26227771927364335\n",
      "epoch 8766: train loss: 0.12291531848677952, test loss: 0.26227774987913177\n",
      "epoch 8767: train loss: 0.12291161592919357, test loss: 0.2622777806694971\n",
      "epoch 8768: train loss: 0.12290791396235687, test loss: 0.26227781164468295\n",
      "epoch 8769: train loss: 0.1229042125861023, test loss: 0.26227784280463273\n",
      "epoch 8770: train loss: 0.122900511800263, test loss: 0.2622778741492901\n",
      "epoch 8771: train loss: 0.12289681160467197, test loss: 0.26227790567859866\n",
      "epoch 8772: train loss: 0.12289311199916238, test loss: 0.2622779373925018\n",
      "epoch 8773: train loss: 0.12288941298356745, test loss: 0.26227796929094344\n",
      "epoch 8774: train loss: 0.12288571455772047, test loss: 0.2622780013738671\n",
      "epoch 8775: train loss: 0.12288201672145478, test loss: 0.2622780336412165\n",
      "epoch 8776: train loss: 0.12287831947460379, test loss: 0.26227806609293514\n",
      "epoch 8777: train loss: 0.12287462281700105, test loss: 0.26227809872896707\n",
      "epoch 8778: train loss: 0.12287092674848005, test loss: 0.26227813154925583\n",
      "epoch 8779: train loss: 0.12286723126887439, test loss: 0.2622781645537453\n",
      "epoch 8780: train loss: 0.12286353637801783, test loss: 0.2622781977423792\n",
      "epoch 8781: train loss: 0.12285984207574409, test loss: 0.26227823111510135\n",
      "epoch 8782: train loss: 0.12285614836188696, test loss: 0.26227826467185555\n",
      "epoch 8783: train loss: 0.12285245523628037, test loss: 0.2622782984125858\n",
      "epoch 8784: train loss: 0.12284876269875823, test loss: 0.2622783323372359\n",
      "epoch 8785: train loss: 0.1228450707491546, test loss: 0.26227836644574976\n",
      "epoch 8786: train loss: 0.12284137938730356, test loss: 0.26227840073807135\n",
      "epoch 8787: train loss: 0.12283768861303926, test loss: 0.26227843521414457\n",
      "epoch 8788: train loss: 0.1228339984261959, test loss: 0.26227846987391346\n",
      "epoch 8789: train loss: 0.12283030882660778, test loss: 0.26227850471732195\n",
      "epoch 8790: train loss: 0.12282661981410926, test loss: 0.2622785397443141\n",
      "epoch 8791: train loss: 0.12282293138853476, test loss: 0.262278574954834\n",
      "epoch 8792: train loss: 0.12281924354971872, test loss: 0.26227861034882566\n",
      "epoch 8793: train loss: 0.12281555629749576, test loss: 0.2622786459262332\n",
      "epoch 8794: train loss: 0.12281186963170047, test loss: 0.2622786816870007\n",
      "epoch 8795: train loss: 0.12280818355216751, test loss: 0.26227871763107236\n",
      "epoch 8796: train loss: 0.12280449805873167, test loss: 0.2622787537583924\n",
      "epoch 8797: train loss: 0.12280081315122773, test loss: 0.2622787900689048\n",
      "epoch 8798: train loss: 0.12279712882949059, test loss: 0.26227882656255397\n",
      "epoch 8799: train loss: 0.1227934450933552, test loss: 0.262278863239284\n",
      "epoch 8800: train loss: 0.12278976194265656, test loss: 0.2622789000990393\n",
      "epoch 8801: train loss: 0.12278607937722978, test loss: 0.2622789371417639\n",
      "epoch 8802: train loss: 0.12278239739690999, test loss: 0.26227897436740244\n",
      "epoch 8803: train loss: 0.1227787160015324, test loss: 0.26227901177589896\n",
      "epoch 8804: train loss: 0.12277503519093232, test loss: 0.262279049367198\n",
      "epoch 8805: train loss: 0.12277135496494504, test loss: 0.26227908714124376\n",
      "epoch 8806: train loss: 0.12276767532340599, test loss: 0.2622791250979807\n",
      "epoch 8807: train loss: 0.12276399626615067, test loss: 0.26227916323735323\n",
      "epoch 8808: train loss: 0.1227603177930146, test loss: 0.2622792015593058\n",
      "epoch 8809: train loss: 0.1227566399038334, test loss: 0.2622792400637829\n",
      "epoch 8810: train loss: 0.12275296259844272, test loss: 0.26227927875072893\n",
      "epoch 8811: train loss: 0.12274928587667834, test loss: 0.2622793176200885\n",
      "epoch 8812: train loss: 0.12274560973837602, test loss: 0.2622793566718061\n",
      "epoch 8813: train loss: 0.1227419341833717, test loss: 0.2622793959058262\n",
      "epoch 8814: train loss: 0.12273825921150122, test loss: 0.26227943532209347\n",
      "epoch 8815: train loss: 0.12273458482260065, test loss: 0.2622794749205525\n",
      "epoch 8816: train loss: 0.12273091101650606, test loss: 0.2622795147011478\n",
      "epoch 8817: train loss: 0.12272723779305356, test loss: 0.26227955466382413\n",
      "epoch 8818: train loss: 0.12272356515207931, test loss: 0.26227959480852614\n",
      "epoch 8819: train loss: 0.12271989309341967, test loss: 0.26227963513519853\n",
      "epoch 8820: train loss: 0.12271622161691088, test loss: 0.2622796756437859\n",
      "epoch 8821: train loss: 0.12271255072238939, test loss: 0.2622797163342331\n",
      "epoch 8822: train loss: 0.12270888040969162, test loss: 0.2622797572064848\n",
      "epoch 8823: train loss: 0.12270521067865413, test loss: 0.26227979826048586\n",
      "epoch 8824: train loss: 0.12270154152911349, test loss: 0.26227983949618094\n",
      "epoch 8825: train loss: 0.1226978729609064, test loss: 0.26227988091351506\n",
      "epoch 8826: train loss: 0.1226942049738695, test loss: 0.26227992251243293\n",
      "epoch 8827: train loss: 0.12269053756783962, test loss: 0.2622799642928795\n",
      "epoch 8828: train loss: 0.12268687074265365, test loss: 0.26228000625479947\n",
      "epoch 8829: train loss: 0.12268320449814844, test loss: 0.262280048398138\n",
      "epoch 8830: train loss: 0.122679538834161, test loss: 0.2622800907228399\n",
      "epoch 8831: train loss: 0.12267587375052838, test loss: 0.26228013322885013\n",
      "epoch 8832: train loss: 0.1226722092470877, test loss: 0.26228017591611363\n",
      "epoch 8833: train loss: 0.1226685453236761, test loss: 0.2622802187845755\n",
      "epoch 8834: train loss: 0.12266488198013085, test loss: 0.2622802618341808\n",
      "epoch 8835: train loss: 0.1226612192162893, test loss: 0.2622803050648744\n",
      "epoch 8836: train loss: 0.12265755703198873, test loss: 0.2622803484766014\n",
      "epoch 8837: train loss: 0.12265389542706663, test loss: 0.2622803920693071\n",
      "epoch 8838: train loss: 0.1226502344013605, test loss: 0.2622804358429364\n",
      "epoch 8839: train loss: 0.12264657395470788, test loss: 0.2622804797974345\n",
      "epoch 8840: train loss: 0.12264291408694644, test loss: 0.2622805239327466\n",
      "epoch 8841: train loss: 0.12263925479791384, test loss: 0.26228056824881774\n",
      "epoch 8842: train loss: 0.12263559608744785, test loss: 0.2622806127455933\n",
      "epoch 8843: train loss: 0.12263193795538632, test loss: 0.2622806574230185\n",
      "epoch 8844: train loss: 0.12262828040156709, test loss: 0.2622807022810384\n",
      "epoch 8845: train loss: 0.12262462342582815, test loss: 0.26228074731959844\n",
      "epoch 8846: train loss: 0.12262096702800751, test loss: 0.26228079253864384\n",
      "epoch 8847: train loss: 0.12261731120794325, test loss: 0.26228083793811996\n",
      "epoch 8848: train loss: 0.12261365596547354, test loss: 0.2622808835179721\n",
      "epoch 8849: train loss: 0.12261000130043652, test loss: 0.26228092927814556\n",
      "epoch 8850: train loss: 0.12260634721267052, test loss: 0.2622809752185859\n",
      "epoch 8851: train loss: 0.12260269370201392, test loss: 0.26228102133923836\n",
      "epoch 8852: train loss: 0.12259904076830506, test loss: 0.26228106764004844\n",
      "epoch 8853: train loss: 0.12259538841138243, test loss: 0.26228111412096156\n",
      "epoch 8854: train loss: 0.12259173663108455, test loss: 0.26228116078192315\n",
      "epoch 8855: train loss: 0.12258808542725007, test loss: 0.2622812076228788\n",
      "epoch 8856: train loss: 0.12258443479971756, test loss: 0.262281254643774\n",
      "epoch 8857: train loss: 0.12258078474832584, test loss: 0.2622813018445542\n",
      "epoch 8858: train loss: 0.12257713527291361, test loss: 0.26228134922516505\n",
      "epoch 8859: train loss: 0.12257348637331983, test loss: 0.26228139678555207\n",
      "epoch 8860: train loss: 0.12256983804938332, test loss: 0.26228144452566093\n",
      "epoch 8861: train loss: 0.1225661903009431, test loss: 0.26228149244543725\n",
      "epoch 8862: train loss: 0.12256254312783824, test loss: 0.2622815405448265\n",
      "epoch 8863: train loss: 0.1225588965299078, test loss: 0.2622815888237746\n",
      "epoch 8864: train loss: 0.12255525050699098, test loss: 0.2622816372822271\n",
      "epoch 8865: train loss: 0.12255160505892708, test loss: 0.26228168592012974\n",
      "epoch 8866: train loss: 0.12254796018555528, test loss: 0.2622817347374283\n",
      "epoch 8867: train loss: 0.12254431588671504, test loss: 0.2622817837340684\n",
      "epoch 8868: train loss: 0.12254067216224573, test loss: 0.26228183290999596\n",
      "epoch 8869: train loss: 0.12253702901198688, test loss: 0.26228188226515675\n",
      "epoch 8870: train loss: 0.12253338643577806, test loss: 0.26228193179949655\n",
      "epoch 8871: train loss: 0.12252974443345885, test loss: 0.2622819815129612\n",
      "epoch 8872: train loss: 0.12252610300486894, test loss: 0.26228203140549666\n",
      "epoch 8873: train loss: 0.12252246214984808, test loss: 0.2622820814770487\n",
      "epoch 8874: train loss: 0.12251882186823611, test loss: 0.2622821317275633\n",
      "epoch 8875: train loss: 0.12251518215987289, test loss: 0.2622821821569864\n",
      "epoch 8876: train loss: 0.12251154302459837, test loss: 0.26228223276526386\n",
      "epoch 8877: train loss: 0.12250790446225249, test loss: 0.26228228355234184\n",
      "epoch 8878: train loss: 0.1225042664726754, test loss: 0.2622823345181661\n",
      "epoch 8879: train loss: 0.12250062905570717, test loss: 0.2622823856626828\n",
      "epoch 8880: train loss: 0.12249699221118802, test loss: 0.26228243698583803\n",
      "epoch 8881: train loss: 0.12249335593895821, test loss: 0.26228248848757774\n",
      "epoch 8882: train loss: 0.12248972023885805, test loss: 0.262282540167848\n",
      "epoch 8883: train loss: 0.12248608511072791, test loss: 0.26228259202659504\n",
      "epoch 8884: train loss: 0.12248245055440826, test loss: 0.26228264406376495\n",
      "epoch 8885: train loss: 0.1224788165697396, test loss: 0.2622826962793038\n",
      "epoch 8886: train loss: 0.12247518315656247, test loss: 0.2622827486731579\n",
      "epoch 8887: train loss: 0.12247155031471756, test loss: 0.26228280124527326\n",
      "epoch 8888: train loss: 0.12246791804404551, test loss: 0.26228285399559614\n",
      "epoch 8889: train loss: 0.12246428634438715, test loss: 0.26228290692407297\n",
      "epoch 8890: train loss: 0.12246065521558325, test loss: 0.26228296003064977\n",
      "epoch 8891: train loss: 0.12245702465747471, test loss: 0.262283013315273\n",
      "epoch 8892: train loss: 0.1224533946699025, test loss: 0.2622830667778888\n",
      "epoch 8893: train loss: 0.12244976525270762, test loss: 0.2622831204184436\n",
      "epoch 8894: train loss: 0.12244613640573114, test loss: 0.2622831742368837\n",
      "epoch 8895: train loss: 0.12244250812881424, test loss: 0.26228322823315553\n",
      "epoch 8896: train loss: 0.12243888042179804, test loss: 0.26228328240720533\n",
      "epoch 8897: train loss: 0.12243525328452388, test loss: 0.2622833367589797\n",
      "epoch 8898: train loss: 0.12243162671683307, test loss: 0.262283391288425\n",
      "epoch 8899: train loss: 0.12242800071856698, test loss: 0.26228344599548764\n",
      "epoch 8900: train loss: 0.12242437528956708, test loss: 0.2622835008801141\n",
      "epoch 8901: train loss: 0.12242075042967489, test loss: 0.2622835559422509\n",
      "epoch 8902: train loss: 0.12241712613873197, test loss: 0.2622836111818445\n",
      "epoch 8903: train loss: 0.12241350241658001, test loss: 0.2622836665988416\n",
      "epoch 8904: train loss: 0.12240987926306066, test loss: 0.2622837221931887\n",
      "epoch 8905: train loss: 0.12240625667801566, test loss: 0.2622837779648322\n",
      "epoch 8906: train loss: 0.12240263466128695, test loss: 0.2622838339137189\n",
      "epoch 8907: train loss: 0.12239901321271635, test loss: 0.26228389003979535\n",
      "epoch 8908: train loss: 0.12239539233214579, test loss: 0.2622839463430082\n",
      "epoch 8909: train loss: 0.12239177201941737, test loss: 0.2622840028233041\n",
      "epoch 8910: train loss: 0.1223881522743731, test loss: 0.2622840594806298\n",
      "epoch 8911: train loss: 0.12238453309685511, test loss: 0.26228411631493204\n",
      "epoch 8912: train loss: 0.12238091448670567, test loss: 0.2622841733261575\n",
      "epoch 8913: train loss: 0.12237729644376698, test loss: 0.2622842305142528\n",
      "epoch 8914: train loss: 0.12237367896788143, test loss: 0.2622842878791649\n",
      "epoch 8915: train loss: 0.12237006205889138, test loss: 0.2622843454208406\n",
      "epoch 8916: train loss: 0.12236644571663927, test loss: 0.26228440313922663\n",
      "epoch 8917: train loss: 0.12236282994096764, test loss: 0.2622844610342698\n",
      "epoch 8918: train loss: 0.12235921473171907, test loss: 0.2622845191059171\n",
      "epoch 8919: train loss: 0.12235560008873617, test loss: 0.2622845773541152\n",
      "epoch 8920: train loss: 0.12235198601186166, test loss: 0.2622846357788113\n",
      "epoch 8921: train loss: 0.12234837250093836, test loss: 0.2622846943799521\n",
      "epoch 8922: train loss: 0.12234475955580898, test loss: 0.26228475315748456\n",
      "epoch 8923: train loss: 0.12234114717631651, test loss: 0.26228481211135574\n",
      "epoch 8924: train loss: 0.12233753536230385, test loss: 0.26228487124151256\n",
      "epoch 8925: train loss: 0.122333924113614, test loss: 0.26228493054790203\n",
      "epoch 8926: train loss: 0.12233031343009007, test loss: 0.2622849900304713\n",
      "epoch 8927: train loss: 0.12232670331157518, test loss: 0.2622850496891672\n",
      "epoch 8928: train loss: 0.12232309375791256, test loss: 0.2622851095239371\n",
      "epoch 8929: train loss: 0.12231948476894541, test loss: 0.2622851695347278\n",
      "epoch 8930: train loss: 0.1223158763445171, test loss: 0.26228522972148655\n",
      "epoch 8931: train loss: 0.12231226848447097, test loss: 0.26228529008416046\n",
      "epoch 8932: train loss: 0.12230866118865052, test loss: 0.26228535062269676\n",
      "epoch 8933: train loss: 0.12230505445689921, test loss: 0.26228541133704253\n",
      "epoch 8934: train loss: 0.12230144828906063, test loss: 0.26228547222714504\n",
      "epoch 8935: train loss: 0.12229784268497841, test loss: 0.26228553329295146\n",
      "epoch 8936: train loss: 0.12229423764449622, test loss: 0.26228559453440903\n",
      "epoch 8937: train loss: 0.12229063316745785, test loss: 0.2622856559514651\n",
      "epoch 8938: train loss: 0.12228702925370709, test loss: 0.2622857175440668\n",
      "epoch 8939: train loss: 0.12228342590308781, test loss: 0.2622857793121616\n",
      "epoch 8940: train loss: 0.12227982311544398, test loss: 0.26228584125569676\n",
      "epoch 8941: train loss: 0.12227622089061957, test loss: 0.2622859033746196\n",
      "epoch 8942: train loss: 0.12227261922845864, test loss: 0.2622859656688775\n",
      "epoch 8943: train loss: 0.12226901812880535, test loss: 0.2622860281384179\n",
      "epoch 8944: train loss: 0.1222654175915038, test loss: 0.2622860907831881\n",
      "epoch 8945: train loss: 0.12226181761639833, test loss: 0.2622861536031357\n",
      "epoch 8946: train loss: 0.1222582182033332, test loss: 0.262286216598208\n",
      "epoch 8947: train loss: 0.12225461935215277, test loss: 0.2622862797683525\n",
      "epoch 8948: train loss: 0.1222510210627015, test loss: 0.2622863431135168\n",
      "epoch 8949: train loss: 0.12224742333482382, test loss: 0.2622864066336483\n",
      "epoch 8950: train loss: 0.12224382616836436, test loss: 0.2622864703286945\n",
      "epoch 8951: train loss: 0.12224022956316767, test loss: 0.26228653419860315\n",
      "epoch 8952: train loss: 0.12223663351907847, test loss: 0.2622865982433217\n",
      "epoch 8953: train loss: 0.12223303803594146, test loss: 0.26228666246279775\n",
      "epoch 8954: train loss: 0.12222944311360143, test loss: 0.26228672685697885\n",
      "epoch 8955: train loss: 0.12222584875190325, test loss: 0.26228679142581274\n",
      "epoch 8956: train loss: 0.12222225495069186, test loss: 0.26228685616924713\n",
      "epoch 8957: train loss: 0.12221866170981223, test loss: 0.2622869210872296\n",
      "epoch 8958: train loss: 0.12221506902910935, test loss: 0.2622869861797079\n",
      "epoch 8959: train loss: 0.12221147690842836, test loss: 0.2622870514466297\n",
      "epoch 8960: train loss: 0.12220788534761443, test loss: 0.2622871168879428\n",
      "epoch 8961: train loss: 0.12220429434651275, test loss: 0.26228718250359506\n",
      "epoch 8962: train loss: 0.12220070390496865, test loss: 0.2622872482935341\n",
      "epoch 8963: train loss: 0.12219711402282743, test loss: 0.26228731425770774\n",
      "epoch 8964: train loss: 0.1221935246999345, test loss: 0.2622873803960639\n",
      "epoch 8965: train loss: 0.12218993593613532, test loss: 0.26228744670855036\n",
      "epoch 8966: train loss: 0.12218634773127546, test loss: 0.2622875131951151\n",
      "epoch 8967: train loss: 0.12218276008520046, test loss: 0.26228757985570583\n",
      "epoch 8968: train loss: 0.12217917299775596, test loss: 0.26228764669027055\n",
      "epoch 8969: train loss: 0.12217558646878772, test loss: 0.2622877136987573\n",
      "epoch 8970: train loss: 0.12217200049814146, test loss: 0.26228778088111393\n",
      "epoch 8971: train loss: 0.122168415085663, test loss: 0.2622878482372883\n",
      "epoch 8972: train loss: 0.12216483023119828, test loss: 0.26228791576722854\n",
      "epoch 8973: train loss: 0.12216124593459321, test loss: 0.2622879834708827\n",
      "epoch 8974: train loss: 0.12215766219569382, test loss: 0.2622880513481987\n",
      "epoch 8975: train loss: 0.12215407901434615, test loss: 0.2622881193991248\n",
      "epoch 8976: train loss: 0.12215049639039637, test loss: 0.2622881876236087\n",
      "epoch 8977: train loss: 0.12214691432369061, test loss: 0.26228825602159894\n",
      "epoch 8978: train loss: 0.12214333281407519, test loss: 0.2622883245930433\n",
      "epoch 8979: train loss: 0.1221397518613964, test loss: 0.2622883933378901\n",
      "epoch 8980: train loss: 0.1221361714655006, test loss: 0.2622884622560875\n",
      "epoch 8981: train loss: 0.12213259162623419, test loss: 0.2622885313475835\n",
      "epoch 8982: train loss: 0.12212901234344375, test loss: 0.2622886006123265\n",
      "epoch 8983: train loss: 0.12212543361697577, test loss: 0.2622886700502647\n",
      "epoch 8984: train loss: 0.12212185544667686, test loss: 0.2622887396613462\n",
      "epoch 8985: train loss: 0.12211827783239368, test loss: 0.2622888094455193\n",
      "epoch 8986: train loss: 0.12211470077397299, test loss: 0.2622888794027324\n",
      "epoch 8987: train loss: 0.1221111242712616, test loss: 0.26228894953293375\n",
      "epoch 8988: train loss: 0.12210754832410636, test loss: 0.26228901983607167\n",
      "epoch 8989: train loss: 0.1221039729323541, test loss: 0.26228909031209446\n",
      "epoch 8990: train loss: 0.1221003980958519, test loss: 0.2622891609609505\n",
      "epoch 8991: train loss: 0.12209682381444674, test loss: 0.2622892317825882\n",
      "epoch 8992: train loss: 0.12209325008798569, test loss: 0.26228930277695595\n",
      "epoch 8993: train loss: 0.12208967691631596, test loss: 0.2622893739440022\n",
      "epoch 8994: train loss: 0.1220861042992847, test loss: 0.26228944528367526\n",
      "epoch 8995: train loss: 0.12208253223673925, test loss: 0.2622895167959238\n",
      "epoch 8996: train loss: 0.12207896072852688, test loss: 0.26228958848069633\n",
      "epoch 8997: train loss: 0.12207538977449499, test loss: 0.2622896603379411\n",
      "epoch 8998: train loss: 0.12207181937449105, test loss: 0.2622897323676068\n",
      "epoch 8999: train loss: 0.1220682495283626, test loss: 0.26228980456964196\n",
      "epoch 9000: train loss: 0.12206468023595711, test loss: 0.26228987694399514\n",
      "epoch 9001: train loss: 0.12206111149712232, test loss: 0.26228994949061496\n",
      "epoch 9002: train loss: 0.12205754331170585, test loss: 0.26229002220945\n",
      "epoch 9003: train loss: 0.1220539756795555, test loss: 0.2622900951004489\n",
      "epoch 9004: train loss: 0.12205040860051902, test loss: 0.2622901681635603\n",
      "epoch 9005: train loss: 0.12204684207444431, test loss: 0.2622902413987329\n",
      "epoch 9006: train loss: 0.1220432761011793, test loss: 0.2622903148059153\n",
      "epoch 9007: train loss: 0.12203971068057197, test loss: 0.2622903883850563\n",
      "epoch 9008: train loss: 0.12203614581247034, test loss: 0.2622904621361046\n",
      "epoch 9009: train loss: 0.12203258149672255, test loss: 0.2622905360590089\n",
      "epoch 9010: train loss: 0.12202901773317677, test loss: 0.26229061015371813\n",
      "epoch 9011: train loss: 0.12202545452168118, test loss: 0.2622906844201809\n",
      "epoch 9012: train loss: 0.1220218918620841, test loss: 0.26229075885834613\n",
      "epoch 9013: train loss: 0.12201832975423389, test loss: 0.2622908334681626\n",
      "epoch 9014: train loss: 0.12201476819797889, test loss: 0.2622909082495791\n",
      "epoch 9015: train loss: 0.1220112071931676, test loss: 0.26229098320254474\n",
      "epoch 9016: train loss: 0.12200764673964855, test loss: 0.26229105832700816\n",
      "epoch 9017: train loss: 0.12200408683727028, test loss: 0.2622911336229184\n",
      "epoch 9018: train loss: 0.12200052748588147, test loss: 0.2622912090902243\n",
      "epoch 9019: train loss: 0.12199696868533078, test loss: 0.26229128472887486\n",
      "epoch 9020: train loss: 0.12199341043546699, test loss: 0.2622913605388192\n",
      "epoch 9021: train loss: 0.1219898527361389, test loss: 0.262291436520006\n",
      "epoch 9022: train loss: 0.12198629558719538, test loss: 0.2622915126723846\n",
      "epoch 9023: train loss: 0.12198273898848541, test loss: 0.2622915889959037\n",
      "epoch 9024: train loss: 0.1219791829398579, test loss: 0.2622916654905127\n",
      "epoch 9025: train loss: 0.12197562744116201, test loss: 0.2622917421561604\n",
      "epoch 9026: train loss: 0.12197207249224676, test loss: 0.262291818992796\n",
      "epoch 9027: train loss: 0.12196851809296132, test loss: 0.2622918960003687\n",
      "epoch 9028: train loss: 0.12196496424315491, test loss: 0.2622919731788275\n",
      "epoch 9029: train loss: 0.12196141094267691, test loss: 0.26229205052812155\n",
      "epoch 9030: train loss: 0.12195785819137658, test loss: 0.26229212804820007\n",
      "epoch 9031: train loss: 0.12195430598910334, test loss: 0.26229220573901235\n",
      "epoch 9032: train loss: 0.12195075433570665, test loss: 0.2622922836005074\n",
      "epoch 9033: train loss: 0.12194720323103604, test loss: 0.26229236163263464\n",
      "epoch 9034: train loss: 0.1219436526749411, test loss: 0.2622924398353431\n",
      "epoch 9035: train loss: 0.12194010266727145, test loss: 0.2622925182085823\n",
      "epoch 9036: train loss: 0.12193655320787679, test loss: 0.2622925967523015\n",
      "epoch 9037: train loss: 0.12193300429660686, test loss: 0.26229267546644974\n",
      "epoch 9038: train loss: 0.12192945593331152, test loss: 0.2622927543509767\n",
      "epoch 9039: train loss: 0.12192590811784058, test loss: 0.26229283340583154\n",
      "epoch 9040: train loss: 0.12192236085004399, test loss: 0.26229291263096366\n",
      "epoch 9041: train loss: 0.12191881412977176, test loss: 0.2622929920263225\n",
      "epoch 9042: train loss: 0.12191526795687395, test loss: 0.2622930715918574\n",
      "epoch 9043: train loss: 0.1219117223312006, test loss: 0.26229315132751785\n",
      "epoch 9044: train loss: 0.12190817725260192, test loss: 0.2622932312332532\n",
      "epoch 9045: train loss: 0.12190463272092815, test loss: 0.2622933113090131\n",
      "epoch 9046: train loss: 0.12190108873602952, test loss: 0.26229339155474696\n",
      "epoch 9047: train loss: 0.12189754529775644, test loss: 0.26229347197040426\n",
      "epoch 9048: train loss: 0.12189400240595921, test loss: 0.26229355255593456\n",
      "epoch 9049: train loss: 0.12189046006048834, test loss: 0.2622936333112873\n",
      "epoch 9050: train loss: 0.12188691826119437, test loss: 0.2622937142364122\n",
      "epoch 9051: train loss: 0.12188337700792781, test loss: 0.26229379533125885\n",
      "epoch 9052: train loss: 0.1218798363005393, test loss: 0.26229387659577674\n",
      "epoch 9053: train loss: 0.12187629613887958, test loss: 0.26229395802991556\n",
      "epoch 9054: train loss: 0.12187275652279933, test loss: 0.262294039633625\n",
      "epoch 9055: train loss: 0.12186921745214942, test loss: 0.26229412140685476\n",
      "epoch 9056: train loss: 0.12186567892678067, test loss: 0.2622942033495544\n",
      "epoch 9057: train loss: 0.12186214094654396, test loss: 0.2622942854616736\n",
      "epoch 9058: train loss: 0.12185860351129035, test loss: 0.26229436774316234\n",
      "epoch 9059: train loss: 0.12185506662087082, test loss: 0.26229445019397013\n",
      "epoch 9060: train loss: 0.12185153027513645, test loss: 0.2622945328140468\n",
      "epoch 9061: train loss: 0.12184799447393846, test loss: 0.2622946156033422\n",
      "epoch 9062: train loss: 0.121844459217128, test loss: 0.2622946985618061\n",
      "epoch 9063: train loss: 0.12184092450455633, test loss: 0.26229478168938825\n",
      "epoch 9064: train loss: 0.12183739033607482, test loss: 0.2622948649860386\n",
      "epoch 9065: train loss: 0.12183385671153482, test loss: 0.262294948451707\n",
      "epoch 9066: train loss: 0.12183032363078777, test loss: 0.26229503208634325\n",
      "epoch 9067: train loss: 0.12182679109368517, test loss: 0.2622951158898974\n",
      "epoch 9068: train loss: 0.12182325910007857, test loss: 0.2622951998623192\n",
      "epoch 9069: train loss: 0.12181972764981962, test loss: 0.2622952840035587\n",
      "epoch 9070: train loss: 0.12181619674275991, test loss: 0.262295368313566\n",
      "epoch 9071: train loss: 0.12181266637875127, test loss: 0.2622954527922908\n",
      "epoch 9072: train loss: 0.12180913655764539, test loss: 0.26229553743968326\n",
      "epoch 9073: train loss: 0.12180560727929414, test loss: 0.2622956222556933\n",
      "epoch 9074: train loss: 0.12180207854354945, test loss: 0.2622957072402711\n",
      "epoch 9075: train loss: 0.12179855035026325, test loss: 0.26229579239336664\n",
      "epoch 9076: train loss: 0.12179502269928755, test loss: 0.2622958777149301\n",
      "epoch 9077: train loss: 0.12179149559047442, test loss: 0.2622959632049115\n",
      "epoch 9078: train loss: 0.12178796902367603, test loss: 0.26229604886326086\n",
      "epoch 9079: train loss: 0.12178444299874452, test loss: 0.26229613468992846\n",
      "epoch 9080: train loss: 0.12178091751553216, test loss: 0.2622962206848644\n",
      "epoch 9081: train loss: 0.12177739257389124, test loss: 0.2622963068480189\n",
      "epoch 9082: train loss: 0.1217738681736741, test loss: 0.26229639317934217\n",
      "epoch 9083: train loss: 0.12177034431473316, test loss: 0.26229647967878433\n",
      "epoch 9084: train loss: 0.12176682099692095, test loss: 0.2622965663462957\n",
      "epoch 9085: train loss: 0.12176329822008991, test loss: 0.2622966531818265\n",
      "epoch 9086: train loss: 0.12175977598409268, test loss: 0.26229674018532695\n",
      "epoch 9087: train loss: 0.12175625428878188, test loss: 0.2622968273567475\n",
      "epoch 9088: train loss: 0.12175273313401025, test loss: 0.26229691469603833\n",
      "epoch 9089: train loss: 0.12174921251963049, test loss: 0.26229700220314983\n",
      "epoch 9090: train loss: 0.12174569244549546, test loss: 0.2622970898780323\n",
      "epoch 9091: train loss: 0.12174217291145803, test loss: 0.2622971777206362\n",
      "epoch 9092: train loss: 0.1217386539173711, test loss: 0.26229726573091183\n",
      "epoch 9093: train loss: 0.12173513546308765, test loss: 0.26229735390880965\n",
      "epoch 9094: train loss: 0.12173161754846078, test loss: 0.2622974422542801\n",
      "epoch 9095: train loss: 0.12172810017334353, test loss: 0.2622975307672736\n",
      "epoch 9096: train loss: 0.12172458333758905, test loss: 0.2622976194477406\n",
      "epoch 9097: train loss: 0.12172106704105062, test loss: 0.2622977082956316\n",
      "epoch 9098: train loss: 0.12171755128358143, test loss: 0.26229779731089714\n",
      "epoch 9099: train loss: 0.12171403606503485, test loss: 0.2622978864934877\n",
      "epoch 9100: train loss: 0.12171052138526427, test loss: 0.26229797584335385\n",
      "epoch 9101: train loss: 0.1217070072441231, test loss: 0.26229806536044614\n",
      "epoch 9102: train loss: 0.12170349364146484, test loss: 0.26229815504471526\n",
      "epoch 9103: train loss: 0.12169998057714305, test loss: 0.2622982448961117\n",
      "epoch 9104: train loss: 0.12169646805101136, test loss: 0.262298334914586\n",
      "epoch 9105: train loss: 0.12169295606292341, test loss: 0.26229842510008894\n",
      "epoch 9106: train loss: 0.12168944461273293, test loss: 0.2622985154525711\n",
      "epoch 9107: train loss: 0.12168593370029368, test loss: 0.2622986059719833\n",
      "epoch 9108: train loss: 0.1216824233254595, test loss: 0.2622986966582761\n",
      "epoch 9109: train loss: 0.12167891348808435, test loss: 0.2622987875114002\n",
      "epoch 9110: train loss: 0.12167540418802206, test loss: 0.26229887853130646\n",
      "epoch 9111: train loss: 0.12167189542512671, test loss: 0.2622989697179455\n",
      "epoch 9112: train loss: 0.12166838719925237, test loss: 0.26229906107126816\n",
      "epoch 9113: train loss: 0.12166487951025311, test loss: 0.26229915259122527\n",
      "epoch 9114: train loss: 0.12166137235798312, test loss: 0.2622992442777676\n",
      "epoch 9115: train loss: 0.12165786574229663, test loss: 0.262299336130846\n",
      "epoch 9116: train loss: 0.12165435966304793, test loss: 0.2622994281504112\n",
      "epoch 9117: train loss: 0.12165085412009137, test loss: 0.2622995203364143\n",
      "epoch 9118: train loss: 0.12164734911328134, test loss: 0.262299612688806\n",
      "epoch 9119: train loss: 0.12164384464247231, test loss: 0.26229970520753737\n",
      "epoch 9120: train loss: 0.12164034070751875, test loss: 0.26229979789255914\n",
      "epoch 9121: train loss: 0.12163683730827524, test loss: 0.2622998907438224\n",
      "epoch 9122: train loss: 0.12163333444459644, test loss: 0.2622999837612781\n",
      "epoch 9123: train loss: 0.12162983211633698, test loss: 0.2623000769448771\n",
      "epoch 9124: train loss: 0.1216263303233516, test loss: 0.2623001702945706\n",
      "epoch 9125: train loss: 0.12162282906549514, test loss: 0.26230026381030946\n",
      "epoch 9126: train loss: 0.12161932834262239, test loss: 0.26230035749204483\n",
      "epoch 9127: train loss: 0.12161582815458827, test loss: 0.2623004513397277\n",
      "epoch 9128: train loss: 0.12161232850124776, test loss: 0.26230054535330916\n",
      "epoch 9129: train loss: 0.12160882938245585, test loss: 0.2623006395327403\n",
      "epoch 9130: train loss: 0.12160533079806758, test loss: 0.26230073387797226\n",
      "epoch 9131: train loss: 0.12160183274793816, test loss: 0.2623008283889562\n",
      "epoch 9132: train loss: 0.12159833523192272, test loss: 0.2623009230656432\n",
      "epoch 9133: train loss: 0.12159483824987646, test loss: 0.2623010179079845\n",
      "epoch 9134: train loss: 0.12159134180165475, test loss: 0.2623011129159313\n",
      "epoch 9135: train loss: 0.12158784588711288, test loss: 0.2623012080894347\n",
      "epoch 9136: train loss: 0.12158435050610632, test loss: 0.26230130342844604\n",
      "epoch 9137: train loss: 0.12158085565849044, test loss: 0.2623013989329166\n",
      "epoch 9138: train loss: 0.1215773613441208, test loss: 0.2623014946027975\n",
      "epoch 9139: train loss: 0.12157386756285299, test loss: 0.26230159043804013\n",
      "epoch 9140: train loss: 0.1215703743145426, test loss: 0.2623016864385958\n",
      "epoch 9141: train loss: 0.12156688159904537, test loss: 0.2623017826044158\n",
      "epoch 9142: train loss: 0.12156338941621696, test loss: 0.2623018789354514\n",
      "epoch 9143: train loss: 0.12155989776591322, test loss: 0.2623019754316541\n",
      "epoch 9144: train loss: 0.12155640664798999, test loss: 0.26230207209297524\n",
      "epoch 9145: train loss: 0.12155291606230316, test loss: 0.2623021689193662\n",
      "epoch 9146: train loss: 0.1215494260087087, test loss: 0.26230226591077843\n",
      "epoch 9147: train loss: 0.1215459364870626, test loss: 0.2623023630671632\n",
      "epoch 9148: train loss: 0.12154244749722098, test loss: 0.2623024603884722\n",
      "epoch 9149: train loss: 0.12153895903903993, test loss: 0.2623025578746568\n",
      "epoch 9150: train loss: 0.12153547111237561, test loss: 0.26230265552566845\n",
      "epoch 9151: train loss: 0.1215319837170843, test loss: 0.26230275334145875\n",
      "epoch 9152: train loss: 0.12152849685302229, test loss: 0.2623028513219791\n",
      "epoch 9153: train loss: 0.12152501052004588, test loss: 0.26230294946718125\n",
      "epoch 9154: train loss: 0.12152152471801152, test loss: 0.2623030477770166\n",
      "epoch 9155: train loss: 0.12151803944677565, test loss: 0.26230314625143675\n",
      "epoch 9156: train loss: 0.12151455470619478, test loss: 0.2623032448903933\n",
      "epoch 9157: train loss: 0.12151107049612549, test loss: 0.26230334369383795\n",
      "epoch 9158: train loss: 0.12150758681642439, test loss: 0.2623034426617224\n",
      "epoch 9159: train loss: 0.12150410366694814, test loss: 0.2623035417939981\n",
      "epoch 9160: train loss: 0.12150062104755349, test loss: 0.2623036410906169\n",
      "epoch 9161: train loss: 0.12149713895809726, test loss: 0.2623037405515304\n",
      "epoch 9162: train loss: 0.12149365739843622, test loss: 0.26230384017669034\n",
      "epoch 9163: train loss: 0.12149017636842732, test loss: 0.2623039399660485\n",
      "epoch 9164: train loss: 0.1214866958679275, test loss: 0.26230403991955653\n",
      "epoch 9165: train loss: 0.12148321589679377, test loss: 0.26230414003716634\n",
      "epoch 9166: train loss: 0.12147973645488318, test loss: 0.2623042403188296\n",
      "epoch 9167: train loss: 0.12147625754205285, test loss: 0.26230434076449827\n",
      "epoch 9168: train loss: 0.12147277915815996, test loss: 0.26230444137412395\n",
      "epoch 9169: train loss: 0.12146930130306172, test loss: 0.2623045421476587\n",
      "epoch 9170: train loss: 0.12146582397661546, test loss: 0.2623046430850542\n",
      "epoch 9171: train loss: 0.12146234717867843, test loss: 0.2623047441862625\n",
      "epoch 9172: train loss: 0.12145887090910808, test loss: 0.2623048454512354\n",
      "epoch 9173: train loss: 0.12145539516776185, test loss: 0.26230494687992484\n",
      "epoch 9174: train loss: 0.12145191995449725, test loss: 0.2623050484722828\n",
      "epoch 9175: train loss: 0.1214484452691718, test loss: 0.26230515022826123\n",
      "epoch 9176: train loss: 0.1214449711116431, test loss: 0.262305252147812\n",
      "epoch 9177: train loss: 0.12144149748176886, test loss: 0.2623053542308872\n",
      "epoch 9178: train loss: 0.12143802437940675, test loss: 0.26230545647743875\n",
      "epoch 9179: train loss: 0.1214345518044146, test loss: 0.2623055588874188\n",
      "epoch 9180: train loss: 0.1214310797566502, test loss: 0.2623056614607794\n",
      "epoch 9181: train loss: 0.12142760823597143, test loss: 0.26230576419747254\n",
      "epoch 9182: train loss: 0.12142413724223623, test loss: 0.2623058670974503\n",
      "epoch 9183: train loss: 0.1214206667753026, test loss: 0.2623059701606648\n",
      "epoch 9184: train loss: 0.12141719683502855, test loss: 0.2623060733870682\n",
      "epoch 9185: train loss: 0.12141372742127221, test loss: 0.26230617677661255\n",
      "epoch 9186: train loss: 0.12141025853389172, test loss: 0.26230628032925013\n",
      "epoch 9187: train loss: 0.1214067901727453, test loss: 0.2623063840449331\n",
      "epoch 9188: train loss: 0.12140332233769122, test loss: 0.26230648792361355\n",
      "epoch 9189: train loss: 0.12139985502858776, test loss: 0.2623065919652437\n",
      "epoch 9190: train loss: 0.12139638824529332, test loss: 0.2623066961697759\n",
      "epoch 9191: train loss: 0.1213929219876663, test loss: 0.26230680053716243\n",
      "epoch 9192: train loss: 0.1213894562555652, test loss: 0.26230690506735527\n",
      "epoch 9193: train loss: 0.12138599104884856, test loss: 0.262307009760307\n",
      "epoch 9194: train loss: 0.12138252636737493, test loss: 0.2623071146159697\n",
      "epoch 9195: train loss: 0.12137906221100297, test loss: 0.26230721963429604\n",
      "epoch 9196: train loss: 0.1213755985795914, test loss: 0.262307324815238\n",
      "epoch 9197: train loss: 0.1213721354729989, test loss: 0.26230743015874813\n",
      "epoch 9198: train loss: 0.12136867289108434, test loss: 0.26230753566477866\n",
      "epoch 9199: train loss: 0.12136521083370656, test loss: 0.26230764133328216\n",
      "epoch 9200: train loss: 0.12136174930072446, test loss: 0.26230774716421096\n",
      "epoch 9201: train loss: 0.12135828829199699, test loss: 0.2623078531575175\n",
      "epoch 9202: train loss: 0.1213548278073832, test loss: 0.26230795931315415\n",
      "epoch 9203: train loss: 0.12135136784674212, test loss: 0.2623080656310735\n",
      "epoch 9204: train loss: 0.12134790840993294, test loss: 0.262308172111228\n",
      "epoch 9205: train loss: 0.12134444949681476, test loss: 0.2623082787535701\n",
      "epoch 9206: train loss: 0.12134099110724689, test loss: 0.26230838555805236\n",
      "epoch 9207: train loss: 0.12133753324108858, test loss: 0.26230849252462735\n",
      "epoch 9208: train loss: 0.12133407589819915, test loss: 0.2623085996532476\n",
      "epoch 9209: train loss: 0.12133061907843803, test loss: 0.26230870694386565\n",
      "epoch 9210: train loss: 0.12132716278166462, test loss: 0.2623088143964342\n",
      "epoch 9211: train loss: 0.12132370700773853, test loss: 0.26230892201090583\n",
      "epoch 9212: train loss: 0.12132025175651918, test loss: 0.2623090297872331\n",
      "epoch 9213: train loss: 0.12131679702786627, test loss: 0.2623091377253686\n",
      "epoch 9214: train loss: 0.12131334282163941, test loss: 0.2623092458252652\n",
      "epoch 9215: train loss: 0.12130988913769837, test loss: 0.26230935408687545\n",
      "epoch 9216: train loss: 0.12130643597590286, test loss: 0.2623094625101521\n",
      "epoch 9217: train loss: 0.12130298333611272, test loss: 0.2623095710950478\n",
      "epoch 9218: train loss: 0.12129953121818787, test loss: 0.26230967984151543\n",
      "epoch 9219: train loss: 0.12129607962198818, test loss: 0.26230978874950756\n",
      "epoch 9220: train loss: 0.12129262854737366, test loss: 0.2623098978189772\n",
      "epoch 9221: train loss: 0.12128917799420436, test loss: 0.26231000704987684\n",
      "epoch 9222: train loss: 0.12128572796234036, test loss: 0.26231011644215957\n",
      "epoch 9223: train loss: 0.12128227845164181, test loss: 0.2623102259957781\n",
      "epoch 9224: train loss: 0.12127882946196886, test loss: 0.26231033571068524\n",
      "epoch 9225: train loss: 0.12127538099318183, test loss: 0.2623104455868339\n",
      "epoch 9226: train loss: 0.12127193304514097, test loss: 0.262310555624177\n",
      "epoch 9227: train loss: 0.12126848561770665, test loss: 0.26231066582266743\n",
      "epoch 9228: train loss: 0.12126503871073929, test loss: 0.26231077618225807\n",
      "epoch 9229: train loss: 0.12126159232409936, test loss: 0.2623108867029018\n",
      "epoch 9230: train loss: 0.12125814645764735, test loss: 0.2623109973845517\n",
      "epoch 9231: train loss: 0.12125470111124383, test loss: 0.26231110822716075\n",
      "epoch 9232: train loss: 0.12125125628474943, test loss: 0.26231121923068174\n",
      "epoch 9233: train loss: 0.12124781197802484, test loss: 0.26231133039506793\n",
      "epoch 9234: train loss: 0.12124436819093078, test loss: 0.26231144172027215\n",
      "epoch 9235: train loss: 0.121240924923328, test loss: 0.2623115532062475\n",
      "epoch 9236: train loss: 0.12123748217507738, test loss: 0.2623116648529471\n",
      "epoch 9237: train loss: 0.12123403994603979, test loss: 0.262311776660324\n",
      "epoch 9238: train loss: 0.12123059823607615, test loss: 0.2623118886283314\n",
      "epoch 9239: train loss: 0.1212271570450475, test loss: 0.26231200075692207\n",
      "epoch 9240: train loss: 0.12122371637281482, test loss: 0.26231211304604957\n",
      "epoch 9241: train loss: 0.12122027621923927, test loss: 0.2623122254956668\n",
      "epoch 9242: train loss: 0.12121683658418196, test loss: 0.26231233810572696\n",
      "epoch 9243: train loss: 0.12121339746750409, test loss: 0.2623124508761833\n",
      "epoch 9244: train loss: 0.12120995886906695, test loss: 0.262312563806989\n",
      "epoch 9245: train loss: 0.12120652078873183, test loss: 0.2623126768980972\n",
      "epoch 9246: train loss: 0.1212030832263601, test loss: 0.26231279014946124\n",
      "epoch 9247: train loss: 0.12119964618181317, test loss: 0.26231290356103437\n",
      "epoch 9248: train loss: 0.12119620965495251, test loss: 0.26231301713276983\n",
      "epoch 9249: train loss: 0.12119277364563961, test loss: 0.26231313086462094\n",
      "epoch 9250: train loss: 0.1211893381537361, test loss: 0.262313244756541\n",
      "epoch 9251: train loss: 0.12118590317910356, test loss: 0.26231335880848333\n",
      "epoch 9252: train loss: 0.12118246872160371, test loss: 0.2623134730204012\n",
      "epoch 9253: train loss: 0.12117903478109822, test loss: 0.2623135873922481\n",
      "epoch 9254: train loss: 0.12117560135744893, test loss: 0.26231370192397735\n",
      "epoch 9255: train loss: 0.12117216845051765, test loss: 0.2623138166155424\n",
      "epoch 9256: train loss: 0.12116873606016626, test loss: 0.2623139314668965\n",
      "epoch 9257: train loss: 0.1211653041862567, test loss: 0.2623140464779933\n",
      "epoch 9258: train loss: 0.12116187282865098, test loss: 0.2623141616487862\n",
      "epoch 9259: train loss: 0.12115844198721114, test loss: 0.26231427697922854\n",
      "epoch 9260: train loss: 0.12115501166179926, test loss: 0.26231439246927396\n",
      "epoch 9261: train loss: 0.12115158185227753, test loss: 0.26231450811887586\n",
      "epoch 9262: train loss: 0.12114815255850811, test loss: 0.2623146239279878\n",
      "epoch 9263: train loss: 0.12114472378035329, test loss: 0.26231473989656334\n",
      "epoch 9264: train loss: 0.12114129551767534, test loss: 0.26231485602455595\n",
      "epoch 9265: train loss: 0.12113786777033664, test loss: 0.2623149723119194\n",
      "epoch 9266: train loss: 0.12113444053819958, test loss: 0.262315088758607\n",
      "epoch 9267: train loss: 0.12113101382112665, test loss: 0.2623152053645726\n",
      "epoch 9268: train loss: 0.12112758761898033, test loss: 0.2623153221297697\n",
      "epoch 9269: train loss: 0.12112416193162324, test loss: 0.262315439054152\n",
      "epoch 9270: train loss: 0.121120736758918, test loss: 0.2623155561376732\n",
      "epoch 9271: train loss: 0.1211173121007272, test loss: 0.26231567338028683\n",
      "epoch 9272: train loss: 0.12111388795691362, test loss: 0.2623157907819467\n",
      "epoch 9273: train loss: 0.12111046432734004, test loss: 0.2623159083426066\n",
      "epoch 9274: train loss: 0.12110704121186931, test loss: 0.26231602606222004\n",
      "epoch 9275: train loss: 0.12110361861036424, test loss: 0.262316143940741\n",
      "epoch 9276: train loss: 0.12110019652268779, test loss: 0.262316261978123\n",
      "epoch 9277: train loss: 0.121096774948703, test loss: 0.2623163801743201\n",
      "epoch 9278: train loss: 0.12109335388827279, test loss: 0.2623164985292859\n",
      "epoch 9279: train loss: 0.12108993334126036, test loss: 0.2623166170429743\n",
      "epoch 9280: train loss: 0.12108651330752881, test loss: 0.2623167357153391\n",
      "epoch 9281: train loss: 0.12108309378694129, test loss: 0.2623168545463342\n",
      "epoch 9282: train loss: 0.1210796747793611, test loss: 0.2623169735359135\n",
      "epoch 9283: train loss: 0.12107625628465149, test loss: 0.2623170926840308\n",
      "epoch 9284: train loss: 0.12107283830267583, test loss: 0.26231721199064\n",
      "epoch 9285: train loss: 0.12106942083329755, test loss: 0.26231733145569514\n",
      "epoch 9286: train loss: 0.12106600387638004, test loss: 0.26231745107915005\n",
      "epoch 9287: train loss: 0.12106258743178681, test loss: 0.2623175708609587\n",
      "epoch 9288: train loss: 0.12105917149938143, test loss: 0.26231769080107514\n",
      "epoch 9289: train loss: 0.12105575607902751, test loss: 0.2623178108994533\n",
      "epoch 9290: train loss: 0.12105234117058866, test loss: 0.2623179311560471\n",
      "epoch 9291: train loss: 0.12104892677392869, test loss: 0.2623180515708108\n",
      "epoch 9292: train loss: 0.12104551288891127, test loss: 0.26231817214369824\n",
      "epoch 9293: train loss: 0.12104209951540021, test loss: 0.26231829287466363\n",
      "epoch 9294: train loss: 0.1210386866532594, test loss: 0.26231841376366094\n",
      "epoch 9295: train loss: 0.12103527430235275, test loss: 0.2623185348106443\n",
      "epoch 9296: train loss: 0.12103186246254422, test loss: 0.26231865601556775\n",
      "epoch 9297: train loss: 0.12102845113369785, test loss: 0.2623187773783856\n",
      "epoch 9298: train loss: 0.12102504031567765, test loss: 0.26231889889905186\n",
      "epoch 9299: train loss: 0.12102163000834781, test loss: 0.2623190205775207\n",
      "epoch 9300: train loss: 0.12101822021157244, test loss: 0.2623191424137464\n",
      "epoch 9301: train loss: 0.1210148109252158, test loss: 0.26231926440768305\n",
      "epoch 9302: train loss: 0.12101140214914213, test loss: 0.2623193865592849\n",
      "epoch 9303: train loss: 0.1210079938832158, test loss: 0.26231950886850613\n",
      "epoch 9304: train loss: 0.12100458612730114, test loss: 0.2623196313353012\n",
      "epoch 9305: train loss: 0.12100117888126258, test loss: 0.2623197539596241\n",
      "epoch 9306: train loss: 0.12099777214496463, test loss: 0.26231987674142926\n",
      "epoch 9307: train loss: 0.12099436591827181, test loss: 0.2623199996806711\n",
      "epoch 9308: train loss: 0.12099096020104869, test loss: 0.2623201227773037\n",
      "epoch 9309: train loss: 0.12098755499315988, test loss: 0.2623202460312815\n",
      "epoch 9310: train loss: 0.12098415029447011, test loss: 0.2623203694425589\n",
      "epoch 9311: train loss: 0.12098074610484406, test loss: 0.2623204930110903\n",
      "epoch 9312: train loss: 0.12097734242414657, test loss: 0.2623206167368299\n",
      "epoch 9313: train loss: 0.12097393925224242, test loss: 0.2623207406197322\n",
      "epoch 9314: train loss: 0.12097053658899654, test loss: 0.2623208646597518\n",
      "epoch 9315: train loss: 0.12096713443427384, test loss: 0.26232098885684285\n",
      "epoch 9316: train loss: 0.1209637327879393, test loss: 0.2623211132109601\n",
      "epoch 9317: train loss: 0.12096033164985802, test loss: 0.26232123772205773\n",
      "epoch 9318: train loss: 0.120956931019895, test loss: 0.2623213623900904\n",
      "epoch 9319: train loss: 0.12095353089791545, test loss: 0.2623214872150127\n",
      "epoch 9320: train loss: 0.12095013128378453, test loss: 0.2623216121967789\n",
      "epoch 9321: train loss: 0.12094673217736748, test loss: 0.2623217373353437\n",
      "epoch 9322: train loss: 0.1209433335785296, test loss: 0.26232186263066176\n",
      "epoch 9323: train loss: 0.12093993548713625, test loss: 0.2623219880826875\n",
      "epoch 9324: train loss: 0.12093653790305282, test loss: 0.26232211369137554\n",
      "epoch 9325: train loss: 0.12093314082614473, test loss: 0.2623222394566805\n",
      "epoch 9326: train loss: 0.12092974425627748, test loss: 0.2623223653785569\n",
      "epoch 9327: train loss: 0.12092634819331664, test loss: 0.2623224914569597\n",
      "epoch 9328: train loss: 0.12092295263712781, test loss: 0.2623226176918432\n",
      "epoch 9329: train loss: 0.1209195575875766, test loss: 0.2623227440831623\n",
      "epoch 9330: train loss: 0.12091616304452873, test loss: 0.2623228706308716\n",
      "epoch 9331: train loss: 0.12091276900784997, test loss: 0.26232299733492587\n",
      "epoch 9332: train loss: 0.12090937547740609, test loss: 0.2623231241952798\n",
      "epoch 9333: train loss: 0.12090598245306293, test loss: 0.26232325121188815\n",
      "epoch 9334: train loss: 0.12090258993468643, test loss: 0.2623233783847057\n",
      "epoch 9335: train loss: 0.1208991979221425, test loss: 0.2623235057136872\n",
      "epoch 9336: train loss: 0.12089580641529718, test loss: 0.2623236331987874\n",
      "epoch 9337: train loss: 0.12089241541401648, test loss: 0.26232376083996123\n",
      "epoch 9338: train loss: 0.12088902491816655, test loss: 0.26232388863716344\n",
      "epoch 9339: train loss: 0.1208856349276135, test loss: 0.2623240165903489\n",
      "epoch 9340: train loss: 0.12088224544222356, test loss: 0.26232414469947246\n",
      "epoch 9341: train loss: 0.12087885646186299, test loss: 0.262324272964489\n",
      "epoch 9342: train loss: 0.12087546798639803, test loss: 0.26232440138535346\n",
      "epoch 9343: train loss: 0.1208720800156951, test loss: 0.26232452996202066\n",
      "epoch 9344: train loss: 0.1208686925496206, test loss: 0.2623246586944456\n",
      "epoch 9345: train loss: 0.12086530558804096, test loss: 0.26232478758258326\n",
      "epoch 9346: train loss: 0.12086191913082267, test loss: 0.26232491662638857\n",
      "epoch 9347: train loss: 0.1208585331778323, test loss: 0.2623250458258165\n",
      "epoch 9348: train loss: 0.12085514772893649, test loss: 0.262325175180822\n",
      "epoch 9349: train loss: 0.12085176278400184, test loss: 0.26232530469136023\n",
      "epoch 9350: train loss: 0.1208483783428951, test loss: 0.26232543435738603\n",
      "epoch 9351: train loss: 0.12084499440548298, test loss: 0.26232556417885466\n",
      "epoch 9352: train loss: 0.12084161097163232, test loss: 0.262325694155721\n",
      "epoch 9353: train loss: 0.12083822804120996, test loss: 0.26232582428794027\n",
      "epoch 9354: train loss: 0.12083484561408281, test loss: 0.2623259545754674\n",
      "epoch 9355: train loss: 0.12083146369011781, test loss: 0.2623260850182577\n",
      "epoch 9356: train loss: 0.120828082269182, test loss: 0.26232621561626623\n",
      "epoch 9357: train loss: 0.12082470135114239, test loss: 0.26232634636944807\n",
      "epoch 9358: train loss: 0.12082132093586612, test loss: 0.2623264772777585\n",
      "epoch 9359: train loss: 0.12081794102322033, test loss: 0.26232660834115257\n",
      "epoch 9360: train loss: 0.12081456161307219, test loss: 0.26232673955958563\n",
      "epoch 9361: train loss: 0.12081118270528901, test loss: 0.26232687093301277\n",
      "epoch 9362: train loss: 0.12080780429973807, test loss: 0.26232700246138924\n",
      "epoch 9363: train loss: 0.12080442639628672, test loss: 0.26232713414467035\n",
      "epoch 9364: train loss: 0.1208010489948024, test loss: 0.2623272659828113\n",
      "epoch 9365: train loss: 0.1207976720951525, test loss: 0.26232739797576743\n",
      "epoch 9366: train loss: 0.12079429569720453, test loss: 0.2623275301234941\n",
      "epoch 9367: train loss: 0.12079091980082612, test loss: 0.2623276624259464\n",
      "epoch 9368: train loss: 0.12078754440588478, test loss: 0.26232779488308\n",
      "epoch 9369: train loss: 0.12078416951224823, test loss: 0.26232792749484984\n",
      "epoch 9370: train loss: 0.12078079511978411, test loss: 0.2623280602612115\n",
      "epoch 9371: train loss: 0.12077742122836022, test loss: 0.26232819318212053\n",
      "epoch 9372: train loss: 0.12077404783784433, test loss: 0.26232832625753205\n",
      "epoch 9373: train loss: 0.12077067494810428, test loss: 0.26232845948740163\n",
      "epoch 9374: train loss: 0.12076730255900803, test loss: 0.2623285928716846\n",
      "epoch 9375: train loss: 0.12076393067042349, test loss: 0.2623287264103365\n",
      "epoch 9376: train loss: 0.12076055928221863, test loss: 0.2623288601033128\n",
      "epoch 9377: train loss: 0.12075718839426154, test loss: 0.26232899395056886\n",
      "epoch 9378: train loss: 0.12075381800642031, test loss: 0.2623291279520602\n",
      "epoch 9379: train loss: 0.12075044811856304, test loss: 0.26232926210774254\n",
      "epoch 9380: train loss: 0.120747078730558, test loss: 0.26232939641757114\n",
      "epoch 9381: train loss: 0.1207437098422734, test loss: 0.26232953088150174\n",
      "epoch 9382: train loss: 0.12074034145357752, test loss: 0.26232966549948983\n",
      "epoch 9383: train loss: 0.12073697356433871, test loss: 0.2623298002714909\n",
      "epoch 9384: train loss: 0.12073360617442538, test loss: 0.26232993519746073\n",
      "epoch 9385: train loss: 0.12073023928370599, test loss: 0.2623300702773549\n",
      "epoch 9386: train loss: 0.12072687289204896, test loss: 0.26233020551112884\n",
      "epoch 9387: train loss: 0.1207235069993229, test loss: 0.2623303408987384\n",
      "epoch 9388: train loss: 0.12072014160539637, test loss: 0.26233047644013924\n",
      "epoch 9389: train loss: 0.12071677671013802, test loss: 0.2623306121352869\n",
      "epoch 9390: train loss: 0.12071341231341652, test loss: 0.2623307479841372\n",
      "epoch 9391: train loss: 0.12071004841510062, test loss: 0.2623308839866458\n",
      "epoch 9392: train loss: 0.1207066850150591, test loss: 0.2623310201427684\n",
      "epoch 9393: train loss: 0.12070332211316082, test loss: 0.2623311564524608\n",
      "epoch 9394: train loss: 0.12069995970927462, test loss: 0.2623312929156787\n",
      "epoch 9395: train loss: 0.1206965978032695, test loss: 0.262331429532378\n",
      "epoch 9396: train loss: 0.12069323639501435, test loss: 0.26233156630251436\n",
      "epoch 9397: train loss: 0.12068987548437829, test loss: 0.26233170322604366\n",
      "epoch 9398: train loss: 0.12068651507123032, test loss: 0.26233184030292167\n",
      "epoch 9399: train loss: 0.12068315515543965, test loss: 0.26233197753310433\n",
      "epoch 9400: train loss: 0.12067979573687541, test loss: 0.2623321149165474\n",
      "epoch 9401: train loss: 0.12067643681540684, test loss: 0.2623322524532068\n",
      "epoch 9402: train loss: 0.12067307839090319, test loss: 0.2623323901430384\n",
      "epoch 9403: train loss: 0.1206697204632338, test loss: 0.26233252798599815\n",
      "epoch 9404: train loss: 0.12066636303226808, test loss: 0.26233266598204197\n",
      "epoch 9405: train loss: 0.12066300609787539, test loss: 0.2623328041311257\n",
      "epoch 9406: train loss: 0.12065964965992525, test loss: 0.26233294243320543\n",
      "epoch 9407: train loss: 0.12065629371828715, test loss: 0.2623330808882371\n",
      "epoch 9408: train loss: 0.12065293827283068, test loss: 0.2623332194961766\n",
      "epoch 9409: train loss: 0.12064958332342543, test loss: 0.26233335825698006\n",
      "epoch 9410: train loss: 0.12064622886994111, test loss: 0.2623334971706035\n",
      "epoch 9411: train loss: 0.12064287491224737, test loss: 0.26233363623700284\n",
      "epoch 9412: train loss: 0.12063952145021403, test loss: 0.26233377545613423\n",
      "epoch 9413: train loss: 0.12063616848371086, test loss: 0.26233391482795376\n",
      "epoch 9414: train loss: 0.12063281601260777, test loss: 0.2623340543524174\n",
      "epoch 9415: train loss: 0.12062946403677462, test loss: 0.2623341940294814\n",
      "epoch 9416: train loss: 0.12062611255608136, test loss: 0.26233433385910176\n",
      "epoch 9417: train loss: 0.12062276157039804, test loss: 0.26233447384123465\n",
      "epoch 9418: train loss: 0.1206194110795947, test loss: 0.26233461397583624\n",
      "epoch 9419: train loss: 0.1206160610835414, test loss: 0.26233475426286273\n",
      "epoch 9420: train loss: 0.12061271158210836, test loss: 0.2623348947022702\n",
      "epoch 9421: train loss: 0.12060936257516577, test loss: 0.262335035294015\n",
      "epoch 9422: train loss: 0.1206060140625838, test loss: 0.2623351760380532\n",
      "epoch 9423: train loss: 0.12060266604423281, test loss: 0.2623353169343411\n",
      "epoch 9424: train loss: 0.12059931851998314, test loss: 0.2623354579828349\n",
      "epoch 9425: train loss: 0.12059597148970516, test loss: 0.26233559918349103\n",
      "epoch 9426: train loss: 0.12059262495326932, test loss: 0.26233574053626557\n",
      "epoch 9427: train loss: 0.12058927891054612, test loss: 0.26233588204111485\n",
      "epoch 9428: train loss: 0.1205859333614061, test loss: 0.26233602369799525\n",
      "epoch 9429: train loss: 0.12058258830571983, test loss: 0.2623361655068631\n",
      "epoch 9430: train loss: 0.12057924374335795, test loss: 0.2623363074676747\n",
      "epoch 9431: train loss: 0.12057589967419112, test loss: 0.26233644958038643\n",
      "epoch 9432: train loss: 0.12057255609809012, test loss: 0.2623365918449547\n",
      "epoch 9433: train loss: 0.12056921301492565, test loss: 0.26233673426133586\n",
      "epoch 9434: train loss: 0.12056587042456862, test loss: 0.26233687682948637\n",
      "epoch 9435: train loss: 0.12056252832688985, test loss: 0.2623370195493625\n",
      "epoch 9436: train loss: 0.12055918672176028, test loss: 0.2623371624209209\n",
      "epoch 9437: train loss: 0.12055584560905089, test loss: 0.26233730544411793\n",
      "epoch 9438: train loss: 0.12055250498863268, test loss: 0.26233744861891\n",
      "epoch 9439: train loss: 0.12054916486037673, test loss: 0.2623375919452537\n",
      "epoch 9440: train loss: 0.12054582522415412, test loss: 0.26233773542310546\n",
      "epoch 9441: train loss: 0.12054248607983606, test loss: 0.2623378790524219\n",
      "epoch 9442: train loss: 0.12053914742729371, test loss: 0.26233802283315955\n",
      "epoch 9443: train loss: 0.12053580926639838, test loss: 0.26233816676527477\n",
      "epoch 9444: train loss: 0.12053247159702134, test loss: 0.26233831084872433\n",
      "epoch 9445: train loss: 0.12052913441903396, test loss: 0.26233845508346476\n",
      "epoch 9446: train loss: 0.12052579773230761, test loss: 0.26233859946945265\n",
      "epoch 9447: train loss: 0.1205224615367138, test loss: 0.2623387440066447\n",
      "epoch 9448: train loss: 0.12051912583212396, test loss: 0.2623388886949974\n",
      "epoch 9449: train loss: 0.12051579061840967, test loss: 0.26233903353446747\n",
      "epoch 9450: train loss: 0.12051245589544252, test loss: 0.26233917852501154\n",
      "epoch 9451: train loss: 0.12050912166309413, test loss: 0.26233932366658635\n",
      "epoch 9452: train loss: 0.12050578792123622, test loss: 0.2623394689591485\n",
      "epoch 9453: train loss: 0.1205024546697405, test loss: 0.2623396144026548\n",
      "epoch 9454: train loss: 0.12049912190847875, test loss: 0.2623397599970621\n",
      "epoch 9455: train loss: 0.1204957896373228, test loss: 0.2623399057423268\n",
      "epoch 9456: train loss: 0.12049245785614458, test loss: 0.2623400516384059\n",
      "epoch 9457: train loss: 0.12048912656481595, test loss: 0.2623401976852562\n",
      "epoch 9458: train loss: 0.12048579576320888, test loss: 0.26234034388283434\n",
      "epoch 9459: train loss: 0.12048246545119543, test loss: 0.26234049023109723\n",
      "epoch 9460: train loss: 0.12047913562864766, test loss: 0.2623406367300017\n",
      "epoch 9461: train loss: 0.12047580629543767, test loss: 0.26234078337950445\n",
      "epoch 9462: train loss: 0.12047247745143766, test loss: 0.26234093017956256\n",
      "epoch 9463: train loss: 0.12046914909651978, test loss: 0.2623410771301327\n",
      "epoch 9464: train loss: 0.12046582123055632, test loss: 0.2623412242311719\n",
      "epoch 9465: train loss: 0.12046249385341959, test loss: 0.262341371482637\n",
      "epoch 9466: train loss: 0.12045916696498192, test loss: 0.26234151888448487\n",
      "epoch 9467: train loss: 0.12045584056511575, test loss: 0.2623416664366725\n",
      "epoch 9468: train loss: 0.12045251465369348, test loss: 0.2623418141391569\n",
      "epoch 9469: train loss: 0.12044918923058766, test loss: 0.2623419619918949\n",
      "epoch 9470: train loss: 0.12044586429567077, test loss: 0.2623421099948436\n",
      "epoch 9471: train loss: 0.12044253984881542, test loss: 0.2623422581479599\n",
      "epoch 9472: train loss: 0.12043921588989427, test loss: 0.2623424064512009\n",
      "epoch 9473: train loss: 0.12043589241877996, test loss: 0.2623425549045236\n",
      "epoch 9474: train loss: 0.12043256943534526, test loss: 0.2623427035078849\n",
      "epoch 9475: train loss: 0.12042924693946291, test loss: 0.26234285226124215\n",
      "epoch 9476: train loss: 0.12042592493100578, test loss: 0.26234300116455217\n",
      "epoch 9477: train loss: 0.12042260340984673, test loss: 0.26234315021777216\n",
      "epoch 9478: train loss: 0.12041928237585864, test loss: 0.26234329942085927\n",
      "epoch 9479: train loss: 0.1204159618289145, test loss: 0.26234344877377047\n",
      "epoch 9480: train loss: 0.12041264176888734, test loss: 0.2623435982764631\n",
      "epoch 9481: train loss: 0.12040932219565019, test loss: 0.26234374792889414\n",
      "epoch 9482: train loss: 0.12040600310907618, test loss: 0.26234389773102085\n",
      "epoch 9483: train loss: 0.12040268450903845, test loss: 0.26234404768280034\n",
      "epoch 9484: train loss: 0.12039936639541018, test loss: 0.2623441977841899\n",
      "epoch 9485: train loss: 0.12039604876806469, test loss: 0.2623443480351467\n",
      "epoch 9486: train loss: 0.12039273162687518, test loss: 0.262344498435628\n",
      "epoch 9487: train loss: 0.12038941497171503, test loss: 0.26234464898559107\n",
      "epoch 9488: train loss: 0.1203860988024577, test loss: 0.2623447996849931\n",
      "epoch 9489: train loss: 0.1203827831189765, test loss: 0.26234495053379137\n",
      "epoch 9490: train loss: 0.120379467921145, test loss: 0.26234510153194324\n",
      "epoch 9491: train loss: 0.12037615320883667, test loss: 0.2623452526794059\n",
      "epoch 9492: train loss: 0.12037283898192513, test loss: 0.26234540397613676\n",
      "epoch 9493: train loss: 0.12036952524028399, test loss: 0.2623455554220932\n",
      "epoch 9494: train loss: 0.12036621198378691, test loss: 0.2623457070172326\n",
      "epoch 9495: train loss: 0.12036289921230761, test loss: 0.26234585876151223\n",
      "epoch 9496: train loss: 0.12035958692571988, test loss: 0.2623460106548895\n",
      "epoch 9497: train loss: 0.12035627512389745, test loss: 0.26234616269732186\n",
      "epoch 9498: train loss: 0.12035296380671426, test loss: 0.2623463148887666\n",
      "epoch 9499: train loss: 0.12034965297404415, test loss: 0.2623464672291813\n",
      "epoch 9500: train loss: 0.12034634262576112, test loss: 0.2623466197185234\n",
      "epoch 9501: train loss: 0.12034303276173915, test loss: 0.2623467723567503\n",
      "epoch 9502: train loss: 0.12033972338185223, test loss: 0.2623469251438195\n",
      "epoch 9503: train loss: 0.12033641448597451, test loss: 0.26234707807968854\n",
      "epoch 9504: train loss: 0.12033310607398011, test loss: 0.26234723116431485\n",
      "epoch 9505: train loss: 0.1203297981457432, test loss: 0.2623473843976561\n",
      "epoch 9506: train loss: 0.12032649070113802, test loss: 0.26234753777966957\n",
      "epoch 9507: train loss: 0.12032318374003882, test loss: 0.26234769131031305\n",
      "epoch 9508: train loss: 0.12031987726231996, test loss: 0.26234784498954405\n",
      "epoch 9509: train loss: 0.12031657126785576, test loss: 0.2623479988173202\n",
      "epoch 9510: train loss: 0.12031326575652067, test loss: 0.262348152793599\n",
      "epoch 9511: train loss: 0.12030996072818911, test loss: 0.26234830691833805\n",
      "epoch 9512: train loss: 0.12030665618273567, test loss: 0.2623484611914952\n",
      "epoch 9513: train loss: 0.12030335212003479, test loss: 0.2623486156130279\n",
      "epoch 9514: train loss: 0.12030004853996111, test loss: 0.26234877018289393\n",
      "epoch 9515: train loss: 0.12029674544238933, test loss: 0.26234892490105083\n",
      "epoch 9516: train loss: 0.12029344282719406, test loss: 0.2623490797674564\n",
      "epoch 9517: train loss: 0.12029014069425009, test loss: 0.2623492347820684\n",
      "epoch 9518: train loss: 0.12028683904343217, test loss: 0.2623493899448444\n",
      "epoch 9519: train loss: 0.12028353787461514, test loss: 0.26234954525574233\n",
      "epoch 9520: train loss: 0.1202802371876739, test loss: 0.2623497007147198\n",
      "epoch 9521: train loss: 0.12027693698248335, test loss: 0.26234985632173463\n",
      "epoch 9522: train loss: 0.12027363725891842, test loss: 0.26235001207674463\n",
      "epoch 9523: train loss: 0.1202703380168542, test loss: 0.26235016797970767\n",
      "epoch 9524: train loss: 0.12026703925616569, test loss: 0.2623503240305814\n",
      "epoch 9525: train loss: 0.12026374097672803, test loss: 0.2623504802293237\n",
      "epoch 9526: train loss: 0.12026044317841635, test loss: 0.2623506365758926\n",
      "epoch 9527: train loss: 0.12025714586110585, test loss: 0.2623507930702457\n",
      "epoch 9528: train loss: 0.12025384902467177, test loss: 0.2623509497123411\n",
      "epoch 9529: train loss: 0.12025055266898942, test loss: 0.2623511065021365\n",
      "epoch 9530: train loss: 0.12024725679393412, test loss: 0.26235126343959\n",
      "epoch 9531: train loss: 0.12024396139938126, test loss: 0.26235142052465943\n",
      "epoch 9532: train loss: 0.12024066648520625, test loss: 0.2623515777573027\n",
      "epoch 9533: train loss: 0.12023737205128457, test loss: 0.26235173513747784\n",
      "epoch 9534: train loss: 0.12023407809749175, test loss: 0.2623518926651428\n",
      "epoch 9535: train loss: 0.12023078462370336, test loss: 0.26235205034025555\n",
      "epoch 9536: train loss: 0.12022749162979497, test loss: 0.2623522081627741\n",
      "epoch 9537: train loss: 0.12022419911564228, test loss: 0.2623523661326565\n",
      "epoch 9538: train loss: 0.12022090708112096, test loss: 0.26235252424986066\n",
      "epoch 9539: train loss: 0.12021761552610678, test loss: 0.26235268251434485\n",
      "epoch 9540: train loss: 0.12021432445047553, test loss: 0.2623528409260669\n",
      "epoch 9541: train loss: 0.12021103385410305, test loss: 0.26235299948498503\n",
      "epoch 9542: train loss: 0.1202077437368652, test loss: 0.2623531581910572\n",
      "epoch 9543: train loss: 0.12020445409863795, test loss: 0.26235331704424175\n",
      "epoch 9544: train loss: 0.12020116493929724, test loss: 0.2623534760444966\n",
      "epoch 9545: train loss: 0.1201978762587191, test loss: 0.26235363519177995\n",
      "epoch 9546: train loss: 0.1201945880567796, test loss: 0.26235379448605\n",
      "epoch 9547: train loss: 0.12019130033335486, test loss: 0.26235395392726485\n",
      "epoch 9548: train loss: 0.12018801308832103, test loss: 0.26235411351538274\n",
      "epoch 9549: train loss: 0.12018472632155433, test loss: 0.26235427325036176\n",
      "epoch 9550: train loss: 0.12018144003293099, test loss: 0.26235443313216034\n",
      "epoch 9551: train loss: 0.12017815422232729, test loss: 0.2623545931607364\n",
      "epoch 9552: train loss: 0.1201748688896196, test loss: 0.26235475333604846\n",
      "epoch 9553: train loss: 0.12017158403468431, test loss: 0.2623549136580547\n",
      "epoch 9554: train loss: 0.12016829965739782, test loss: 0.2623550741267134\n",
      "epoch 9555: train loss: 0.12016501575763662, test loss: 0.26235523474198275\n",
      "epoch 9556: train loss: 0.12016173233527723, test loss: 0.26235539550382114\n",
      "epoch 9557: train loss: 0.12015844939019622, test loss: 0.2623555564121869\n",
      "epoch 9558: train loss: 0.1201551669222702, test loss: 0.26235571746703834\n",
      "epoch 9559: train loss: 0.12015188493137582, test loss: 0.2623558786683338\n",
      "epoch 9560: train loss: 0.12014860341738981, test loss: 0.2623560400160317\n",
      "epoch 9561: train loss: 0.1201453223801889, test loss: 0.26235620151009037\n",
      "epoch 9562: train loss: 0.12014204181964985, test loss: 0.2623563631504682\n",
      "epoch 9563: train loss: 0.12013876173564954, test loss: 0.2623565249371236\n",
      "epoch 9564: train loss: 0.12013548212806484, test loss: 0.26235668687001507\n",
      "epoch 9565: train loss: 0.12013220299677269, test loss: 0.2623568489491009\n",
      "epoch 9566: train loss: 0.12012892434165004, test loss: 0.26235701117433974\n",
      "epoch 9567: train loss: 0.12012564616257393, test loss: 0.2623571735456899\n",
      "epoch 9568: train loss: 0.1201223684594214, test loss: 0.2623573360631099\n",
      "epoch 9569: train loss: 0.12011909123206958, test loss: 0.2623574987265583\n",
      "epoch 9570: train loss: 0.12011581448039564, test loss: 0.26235766153599355\n",
      "epoch 9571: train loss: 0.12011253820427673, test loss: 0.2623578244913743\n",
      "epoch 9572: train loss: 0.12010926240359013, test loss: 0.2623579875926589\n",
      "epoch 9573: train loss: 0.12010598707821311, test loss: 0.26235815083980607\n",
      "epoch 9574: train loss: 0.12010271222802302, test loss: 0.2623583142327743\n",
      "epoch 9575: train loss: 0.12009943785289721, test loss: 0.2623584777715222\n",
      "epoch 9576: train loss: 0.12009616395271315, test loss: 0.26235864145600835\n",
      "epoch 9577: train loss: 0.12009289052734828, test loss: 0.2623588052861915\n",
      "epoch 9578: train loss: 0.12008961757668013, test loss: 0.2623589692620301\n",
      "epoch 9579: train loss: 0.12008634510058623, test loss: 0.2623591333834829\n",
      "epoch 9580: train loss: 0.1200830730989442, test loss: 0.2623592976505086\n",
      "epoch 9581: train loss: 0.12007980157163167, test loss: 0.2623594620630658\n",
      "epoch 9582: train loss: 0.12007653051852638, test loss: 0.2623596266211132\n",
      "epoch 9583: train loss: 0.12007325993950602, test loss: 0.26235979132460957\n",
      "epoch 9584: train loss: 0.12006998983444837, test loss: 0.26235995617351354\n",
      "epoch 9585: train loss: 0.1200667202032313, test loss: 0.26236012116778396\n",
      "epoch 9586: train loss: 0.12006345104573263, test loss: 0.26236028630737945\n",
      "epoch 9587: train loss: 0.12006018236183032, test loss: 0.26236045159225896\n",
      "epoch 9588: train loss: 0.12005691415140231, test loss: 0.2623606170223811\n",
      "epoch 9589: train loss: 0.1200536464143266, test loss: 0.2623607825977048\n",
      "epoch 9590: train loss: 0.12005037915048128, test loss: 0.26236094831818874\n",
      "epoch 9591: train loss: 0.12004711235974437, test loss: 0.2623611141837918\n",
      "epoch 9592: train loss: 0.12004384604199408, test loss: 0.2623612801944729\n",
      "epoch 9593: train loss: 0.12004058019710856, test loss: 0.2623614463501908\n",
      "epoch 9594: train loss: 0.12003731482496605, test loss: 0.26236161265090446\n",
      "epoch 9595: train loss: 0.12003404992544482, test loss: 0.2623617790965727\n",
      "epoch 9596: train loss: 0.12003078549842314, test loss: 0.26236194568715443\n",
      "epoch 9597: train loss: 0.12002752154377948, test loss: 0.2623621124226085\n",
      "epoch 9598: train loss: 0.12002425806139215, test loss: 0.26236227930289396\n",
      "epoch 9599: train loss: 0.12002099505113964, test loss: 0.26236244632796973\n",
      "epoch 9600: train loss: 0.12001773251290045, test loss: 0.26236261349779466\n",
      "epoch 9601: train loss: 0.1200144704465531, test loss: 0.26236278081232794\n",
      "epoch 9602: train loss: 0.12001120885197616, test loss: 0.2623629482715284\n",
      "epoch 9603: train loss: 0.12000794772904833, test loss: 0.26236311587535505\n",
      "epoch 9604: train loss: 0.12000468707764819, test loss: 0.262363283623767\n",
      "epoch 9605: train loss: 0.1200014268976545, test loss: 0.2623634515167231\n",
      "epoch 9606: train loss: 0.11999816718894603, test loss: 0.2623636195541825\n",
      "epoch 9607: train loss: 0.1199949079514016, test loss: 0.2623637877361044\n",
      "epoch 9608: train loss: 0.11999164918489999, test loss: 0.2623639560624478\n",
      "epoch 9609: train loss: 0.11998839088932019, test loss: 0.26236412453317165\n",
      "epoch 9610: train loss: 0.11998513306454103, test loss: 0.2623642931482352\n",
      "epoch 9611: train loss: 0.1199818757104416, test loss: 0.2623644619075975\n",
      "epoch 9612: train loss: 0.11997861882690083, test loss: 0.2623646308112177\n",
      "epoch 9613: train loss: 0.11997536241379787, test loss: 0.262364799859055\n",
      "epoch 9614: train loss: 0.1199721064710118, test loss: 0.2623649690510685\n",
      "epoch 9615: train loss: 0.11996885099842178, test loss: 0.2623651383872175\n",
      "epoch 9616: train loss: 0.119965595995907, test loss: 0.26236530786746104\n",
      "epoch 9617: train loss: 0.11996234146334672, test loss: 0.2623654774917584\n",
      "epoch 9618: train loss: 0.11995908740062022, test loss: 0.26236564726006883\n",
      "epoch 9619: train loss: 0.11995583380760685, test loss: 0.2623658171723515\n",
      "epoch 9620: train loss: 0.119952580684186, test loss: 0.2623659872285658\n",
      "epoch 9621: train loss: 0.11994932803023704, test loss: 0.2623661574286708\n",
      "epoch 9622: train loss: 0.11994607584563952, test loss: 0.262366327772626\n",
      "epoch 9623: train loss: 0.11994282413027287, test loss: 0.2623664982603905\n",
      "epoch 9624: train loss: 0.11993957288401667, test loss: 0.2623666688919236\n",
      "epoch 9625: train loss: 0.11993632210675056, test loss: 0.26236683966718494\n",
      "epoch 9626: train loss: 0.11993307179835412, test loss: 0.2623670105861335\n",
      "epoch 9627: train loss: 0.11992982195870708, test loss: 0.26236718164872874\n",
      "epoch 9628: train loss: 0.11992657258768916, test loss: 0.2623673528549302\n",
      "epoch 9629: train loss: 0.1199233236851801, test loss: 0.262367524204697\n",
      "epoch 9630: train loss: 0.11992007525105976, test loss: 0.2623676956979888\n",
      "epoch 9631: train loss: 0.11991682728520797, test loss: 0.2623678673347648\n",
      "epoch 9632: train loss: 0.11991357978750465, test loss: 0.2623680391149845\n",
      "epoch 9633: train loss: 0.11991033275782978, test loss: 0.26236821103860736\n",
      "epoch 9634: train loss: 0.1199070861960633, test loss: 0.2623683831055929\n",
      "epoch 9635: train loss: 0.11990384010208528, test loss: 0.2623685553159005\n",
      "epoch 9636: train loss: 0.11990059447577579, test loss: 0.26236872766948965\n",
      "epoch 9637: train loss: 0.11989734931701491, test loss: 0.2623689001663198\n",
      "epoch 9638: train loss: 0.11989410462568291, test loss: 0.2623690728063506\n",
      "epoch 9639: train loss: 0.1198908604016599, test loss: 0.26236924558954144\n",
      "epoch 9640: train loss: 0.11988761664482617, test loss: 0.26236941851585205\n",
      "epoch 9641: train loss: 0.11988437335506204, test loss: 0.2623695915852418\n",
      "epoch 9642: train loss: 0.11988113053224782, test loss: 0.2623697647976704\n",
      "epoch 9643: train loss: 0.11987788817626388, test loss: 0.2623699381530972\n",
      "epoch 9644: train loss: 0.11987464628699071, test loss: 0.26237011165148216\n",
      "epoch 9645: train loss: 0.11987140486430875, test loss: 0.2623702852927846\n",
      "epoch 9646: train loss: 0.1198681639080985, test loss: 0.26237045907696427\n",
      "epoch 9647: train loss: 0.1198649234182405, test loss: 0.26237063300398084\n",
      "epoch 9648: train loss: 0.11986168339461539, test loss: 0.262370807073794\n",
      "epoch 9649: train loss: 0.11985844383710383, test loss: 0.26237098128636316\n",
      "epoch 9650: train loss: 0.11985520474558647, test loss: 0.26237115564164837\n",
      "epoch 9651: train loss: 0.11985196611994407, test loss: 0.26237133013960906\n",
      "epoch 9652: train loss: 0.11984872796005738, test loss: 0.2623715047802051\n",
      "epoch 9653: train loss: 0.11984549026580722, test loss: 0.2623716795633962\n",
      "epoch 9654: train loss: 0.11984225303707449, test loss: 0.262371854489142\n",
      "epoch 9655: train loss: 0.11983901627374006, test loss: 0.26237202955740235\n",
      "epoch 9656: train loss: 0.11983577997568487, test loss: 0.26237220476813705\n",
      "epoch 9657: train loss: 0.11983254414278993, test loss: 0.2623723801213058\n",
      "epoch 9658: train loss: 0.11982930877493629, test loss: 0.2623725556168685\n",
      "epoch 9659: train loss: 0.11982607387200499, test loss: 0.2623727312547849\n",
      "epoch 9660: train loss: 0.11982283943387718, test loss: 0.26237290703501487\n",
      "epoch 9661: train loss: 0.11981960546043402, test loss: 0.2623730829575181\n",
      "epoch 9662: train loss: 0.1198163719515567, test loss: 0.26237325902225467\n",
      "epoch 9663: train loss: 0.11981313890712648, test loss: 0.2623734352291843\n",
      "epoch 9664: train loss: 0.11980990632702468, test loss: 0.2623736115782671\n",
      "epoch 9665: train loss: 0.1198066742111326, test loss: 0.26237378806946265\n",
      "epoch 9666: train loss: 0.11980344255933165, test loss: 0.2623739647027311\n",
      "epoch 9667: train loss: 0.1198002113715032, test loss: 0.2623741414780323\n",
      "epoch 9668: train loss: 0.11979698064752878, test loss: 0.2623743183953262\n",
      "epoch 9669: train loss: 0.11979375038728987, test loss: 0.26237449545457275\n",
      "epoch 9670: train loss: 0.11979052059066801, test loss: 0.26237467265573194\n",
      "epoch 9671: train loss: 0.11978729125754482, test loss: 0.2623748499987638\n",
      "epoch 9672: train loss: 0.1197840623878019, test loss: 0.26237502748362823\n",
      "epoch 9673: train loss: 0.119780833981321, test loss: 0.26237520511028534\n",
      "epoch 9674: train loss: 0.11977760603798374, test loss: 0.2623753828786951\n",
      "epoch 9675: train loss: 0.11977437855767198, test loss: 0.2623755607888176\n",
      "epoch 9676: train loss: 0.11977115154026746, test loss: 0.26237573884061277\n",
      "epoch 9677: train loss: 0.1197679249856521, test loss: 0.26237591703404095\n",
      "epoch 9678: train loss: 0.11976469889370774, test loss: 0.2623760953690619\n",
      "epoch 9679: train loss: 0.11976147326431637, test loss: 0.262376273845636\n",
      "epoch 9680: train loss: 0.11975824809735991, test loss: 0.26237645246372315\n",
      "epoch 9681: train loss: 0.11975502339272039, test loss: 0.2623766312232837\n",
      "epoch 9682: train loss: 0.11975179915027992, test loss: 0.26237681012427766\n",
      "epoch 9683: train loss: 0.11974857536992058, test loss: 0.26237698916666513\n",
      "epoch 9684: train loss: 0.11974535205152449, test loss: 0.2623771683504064\n",
      "epoch 9685: train loss: 0.11974212919497393, test loss: 0.26237734767546156\n",
      "epoch 9686: train loss: 0.11973890680015106, test loss: 0.26237752714179086\n",
      "epoch 9687: train loss: 0.1197356848669382, test loss: 0.26237770674935457\n",
      "epoch 9688: train loss: 0.11973246339521762, test loss: 0.2623778864981128\n",
      "epoch 9689: train loss: 0.11972924238487175, test loss: 0.2623780663880259\n",
      "epoch 9690: train loss: 0.11972602183578294, test loss: 0.26237824641905405\n",
      "epoch 9691: train loss: 0.11972280174783367, test loss: 0.26237842659115757\n",
      "epoch 9692: train loss: 0.11971958212090641, test loss: 0.2623786069042967\n",
      "epoch 9693: train loss: 0.11971636295488373, test loss: 0.2623787873584318\n",
      "epoch 9694: train loss: 0.1197131442496482, test loss: 0.26237896795352306\n",
      "epoch 9695: train loss: 0.1197099260050824, test loss: 0.2623791486895309\n",
      "epoch 9696: train loss: 0.119706708221069, test loss: 0.26237932956641574\n",
      "epoch 9697: train loss: 0.11970349089749074, test loss: 0.2623795105841378\n",
      "epoch 9698: train loss: 0.11970027403423038, test loss: 0.2623796917426575\n",
      "epoch 9699: train loss: 0.11969705763117063, test loss: 0.2623798730419352\n",
      "epoch 9700: train loss: 0.11969384168819436, test loss: 0.2623800544819313\n",
      "epoch 9701: train loss: 0.11969062620518449, test loss: 0.26238023606260624\n",
      "epoch 9702: train loss: 0.11968741118202385, test loss: 0.26238041778392046\n",
      "epoch 9703: train loss: 0.11968419661859546, test loss: 0.2623805996458344\n",
      "epoch 9704: train loss: 0.11968098251478232, test loss: 0.26238078164830836\n",
      "epoch 9705: train loss: 0.11967776887046742, test loss: 0.26238096379130305\n",
      "epoch 9706: train loss: 0.11967455568553392, test loss: 0.26238114607477875\n",
      "epoch 9707: train loss: 0.11967134295986487, test loss: 0.26238132849869605\n",
      "epoch 9708: train loss: 0.1196681306933435, test loss: 0.26238151106301544\n",
      "epoch 9709: train loss: 0.119664918885853, test loss: 0.2623816937676975\n",
      "epoch 9710: train loss: 0.1196617075372766, test loss: 0.2623818766127026\n",
      "epoch 9711: train loss: 0.11965849664749761, test loss: 0.26238205959799143\n",
      "epoch 9712: train loss: 0.11965528621639938, test loss: 0.2623822427235247\n",
      "epoch 9713: train loss: 0.1196520762438653, test loss: 0.26238242598926254\n",
      "epoch 9714: train loss: 0.11964886672977876, test loss: 0.262382609395166\n",
      "epoch 9715: train loss: 0.11964565767402326, test loss: 0.26238279294119543\n",
      "epoch 9716: train loss: 0.11964244907648226, test loss: 0.2623829766273116\n",
      "epoch 9717: train loss: 0.11963924093703934, test loss: 0.262383160453475\n",
      "epoch 9718: train loss: 0.11963603325557809, test loss: 0.2623833444196462\n",
      "epoch 9719: train loss: 0.11963282603198212, test loss: 0.2623835285257862\n",
      "epoch 9720: train loss: 0.11962961926613515, test loss: 0.2623837127718554\n",
      "epoch 9721: train loss: 0.11962641295792081, test loss: 0.2623838971578147\n",
      "epoch 9722: train loss: 0.11962320710722298, test loss: 0.2623840816836246\n",
      "epoch 9723: train loss: 0.11962000171392534, test loss: 0.2623842663492459\n",
      "epoch 9724: train loss: 0.11961679677791182, test loss: 0.26238445115463915\n",
      "epoch 9725: train loss: 0.11961359229906623, test loss: 0.2623846360997654\n",
      "epoch 9726: train loss: 0.11961038827727254, test loss: 0.26238482118458534\n",
      "epoch 9727: train loss: 0.11960718471241474, test loss: 0.2623850064090596\n",
      "epoch 9728: train loss: 0.11960398160437677, test loss: 0.2623851917731491\n",
      "epoch 9729: train loss: 0.11960077895304277, test loss: 0.26238537727681455\n",
      "epoch 9730: train loss: 0.11959757675829674, test loss: 0.26238556292001675\n",
      "epoch 9731: train loss: 0.11959437502002286, test loss: 0.26238574870271664\n",
      "epoch 9732: train loss: 0.1195911737381053, test loss: 0.262385934624875\n",
      "epoch 9733: train loss: 0.11958797291242827, test loss: 0.2623861206864526\n",
      "epoch 9734: train loss: 0.11958477254287607, test loss: 0.2623863068874105\n",
      "epoch 9735: train loss: 0.11958157262933294, test loss: 0.2623864932277094\n",
      "epoch 9736: train loss: 0.11957837317168327, test loss: 0.2623866797073103\n",
      "epoch 9737: train loss: 0.11957517416981141, test loss: 0.2623868663261741\n",
      "epoch 9738: train loss: 0.1195719756236018, test loss: 0.26238705308426163\n",
      "epoch 9739: train loss: 0.11956877753293893, test loss: 0.26238723998153396\n",
      "epoch 9740: train loss: 0.11956557989770726, test loss: 0.262387427017952\n",
      "epoch 9741: train loss: 0.1195623827177914, test loss: 0.26238761419347667\n",
      "epoch 9742: train loss: 0.11955918599307588, test loss: 0.262387801508069\n",
      "epoch 9743: train loss: 0.11955598972344539, test loss: 0.26238798896168997\n",
      "epoch 9744: train loss: 0.11955279390878458, test loss: 0.2623881765543005\n",
      "epoch 9745: train loss: 0.11954959854897815, test loss: 0.2623883642858618\n",
      "epoch 9746: train loss: 0.11954640364391089, test loss: 0.2623885521563347\n",
      "epoch 9747: train loss: 0.11954320919346756, test loss: 0.2623887401656804\n",
      "epoch 9748: train loss: 0.11954001519753303, test loss: 0.2623889283138599\n",
      "epoch 9749: train loss: 0.1195368216559922, test loss: 0.2623891166008343\n",
      "epoch 9750: train loss: 0.11953362856872993, test loss: 0.2623893050265646\n",
      "epoch 9751: train loss: 0.11953043593563123, test loss: 0.26238949359101205\n",
      "epoch 9752: train loss: 0.11952724375658114, test loss: 0.26238968229413756\n",
      "epoch 9753: train loss: 0.11952405203146466, test loss: 0.2623898711359025\n",
      "epoch 9754: train loss: 0.11952086076016688, test loss: 0.26239006011626786\n",
      "epoch 9755: train loss: 0.1195176699425729, test loss: 0.26239024923519483\n",
      "epoch 9756: train loss: 0.11951447957856796, test loss: 0.2623904384926446\n",
      "epoch 9757: train loss: 0.11951128966803726, test loss: 0.2623906278885783\n",
      "epoch 9758: train loss: 0.11950810021086601, test loss: 0.26239081742295717\n",
      "epoch 9759: train loss: 0.11950491120693954, test loss: 0.2623910070957425\n",
      "epoch 9760: train loss: 0.11950172265614317, test loss: 0.2623911969068953\n",
      "epoch 9761: train loss: 0.11949853455836229, test loss: 0.26239138685637703\n",
      "epoch 9762: train loss: 0.1194953469134823, test loss: 0.26239157694414883\n",
      "epoch 9763: train loss: 0.1194921597213887, test loss: 0.26239176717017204\n",
      "epoch 9764: train loss: 0.11948897298196695, test loss: 0.2623919575344078\n",
      "epoch 9765: train loss: 0.1194857866951026, test loss: 0.26239214803681754\n",
      "epoch 9766: train loss: 0.11948260086068124, test loss: 0.2623923386773626\n",
      "epoch 9767: train loss: 0.1194794154785885, test loss: 0.26239252945600405\n",
      "epoch 9768: train loss: 0.11947623054871002, test loss: 0.26239272037270356\n",
      "epoch 9769: train loss: 0.11947304607093154, test loss: 0.26239291142742216\n",
      "epoch 9770: train loss: 0.11946986204513876, test loss: 0.26239310262012144\n",
      "epoch 9771: train loss: 0.11946667847121756, test loss: 0.2623932939507628\n",
      "epoch 9772: train loss: 0.11946349534905366, test loss: 0.26239348541930735\n",
      "epoch 9773: train loss: 0.11946031267853296, test loss: 0.2623936770257167\n",
      "epoch 9774: train loss: 0.11945713045954143, test loss: 0.26239386876995224\n",
      "epoch 9775: train loss: 0.11945394869196496, test loss: 0.2623940606519754\n",
      "epoch 9776: train loss: 0.11945076737568958, test loss: 0.26239425267174765\n",
      "epoch 9777: train loss: 0.1194475865106013, test loss: 0.26239444482923036\n",
      "epoch 9778: train loss: 0.11944440609658619, test loss: 0.262394637124385\n",
      "epoch 9779: train loss: 0.11944122613353038, test loss: 0.2623948295571732\n",
      "epoch 9780: train loss: 0.11943804662132002, test loss: 0.2623950221275563\n",
      "epoch 9781: train loss: 0.11943486755984128, test loss: 0.2623952148354959\n",
      "epoch 9782: train loss: 0.11943168894898047, test loss: 0.2623954076809534\n",
      "epoch 9783: train loss: 0.1194285107886238, test loss: 0.26239560066389045\n",
      "epoch 9784: train loss: 0.11942533307865762, test loss: 0.2623957937842686\n",
      "epoch 9785: train loss: 0.11942215581896828, test loss: 0.2623959870420494\n",
      "epoch 9786: train loss: 0.11941897900944218, test loss: 0.26239618043719437\n",
      "epoch 9787: train loss: 0.11941580264996576, test loss: 0.2623963739696651\n",
      "epoch 9788: train loss: 0.11941262674042552, test loss: 0.26239656763942326\n",
      "epoch 9789: train loss: 0.11940945128070794, test loss: 0.2623967614464305\n",
      "epoch 9790: train loss: 0.11940627627069964, test loss: 0.26239695539064833\n",
      "epoch 9791: train loss: 0.11940310171028717, test loss: 0.26239714947203846\n",
      "epoch 9792: train loss: 0.11939992759935719, test loss: 0.26239734369056256\n",
      "epoch 9793: train loss: 0.11939675393779642, test loss: 0.26239753804618227\n",
      "epoch 9794: train loss: 0.11939358072549154, test loss: 0.2623977325388593\n",
      "epoch 9795: train loss: 0.11939040796232932, test loss: 0.2623979271685553\n",
      "epoch 9796: train loss: 0.1193872356481966, test loss: 0.262398121935232\n",
      "epoch 9797: train loss: 0.1193840637829802, test loss: 0.26239831683885123\n",
      "epoch 9798: train loss: 0.11938089236656702, test loss: 0.2623985118793745\n",
      "epoch 9799: train loss: 0.11937772139884395, test loss: 0.2623987070567638\n",
      "epoch 9800: train loss: 0.11937455087969802, test loss: 0.26239890237098074\n",
      "epoch 9801: train loss: 0.11937138080901619, test loss: 0.26239909782198717\n",
      "epoch 9802: train loss: 0.1193682111866855, test loss: 0.2623992934097448\n",
      "epoch 9803: train loss: 0.11936504201259311, test loss: 0.2623994891342155\n",
      "epoch 9804: train loss: 0.11936187328662609, test loss: 0.26239968499536115\n",
      "epoch 9805: train loss: 0.1193587050086716, test loss: 0.26239988099314343\n",
      "epoch 9806: train loss: 0.11935553717861691, test loss: 0.2624000771275243\n",
      "epoch 9807: train loss: 0.11935236979634922, test loss: 0.26240027339846556\n",
      "epoch 9808: train loss: 0.11934920286175581, test loss: 0.2624004698059291\n",
      "epoch 9809: train loss: 0.11934603637472405, test loss: 0.2624006663498768\n",
      "epoch 9810: train loss: 0.1193428703351413, test loss: 0.26240086303027055\n",
      "epoch 9811: train loss: 0.11933970474289496, test loss: 0.26240105984707235\n",
      "epoch 9812: train loss: 0.11933653959787247, test loss: 0.26240125680024395\n",
      "epoch 9813: train loss: 0.11933337489996136, test loss: 0.2624014538897474\n",
      "epoch 9814: train loss: 0.11933021064904914, test loss: 0.26240165111554464\n",
      "epoch 9815: train loss: 0.11932704684502339, test loss: 0.2624018484775976\n",
      "epoch 9816: train loss: 0.1193238834877717, test loss: 0.2624020459758683\n",
      "epoch 9817: train loss: 0.11932072057718177, test loss: 0.2624022436103188\n",
      "epoch 9818: train loss: 0.11931755811314122, test loss: 0.26240244138091084\n",
      "epoch 9819: train loss: 0.11931439609553784, test loss: 0.2624026392876067\n",
      "epoch 9820: train loss: 0.1193112345242594, test loss: 0.26240283733036834\n",
      "epoch 9821: train loss: 0.11930807339919365, test loss: 0.2624030355091578\n",
      "epoch 9822: train loss: 0.11930491272022853, test loss: 0.2624032338239371\n",
      "epoch 9823: train loss: 0.11930175248725189, test loss: 0.2624034322746682\n",
      "epoch 9824: train loss: 0.11929859270015167, test loss: 0.26240363086131346\n",
      "epoch 9825: train loss: 0.11929543335881582, test loss: 0.2624038295838348\n",
      "epoch 9826: train loss: 0.11929227446313238, test loss: 0.2624040284421943\n",
      "epoch 9827: train loss: 0.11928911601298937, test loss: 0.2624042274363541\n",
      "epoch 9828: train loss: 0.11928595800827495, test loss: 0.2624044265662765\n",
      "epoch 9829: train loss: 0.11928280044887718, test loss: 0.2624046258319234\n",
      "epoch 9830: train loss: 0.11927964333468424, test loss: 0.26240482523325714\n",
      "epoch 9831: train loss: 0.11927648666558437, test loss: 0.26240502477023986\n",
      "epoch 9832: train loss: 0.11927333044146582, test loss: 0.26240522444283365\n",
      "epoch 9833: train loss: 0.11927017466221686, test loss: 0.26240542425100083\n",
      "epoch 9834: train loss: 0.11926701932772583, test loss: 0.2624056241947036\n",
      "epoch 9835: train loss: 0.11926386443788112, test loss: 0.262405824273904\n",
      "epoch 9836: train loss: 0.11926070999257112, test loss: 0.2624060244885646\n",
      "epoch 9837: train loss: 0.11925755599168428, test loss: 0.2624062248386474\n",
      "epoch 9838: train loss: 0.11925440243510912, test loss: 0.26240642532411473\n",
      "epoch 9839: train loss: 0.11925124932273412, test loss: 0.26240662594492886\n",
      "epoch 9840: train loss: 0.11924809665444787, test loss: 0.26240682670105214\n",
      "epoch 9841: train loss: 0.11924494443013903, test loss: 0.26240702759244683\n",
      "epoch 9842: train loss: 0.11924179264969614, test loss: 0.26240722861907523\n",
      "epoch 9843: train loss: 0.119238641313008, test loss: 0.26240742978089976\n",
      "epoch 9844: train loss: 0.11923549041996326, test loss: 0.2624076310778827\n",
      "epoch 9845: train loss: 0.1192323399704507, test loss: 0.2624078325099864\n",
      "epoch 9846: train loss: 0.11922918996435918, test loss: 0.2624080340771732\n",
      "epoch 9847: train loss: 0.11922604040157751, test loss: 0.2624082357794056\n",
      "epoch 9848: train loss: 0.11922289128199455, test loss: 0.26240843761664584\n",
      "epoch 9849: train loss: 0.11921974260549927, test loss: 0.2624086395888565\n",
      "epoch 9850: train loss: 0.11921659437198061, test loss: 0.2624088416959999\n",
      "epoch 9851: train loss: 0.1192134465813276, test loss: 0.26240904393803843\n",
      "epoch 9852: train loss: 0.11921029923342924, test loss: 0.2624092463149346\n",
      "epoch 9853: train loss: 0.11920715232817468, test loss: 0.26240944882665096\n",
      "epoch 9854: train loss: 0.11920400586545296, test loss: 0.2624096514731499\n",
      "epoch 9855: train loss: 0.11920085984515331, test loss: 0.2624098542543939\n",
      "epoch 9856: train loss: 0.11919771426716491, test loss: 0.2624100571703454\n",
      "epoch 9857: train loss: 0.119194569131377, test loss: 0.26241026022096703\n",
      "epoch 9858: train loss: 0.11919142443767886, test loss: 0.2624104634062212\n",
      "epoch 9859: train loss: 0.11918828018595978, test loss: 0.2624106667260706\n",
      "epoch 9860: train loss: 0.11918513637610918, test loss: 0.2624108701804777\n",
      "epoch 9861: train loss: 0.11918199300801642, test loss: 0.26241107376940503\n",
      "epoch 9862: train loss: 0.11917885008157093, test loss: 0.2624112774928152\n",
      "epoch 9863: train loss: 0.11917570759666223, test loss: 0.2624114813506708\n",
      "epoch 9864: train loss: 0.1191725655531798, test loss: 0.2624116853429345\n",
      "epoch 9865: train loss: 0.1191694239510132, test loss: 0.26241188946956884\n",
      "epoch 9866: train loss: 0.11916628279005202, test loss: 0.2624120937305365\n",
      "epoch 9867: train loss: 0.11916314207018593, test loss: 0.2624122981258\n",
      "epoch 9868: train loss: 0.11916000179130454, test loss: 0.2624125026553222\n",
      "epoch 9869: train loss: 0.11915686195329761, test loss: 0.26241270731906563\n",
      "epoch 9870: train loss: 0.1191537225560549, test loss: 0.262412912116993\n",
      "epoch 9871: train loss: 0.11915058359946616, test loss: 0.262413117049067\n",
      "epoch 9872: train loss: 0.11914744508342125, test loss: 0.26241332211525037\n",
      "epoch 9873: train loss: 0.11914430700781001, test loss: 0.26241352731550577\n",
      "epoch 9874: train loss: 0.11914116937252237, test loss: 0.262413732649796\n",
      "epoch 9875: train loss: 0.11913803217744828, test loss: 0.2624139381180838\n",
      "epoch 9876: train loss: 0.11913489542247774, test loss: 0.26241414372033184\n",
      "epoch 9877: train loss: 0.11913175910750072, test loss: 0.262414349456503\n",
      "epoch 9878: train loss: 0.11912862323240732, test loss: 0.26241455532656\n",
      "epoch 9879: train loss: 0.11912548779708766, test loss: 0.2624147613304657\n",
      "epoch 9880: train loss: 0.11912235280143184, test loss: 0.2624149674681828\n",
      "epoch 9881: train loss: 0.11911921824533006, test loss: 0.2624151737396743\n",
      "epoch 9882: train loss: 0.11911608412867256, test loss: 0.2624153801449029\n",
      "epoch 9883: train loss: 0.11911295045134958, test loss: 0.2624155866838315\n",
      "epoch 9884: train loss: 0.11910981721325141, test loss: 0.2624157933564229\n",
      "epoch 9885: train loss: 0.11910668441426842, test loss: 0.2624160001626401\n",
      "epoch 9886: train loss: 0.11910355205429092, test loss: 0.26241620710244584\n",
      "epoch 9887: train loss: 0.11910042013320937, test loss: 0.26241641417580314\n",
      "epoch 9888: train loss: 0.11909728865091423, test loss: 0.26241662138267485\n",
      "epoch 9889: train loss: 0.11909415760729597, test loss: 0.26241682872302385\n",
      "epoch 9890: train loss: 0.11909102700224512, test loss: 0.2624170361968132\n",
      "epoch 9891: train loss: 0.11908789683565227, test loss: 0.2624172438040058\n",
      "epoch 9892: train loss: 0.11908476710740802, test loss: 0.2624174515445646\n",
      "epoch 9893: train loss: 0.119081637817403, test loss: 0.2624176594184525\n",
      "epoch 9894: train loss: 0.11907850896552788, test loss: 0.2624178674256327\n",
      "epoch 9895: train loss: 0.11907538055167344, test loss: 0.262418075566068\n",
      "epoch 9896: train loss: 0.11907225257573041, test loss: 0.26241828383972154\n",
      "epoch 9897: train loss: 0.11906912503758958, test loss: 0.2624184922465562\n",
      "epoch 9898: train loss: 0.11906599793714183, test loss: 0.26241870078653523\n",
      "epoch 9899: train loss: 0.11906287127427798, test loss: 0.26241890945962154\n",
      "epoch 9900: train loss: 0.119059745048889, test loss: 0.26241911826577824\n",
      "epoch 9901: train loss: 0.1190566192608658, test loss: 0.26241932720496836\n",
      "epoch 9902: train loss: 0.11905349391009942, test loss: 0.2624195362771551\n",
      "epoch 9903: train loss: 0.11905036899648085, test loss: 0.2624197454823014\n",
      "epoch 9904: train loss: 0.1190472445199012, test loss: 0.26241995482037056\n",
      "epoch 9905: train loss: 0.11904412048025156, test loss: 0.26242016429132564\n",
      "epoch 9906: train loss: 0.11904099687742306, test loss: 0.2624203738951297\n",
      "epoch 9907: train loss: 0.11903787371130693, test loss: 0.262420583631746\n",
      "epoch 9908: train loss: 0.11903475098179436, test loss: 0.2624207935011376\n",
      "epoch 9909: train loss: 0.11903162868877665, test loss: 0.2624210035032678\n",
      "epoch 9910: train loss: 0.11902850683214507, test loss: 0.2624212136380998\n",
      "epoch 9911: train loss: 0.11902538541179095, test loss: 0.2624214239055967\n",
      "epoch 9912: train loss: 0.11902226442760569, test loss: 0.26242163430572174\n",
      "epoch 9913: train loss: 0.1190191438794807, test loss: 0.2624218448384383\n",
      "epoch 9914: train loss: 0.11901602376730742, test loss: 0.2624220555037094\n",
      "epoch 9915: train loss: 0.1190129040909774, test loss: 0.26242226630149834\n",
      "epoch 9916: train loss: 0.11900978485038215, test loss: 0.26242247723176854\n",
      "epoch 9917: train loss: 0.1190066660454132, test loss: 0.26242268829448323\n",
      "epoch 9918: train loss: 0.11900354767596218, test loss: 0.2624228994896056\n",
      "epoch 9919: train loss: 0.11900042974192075, test loss: 0.2624231108170991\n",
      "epoch 9920: train loss: 0.11899731224318057, test loss: 0.2624233222769269\n",
      "epoch 9921: train loss: 0.1189941951796334, test loss: 0.2624235338690524\n",
      "epoch 9922: train loss: 0.11899107855117098, test loss: 0.26242374559343895\n",
      "epoch 9923: train loss: 0.1189879623576851, test loss: 0.26242395745005004\n",
      "epoch 9924: train loss: 0.11898484659906762, test loss: 0.2624241694388488\n",
      "epoch 9925: train loss: 0.1189817312752104, test loss: 0.2624243815597987\n",
      "epoch 9926: train loss: 0.11897861638600539, test loss: 0.26242459381286315\n",
      "epoch 9927: train loss: 0.11897550193134449, test loss: 0.2624248061980057\n",
      "epoch 9928: train loss: 0.1189723879111197, test loss: 0.2624250187151895\n",
      "epoch 9929: train loss: 0.11896927432522306, test loss: 0.26242523136437806\n",
      "epoch 9930: train loss: 0.11896616117354665, test loss: 0.262425444145535\n",
      "epoch 9931: train loss: 0.11896304845598257, test loss: 0.26242565705862364\n",
      "epoch 9932: train loss: 0.11895993617242297, test loss: 0.26242587010360746\n",
      "epoch 9933: train loss: 0.11895682432275997, test loss: 0.2624260832804499\n",
      "epoch 9934: train loss: 0.11895371290688589, test loss: 0.2624262965891146\n",
      "epoch 9935: train loss: 0.1189506019246929, test loss: 0.2624265100295649\n",
      "epoch 9936: train loss: 0.11894749137607333, test loss: 0.2624267236017645\n",
      "epoch 9937: train loss: 0.11894438126091952, test loss: 0.2624269373056768\n",
      "epoch 9938: train loss: 0.11894127157912383, test loss: 0.26242715114126525\n",
      "epoch 9939: train loss: 0.11893816233057866, test loss: 0.2624273651084936\n",
      "epoch 9940: train loss: 0.11893505351517647, test loss: 0.26242757920732535\n",
      "epoch 9941: train loss: 0.11893194513280973, test loss: 0.2624277934377242\n",
      "epoch 9942: train loss: 0.118928837183371, test loss: 0.26242800779965353\n",
      "epoch 9943: train loss: 0.11892572966675279, test loss: 0.26242822229307705\n",
      "epoch 9944: train loss: 0.1189226225828477, test loss: 0.2624284369179584\n",
      "epoch 9945: train loss: 0.11891951593154843, test loss: 0.26242865167426127\n",
      "epoch 9946: train loss: 0.11891640971274758, test loss: 0.26242886656194914\n",
      "epoch 9947: train loss: 0.11891330392633791, test loss: 0.2624290815809857\n",
      "epoch 9948: train loss: 0.11891019857221213, test loss: 0.26242929673133475\n",
      "epoch 9949: train loss: 0.11890709365026307, test loss: 0.2624295120129599\n",
      "epoch 9950: train loss: 0.11890398916038353, test loss: 0.2624297274258248\n",
      "epoch 9951: train loss: 0.11890088510246635, test loss: 0.26242994296989325\n",
      "epoch 9952: train loss: 0.11889778147640444, test loss: 0.2624301586451289\n",
      "epoch 9953: train loss: 0.11889467828209078, test loss: 0.26243037445149553\n",
      "epoch 9954: train loss: 0.11889157551941831, test loss: 0.26243059038895683\n",
      "epoch 9955: train loss: 0.11888847318828004, test loss: 0.2624308064574765\n",
      "epoch 9956: train loss: 0.11888537128856903, test loss: 0.26243102265701856\n",
      "epoch 9957: train loss: 0.11888226982017837, test loss: 0.2624312389875465\n",
      "epoch 9958: train loss: 0.11887916878300117, test loss: 0.2624314554490243\n",
      "epoch 9959: train loss: 0.11887606817693061, test loss: 0.2624316720414157\n",
      "epoch 9960: train loss: 0.1188729680018599, test loss: 0.26243188876468454\n",
      "epoch 9961: train loss: 0.11886986825768225, test loss: 0.2624321056187946\n",
      "epoch 9962: train loss: 0.11886676894429096, test loss: 0.26243232260370974\n",
      "epoch 9963: train loss: 0.11886367006157934, test loss: 0.26243253971939384\n",
      "epoch 9964: train loss: 0.11886057160944069, test loss: 0.2624327569658108\n",
      "epoch 9965: train loss: 0.1188574735877685, test loss: 0.26243297434292445\n",
      "epoch 9966: train loss: 0.11885437599645612, test loss: 0.26243319185069874\n",
      "epoch 9967: train loss: 0.11885127883539702, test loss: 0.2624334094890975\n",
      "epoch 9968: train loss: 0.11884818210448471, test loss: 0.2624336272580846\n",
      "epoch 9969: train loss: 0.11884508580361272, test loss: 0.26243384515762413\n",
      "epoch 9970: train loss: 0.11884198993267465, test loss: 0.26243406318768\n",
      "epoch 9971: train loss: 0.1188388944915641, test loss: 0.26243428134821595\n",
      "epoch 9972: train loss: 0.1188357994801747, test loss: 0.26243449963919624\n",
      "epoch 9973: train loss: 0.11883270489840018, test loss: 0.2624347180605847\n",
      "epoch 9974: train loss: 0.11882961074613423, test loss: 0.2624349366123453\n",
      "epoch 9975: train loss: 0.11882651702327061, test loss: 0.2624351552944421\n",
      "epoch 9976: train loss: 0.11882342372970311, test loss: 0.26243537410683915\n",
      "epoch 9977: train loss: 0.11882033086532562, test loss: 0.2624355930495005\n",
      "epoch 9978: train loss: 0.11881723843003197, test loss: 0.26243581212239\n",
      "epoch 9979: train loss: 0.11881414642371607, test loss: 0.26243603132547183\n",
      "epoch 9980: train loss: 0.11881105484627191, test loss: 0.26243625065871007\n",
      "epoch 9981: train loss: 0.11880796369759342, test loss: 0.2624364701220687\n",
      "epoch 9982: train loss: 0.11880487297757465, test loss: 0.26243668971551204\n",
      "epoch 9983: train loss: 0.11880178268610965, test loss: 0.2624369094390039\n",
      "epoch 9984: train loss: 0.11879869282309255, test loss: 0.2624371292925086\n",
      "epoch 9985: train loss: 0.11879560338841742, test loss: 0.2624373492759902\n",
      "epoch 9986: train loss: 0.11879251438197849, test loss: 0.2624375693894128\n",
      "epoch 9987: train loss: 0.11878942580366993, test loss: 0.26243778963274067\n",
      "epoch 9988: train loss: 0.11878633765338602, test loss: 0.26243801000593775\n",
      "epoch 9989: train loss: 0.11878324993102102, test loss: 0.26243823050896853\n",
      "epoch 9990: train loss: 0.11878016263646925, test loss: 0.26243845114179687\n",
      "epoch 9991: train loss: 0.11877707576962507, test loss: 0.2624386719043871\n",
      "epoch 9992: train loss: 0.11877398933038286, test loss: 0.2624388927967035\n",
      "epoch 9993: train loss: 0.11877090331863707, test loss: 0.2624391138187103\n",
      "epoch 9994: train loss: 0.11876781773428217, test loss: 0.26243933497037164\n",
      "epoch 9995: train loss: 0.11876473257721264, test loss: 0.2624395562516518\n",
      "epoch 9996: train loss: 0.11876164784732306, test loss: 0.2624397776625151\n",
      "epoch 9997: train loss: 0.11875856354450795, test loss: 0.2624399992029257\n",
      "epoch 9998: train loss: 0.11875547966866197, test loss: 0.2624402208728479\n",
      "epoch 9999: train loss: 0.11875239621967978, test loss: 0.26244044267224614\n",
      "epoch 10000: train loss: 0.11874931319745602, test loss: 0.26244066460108456\n",
      "epoch 10001: train loss: 0.11874623060188547, test loss: 0.26244088665932763\n",
      "epoch 10002: train loss: 0.11874314843286285, test loss: 0.26244110884693955\n",
      "epoch 10003: train loss: 0.11874006669028297, test loss: 0.26244133116388474\n",
      "epoch 10004: train loss: 0.11873698537404068, test loss: 0.26244155361012744\n",
      "epoch 10005: train loss: 0.11873390448403086, test loss: 0.26244177618563225\n",
      "epoch 10006: train loss: 0.1187308240201484, test loss: 0.2624419988903633\n",
      "epoch 10007: train loss: 0.11872774398228823, test loss: 0.2624422217242852\n",
      "epoch 10008: train loss: 0.11872466437034536, test loss: 0.2624424446873622\n",
      "epoch 10009: train loss: 0.11872158518421483, test loss: 0.2624426677795589\n",
      "epoch 10010: train loss: 0.11871850642379166, test loss: 0.26244289100083945\n",
      "epoch 10011: train loss: 0.11871542808897093, test loss: 0.2624431143511685\n",
      "epoch 10012: train loss: 0.11871235017964782, test loss: 0.2624433378305104\n",
      "epoch 10013: train loss: 0.11870927269571746, test loss: 0.26244356143882974\n",
      "epoch 10014: train loss: 0.11870619563707507, test loss: 0.26244378517609085\n",
      "epoch 10015: train loss: 0.11870311900361585, test loss: 0.26244400904225834\n",
      "epoch 10016: train loss: 0.11870004279523515, test loss: 0.2624442330372967\n",
      "epoch 10017: train loss: 0.11869696701182825, test loss: 0.26244445716117026\n",
      "epoch 10018: train loss: 0.11869389165329046, test loss: 0.26244468141384386\n",
      "epoch 10019: train loss: 0.11869081671951723, test loss: 0.26244490579528185\n",
      "epoch 10020: train loss: 0.1186877422104039, test loss: 0.26244513030544875\n",
      "epoch 10021: train loss: 0.11868466812584602, test loss: 0.2624453549443092\n",
      "epoch 10022: train loss: 0.11868159446573903, test loss: 0.2624455797118277\n",
      "epoch 10023: train loss: 0.11867852122997848, test loss: 0.26244580460796896\n",
      "epoch 10024: train loss: 0.11867544841845995, test loss: 0.2624460296326975\n",
      "epoch 10025: train loss: 0.11867237603107902, test loss: 0.26244625478597794\n",
      "epoch 10026: train loss: 0.11866930406773134, test loss: 0.2624464800677748\n",
      "epoch 10027: train loss: 0.1186662325283126, test loss: 0.26244670547805293\n",
      "epoch 10028: train loss: 0.11866316141271853, test loss: 0.2624469310167768\n",
      "epoch 10029: train loss: 0.11866009072084481, test loss: 0.26244715668391116\n",
      "epoch 10030: train loss: 0.11865702045258729, test loss: 0.2624473824794206\n",
      "epoch 10031: train loss: 0.11865395060784177, test loss: 0.2624476084032699\n",
      "epoch 10032: train loss: 0.11865088118650415, test loss: 0.2624478344554236\n",
      "epoch 10033: train loss: 0.11864781218847024, test loss: 0.26244806063584664\n",
      "epoch 10034: train loss: 0.11864474361363607, test loss: 0.26244828694450356\n",
      "epoch 10035: train loss: 0.11864167546189756, test loss: 0.2624485133813591\n",
      "epoch 10036: train loss: 0.11863860773315071, test loss: 0.26244873994637813\n",
      "epoch 10037: train loss: 0.11863554042729156, test loss: 0.2624489666395252\n",
      "epoch 10038: train loss: 0.11863247354421622, test loss: 0.2624491934607653\n",
      "epoch 10039: train loss: 0.11862940708382076, test loss: 0.26244942041006303\n",
      "epoch 10040: train loss: 0.11862634104600137, test loss: 0.2624496474873832\n",
      "epoch 10041: train loss: 0.11862327543065421, test loss: 0.2624498746926907\n",
      "epoch 10042: train loss: 0.1186202102376755, test loss: 0.2624501020259503\n",
      "epoch 10043: train loss: 0.11861714546696152, test loss: 0.2624503294871268\n",
      "epoch 10044: train loss: 0.11861408111840856, test loss: 0.26245055707618514\n",
      "epoch 10045: train loss: 0.11861101719191293, test loss: 0.26245078479309003\n",
      "epoch 10046: train loss: 0.11860795368737101, test loss: 0.2624510126378064\n",
      "epoch 10047: train loss: 0.1186048906046792, test loss: 0.26245124061029906\n",
      "epoch 10048: train loss: 0.11860182794373397, test loss: 0.2624514687105331\n",
      "epoch 10049: train loss: 0.11859876570443176, test loss: 0.26245169693847314\n",
      "epoch 10050: train loss: 0.11859570388666907, test loss: 0.2624519252940843\n",
      "epoch 10051: train loss: 0.11859264249034247, test loss: 0.26245215377733133\n",
      "epoch 10052: train loss: 0.11858958151534854, test loss: 0.2624523823881793\n",
      "epoch 10053: train loss: 0.11858652096158391, test loss: 0.26245261112659307\n",
      "epoch 10054: train loss: 0.1185834608289452, test loss: 0.26245283999253766\n",
      "epoch 10055: train loss: 0.11858040111732915, test loss: 0.2624530689859779\n",
      "epoch 10056: train loss: 0.11857734182663245, test loss: 0.26245329810687906\n",
      "epoch 10057: train loss: 0.11857428295675188, test loss: 0.2624535273552058\n",
      "epoch 10058: train loss: 0.11857122450758424, test loss: 0.2624537567309233\n",
      "epoch 10059: train loss: 0.11856816647902635, test loss: 0.2624539862339966\n",
      "epoch 10060: train loss: 0.1185651088709751, test loss: 0.26245421586439055\n",
      "epoch 10061: train loss: 0.11856205168332737, test loss: 0.26245444562207043\n",
      "epoch 10062: train loss: 0.11855899491598013, test loss: 0.26245467550700113\n",
      "epoch 10063: train loss: 0.11855593856883034, test loss: 0.2624549055191477\n",
      "epoch 10064: train loss: 0.11855288264177503, test loss: 0.26245513565847534\n",
      "epoch 10065: train loss: 0.11854982713471122, test loss: 0.26245536592494895\n",
      "epoch 10066: train loss: 0.11854677204753603, test loss: 0.2624555963185338\n",
      "epoch 10067: train loss: 0.11854371738014656, test loss: 0.262455826839195\n",
      "epoch 10068: train loss: 0.11854066313244, test loss: 0.26245605748689754\n",
      "epoch 10069: train loss: 0.1185376093043135, test loss: 0.2624562882616066\n",
      "epoch 10070: train loss: 0.11853455589566429, test loss: 0.2624565191632874\n",
      "epoch 10071: train loss: 0.11853150290638968, test loss: 0.26245675019190506\n",
      "epoch 10072: train loss: 0.1185284503363869, test loss: 0.2624569813474246\n",
      "epoch 10073: train loss: 0.11852539818555335, test loss: 0.2624572126298114\n",
      "epoch 10074: train loss: 0.11852234645378637, test loss: 0.26245744403903065\n",
      "epoch 10075: train loss: 0.11851929514098339, test loss: 0.2624576755750474\n",
      "epoch 10076: train loss: 0.11851624424704184, test loss: 0.2624579072378269\n",
      "epoch 10077: train loss: 0.11851319377185918, test loss: 0.2624581390273345\n",
      "epoch 10078: train loss: 0.11851014371533294, test loss: 0.2624583709435354\n",
      "epoch 10079: train loss: 0.1185070940773607, test loss: 0.26245860298639473\n",
      "epoch 10080: train loss: 0.11850404485783997, test loss: 0.26245883515587787\n",
      "epoch 10081: train loss: 0.11850099605666843, test loss: 0.2624590674519501\n",
      "epoch 10082: train loss: 0.11849794767374369, test loss: 0.26245929987457656\n",
      "epoch 10083: train loss: 0.11849489970896354, test loss: 0.26245953242372266\n",
      "epoch 10084: train loss: 0.11849185216222559, test loss: 0.26245976509935376\n",
      "epoch 10085: train loss: 0.11848880503342764, test loss: 0.2624599979014351\n",
      "epoch 10086: train loss: 0.11848575832246752, test loss: 0.26246023082993203\n",
      "epoch 10087: train loss: 0.11848271202924306, test loss: 0.2624604638848098\n",
      "epoch 10088: train loss: 0.11847966615365209, test loss: 0.262460697066034\n",
      "epoch 10089: train loss: 0.11847662069559255, test loss: 0.26246093037356977\n",
      "epoch 10090: train loss: 0.11847357565496232, test loss: 0.2624611638073826\n",
      "epoch 10091: train loss: 0.11847053103165946, test loss: 0.26246139736743784\n",
      "epoch 10092: train loss: 0.11846748682558193, test loss: 0.26246163105370085\n",
      "epoch 10093: train loss: 0.1184644430366278, test loss: 0.26246186486613715\n",
      "epoch 10094: train loss: 0.11846139966469511, test loss: 0.26246209880471205\n",
      "epoch 10095: train loss: 0.11845835670968202, test loss: 0.2624623328693911\n",
      "epoch 10096: train loss: 0.11845531417148666, test loss: 0.2624625670601397\n",
      "epoch 10097: train loss: 0.11845227205000723, test loss: 0.2624628013769233\n",
      "epoch 10098: train loss: 0.11844923034514196, test loss: 0.2624630358197074\n",
      "epoch 10099: train loss: 0.11844618905678907, test loss: 0.2624632703884574\n",
      "epoch 10100: train loss: 0.11844314818484687, test loss: 0.26246350508313887\n",
      "epoch 10101: train loss: 0.1184401077292137, test loss: 0.2624637399037173\n",
      "epoch 10102: train loss: 0.11843706768978793, test loss: 0.26246397485015827\n",
      "epoch 10103: train loss: 0.11843402806646793, test loss: 0.26246420992242714\n",
      "epoch 10104: train loss: 0.11843098885915214, test loss: 0.2624644451204896\n",
      "epoch 10105: train loss: 0.11842795006773907, test loss: 0.2624646804443112\n",
      "epoch 10106: train loss: 0.11842491169212715, test loss: 0.2624649158938574\n",
      "epoch 10107: train loss: 0.118421873732215, test loss: 0.2624651514690939\n",
      "epoch 10108: train loss: 0.11841883618790114, test loss: 0.2624653871699861\n",
      "epoch 10109: train loss: 0.11841579905908416, test loss: 0.2624656229964998\n",
      "epoch 10110: train loss: 0.11841276234566277, test loss: 0.26246585894860053\n",
      "epoch 10111: train loss: 0.1184097260475356, test loss: 0.26246609502625395\n",
      "epoch 10112: train loss: 0.1184066901646014, test loss: 0.26246633122942564\n",
      "epoch 10113: train loss: 0.1184036546967589, test loss: 0.26246656755808123\n",
      "epoch 10114: train loss: 0.11840061964390688, test loss: 0.26246680401218636\n",
      "epoch 10115: train loss: 0.11839758500594418, test loss: 0.26246704059170683\n",
      "epoch 10116: train loss: 0.11839455078276959, test loss: 0.26246727729660824\n",
      "epoch 10117: train loss: 0.11839151697428209, test loss: 0.26246751412685626\n",
      "epoch 10118: train loss: 0.11838848358038052, test loss: 0.26246775108241654\n",
      "epoch 10119: train loss: 0.11838545060096395, test loss: 0.2624679881632549\n",
      "epoch 10120: train loss: 0.11838241803593127, test loss: 0.26246822536933706\n",
      "epoch 10121: train loss: 0.11837938588518156, test loss: 0.2624684627006286\n",
      "epoch 10122: train loss: 0.11837635414861385, test loss: 0.26246870015709556\n",
      "epoch 10123: train loss: 0.11837332282612728, test loss: 0.2624689377387035\n",
      "epoch 10124: train loss: 0.11837029191762094, test loss: 0.26246917544541815\n",
      "epoch 10125: train loss: 0.11836726142299404, test loss: 0.26246941327720535\n",
      "epoch 10126: train loss: 0.11836423134214574, test loss: 0.26246965123403093\n",
      "epoch 10127: train loss: 0.11836120167497531, test loss: 0.2624698893158607\n",
      "epoch 10128: train loss: 0.11835817242138201, test loss: 0.26247012752266047\n",
      "epoch 10129: train loss: 0.1183551435812652, test loss: 0.2624703658543959\n",
      "epoch 10130: train loss: 0.11835211515452414, test loss: 0.26247060431103314\n",
      "epoch 10131: train loss: 0.11834908714105825, test loss: 0.26247084289253786\n",
      "epoch 10132: train loss: 0.11834605954076696, test loss: 0.26247108159887583\n",
      "epoch 10133: train loss: 0.11834303235354966, test loss: 0.2624713204300131\n",
      "epoch 10134: train loss: 0.11834000557930587, test loss: 0.2624715593859155\n",
      "epoch 10135: train loss: 0.1183369792179351, test loss: 0.2624717984665489\n",
      "epoch 10136: train loss: 0.11833395326933692, test loss: 0.2624720376718792\n",
      "epoch 10137: train loss: 0.11833092773341089, test loss: 0.26247227700187237\n",
      "epoch 10138: train loss: 0.11832790261005663, test loss: 0.26247251645649433\n",
      "epoch 10139: train loss: 0.1183248778991738, test loss: 0.26247275603571096\n",
      "epoch 10140: train loss: 0.1183218536006621, test loss: 0.26247299573948824\n",
      "epoch 10141: train loss: 0.11831882971442127, test loss: 0.26247323556779223\n",
      "epoch 10142: train loss: 0.11831580624035101, test loss: 0.26247347552058875\n",
      "epoch 10143: train loss: 0.11831278317835116, test loss: 0.2624737155978439\n",
      "epoch 10144: train loss: 0.11830976052832153, test loss: 0.2624739557995237\n",
      "epoch 10145: train loss: 0.11830673829016199, test loss: 0.26247419612559403\n",
      "epoch 10146: train loss: 0.11830371646377244, test loss: 0.26247443657602104\n",
      "epoch 10147: train loss: 0.11830069504905281, test loss: 0.2624746771507707\n",
      "epoch 10148: train loss: 0.11829767404590306, test loss: 0.2624749178498091\n",
      "epoch 10149: train loss: 0.11829465345422321, test loss: 0.2624751586731022\n",
      "epoch 10150: train loss: 0.11829163327391325, test loss: 0.26247539962061617\n",
      "epoch 10151: train loss: 0.1182886135048733, test loss: 0.262475640692317\n",
      "epoch 10152: train loss: 0.11828559414700342, test loss: 0.2624758818881709\n",
      "epoch 10153: train loss: 0.11828257520020377, test loss: 0.26247612320814384\n",
      "epoch 10154: train loss: 0.11827955666437454, test loss: 0.262476364652202\n",
      "epoch 10155: train loss: 0.11827653853941592, test loss: 0.2624766062203115\n",
      "epoch 10156: train loss: 0.11827352082522812, test loss: 0.26247684791243847\n",
      "epoch 10157: train loss: 0.11827050352171145, test loss: 0.2624770897285491\n",
      "epoch 10158: train loss: 0.11826748662876624, test loss: 0.2624773316686094\n",
      "epoch 10159: train loss: 0.11826447014629277, test loss: 0.26247757373258573\n",
      "epoch 10160: train loss: 0.11826145407419149, test loss: 0.26247781592044417\n",
      "epoch 10161: train loss: 0.11825843841236278, test loss: 0.26247805823215087\n",
      "epoch 10162: train loss: 0.11825542316070708, test loss: 0.2624783006676722\n",
      "epoch 10163: train loss: 0.11825240831912484, test loss: 0.26247854322697417\n",
      "epoch 10164: train loss: 0.11824939388751668, test loss: 0.2624787859100231\n",
      "epoch 10165: train loss: 0.11824637986578304, test loss: 0.26247902871678525\n",
      "epoch 10166: train loss: 0.11824336625382456, test loss: 0.26247927164722684\n",
      "epoch 10167: train loss: 0.11824035305154186, test loss: 0.26247951470131414\n",
      "epoch 10168: train loss: 0.11823734025883556, test loss: 0.26247975787901334\n",
      "epoch 10169: train loss: 0.1182343278756064, test loss: 0.2624800011802908\n",
      "epoch 10170: train loss: 0.11823131590175501, test loss: 0.2624802446051129\n",
      "epoch 10171: train loss: 0.11822830433718223, test loss: 0.26248048815344577\n",
      "epoch 10172: train loss: 0.11822529318178883, test loss: 0.26248073182525583\n",
      "epoch 10173: train loss: 0.1182222824354756, test loss: 0.26248097562050937\n",
      "epoch 10174: train loss: 0.11821927209814344, test loss: 0.2624812195391727\n",
      "epoch 10175: train loss: 0.1182162621696932, test loss: 0.26248146358121227\n",
      "epoch 10176: train loss: 0.11821325265002587, test loss: 0.2624817077465943\n",
      "epoch 10177: train loss: 0.11821024353904232, test loss: 0.2624819520352853\n",
      "epoch 10178: train loss: 0.1182072348366436, test loss: 0.2624821964472515\n",
      "epoch 10179: train loss: 0.11820422654273073, test loss: 0.26248244098245954\n",
      "epoch 10180: train loss: 0.1182012186572048, test loss: 0.2624826856408755\n",
      "epoch 10181: train loss: 0.11819821117996682, test loss: 0.26248293042246607\n",
      "epoch 10182: train loss: 0.11819520411091801, test loss: 0.2624831753271975\n",
      "epoch 10183: train loss: 0.11819219744995949, test loss: 0.26248342035503636\n",
      "epoch 10184: train loss: 0.11818919119699245, test loss: 0.2624836655059489\n",
      "epoch 10185: train loss: 0.11818618535191813, test loss: 0.26248391077990185\n",
      "epoch 10186: train loss: 0.11818317991463782, test loss: 0.2624841561768616\n",
      "epoch 10187: train loss: 0.1181801748850528, test loss: 0.2624844016967945\n",
      "epoch 10188: train loss: 0.1181771702630644, test loss: 0.26248464733966703\n",
      "epoch 10189: train loss: 0.11817416604857398, test loss: 0.2624848931054459\n",
      "epoch 10190: train loss: 0.11817116224148293, test loss: 0.26248513899409753\n",
      "epoch 10191: train loss: 0.11816815884169272, test loss: 0.2624853850055884\n",
      "epoch 10192: train loss: 0.11816515584910484, test loss: 0.26248563113988505\n",
      "epoch 10193: train loss: 0.1181621532636207, test loss: 0.26248587739695406\n",
      "epoch 10194: train loss: 0.1181591510851419, test loss: 0.262486123776762\n",
      "epoch 10195: train loss: 0.11815614931357, test loss: 0.26248637027927546\n",
      "epoch 10196: train loss: 0.11815314794880663, test loss: 0.26248661690446096\n",
      "epoch 10197: train loss: 0.11815014699075338, test loss: 0.26248686365228513\n",
      "epoch 10198: train loss: 0.11814714643931193, test loss: 0.2624871105227146\n",
      "epoch 10199: train loss: 0.118144146294384, test loss: 0.262487357515716\n",
      "epoch 10200: train loss: 0.11814114655587132, test loss: 0.26248760463125587\n",
      "epoch 10201: train loss: 0.11813814722367569, test loss: 0.2624878518693009\n",
      "epoch 10202: train loss: 0.11813514829769887, test loss: 0.2624880992298177\n",
      "epoch 10203: train loss: 0.11813214977784274, test loss: 0.262488346712773\n",
      "epoch 10204: train loss: 0.11812915166400913, test loss: 0.26248859431813343\n",
      "epoch 10205: train loss: 0.11812615395609999, test loss: 0.26248884204586564\n",
      "epoch 10206: train loss: 0.11812315665401721, test loss: 0.2624890898959364\n",
      "epoch 10207: train loss: 0.1181201597576628, test loss: 0.26248933786831236\n",
      "epoch 10208: train loss: 0.11811716326693876, test loss: 0.2624895859629603\n",
      "epoch 10209: train loss: 0.11811416718174718, test loss: 0.26248983417984684\n",
      "epoch 10210: train loss: 0.11811117150199003, test loss: 0.26249008251893874\n",
      "epoch 10211: train loss: 0.11810817622756949, test loss: 0.26249033098020286\n",
      "epoch 10212: train loss: 0.11810518135838768, test loss: 0.2624905795636059\n",
      "epoch 10213: train loss: 0.11810218689434678, test loss: 0.2624908282691145\n",
      "epoch 10214: train loss: 0.11809919283534902, test loss: 0.26249107709669567\n",
      "epoch 10215: train loss: 0.11809619918129657, test loss: 0.262491326046316\n",
      "epoch 10216: train loss: 0.11809320593209179, test loss: 0.26249157511794247\n",
      "epoch 10217: train loss: 0.11809021308763695, test loss: 0.2624918243115417\n",
      "epoch 10218: train loss: 0.11808722064783442, test loss: 0.2624920736270807\n",
      "epoch 10219: train loss: 0.11808422861258651, test loss: 0.26249232306452625\n",
      "epoch 10220: train loss: 0.1180812369817957, test loss: 0.2624925726238452\n",
      "epoch 10221: train loss: 0.1180782457553644, test loss: 0.2624928223050043\n",
      "epoch 10222: train loss: 0.11807525493319508, test loss: 0.2624930721079705\n",
      "epoch 10223: train loss: 0.11807226451519029, test loss: 0.26249332203271064\n",
      "epoch 10224: train loss: 0.11806927450125251, test loss: 0.2624935720791918\n",
      "epoch 10225: train loss: 0.11806628489128436, test loss: 0.26249382224738066\n",
      "epoch 10226: train loss: 0.1180632956851884, test loss: 0.2624940725372442\n",
      "epoch 10227: train loss: 0.11806030688286735, test loss: 0.26249432294874936\n",
      "epoch 10228: train loss: 0.11805731848422382, test loss: 0.26249457348186306\n",
      "epoch 10229: train loss: 0.11805433048916056, test loss: 0.26249482413655234\n",
      "epoch 10230: train loss: 0.11805134289758029, test loss: 0.26249507491278395\n",
      "epoch 10231: train loss: 0.1180483557093858, test loss: 0.26249532581052504\n",
      "epoch 10232: train loss: 0.11804536892447988, test loss: 0.26249557682974256\n",
      "epoch 10233: train loss: 0.11804238254276538, test loss: 0.26249582797040344\n",
      "epoch 10234: train loss: 0.11803939656414517, test loss: 0.26249607923247475\n",
      "epoch 10235: train loss: 0.11803641098852218, test loss: 0.26249633061592337\n",
      "epoch 10236: train loss: 0.11803342581579933, test loss: 0.26249658212071647\n",
      "epoch 10237: train loss: 0.11803044104587962, test loss: 0.262496833746821\n",
      "epoch 10238: train loss: 0.11802745667866603, test loss: 0.26249708549420414\n",
      "epoch 10239: train loss: 0.11802447271406159, test loss: 0.2624973373628327\n",
      "epoch 10240: train loss: 0.11802148915196939, test loss: 0.2624975893526739\n",
      "epoch 10241: train loss: 0.11801850599229256, test loss: 0.2624978414636949\n",
      "epoch 10242: train loss: 0.11801552323493421, test loss: 0.26249809369586263\n",
      "epoch 10243: train loss: 0.11801254087979754, test loss: 0.26249834604914424\n",
      "epoch 10244: train loss: 0.11800955892678573, test loss: 0.2624985985235069\n",
      "epoch 10245: train loss: 0.11800657737580203, test loss: 0.26249885111891763\n",
      "epoch 10246: train loss: 0.11800359622674972, test loss: 0.26249910383534364\n",
      "epoch 10247: train loss: 0.11800061547953206, test loss: 0.262499356672752\n",
      "epoch 10248: train loss: 0.11799763513405248, test loss: 0.26249960963110985\n",
      "epoch 10249: train loss: 0.11799465519021425, test loss: 0.2624998627103845\n",
      "epoch 10250: train loss: 0.11799167564792086, test loss: 0.262500115910543\n",
      "epoch 10251: train loss: 0.11798869650707569, test loss: 0.2625003692315526\n",
      "epoch 10252: train loss: 0.11798571776758221, test loss: 0.2625006226733804\n",
      "epoch 10253: train loss: 0.11798273942934395, test loss: 0.2625008762359937\n",
      "epoch 10254: train loss: 0.11797976149226445, test loss: 0.2625011299193597\n",
      "epoch 10255: train loss: 0.11797678395624725, test loss: 0.2625013837234456\n",
      "epoch 10256: train loss: 0.11797380682119599, test loss: 0.2625016376482187\n",
      "epoch 10257: train loss: 0.11797083008701426, test loss: 0.2625018916936462\n",
      "epoch 10258: train loss: 0.11796785375360574, test loss: 0.2625021458596952\n",
      "epoch 10259: train loss: 0.11796487782087416, test loss: 0.26250240014633336\n",
      "epoch 10260: train loss: 0.11796190228872322, test loss: 0.26250265455352767\n",
      "epoch 10261: train loss: 0.1179589271570567, test loss: 0.26250290908124546\n",
      "epoch 10262: train loss: 0.11795595242577842, test loss: 0.26250316372945415\n",
      "epoch 10263: train loss: 0.11795297809479216, test loss: 0.26250341849812087\n",
      "epoch 10264: train loss: 0.1179500041640018, test loss: 0.2625036733872131\n",
      "epoch 10265: train loss: 0.11794703063331127, test loss: 0.26250392839669806\n",
      "epoch 10266: train loss: 0.11794405750262449, test loss: 0.2625041835265433\n",
      "epoch 10267: train loss: 0.11794108477184537, test loss: 0.262504438776716\n",
      "epoch 10268: train loss: 0.11793811244087797, test loss: 0.2625046941471835\n",
      "epoch 10269: train loss: 0.11793514050962628, test loss: 0.2625049496379133\n",
      "epoch 10270: train loss: 0.1179321689779944, test loss: 0.2625052052488728\n",
      "epoch 10271: train loss: 0.11792919784588637, test loss: 0.2625054609800292\n",
      "epoch 10272: train loss: 0.11792622711320636, test loss: 0.2625057168313501\n",
      "epoch 10273: train loss: 0.11792325677985849, test loss: 0.2625059728028029\n",
      "epoch 10274: train loss: 0.117920286845747, test loss: 0.2625062288943551\n",
      "epoch 10275: train loss: 0.11791731731077604, test loss: 0.26250648510597385\n",
      "epoch 10276: train loss: 0.11791434817484994, test loss: 0.2625067414376269\n",
      "epoch 10277: train loss: 0.11791137943787294, test loss: 0.26250699788928167\n",
      "epoch 10278: train loss: 0.11790841109974942, test loss: 0.2625072544609056\n",
      "epoch 10279: train loss: 0.11790544316038364, test loss: 0.26250751115246607\n",
      "epoch 10280: train loss: 0.11790247561968005, test loss: 0.26250776796393077\n",
      "epoch 10281: train loss: 0.11789950847754309, test loss: 0.26250802489526703\n",
      "epoch 10282: train loss: 0.11789654173387717, test loss: 0.2625082819464425\n",
      "epoch 10283: train loss: 0.11789357538858676, test loss: 0.2625085391174246\n",
      "epoch 10284: train loss: 0.11789060944157641, test loss: 0.262508796408181\n",
      "epoch 10285: train loss: 0.11788764389275067, test loss: 0.2625090538186791\n",
      "epoch 10286: train loss: 0.11788467874201408, test loss: 0.26250931134888655\n",
      "epoch 10287: train loss: 0.1178817139892713, test loss: 0.262509568998771\n",
      "epoch 10288: train loss: 0.11787874963442696, test loss: 0.26250982676829987\n",
      "epoch 10289: train loss: 0.11787578567738573, test loss: 0.2625100846574408\n",
      "epoch 10290: train loss: 0.11787282211805232, test loss: 0.26251034266616147\n",
      "epoch 10291: train loss: 0.11786985895633148, test loss: 0.26251060079442945\n",
      "epoch 10292: train loss: 0.117866896192128, test loss: 0.26251085904221233\n",
      "epoch 10293: train loss: 0.11786393382534666, test loss: 0.2625111174094778\n",
      "epoch 10294: train loss: 0.11786097185589231, test loss: 0.2625113758961934\n",
      "epoch 10295: train loss: 0.11785801028366986, test loss: 0.262511634502327\n",
      "epoch 10296: train loss: 0.11785504910858415, test loss: 0.26251189322784607\n",
      "epoch 10297: train loss: 0.11785208833054014, test loss: 0.26251215207271833\n",
      "epoch 10298: train loss: 0.11784912794944281, test loss: 0.26251241103691153\n",
      "epoch 10299: train loss: 0.11784616796519716, test loss: 0.26251267012039337\n",
      "epoch 10300: train loss: 0.1178432083777082, test loss: 0.26251292932313147\n",
      "epoch 10301: train loss: 0.11784024918688103, test loss: 0.2625131886450936\n",
      "epoch 10302: train loss: 0.11783729039262077, test loss: 0.2625134480862475\n",
      "epoch 10303: train loss: 0.11783433199483248, test loss: 0.26251370764656096\n",
      "epoch 10304: train loss: 0.11783137399342135, test loss: 0.26251396732600163\n",
      "epoch 10305: train loss: 0.11782841638829261, test loss: 0.2625142271245373\n",
      "epoch 10306: train loss: 0.11782545917935142, test loss: 0.2625144870421358\n",
      "epoch 10307: train loss: 0.1178225023665031, test loss: 0.262514747078765\n",
      "epoch 10308: train loss: 0.11781954594965292, test loss: 0.26251500723439236\n",
      "epoch 10309: train loss: 0.1178165899287062, test loss: 0.26251526750898596\n",
      "epoch 10310: train loss: 0.1178136343035683, test loss: 0.26251552790251365\n",
      "epoch 10311: train loss: 0.11781067907414461, test loss: 0.26251578841494305\n",
      "epoch 10312: train loss: 0.11780772424034051, test loss: 0.2625160490462422\n",
      "epoch 10313: train loss: 0.11780476980206149, test loss: 0.2625163097963788\n",
      "epoch 10314: train loss: 0.11780181575921306, test loss: 0.2625165706653208\n",
      "epoch 10315: train loss: 0.1177988621117007, test loss: 0.262516831653036\n",
      "epoch 10316: train loss: 0.11779590885942995, test loss: 0.26251709275949237\n",
      "epoch 10317: train loss: 0.1177929560023064, test loss: 0.26251735398465764\n",
      "epoch 10318: train loss: 0.11779000354023567, test loss: 0.2625176153284999\n",
      "epoch 10319: train loss: 0.11778705147312336, test loss: 0.262517876790987\n",
      "epoch 10320: train loss: 0.11778409980087524, test loss: 0.2625181383720867\n",
      "epoch 10321: train loss: 0.11778114852339691, test loss: 0.26251840007176713\n",
      "epoch 10322: train loss: 0.11777819764059416, test loss: 0.2625186618899962\n",
      "epoch 10323: train loss: 0.11777524715237277, test loss: 0.2625189238267418\n",
      "epoch 10324: train loss: 0.11777229705863855, test loss: 0.26251918588197193\n",
      "epoch 10325: train loss: 0.11776934735929728, test loss: 0.2625194480556545\n",
      "epoch 10326: train loss: 0.11776639805425489, test loss: 0.2625197103477576\n",
      "epoch 10327: train loss: 0.11776344914341726, test loss: 0.2625199727582492\n",
      "epoch 10328: train loss: 0.11776050062669029, test loss: 0.2625202352870972\n",
      "epoch 10329: train loss: 0.11775755250398, test loss: 0.2625204979342698\n",
      "epoch 10330: train loss: 0.11775460477519233, test loss: 0.26252076069973473\n",
      "epoch 10331: train loss: 0.11775165744023329, test loss: 0.2625210235834604\n",
      "epoch 10332: train loss: 0.11774871049900902, test loss: 0.26252128658541457\n",
      "epoch 10333: train loss: 0.11774576395142557, test loss: 0.26252154970556546\n",
      "epoch 10334: train loss: 0.11774281779738903, test loss: 0.26252181294388105\n",
      "epoch 10335: train loss: 0.11773987203680558, test loss: 0.2625220763003294\n",
      "epoch 10336: train loss: 0.11773692666958137, test loss: 0.2625223397748787\n",
      "epoch 10337: train loss: 0.1177339816956227, test loss: 0.26252260336749694\n",
      "epoch 10338: train loss: 0.11773103711483575, test loss: 0.26252286707815237\n",
      "epoch 10339: train loss: 0.11772809292712683, test loss: 0.26252313090681295\n",
      "epoch 10340: train loss: 0.1177251491324022, test loss: 0.2625233948534469\n",
      "epoch 10341: train loss: 0.11772220573056828, test loss: 0.2625236589180223\n",
      "epoch 10342: train loss: 0.1177192627215314, test loss: 0.2625239231005074\n",
      "epoch 10343: train loss: 0.11771632010519793, test loss: 0.26252418740087025\n",
      "epoch 10344: train loss: 0.1177133778814744, test loss: 0.2625244518190791\n",
      "epoch 10345: train loss: 0.11771043605026722, test loss: 0.2625247163551021\n",
      "epoch 10346: train loss: 0.1177074946114829, test loss: 0.2625249810089075\n",
      "epoch 10347: train loss: 0.11770455356502797, test loss: 0.2625252457804633\n",
      "epoch 10348: train loss: 0.117701612910809, test loss: 0.26252551066973795\n",
      "epoch 10349: train loss: 0.11769867264873259, test loss: 0.26252577567669966\n",
      "epoch 10350: train loss: 0.11769573277870536, test loss: 0.26252604080131653\n",
      "epoch 10351: train loss: 0.11769279330063398, test loss: 0.2625263060435568\n",
      "epoch 10352: train loss: 0.11768985421442514, test loss: 0.2625265714033888\n",
      "epoch 10353: train loss: 0.11768691551998554, test loss: 0.2625268368807809\n",
      "epoch 10354: train loss: 0.11768397721722192, test loss: 0.2625271024757012\n",
      "epoch 10355: train loss: 0.11768103930604112, test loss: 0.26252736818811806\n",
      "epoch 10356: train loss: 0.11767810178634995, test loss: 0.2625276340179997\n",
      "epoch 10357: train loss: 0.11767516465805522, test loss: 0.26252789996531456\n",
      "epoch 10358: train loss: 0.11767222792106383, test loss: 0.2625281660300308\n",
      "epoch 10359: train loss: 0.11766929157528269, test loss: 0.2625284322121169\n",
      "epoch 10360: train loss: 0.11766635562061874, test loss: 0.2625286985115411\n",
      "epoch 10361: train loss: 0.11766342005697895, test loss: 0.2625289649282719\n",
      "epoch 10362: train loss: 0.11766048488427033, test loss: 0.26252923146227736\n",
      "epoch 10363: train loss: 0.11765755010239992, test loss: 0.262529498113526\n",
      "epoch 10364: train loss: 0.11765461571127478, test loss: 0.2625297648819863\n",
      "epoch 10365: train loss: 0.11765168171080202, test loss: 0.2625300317676266\n",
      "epoch 10366: train loss: 0.11764874810088875, test loss: 0.2625302987704152\n",
      "epoch 10367: train loss: 0.11764581488144214, test loss: 0.26253056589032064\n",
      "epoch 10368: train loss: 0.11764288205236939, test loss: 0.26253083312731124\n",
      "epoch 10369: train loss: 0.11763994961357774, test loss: 0.26253110048135553\n",
      "epoch 10370: train loss: 0.1176370175649744, test loss: 0.26253136795242177\n",
      "epoch 10371: train loss: 0.11763408590646669, test loss: 0.26253163554047865\n",
      "epoch 10372: train loss: 0.11763115463796193, test loss: 0.2625319032454944\n",
      "epoch 10373: train loss: 0.11762822375936745, test loss: 0.26253217106743765\n",
      "epoch 10374: train loss: 0.11762529327059063, test loss: 0.2625324390062768\n",
      "epoch 10375: train loss: 0.1176223631715389, test loss: 0.2625327070619804\n",
      "epoch 10376: train loss: 0.11761943346211971, test loss: 0.26253297523451696\n",
      "epoch 10377: train loss: 0.11761650414224051, test loss: 0.2625332435238548\n",
      "epoch 10378: train loss: 0.1176135752118088, test loss: 0.2625335119299628\n",
      "epoch 10379: train loss: 0.11761064667073214, test loss: 0.2625337804528092\n",
      "epoch 10380: train loss: 0.1176077185189181, test loss: 0.26253404909236266\n",
      "epoch 10381: train loss: 0.11760479075627427, test loss: 0.26253431784859166\n",
      "epoch 10382: train loss: 0.11760186338270824, test loss: 0.26253458672146485\n",
      "epoch 10383: train loss: 0.11759893639812771, test loss: 0.2625348557109507\n",
      "epoch 10384: train loss: 0.11759600980244038, test loss: 0.2625351248170179\n",
      "epoch 10385: train loss: 0.11759308359555398, test loss: 0.262535394039635\n",
      "epoch 10386: train loss: 0.11759015777737623, test loss: 0.2625356633787706\n",
      "epoch 10387: train loss: 0.1175872323478149, test loss: 0.2625359328343933\n",
      "epoch 10388: train loss: 0.11758430730677784, test loss: 0.26253620240647174\n",
      "epoch 10389: train loss: 0.11758138265417291, test loss: 0.2625364720949746\n",
      "epoch 10390: train loss: 0.11757845838990796, test loss: 0.2625367418998705\n",
      "epoch 10391: train loss: 0.11757553451389091, test loss: 0.262537011821128\n",
      "epoch 10392: train loss: 0.11757261102602971, test loss: 0.2625372818587159\n",
      "epoch 10393: train loss: 0.11756968792623229, test loss: 0.2625375520126027\n",
      "epoch 10394: train loss: 0.1175667652144067, test loss: 0.2625378222827573\n",
      "epoch 10395: train loss: 0.11756384289046094, test loss: 0.26253809266914835\n",
      "epoch 10396: train loss: 0.1175609209543031, test loss: 0.2625383631717444\n",
      "epoch 10397: train loss: 0.11755799940584126, test loss: 0.2625386337905143\n",
      "epoch 10398: train loss: 0.11755507824498351, test loss: 0.2625389045254268\n",
      "epoch 10399: train loss: 0.11755215747163808, test loss: 0.2625391753764505\n",
      "epoch 10400: train loss: 0.11754923708571312, test loss: 0.2625394463435542\n",
      "epoch 10401: train loss: 0.11754631708711681, test loss: 0.2625397174267068\n",
      "epoch 10402: train loss: 0.11754339747575746, test loss: 0.26253998862587685\n",
      "epoch 10403: train loss: 0.11754047825154332, test loss: 0.2625402599410333\n",
      "epoch 10404: train loss: 0.1175375594143827, test loss: 0.2625405313721449\n",
      "epoch 10405: train loss: 0.11753464096418394, test loss: 0.2625408029191803\n",
      "epoch 10406: train loss: 0.11753172290085544, test loss: 0.26254107458210846\n",
      "epoch 10407: train loss: 0.11752880522430555, test loss: 0.2625413463608981\n",
      "epoch 10408: train loss: 0.11752588793444275, test loss: 0.2625416182555182\n",
      "epoch 10409: train loss: 0.11752297103117547, test loss: 0.26254189026593744\n",
      "epoch 10410: train loss: 0.11752005451441222, test loss: 0.26254216239212474\n",
      "epoch 10411: train loss: 0.11751713838406155, test loss: 0.26254243463404897\n",
      "epoch 10412: train loss: 0.117514222640032, test loss: 0.262542706991679\n",
      "epoch 10413: train loss: 0.11751130728223214, test loss: 0.26254297946498356\n",
      "epoch 10414: train loss: 0.11750839231057057, test loss: 0.2625432520539318\n",
      "epoch 10415: train loss: 0.11750547772495597, test loss: 0.2625435247584924\n",
      "epoch 10416: train loss: 0.11750256352529703, test loss: 0.26254379757863433\n",
      "epoch 10417: train loss: 0.11749964971150245, test loss: 0.2625440705143265\n",
      "epoch 10418: train loss: 0.11749673628348095, test loss: 0.2625443435655379\n",
      "epoch 10419: train loss: 0.11749382324114134, test loss: 0.26254461673223745\n",
      "epoch 10420: train loss: 0.11749091058439237, test loss: 0.2625448900143941\n",
      "epoch 10421: train loss: 0.1174879983131429, test loss: 0.2625451634119767\n",
      "epoch 10422: train loss: 0.1174850864273018, test loss: 0.26254543692495436\n",
      "epoch 10423: train loss: 0.11748217492677794, test loss: 0.2625457105532959\n",
      "epoch 10424: train loss: 0.11747926381148027, test loss: 0.2625459842969706\n",
      "epoch 10425: train loss: 0.11747635308131771, test loss: 0.2625462581559471\n",
      "epoch 10426: train loss: 0.11747344273619928, test loss: 0.2625465321301947\n",
      "epoch 10427: train loss: 0.11747053277603399, test loss: 0.26254680621968224\n",
      "epoch 10428: train loss: 0.11746762320073084, test loss: 0.26254708042437874\n",
      "epoch 10429: train loss: 0.11746471401019896, test loss: 0.2625473547442534\n",
      "epoch 10430: train loss: 0.11746180520434744, test loss: 0.2625476291792751\n",
      "epoch 10431: train loss: 0.11745889678308544, test loss: 0.26254790372941306\n",
      "epoch 10432: train loss: 0.11745598874632206, test loss: 0.2625481783946362\n",
      "epoch 10433: train loss: 0.11745308109396657, test loss: 0.26254845317491365\n",
      "epoch 10434: train loss: 0.11745017382592814, test loss: 0.26254872807021445\n",
      "epoch 10435: train loss: 0.11744726694211607, test loss: 0.2625490030805079\n",
      "epoch 10436: train loss: 0.11744436044243964, test loss: 0.2625492782057628\n",
      "epoch 10437: train loss: 0.11744145432680815, test loss: 0.2625495534459485\n",
      "epoch 10438: train loss: 0.11743854859513096, test loss: 0.2625498288010341\n",
      "epoch 10439: train loss: 0.11743564324731746, test loss: 0.2625501042709887\n",
      "epoch 10440: train loss: 0.11743273828327705, test loss: 0.26255037985578134\n",
      "epoch 10441: train loss: 0.11742983370291919, test loss: 0.2625506555553814\n",
      "epoch 10442: train loss: 0.1174269295061533, test loss: 0.26255093136975793\n",
      "epoch 10443: train loss: 0.11742402569288896, test loss: 0.26255120729888\n",
      "epoch 10444: train loss: 0.11742112226303561, test loss: 0.26255148334271694\n",
      "epoch 10445: train loss: 0.1174182192165029, test loss: 0.26255175950123794\n",
      "epoch 10446: train loss: 0.11741531655320035, test loss: 0.2625520357744122\n",
      "epoch 10447: train loss: 0.11741241427303763, test loss: 0.262552312162209\n",
      "epoch 10448: train loss: 0.11740951237592437, test loss: 0.26255258866459735\n",
      "epoch 10449: train loss: 0.11740661086177026, test loss: 0.2625528652815467\n",
      "epoch 10450: train loss: 0.11740370973048501, test loss: 0.26255314201302615\n",
      "epoch 10451: train loss: 0.11740080898197836, test loss: 0.2625534188590051\n",
      "epoch 10452: train loss: 0.11739790861616008, test loss: 0.2625536958194528\n",
      "epoch 10453: train loss: 0.11739500863293997, test loss: 0.26255397289433846\n",
      "epoch 10454: train loss: 0.11739210903222791, test loss: 0.2625542500836313\n",
      "epoch 10455: train loss: 0.11738920981393366, test loss: 0.2625545273873008\n",
      "epoch 10456: train loss: 0.11738631097796726, test loss: 0.2625548048053161\n",
      "epoch 10457: train loss: 0.11738341252423849, test loss: 0.26255508233764663\n",
      "epoch 10458: train loss: 0.11738051445265739, test loss: 0.2625553599842617\n",
      "epoch 10459: train loss: 0.11737761676313391, test loss: 0.2625556377451306\n",
      "epoch 10460: train loss: 0.11737471945557809, test loss: 0.2625559156202227\n",
      "epoch 10461: train loss: 0.11737182252989996, test loss: 0.2625561936095073\n",
      "epoch 10462: train loss: 0.11736892598600956, test loss: 0.26255647171295393\n",
      "epoch 10463: train loss: 0.11736602982381708, test loss: 0.26255674993053174\n",
      "epoch 10464: train loss: 0.11736313404323254, test loss: 0.26255702826221033\n",
      "epoch 10465: train loss: 0.11736023864416621, test loss: 0.26255730670795896\n",
      "epoch 10466: train loss: 0.11735734362652821, test loss: 0.26255758526774714\n",
      "epoch 10467: train loss: 0.1173544489902288, test loss: 0.26255786394154407\n",
      "epoch 10468: train loss: 0.11735155473517823, test loss: 0.26255814272931943\n",
      "epoch 10469: train loss: 0.1173486608612868, test loss: 0.2625584216310426\n",
      "epoch 10470: train loss: 0.11734576736846478, test loss: 0.26255870064668296\n",
      "epoch 10471: train loss: 0.11734287425662253, test loss: 0.26255897977620996\n",
      "epoch 10472: train loss: 0.11733998152567043, test loss: 0.2625592590195931\n",
      "epoch 10473: train loss: 0.11733708917551888, test loss: 0.2625595383768018\n",
      "epoch 10474: train loss: 0.11733419720607835, test loss: 0.2625598178478057\n",
      "epoch 10475: train loss: 0.11733130561725924, test loss: 0.262560097432574\n",
      "epoch 10476: train loss: 0.11732841440897207, test loss: 0.26256037713107655\n",
      "epoch 10477: train loss: 0.1173255235811274, test loss: 0.2625606569432826\n",
      "epoch 10478: train loss: 0.11732263313363572, test loss: 0.26256093686916193\n",
      "epoch 10479: train loss: 0.11731974306640765, test loss: 0.2625612169086838\n",
      "epoch 10480: train loss: 0.11731685337935381, test loss: 0.26256149706181786\n",
      "epoch 10481: train loss: 0.11731396407238481, test loss: 0.26256177732853375\n",
      "epoch 10482: train loss: 0.11731107514541134, test loss: 0.2625620577088009\n",
      "epoch 10483: train loss: 0.1173081865983441, test loss: 0.26256233820258906\n",
      "epoch 10484: train loss: 0.11730529843109382, test loss: 0.26256261880986753\n",
      "epoch 10485: train loss: 0.11730241064357128, test loss: 0.2625628995306062\n",
      "epoch 10486: train loss: 0.11729952323568725, test loss: 0.26256318036477455\n",
      "epoch 10487: train loss: 0.11729663620735252, test loss: 0.2625634613123421\n",
      "epoch 10488: train loss: 0.11729374955847799, test loss: 0.26256374237327856\n",
      "epoch 10489: train loss: 0.11729086328897452, test loss: 0.26256402354755365\n",
      "epoch 10490: train loss: 0.11728797739875305, test loss: 0.26256430483513676\n",
      "epoch 10491: train loss: 0.11728509188772447, test loss: 0.26256458623599793\n",
      "epoch 10492: train loss: 0.11728220675579978, test loss: 0.26256486775010635\n",
      "epoch 10493: train loss: 0.11727932200288997, test loss: 0.262565149377432\n",
      "epoch 10494: train loss: 0.11727643762890608, test loss: 0.2625654311179446\n",
      "epoch 10495: train loss: 0.11727355363375913, test loss: 0.26256571297161363\n",
      "epoch 10496: train loss: 0.11727067001736026, test loss: 0.26256599493840893\n",
      "epoch 10497: train loss: 0.11726778677962053, test loss: 0.26256627701830015\n",
      "epoch 10498: train loss: 0.11726490392045115, test loss: 0.26256655921125704\n",
      "epoch 10499: train loss: 0.11726202143976326, test loss: 0.2625668415172494\n",
      "epoch 10500: train loss: 0.11725913933746807, test loss: 0.2625671239362468\n",
      "epoch 10501: train loss: 0.11725625761347679, test loss: 0.262567406468219\n",
      "epoch 10502: train loss: 0.11725337626770074, test loss: 0.262567689113136\n",
      "epoch 10503: train loss: 0.11725049530005116, test loss: 0.26256797187096736\n",
      "epoch 10504: train loss: 0.1172476147104394, test loss: 0.2625682547416828\n",
      "epoch 10505: train loss: 0.11724473449877686, test loss: 0.2625685377252523\n",
      "epoch 10506: train loss: 0.11724185466497479, test loss: 0.26256882082164557\n",
      "epoch 10507: train loss: 0.11723897520894475, test loss: 0.2625691040308324\n",
      "epoch 10508: train loss: 0.11723609613059807, test loss: 0.26256938735278257\n",
      "epoch 10509: train loss: 0.11723321742984631, test loss: 0.26256967078746596\n",
      "epoch 10510: train loss: 0.11723033910660091, test loss: 0.2625699543348524\n",
      "epoch 10511: train loss: 0.11722746116077343, test loss: 0.2625702379949118\n",
      "epoch 10512: train loss: 0.1172245835922754, test loss: 0.26257052176761386\n",
      "epoch 10513: train loss: 0.11722170640101844, test loss: 0.26257080565292856\n",
      "epoch 10514: train loss: 0.11721882958691414, test loss: 0.2625710896508258\n",
      "epoch 10515: train loss: 0.11721595314987415, test loss: 0.2625713737612753\n",
      "epoch 10516: train loss: 0.11721307708981019, test loss: 0.2625716579842472\n",
      "epoch 10517: train loss: 0.11721020140663391, test loss: 0.26257194231971115\n",
      "epoch 10518: train loss: 0.11720732610025708, test loss: 0.2625722267676373\n",
      "epoch 10519: train loss: 0.11720445117059144, test loss: 0.26257251132799536\n",
      "epoch 10520: train loss: 0.1172015766175488, test loss: 0.2625727960007554\n",
      "epoch 10521: train loss: 0.11719870244104101, test loss: 0.2625730807858873\n",
      "epoch 10522: train loss: 0.11719582864097987, test loss: 0.262573365683361\n",
      "epoch 10523: train loss: 0.11719295521727728, test loss: 0.26257365069314653\n",
      "epoch 10524: train loss: 0.11719008216984517, test loss: 0.2625739358152138\n",
      "epoch 10525: train loss: 0.11718720949859546, test loss: 0.2625742210495328\n",
      "epoch 10526: train loss: 0.11718433720344013, test loss: 0.26257450639607355\n",
      "epoch 10527: train loss: 0.11718146528429114, test loss: 0.26257479185480603\n",
      "epoch 10528: train loss: 0.11717859374106059, test loss: 0.2625750774257003\n",
      "epoch 10529: train loss: 0.11717572257366049, test loss: 0.26257536310872626\n",
      "epoch 10530: train loss: 0.11717285178200293, test loss: 0.262575648903854\n",
      "epoch 10531: train loss: 0.11716998136600004, test loss: 0.26257593481105357\n",
      "epoch 10532: train loss: 0.11716711132556394, test loss: 0.26257622083029497\n",
      "epoch 10533: train loss: 0.11716424166060684, test loss: 0.2625765069615484\n",
      "epoch 10534: train loss: 0.11716137237104089, test loss: 0.26257679320478383\n",
      "epoch 10535: train loss: 0.11715850345677836, test loss: 0.26257707955997117\n",
      "epoch 10536: train loss: 0.11715563491773151, test loss: 0.26257736602708087\n",
      "epoch 10537: train loss: 0.11715276675381263, test loss: 0.2625776526060827\n",
      "epoch 10538: train loss: 0.11714989896493404, test loss: 0.26257793929694684\n",
      "epoch 10539: train loss: 0.11714703155100806, test loss: 0.2625782260996435\n",
      "epoch 10540: train loss: 0.11714416451194708, test loss: 0.26257851301414287\n",
      "epoch 10541: train loss: 0.11714129784766353, test loss: 0.26257880004041484\n",
      "epoch 10542: train loss: 0.11713843155806983, test loss: 0.26257908717842965\n",
      "epoch 10543: train loss: 0.11713556564307842, test loss: 0.26257937442815754\n",
      "epoch 10544: train loss: 0.11713270010260184, test loss: 0.2625796617895686\n",
      "epoch 10545: train loss: 0.11712983493655259, test loss: 0.26257994926263306\n",
      "epoch 10546: train loss: 0.11712697014484318, test loss: 0.26258023684732096\n",
      "epoch 10547: train loss: 0.11712410572738627, test loss: 0.26258052454360276\n",
      "epoch 10548: train loss: 0.11712124168409441, test loss: 0.2625808123514483\n",
      "epoch 10549: train loss: 0.11711837801488023, test loss: 0.2625811002708281\n",
      "epoch 10550: train loss: 0.11711551471965645, test loss: 0.2625813883017122\n",
      "epoch 10551: train loss: 0.11711265179833574, test loss: 0.2625816764440709\n",
      "epoch 10552: train loss: 0.11710978925083083, test loss: 0.26258196469787437\n",
      "epoch 10553: train loss: 0.11710692707705445, test loss: 0.262582253063093\n",
      "epoch 10554: train loss: 0.11710406527691938, test loss: 0.26258254153969696\n",
      "epoch 10555: train loss: 0.1171012038503385, test loss: 0.26258283012765643\n",
      "epoch 10556: train loss: 0.11709834279722454, test loss: 0.2625831188269418\n",
      "epoch 10557: train loss: 0.11709548211749049, test loss: 0.2625834076375234\n",
      "epoch 10558: train loss: 0.11709262181104912, test loss: 0.26258369655937136\n",
      "epoch 10559: train loss: 0.11708976187781346, test loss: 0.2625839855924562\n",
      "epoch 10560: train loss: 0.11708690231769642, test loss: 0.262584274736748\n",
      "epoch 10561: train loss: 0.117084043130611, test loss: 0.2625845639922172\n",
      "epoch 10562: train loss: 0.11708118431647019, test loss: 0.2625848533588342\n",
      "epoch 10563: train loss: 0.11707832587518706, test loss: 0.2625851428365692\n",
      "epoch 10564: train loss: 0.11707546780667466, test loss: 0.2625854324253927\n",
      "epoch 10565: train loss: 0.11707261011084608, test loss: 0.2625857221252749\n",
      "epoch 10566: train loss: 0.11706975278761447, test loss: 0.26258601193618625\n",
      "epoch 10567: train loss: 0.11706689583689296, test loss: 0.2625863018580972\n",
      "epoch 10568: train loss: 0.1170640392585948, test loss: 0.262586591890978\n",
      "epoch 10569: train loss: 0.11706118305263313, test loss: 0.2625868820347992\n",
      "epoch 10570: train loss: 0.11705832721892122, test loss: 0.2625871722895311\n",
      "epoch 10571: train loss: 0.11705547175737235, test loss: 0.26258746265514415\n",
      "epoch 10572: train loss: 0.1170526166678998, test loss: 0.26258775313160876\n",
      "epoch 10573: train loss: 0.11704976195041691, test loss: 0.2625880437188954\n",
      "epoch 10574: train loss: 0.11704690760483707, test loss: 0.26258833441697454\n",
      "epoch 10575: train loss: 0.11704405363107359, test loss: 0.2625886252258165\n",
      "epoch 10576: train loss: 0.11704120002903995, test loss: 0.2625889161453919\n",
      "epoch 10577: train loss: 0.11703834679864954, test loss: 0.26258920717567114\n",
      "epoch 10578: train loss: 0.1170354939398159, test loss: 0.26258949831662476\n",
      "epoch 10579: train loss: 0.11703264145245246, test loss: 0.2625897895682232\n",
      "epoch 10580: train loss: 0.1170297893364728, test loss: 0.2625900809304369\n",
      "epoch 10581: train loss: 0.11702693759179043, test loss: 0.2625903724032365\n",
      "epoch 10582: train loss: 0.11702408621831899, test loss: 0.2625906639865925\n",
      "epoch 10583: train loss: 0.11702123521597205, test loss: 0.26259095568047536\n",
      "epoch 10584: train loss: 0.11701838458466327, test loss: 0.2625912474848557\n",
      "epoch 10585: train loss: 0.11701553432430628, test loss: 0.262591539399704\n",
      "epoch 10586: train loss: 0.11701268443481488, test loss: 0.2625918314249908\n",
      "epoch 10587: train loss: 0.11700983491610271, test loss: 0.2625921235606868\n",
      "epoch 10588: train loss: 0.11700698576808355, test loss: 0.26259241580676246\n",
      "epoch 10589: train loss: 0.11700413699067118, test loss: 0.2625927081631884\n",
      "epoch 10590: train loss: 0.11700128858377944, test loss: 0.26259300062993524\n",
      "epoch 10591: train loss: 0.11699844054732216, test loss: 0.26259329320697344\n",
      "epoch 10592: train loss: 0.11699559288121318, test loss: 0.26259358589427384\n",
      "epoch 10593: train loss: 0.11699274558536642, test loss: 0.262593878691807\n",
      "epoch 10594: train loss: 0.11698989865969583, test loss: 0.26259417159954346\n",
      "epoch 10595: train loss: 0.11698705210411532, test loss: 0.262594464617454\n",
      "epoch 10596: train loss: 0.1169842059185389, test loss: 0.26259475774550906\n",
      "epoch 10597: train loss: 0.11698136010288059, test loss: 0.2625950509836795\n",
      "epoch 10598: train loss: 0.1169785146570544, test loss: 0.2625953443319359\n",
      "epoch 10599: train loss: 0.11697566958097444, test loss: 0.262595637790249\n",
      "epoch 10600: train loss: 0.11697282487455476, test loss: 0.26259593135858944\n",
      "epoch 10601: train loss: 0.11696998053770952, test loss: 0.262596225036928\n",
      "epoch 10602: train loss: 0.11696713657035286, test loss: 0.26259651882523516\n",
      "epoch 10603: train loss: 0.11696429297239899, test loss: 0.26259681272348195\n",
      "epoch 10604: train loss: 0.11696144974376205, test loss: 0.2625971067316389\n",
      "epoch 10605: train loss: 0.11695860688435633, test loss: 0.26259740084967675\n",
      "epoch 10606: train loss: 0.1169557643940961, test loss: 0.2625976950775663\n",
      "epoch 10607: train loss: 0.11695292227289565, test loss: 0.2625979894152783\n",
      "epoch 10608: train loss: 0.11695008052066928, test loss: 0.2625982838627835\n",
      "epoch 10609: train loss: 0.11694723913733136, test loss: 0.26259857842005274\n",
      "epoch 10610: train loss: 0.11694439812279629, test loss: 0.2625988730870567\n",
      "epoch 10611: train loss: 0.11694155747697842, test loss: 0.26259916786376614\n",
      "epoch 10612: train loss: 0.11693871719979225, test loss: 0.26259946275015206\n",
      "epoch 10613: train loss: 0.11693587729115217, test loss: 0.26259975774618505\n",
      "epoch 10614: train loss: 0.11693303775097276, test loss: 0.262600052851836\n",
      "epoch 10615: train loss: 0.11693019857916845, test loss: 0.26260034806707594\n",
      "epoch 10616: train loss: 0.11692735977565386, test loss: 0.2626006433918754\n",
      "epoch 10617: train loss: 0.11692452134034353, test loss: 0.2626009388262054\n",
      "epoch 10618: train loss: 0.11692168327315207, test loss: 0.26260123437003663\n",
      "epoch 10619: train loss: 0.11691884557399412, test loss: 0.2626015300233402\n",
      "epoch 10620: train loss: 0.11691600824278434, test loss: 0.2626018257860869\n",
      "epoch 10621: train loss: 0.1169131712794374, test loss: 0.2626021216582475\n",
      "epoch 10622: train loss: 0.11691033468386804, test loss: 0.262602417639793\n",
      "epoch 10623: train loss: 0.11690749845599097, test loss: 0.26260271373069427\n",
      "epoch 10624: train loss: 0.11690466259572102, test loss: 0.2626030099309223\n",
      "epoch 10625: train loss: 0.11690182710297294, test loss: 0.26260330624044786\n",
      "epoch 10626: train loss: 0.11689899197766156, test loss: 0.262603602659242\n",
      "epoch 10627: train loss: 0.11689615721970176, test loss: 0.26260389918727556\n",
      "epoch 10628: train loss: 0.11689332282900843, test loss: 0.2626041958245196\n",
      "epoch 10629: train loss: 0.11689048880549648, test loss: 0.262604492570945\n",
      "epoch 10630: train loss: 0.11688765514908081, test loss: 0.26260478942652277\n",
      "epoch 10631: train loss: 0.11688482185967641, test loss: 0.2626050863912239\n",
      "epoch 10632: train loss: 0.11688198893719833, test loss: 0.26260538346501927\n",
      "epoch 10633: train loss: 0.1168791563815615, test loss: 0.26260568064788004\n",
      "epoch 10634: train loss: 0.11687632419268104, test loss: 0.262605977939777\n",
      "epoch 10635: train loss: 0.116873492370472, test loss: 0.2626062753406814\n",
      "epoch 10636: train loss: 0.11687066091484949, test loss: 0.2626065728505641\n",
      "epoch 10637: train loss: 0.11686782982572864, test loss: 0.26260687046939624\n",
      "epoch 10638: train loss: 0.11686499910302466, test loss: 0.26260716819714874\n",
      "epoch 10639: train loss: 0.1168621687466527, test loss: 0.26260746603379276\n",
      "epoch 10640: train loss: 0.11685933875652799, test loss: 0.2626077639792993\n",
      "epoch 10641: train loss: 0.11685650913256573, test loss: 0.2626080620336394\n",
      "epoch 10642: train loss: 0.11685367987468126, test loss: 0.26260836019678424\n",
      "epoch 10643: train loss: 0.1168508509827899, test loss: 0.2626086584687048\n",
      "epoch 10644: train loss: 0.11684802245680688, test loss: 0.26260895684937224\n",
      "epoch 10645: train loss: 0.11684519429664764, test loss: 0.2626092553387576\n",
      "epoch 10646: train loss: 0.11684236650222754, test loss: 0.26260955393683205\n",
      "epoch 10647: train loss: 0.116839539073462, test loss: 0.2626098526435668\n",
      "epoch 10648: train loss: 0.11683671201026646, test loss: 0.26261015145893274\n",
      "epoch 10649: train loss: 0.1168338853125564, test loss: 0.2626104503829012\n",
      "epoch 10650: train loss: 0.11683105898024727, test loss: 0.26261074941544327\n",
      "epoch 10651: train loss: 0.11682823301325465, test loss: 0.2626110485565302\n",
      "epoch 10652: train loss: 0.11682540741149407, test loss: 0.2626113478061331\n",
      "epoch 10653: train loss: 0.11682258217488109, test loss: 0.262611647164223\n",
      "epoch 10654: train loss: 0.11681975730333136, test loss: 0.2626119466307713\n",
      "epoch 10655: train loss: 0.11681693279676048, test loss: 0.26261224620574913\n",
      "epoch 10656: train loss: 0.11681410865508411, test loss: 0.26261254588912764\n",
      "epoch 10657: train loss: 0.11681128487821799, test loss: 0.262612845680878\n",
      "epoch 10658: train loss: 0.11680846146607778, test loss: 0.2626131455809716\n",
      "epoch 10659: train loss: 0.11680563841857923, test loss: 0.2626134455893796\n",
      "epoch 10660: train loss: 0.11680281573563814, test loss: 0.2626137457060732\n",
      "epoch 10661: train loss: 0.11679999341717032, test loss: 0.2626140459310238\n",
      "epoch 10662: train loss: 0.11679717146309157, test loss: 0.2626143462642024\n",
      "epoch 10663: train loss: 0.11679434987331774, test loss: 0.2626146467055804\n",
      "epoch 10664: train loss: 0.11679152864776475, test loss: 0.2626149472551291\n",
      "epoch 10665: train loss: 0.11678870778634849, test loss: 0.2626152479128198\n",
      "epoch 10666: train loss: 0.11678588728898488, test loss: 0.26261554867862374\n",
      "epoch 10667: train loss: 0.1167830671555899, test loss: 0.2626158495525123\n",
      "epoch 10668: train loss: 0.11678024738607956, test loss: 0.26261615053445675\n",
      "epoch 10669: train loss: 0.11677742798036989, test loss: 0.2626164516244283\n",
      "epoch 10670: train loss: 0.11677460893837686, test loss: 0.2626167528223985\n",
      "epoch 10671: train loss: 0.11677179026001663, test loss: 0.2626170541283386\n",
      "epoch 10672: train loss: 0.11676897194520527, test loss: 0.26261735554221977\n",
      "epoch 10673: train loss: 0.11676615399385891, test loss: 0.2626176570640137\n",
      "epoch 10674: train loss: 0.11676333640589373, test loss: 0.26261795869369153\n",
      "epoch 10675: train loss: 0.11676051918122589, test loss: 0.26261826043122466\n",
      "epoch 10676: train loss: 0.11675770231977162, test loss: 0.2626185622765846\n",
      "epoch 10677: train loss: 0.11675488582144712, test loss: 0.26261886422974257\n",
      "epoch 10678: train loss: 0.11675206968616872, test loss: 0.2626191662906701\n",
      "epoch 10679: train loss: 0.11674925391385266, test loss: 0.26261946845933853\n",
      "epoch 10680: train loss: 0.1167464385044153, test loss: 0.2626197707357194\n",
      "epoch 10681: train loss: 0.11674362345777296, test loss: 0.262620073119784\n",
      "epoch 10682: train loss: 0.11674080877384206, test loss: 0.2626203756115038\n",
      "epoch 10683: train loss: 0.11673799445253896, test loss: 0.26262067821085033\n",
      "epoch 10684: train loss: 0.11673518049378011, test loss: 0.2626209809177949\n",
      "epoch 10685: train loss: 0.11673236689748197, test loss: 0.26262128373230914\n",
      "epoch 10686: train loss: 0.11672955366356103, test loss: 0.26262158665436447\n",
      "epoch 10687: train loss: 0.11672674079193379, test loss: 0.2626218896839324\n",
      "epoch 10688: train loss: 0.11672392828251679, test loss: 0.26262219282098437\n",
      "epoch 10689: train loss: 0.1167211161352266, test loss: 0.2626224960654918\n",
      "epoch 10690: train loss: 0.11671830434997982, test loss: 0.2626227994174264\n",
      "epoch 10691: train loss: 0.1167154929266931, test loss: 0.2626231028767596\n",
      "epoch 10692: train loss: 0.11671268186528305, test loss: 0.26262340644346294\n",
      "epoch 10693: train loss: 0.11670987116566636, test loss: 0.26262371011750785\n",
      "epoch 10694: train loss: 0.11670706082775972, test loss: 0.262624013898866\n",
      "epoch 10695: train loss: 0.1167042508514799, test loss: 0.26262431778750894\n",
      "epoch 10696: train loss: 0.11670144123674361, test loss: 0.2626246217834083\n",
      "epoch 10697: train loss: 0.11669863198346764, test loss: 0.2626249258865354\n",
      "epoch 10698: train loss: 0.11669582309156887, test loss: 0.2626252300968621\n",
      "epoch 10699: train loss: 0.11669301456096408, test loss: 0.26262553441435993\n",
      "epoch 10700: train loss: 0.11669020639157013, test loss: 0.26262583883900037\n",
      "epoch 10701: train loss: 0.11668739858330394, test loss: 0.26262614337075507\n",
      "epoch 10702: train loss: 0.11668459113608243, test loss: 0.2626264480095957\n",
      "epoch 10703: train loss: 0.11668178404982253, test loss: 0.26262675275549396\n",
      "epoch 10704: train loss: 0.11667897732444125, test loss: 0.26262705760842137\n",
      "epoch 10705: train loss: 0.11667617095985552, test loss: 0.26262736256834956\n",
      "epoch 10706: train loss: 0.11667336495598246, test loss: 0.26262766763525025\n",
      "epoch 10707: train loss: 0.11667055931273909, test loss: 0.26262797280909517\n",
      "epoch 10708: train loss: 0.11666775403004247, test loss: 0.2626282780898558\n",
      "epoch 10709: train loss: 0.11666494910780972, test loss: 0.26262858347750395\n",
      "epoch 10710: train loss: 0.11666214454595802, test loss: 0.26262888897201137\n",
      "epoch 10711: train loss: 0.11665934034440446, test loss: 0.2626291945733496\n",
      "epoch 10712: train loss: 0.11665653650306633, test loss: 0.2626295002814905\n",
      "epoch 10713: train loss: 0.11665373302186076, test loss: 0.2626298060964056\n",
      "epoch 10714: train loss: 0.11665092990070501, test loss: 0.26263011201806685\n",
      "epoch 10715: train loss: 0.11664812713951639, test loss: 0.2626304180464459\n",
      "epoch 10716: train loss: 0.11664532473821218, test loss: 0.2626307241815144\n",
      "epoch 10717: train loss: 0.11664252269670967, test loss: 0.2626310304232442\n",
      "epoch 10718: train loss: 0.11663972101492628, test loss: 0.26263133677160705\n",
      "epoch 10719: train loss: 0.11663691969277938, test loss: 0.26263164322657473\n",
      "epoch 10720: train loss: 0.11663411873018631, test loss: 0.26263194978811893\n",
      "epoch 10721: train loss: 0.11663131812706458, test loss: 0.2626322564562116\n",
      "epoch 10722: train loss: 0.1166285178833316, test loss: 0.2626325632308244\n",
      "epoch 10723: train loss: 0.1166257179989049, test loss: 0.26263287011192915\n",
      "epoch 10724: train loss: 0.11662291847370194, test loss: 0.2626331770994977\n",
      "epoch 10725: train loss: 0.11662011930764034, test loss: 0.26263348419350196\n",
      "epoch 10726: train loss: 0.1166173205006376, test loss: 0.2626337913939136\n",
      "epoch 10727: train loss: 0.11661452205261132, test loss: 0.2626340987007045\n",
      "epoch 10728: train loss: 0.11661172396347914, test loss: 0.2626344061138466\n",
      "epoch 10729: train loss: 0.11660892623315874, test loss: 0.26263471363331164\n",
      "epoch 10730: train loss: 0.11660612886156774, test loss: 0.26263502125907157\n",
      "epoch 10731: train loss: 0.11660333184862384, test loss: 0.2626353289910983\n",
      "epoch 10732: train loss: 0.11660053519424483, test loss: 0.26263563682936364\n",
      "epoch 10733: train loss: 0.11659773889834842, test loss: 0.26263594477383945\n",
      "epoch 10734: train loss: 0.11659494296085239, test loss: 0.26263625282449776\n",
      "epoch 10735: train loss: 0.11659214738167459, test loss: 0.2626365609813104\n",
      "epoch 10736: train loss: 0.11658935216073278, test loss: 0.2626368692442493\n",
      "epoch 10737: train loss: 0.1165865572979449, test loss: 0.26263717761328637\n",
      "epoch 10738: train loss: 0.1165837627932288, test loss: 0.2626374860883936\n",
      "epoch 10739: train loss: 0.1165809686465024, test loss: 0.26263779466954285\n",
      "epoch 10740: train loss: 0.11657817485768365, test loss: 0.2626381033567061\n",
      "epoch 10741: train loss: 0.11657538142669051, test loss: 0.2626384121498555\n",
      "epoch 10742: train loss: 0.116572588353441, test loss: 0.2626387210489628\n",
      "epoch 10743: train loss: 0.11656979563785312, test loss: 0.2626390300540001\n",
      "epoch 10744: train loss: 0.1165670032798449, test loss: 0.2626393391649392\n",
      "epoch 10745: train loss: 0.11656421127933445, test loss: 0.26263964838175236\n",
      "epoch 10746: train loss: 0.11656141963623985, test loss: 0.26263995770441145\n",
      "epoch 10747: train loss: 0.11655862835047927, test loss: 0.2626402671328885\n",
      "epoch 10748: train loss: 0.1165558374219708, test loss: 0.26264057666715557\n",
      "epoch 10749: train loss: 0.11655304685063267, test loss: 0.26264088630718463\n",
      "epoch 10750: train loss: 0.11655025663638309, test loss: 0.2626411960529479\n",
      "epoch 10751: train loss: 0.11654746677914027, test loss: 0.2626415059044171\n",
      "epoch 10752: train loss: 0.11654467727882249, test loss: 0.2626418158615646\n",
      "epoch 10753: train loss: 0.11654188813534802, test loss: 0.26264212592436237\n",
      "epoch 10754: train loss: 0.11653909934863518, test loss: 0.26264243609278237\n",
      "epoch 10755: train loss: 0.11653631091860234, test loss: 0.26264274636679696\n",
      "epoch 10756: train loss: 0.11653352284516781, test loss: 0.26264305674637795\n",
      "epoch 10757: train loss: 0.11653073512825005, test loss: 0.2626433672314976\n",
      "epoch 10758: train loss: 0.11652794776776744, test loss: 0.262643677822128\n",
      "epoch 10759: train loss: 0.11652516076363843, test loss: 0.26264398851824117\n",
      "epoch 10760: train loss: 0.1165223741157815, test loss: 0.2626442993198095\n",
      "epoch 10761: train loss: 0.11651958782411515, test loss: 0.2626446102268048\n",
      "epoch 10762: train loss: 0.11651680188855792, test loss: 0.2626449212391994\n",
      "epoch 10763: train loss: 0.11651401630902831, test loss: 0.2626452323569656\n",
      "epoch 10764: train loss: 0.11651123108544498, test loss: 0.26264554358007525\n",
      "epoch 10765: train loss: 0.11650844621772649, test loss: 0.2626458549085007\n",
      "epoch 10766: train loss: 0.11650566170579146, test loss: 0.2626461663422141\n",
      "epoch 10767: train loss: 0.11650287754955857, test loss: 0.2626464778811877\n",
      "epoch 10768: train loss: 0.11650009374894651, test loss: 0.2626467895253936\n",
      "epoch 10769: train loss: 0.11649731030387396, test loss: 0.2626471012748041\n",
      "epoch 10770: train loss: 0.11649452721425968, test loss: 0.26264741312939144\n",
      "epoch 10771: train loss: 0.11649174448002243, test loss: 0.2626477250891277\n",
      "epoch 10772: train loss: 0.11648896210108102, test loss: 0.2626480371539852\n",
      "epoch 10773: train loss: 0.11648618007735424, test loss: 0.2626483493239362\n",
      "epoch 10774: train loss: 0.11648339840876093, test loss: 0.262648661598953\n",
      "epoch 10775: train loss: 0.11648061709521997, test loss: 0.26264897397900777\n",
      "epoch 10776: train loss: 0.11647783613665026, test loss: 0.2626492864640727\n",
      "epoch 10777: train loss: 0.11647505553297072, test loss: 0.26264959905412033\n",
      "epoch 10778: train loss: 0.11647227528410027, test loss: 0.26264991174912267\n",
      "epoch 10779: train loss: 0.11646949538995793, test loss: 0.26265022454905224\n",
      "epoch 10780: train loss: 0.11646671585046268, test loss: 0.26265053745388117\n",
      "epoch 10781: train loss: 0.11646393666553352, test loss: 0.26265085046358194\n",
      "epoch 10782: train loss: 0.11646115783508955, test loss: 0.2626511635781267\n",
      "epoch 10783: train loss: 0.11645837935904982, test loss: 0.2626514767974879\n",
      "epoch 10784: train loss: 0.11645560123733344, test loss: 0.26265179012163775\n",
      "epoch 10785: train loss: 0.11645282346985952, test loss: 0.26265210355054885\n",
      "epoch 10786: train loss: 0.11645004605654727, test loss: 0.26265241708419323\n",
      "epoch 10787: train loss: 0.11644726899731582, test loss: 0.26265273072254347\n",
      "epoch 10788: train loss: 0.11644449229208441, test loss: 0.26265304446557197\n",
      "epoch 10789: train loss: 0.11644171594077227, test loss: 0.262653358313251\n",
      "epoch 10790: train loss: 0.11643893994329861, test loss: 0.2626536722655529\n",
      "epoch 10791: train loss: 0.11643616429958283, test loss: 0.2626539863224503\n",
      "epoch 10792: train loss: 0.11643338900954416, test loss: 0.26265430048391536\n",
      "epoch 10793: train loss: 0.11643061407310196, test loss: 0.26265461474992063\n",
      "epoch 10794: train loss: 0.1164278394901756, test loss: 0.2626549291204385\n",
      "epoch 10795: train loss: 0.11642506526068445, test loss: 0.2626552435954414\n",
      "epoch 10796: train loss: 0.11642229138454795, test loss: 0.2626555581749017\n",
      "epoch 10797: train loss: 0.11641951786168554, test loss: 0.26265587285879205\n",
      "epoch 10798: train loss: 0.11641674469201671, test loss: 0.2626561876470848\n",
      "epoch 10799: train loss: 0.11641397187546092, test loss: 0.26265650253975237\n",
      "epoch 10800: train loss: 0.11641119941193771, test loss: 0.2626568175367673\n",
      "epoch 10801: train loss: 0.11640842730136665, test loss: 0.262657132638102\n",
      "epoch 10802: train loss: 0.11640565554366727, test loss: 0.262657447843729\n",
      "epoch 10803: train loss: 0.1164028841387592, test loss: 0.2626577631536209\n",
      "epoch 10804: train loss: 0.11640011308656206, test loss: 0.2626580785677501\n",
      "epoch 10805: train loss: 0.11639734238699549, test loss: 0.26265839408608904\n",
      "epoch 10806: train loss: 0.11639457203997917, test loss: 0.2626587097086105\n",
      "epoch 10807: train loss: 0.11639180204543285, test loss: 0.2626590254352868\n",
      "epoch 10808: train loss: 0.11638903240327618, test loss: 0.2626593412660906\n",
      "epoch 10809: train loss: 0.116386263113429, test loss: 0.26265965720099443\n",
      "epoch 10810: train loss: 0.11638349417581102, test loss: 0.26265997323997076\n",
      "epoch 10811: train loss: 0.1163807255903421, test loss: 0.2626602893829923\n",
      "epoch 10812: train loss: 0.11637795735694204, test loss: 0.26266060563003146\n",
      "epoch 10813: train loss: 0.1163751894755307, test loss: 0.262660921981061\n",
      "epoch 10814: train loss: 0.11637242194602798, test loss: 0.26266123843605343\n",
      "epoch 10815: train loss: 0.1163696547683538, test loss: 0.2626615549949814\n",
      "epoch 10816: train loss: 0.11636688794242808, test loss: 0.2626618716578175\n",
      "epoch 10817: train loss: 0.11636412146817078, test loss: 0.2626621884245343\n",
      "epoch 10818: train loss: 0.11636135534550189, test loss: 0.2626625052951045\n",
      "epoch 10819: train loss: 0.11635858957434143, test loss: 0.2626628222695008\n",
      "epoch 10820: train loss: 0.11635582415460945, test loss: 0.2626631393476957\n",
      "epoch 10821: train loss: 0.11635305908622601, test loss: 0.2626634565296619\n",
      "epoch 10822: train loss: 0.11635029436911118, test loss: 0.2626637738153721\n",
      "epoch 10823: train loss: 0.11634753000318511, test loss: 0.26266409120479894\n",
      "epoch 10824: train loss: 0.1163447659883679, test loss: 0.2626644086979152\n",
      "epoch 10825: train loss: 0.11634200232457978, test loss: 0.2626647262946934\n",
      "epoch 10826: train loss: 0.1163392390117409, test loss: 0.2626650439951064\n",
      "epoch 10827: train loss: 0.11633647604977149, test loss: 0.26266536179912675\n",
      "epoch 10828: train loss: 0.11633371343859182, test loss: 0.26266567970672733\n",
      "epoch 10829: train loss: 0.11633095117812209, test loss: 0.2626659977178807\n",
      "epoch 10830: train loss: 0.11632818926828267, test loss: 0.2626663158325597\n",
      "epoch 10831: train loss: 0.11632542770899387, test loss: 0.2626666340507371\n",
      "epoch 10832: train loss: 0.11632266650017602, test loss: 0.2626669523723855\n",
      "epoch 10833: train loss: 0.11631990564174949, test loss: 0.26266727079747787\n",
      "epoch 10834: train loss: 0.1163171451336347, test loss: 0.2626675893259868\n",
      "epoch 10835: train loss: 0.11631438497575207, test loss: 0.26266790795788514\n",
      "epoch 10836: train loss: 0.11631162516802206, test loss: 0.26266822669314566\n",
      "epoch 10837: train loss: 0.11630886571036513, test loss: 0.2626685455317411\n",
      "epoch 10838: train loss: 0.11630610660270177, test loss: 0.26266886447364435\n",
      "epoch 10839: train loss: 0.11630334784495254, test loss: 0.26266918351882823\n",
      "epoch 10840: train loss: 0.116300589437038, test loss: 0.26266950266726546\n",
      "epoch 10841: train loss: 0.11629783137887868, test loss: 0.2626698219189289\n",
      "epoch 10842: train loss: 0.11629507367039522, test loss: 0.2626701412737914\n",
      "epoch 10843: train loss: 0.11629231631150827, test loss: 0.26267046073182576\n",
      "epoch 10844: train loss: 0.11628955930213841, test loss: 0.26267078029300495\n",
      "epoch 10845: train loss: 0.11628680264220642, test loss: 0.26267109995730176\n",
      "epoch 10846: train loss: 0.11628404633163295, test loss: 0.26267141972468894\n",
      "epoch 10847: train loss: 0.11628129037033873, test loss: 0.26267173959513945\n",
      "epoch 10848: train loss: 0.11627853475824451, test loss: 0.2626720595686263\n",
      "epoch 10849: train loss: 0.11627577949527111, test loss: 0.26267237964512224\n",
      "epoch 10850: train loss: 0.11627302458133933, test loss: 0.2626726998246002\n",
      "epoch 10851: train loss: 0.11627027001636997, test loss: 0.2626730201070331\n",
      "epoch 10852: train loss: 0.11626751580028394, test loss: 0.2626733404923938\n",
      "epoch 10853: train loss: 0.11626476193300206, test loss: 0.2626736609806553\n",
      "epoch 10854: train loss: 0.11626200841444533, test loss: 0.2626739815717906\n",
      "epoch 10855: train loss: 0.1162592552445346, test loss: 0.2626743022657725\n",
      "epoch 10856: train loss: 0.1162565024231909, test loss: 0.262674623062574\n",
      "epoch 10857: train loss: 0.11625374995033515, test loss: 0.2626749439621681\n",
      "epoch 10858: train loss: 0.11625099782588841, test loss: 0.26267526496452764\n",
      "epoch 10859: train loss: 0.11624824604977169, test loss: 0.2626755860696258\n",
      "epoch 10860: train loss: 0.11624549462190607, test loss: 0.2626759072774354\n",
      "epoch 10861: train loss: 0.11624274354221263, test loss: 0.2626762285879296\n",
      "epoch 10862: train loss: 0.11623999281061252, test loss: 0.2626765500010812\n",
      "epoch 10863: train loss: 0.11623724242702682, test loss: 0.2626768715168633\n",
      "epoch 10864: train loss: 0.11623449239137672, test loss: 0.26267719313524895\n",
      "epoch 10865: train loss: 0.11623174270358343, test loss: 0.26267751485621116\n",
      "epoch 10866: train loss: 0.1162289933635681, test loss: 0.2626778366797229\n",
      "epoch 10867: train loss: 0.11622624437125208, test loss: 0.26267815860575733\n",
      "epoch 10868: train loss: 0.1162234957265565, test loss: 0.2626784806342874\n",
      "epoch 10869: train loss: 0.11622074742940278, test loss: 0.2626788027652862\n",
      "epoch 10870: train loss: 0.11621799947971216, test loss: 0.2626791249987268\n",
      "epoch 10871: train loss: 0.11621525187740599, test loss: 0.2626794473345823\n",
      "epoch 10872: train loss: 0.11621250462240562, test loss: 0.2626797697728258\n",
      "epoch 10873: train loss: 0.11620975771463246, test loss: 0.26268009231343037\n",
      "epoch 10874: train loss: 0.11620701115400799, test loss: 0.26268041495636907\n",
      "epoch 10875: train loss: 0.11620426494045356, test loss: 0.2626807377016151\n",
      "epoch 10876: train loss: 0.11620151907389067, test loss: 0.2626810605491414\n",
      "epoch 10877: train loss: 0.1161987735542408, test loss: 0.26268138349892134\n",
      "epoch 10878: train loss: 0.11619602838142554, test loss: 0.2626817065509279\n",
      "epoch 10879: train loss: 0.11619328355536633, test loss: 0.2626820297051342\n",
      "epoch 10880: train loss: 0.11619053907598477, test loss: 0.26268235296151354\n",
      "epoch 10881: train loss: 0.11618779494320247, test loss: 0.26268267632003894\n",
      "epoch 10882: train loss: 0.11618505115694105, test loss: 0.2626829997806836\n",
      "epoch 10883: train loss: 0.11618230771712217, test loss: 0.2626833233434207\n",
      "epoch 10884: train loss: 0.11617956462366744, test loss: 0.26268364700822344\n",
      "epoch 10885: train loss: 0.11617682187649861, test loss: 0.2626839707750651\n",
      "epoch 10886: train loss: 0.11617407947553737, test loss: 0.26268429464391874\n",
      "epoch 10887: train loss: 0.11617133742070544, test loss: 0.2626846186147576\n",
      "epoch 10888: train loss: 0.11616859571192464, test loss: 0.262684942687555\n",
      "epoch 10889: train loss: 0.11616585434911678, test loss: 0.262685266862284\n",
      "epoch 10890: train loss: 0.11616311333220355, test loss: 0.262685591138918\n",
      "epoch 10891: train loss: 0.11616037266110699, test loss: 0.26268591551743004\n",
      "epoch 10892: train loss: 0.1161576323357488, test loss: 0.26268623999779356\n",
      "epoch 10893: train loss: 0.11615489235605093, test loss: 0.26268656457998174\n",
      "epoch 10894: train loss: 0.11615215272193534, test loss: 0.26268688926396794\n",
      "epoch 10895: train loss: 0.11614941343332394, test loss: 0.26268721404972534\n",
      "epoch 10896: train loss: 0.11614667449013866, test loss: 0.2626875389372271\n",
      "epoch 10897: train loss: 0.11614393589230156, test loss: 0.2626878639264468\n",
      "epoch 10898: train loss: 0.11614119763973463, test loss: 0.2626881890173576\n",
      "epoch 10899: train loss: 0.11613845973235992, test loss: 0.2626885142099327\n",
      "epoch 10900: train loss: 0.11613572217009949, test loss: 0.26268883950414557\n",
      "epoch 10901: train loss: 0.11613298495287541, test loss: 0.2626891648999695\n",
      "epoch 10902: train loss: 0.11613024808060984, test loss: 0.2626894903973778\n",
      "epoch 10903: train loss: 0.11612751155322493, test loss: 0.2626898159963437\n",
      "epoch 10904: train loss: 0.11612477537064281, test loss: 0.26269014169684085\n",
      "epoch 10905: train loss: 0.1161220395327857, test loss: 0.26269046749884234\n",
      "epoch 10906: train loss: 0.1161193040395758, test loss: 0.2626907934023217\n",
      "epoch 10907: train loss: 0.11611656889093534, test loss: 0.26269111940725215\n",
      "epoch 10908: train loss: 0.11611383408678665, test loss: 0.2626914455136072\n",
      "epoch 10909: train loss: 0.11611109962705196, test loss: 0.2626917717213601\n",
      "epoch 10910: train loss: 0.11610836551165361, test loss: 0.2626920980304845\n",
      "epoch 10911: train loss: 0.11610563174051394, test loss: 0.2626924244409535\n",
      "epoch 10912: train loss: 0.11610289831355532, test loss: 0.26269275095274075\n",
      "epoch 10913: train loss: 0.11610016523070016, test loss: 0.2626930775658195\n",
      "epoch 10914: train loss: 0.11609743249187084, test loss: 0.26269340428016336\n",
      "epoch 10915: train loss: 0.11609470009698981, test loss: 0.26269373109574573\n",
      "epoch 10916: train loss: 0.11609196804597956, test loss: 0.26269405801253987\n",
      "epoch 10917: train loss: 0.11608923633876253, test loss: 0.26269438503051945\n",
      "epoch 10918: train loss: 0.11608650497526131, test loss: 0.26269471214965784\n",
      "epoch 10919: train loss: 0.1160837739553984, test loss: 0.26269503936992855\n",
      "epoch 10920: train loss: 0.11608104327909637, test loss: 0.26269536669130494\n",
      "epoch 10921: train loss: 0.1160783129462778, test loss: 0.26269569411376076\n",
      "epoch 10922: train loss: 0.1160755829568653, test loss: 0.2626960216372693\n",
      "epoch 10923: train loss: 0.11607285331078154, test loss: 0.2626963492618041\n",
      "epoch 10924: train loss: 0.11607012400794915, test loss: 0.2626966769873386\n",
      "epoch 10925: train loss: 0.11606739504829086, test loss: 0.26269700481384656\n",
      "epoch 10926: train loss: 0.11606466643172933, test loss: 0.2626973327413013\n",
      "epoch 10927: train loss: 0.11606193815818734, test loss: 0.2626976607696763\n",
      "epoch 10928: train loss: 0.11605921022758764, test loss: 0.26269798889894547\n",
      "epoch 10929: train loss: 0.11605648263985302, test loss: 0.2626983171290819\n",
      "epoch 10930: train loss: 0.11605375539490627, test loss: 0.2626986454600595\n",
      "epoch 10931: train loss: 0.11605102849267027, test loss: 0.2626989738918517\n",
      "epoch 10932: train loss: 0.11604830193306785, test loss: 0.26269930242443207\n",
      "epoch 10933: train loss: 0.11604557571602189, test loss: 0.26269963105777416\n",
      "epoch 10934: train loss: 0.11604284984145533, test loss: 0.2626999597918517\n",
      "epoch 10935: train loss: 0.11604012430929109, test loss: 0.2627002886266382\n",
      "epoch 10936: train loss: 0.11603739911945211, test loss: 0.2627006175621072\n",
      "epoch 10937: train loss: 0.11603467427186143, test loss: 0.26270094659823257\n",
      "epoch 10938: train loss: 0.11603194976644199, test loss: 0.2627012757349877\n",
      "epoch 10939: train loss: 0.11602922560311685, test loss: 0.2627016049723463\n",
      "epoch 10940: train loss: 0.1160265017818091, test loss: 0.262701934310282\n",
      "epoch 10941: train loss: 0.11602377830244176, test loss: 0.2627022637487684\n",
      "epoch 10942: train loss: 0.116021055164938, test loss: 0.26270259328777934\n",
      "epoch 10943: train loss: 0.1160183323692209, test loss: 0.2627029229272883\n",
      "epoch 10944: train loss: 0.11601560991521366, test loss: 0.26270325266726907\n",
      "epoch 10945: train loss: 0.11601288780283944, test loss: 0.2627035825076953\n",
      "epoch 10946: train loss: 0.11601016603202143, test loss: 0.2627039124485407\n",
      "epoch 10947: train loss: 0.11600744460268288, test loss: 0.2627042424897788\n",
      "epoch 10948: train loss: 0.11600472351474704, test loss: 0.2627045726313837\n",
      "epoch 10949: train loss: 0.1160020027681372, test loss: 0.2627049028733287\n",
      "epoch 10950: train loss: 0.11599928236277662, test loss: 0.2627052332155877\n",
      "epoch 10951: train loss: 0.11599656229858868, test loss: 0.26270556365813447\n",
      "epoch 10952: train loss: 0.11599384257549666, test loss: 0.2627058942009427\n",
      "epoch 10953: train loss: 0.11599112319342403, test loss: 0.26270622484398615\n",
      "epoch 10954: train loss: 0.11598840415229414, test loss: 0.26270655558723854\n",
      "epoch 10955: train loss: 0.1159856854520304, test loss: 0.26270688643067364\n",
      "epoch 10956: train loss: 0.1159829670925563, test loss: 0.2627072173742654\n",
      "epoch 10957: train loss: 0.11598024907379528, test loss: 0.2627075484179874\n",
      "epoch 10958: train loss: 0.11597753139567082, test loss: 0.26270787956181346\n",
      "epoch 10959: train loss: 0.11597481405810652, test loss: 0.2627082108057174\n",
      "epoch 10960: train loss: 0.11597209706102586, test loss: 0.262708542149673\n",
      "epoch 10961: train loss: 0.11596938040435241, test loss: 0.2627088735936541\n",
      "epoch 10962: train loss: 0.11596666408800978, test loss: 0.2627092051376346\n",
      "epoch 10963: train loss: 0.11596394811192161, test loss: 0.2627095367815882\n",
      "epoch 10964: train loss: 0.11596123247601152, test loss: 0.2627098685254888\n",
      "epoch 10965: train loss: 0.11595851718020317, test loss: 0.26271020036931014\n",
      "epoch 10966: train loss: 0.1159558022244203, test loss: 0.2627105323130262\n",
      "epoch 10967: train loss: 0.11595308760858654, test loss: 0.2627108643566109\n",
      "epoch 10968: train loss: 0.11595037333262571, test loss: 0.262711196500038\n",
      "epoch 10969: train loss: 0.11594765939646155, test loss: 0.26271152874328135\n",
      "epoch 10970: train loss: 0.11594494580001787, test loss: 0.2627118610863149\n",
      "epoch 10971: train loss: 0.11594223254321841, test loss: 0.26271219352911246\n",
      "epoch 10972: train loss: 0.11593951962598709, test loss: 0.262712526071648\n",
      "epoch 10973: train loss: 0.11593680704824774, test loss: 0.2627128587138956\n",
      "epoch 10974: train loss: 0.1159340948099242, test loss: 0.26271319145582883\n",
      "epoch 10975: train loss: 0.11593138291094048, test loss: 0.2627135242974218\n",
      "epoch 10976: train loss: 0.11592867135122045, test loss: 0.26271385723864843\n",
      "epoch 10977: train loss: 0.11592596013068807, test loss: 0.2627141902794827\n",
      "epoch 10978: train loss: 0.11592324924926732, test loss: 0.26271452341989854\n",
      "epoch 10979: train loss: 0.11592053870688222, test loss: 0.2627148566598698\n",
      "epoch 10980: train loss: 0.11591782850345682, test loss: 0.2627151899993706\n",
      "epoch 10981: train loss: 0.11591511863891511, test loss: 0.26271552343837484\n",
      "epoch 10982: train loss: 0.11591240911318128, test loss: 0.2627158569768565\n",
      "epoch 10983: train loss: 0.11590969992617932, test loss: 0.26271619061478957\n",
      "epoch 10984: train loss: 0.11590699107783341, test loss: 0.26271652435214804\n",
      "epoch 10985: train loss: 0.11590428256806767, test loss: 0.26271685818890594\n",
      "epoch 10986: train loss: 0.11590157439680634, test loss: 0.2627171921250373\n",
      "epoch 10987: train loss: 0.11589886656397358, test loss: 0.2627175261605162\n",
      "epoch 10988: train loss: 0.1158961590694936, test loss: 0.2627178602953164\n",
      "epoch 10989: train loss: 0.11589345191329066, test loss: 0.26271819452941225\n",
      "epoch 10990: train loss: 0.11589074509528903, test loss: 0.26271852886277763\n",
      "epoch 10991: train loss: 0.11588803861541301, test loss: 0.2627188632953866\n",
      "epoch 10992: train loss: 0.11588533247358694, test loss: 0.26271919782721326\n",
      "epoch 10993: train loss: 0.11588262666973512, test loss: 0.2627195324582317\n",
      "epoch 10994: train loss: 0.11587992120378193, test loss: 0.26271986718841595\n",
      "epoch 10995: train loss: 0.11587721607565178, test loss: 0.2627202020177401\n",
      "epoch 10996: train loss: 0.11587451128526906, test loss: 0.26272053694617825\n",
      "epoch 10997: train loss: 0.11587180683255827, test loss: 0.26272087197370453\n",
      "epoch 10998: train loss: 0.11586910271744377, test loss: 0.2627212071002929\n",
      "epoch 10999: train loss: 0.11586639893985018, test loss: 0.2627215423259177\n",
      "epoch 11000: train loss: 0.11586369549970188, test loss: 0.262721877650553\n",
      "epoch 11001: train loss: 0.11586099239692349, test loss: 0.2627222130741728\n",
      "epoch 11002: train loss: 0.11585828963143953, test loss: 0.26272254859675137\n",
      "epoch 11003: train loss: 0.11585558720317461, test loss: 0.26272288421826273\n",
      "epoch 11004: train loss: 0.11585288511205331, test loss: 0.26272321993868125\n",
      "epoch 11005: train loss: 0.11585018335800029, test loss: 0.26272355575798095\n",
      "epoch 11006: train loss: 0.11584748194094016, test loss: 0.2627238916761359\n",
      "epoch 11007: train loss: 0.11584478086079768, test loss: 0.26272422769312054\n",
      "epoch 11008: train loss: 0.11584208011749747, test loss: 0.26272456380890874\n",
      "epoch 11009: train loss: 0.1158393797109643, test loss: 0.2627249000234749\n",
      "epoch 11010: train loss: 0.11583667964112287, test loss: 0.2627252363367933\n",
      "epoch 11011: train loss: 0.11583397990789804, test loss: 0.26272557274883795\n",
      "epoch 11012: train loss: 0.11583128051121452, test loss: 0.26272590925958317\n",
      "epoch 11013: train loss: 0.11582858145099721, test loss: 0.2627262458690032\n",
      "epoch 11014: train loss: 0.11582588272717087, test loss: 0.26272658257707227\n",
      "epoch 11015: train loss: 0.11582318433966046, test loss: 0.2627269193837645\n",
      "epoch 11016: train loss: 0.11582048628839081, test loss: 0.26272725628905436\n",
      "epoch 11017: train loss: 0.11581778857328684, test loss: 0.26272759329291595\n",
      "epoch 11018: train loss: 0.11581509119427352, test loss: 0.26272793039532355\n",
      "epoch 11019: train loss: 0.11581239415127582, test loss: 0.2627282675962515\n",
      "epoch 11020: train loss: 0.11580969744421869, test loss: 0.262728604895674\n",
      "epoch 11021: train loss: 0.11580700107302715, test loss: 0.26272894229356547\n",
      "epoch 11022: train loss: 0.11580430503762625, test loss: 0.26272927978990007\n",
      "epoch 11023: train loss: 0.11580160933794106, test loss: 0.2627296173846521\n",
      "epoch 11024: train loss: 0.11579891397389666, test loss: 0.26272995507779606\n",
      "epoch 11025: train loss: 0.11579621894541811, test loss: 0.26273029286930605\n",
      "epoch 11026: train loss: 0.11579352425243059, test loss: 0.26273063075915654\n",
      "epoch 11027: train loss: 0.11579082989485923, test loss: 0.26273096874732177\n",
      "epoch 11028: train loss: 0.11578813587262923, test loss: 0.2627313068337761\n",
      "epoch 11029: train loss: 0.11578544218566578, test loss: 0.262731645018494\n",
      "epoch 11030: train loss: 0.11578274883389408, test loss: 0.2627319833014497\n",
      "epoch 11031: train loss: 0.11578005581723941, test loss: 0.2627323216826176\n",
      "epoch 11032: train loss: 0.11577736313562705, test loss: 0.2627326601619721\n",
      "epoch 11033: train loss: 0.11577467078898225, test loss: 0.2627329987394876\n",
      "epoch 11034: train loss: 0.11577197877723037, test loss: 0.2627333374151385\n",
      "epoch 11035: train loss: 0.11576928710029674, test loss: 0.26273367618889915\n",
      "epoch 11036: train loss: 0.11576659575810673, test loss: 0.26273401506074384\n",
      "epoch 11037: train loss: 0.11576390475058573, test loss: 0.2627343540306472\n",
      "epoch 11038: train loss: 0.11576121407765916, test loss: 0.26273469309858355\n",
      "epoch 11039: train loss: 0.11575852373925244, test loss: 0.26273503226452743\n",
      "epoch 11040: train loss: 0.11575583373529102, test loss: 0.2627353715284531\n",
      "epoch 11041: train loss: 0.11575314406570043, test loss: 0.2627357108903351\n",
      "epoch 11042: train loss: 0.11575045473040614, test loss: 0.2627360503501479\n",
      "epoch 11043: train loss: 0.11574776572933371, test loss: 0.26273638990786596\n",
      "epoch 11044: train loss: 0.11574507706240865, test loss: 0.26273672956346367\n",
      "epoch 11045: train loss: 0.11574238872955657, test loss: 0.26273706931691554\n",
      "epoch 11046: train loss: 0.1157397007307031, test loss: 0.26273740916819627\n",
      "epoch 11047: train loss: 0.1157370130657738, test loss: 0.26273774911728\n",
      "epoch 11048: train loss: 0.11573432573469436, test loss: 0.2627380891641414\n",
      "epoch 11049: train loss: 0.11573163873739045, test loss: 0.262738429308755\n",
      "epoch 11050: train loss: 0.11572895207378775, test loss: 0.2627387695510953\n",
      "epoch 11051: train loss: 0.115726265743812, test loss: 0.2627391098911368\n",
      "epoch 11052: train loss: 0.1157235797473889, test loss: 0.26273945032885404\n",
      "epoch 11053: train loss: 0.11572089408444426, test loss: 0.2627397908642216\n",
      "epoch 11054: train loss: 0.11571820875490386, test loss: 0.2627401314972139\n",
      "epoch 11055: train loss: 0.11571552375869353, test loss: 0.2627404722278057\n",
      "epoch 11056: train loss: 0.11571283909573907, test loss: 0.2627408130559714\n",
      "epoch 11057: train loss: 0.11571015476596636, test loss: 0.2627411539816856\n",
      "epoch 11058: train loss: 0.11570747076930125, test loss: 0.262741495004923\n",
      "epoch 11059: train loss: 0.1157047871056697, test loss: 0.26274183612565793\n",
      "epoch 11060: train loss: 0.11570210377499762, test loss: 0.2627421773438652\n",
      "epoch 11061: train loss: 0.11569942077721095, test loss: 0.26274251865951936\n",
      "epoch 11062: train loss: 0.11569673811223566, test loss: 0.26274286007259495\n",
      "epoch 11063: train loss: 0.11569405577999778, test loss: 0.26274320158306674\n",
      "epoch 11064: train loss: 0.11569137378042328, test loss: 0.2627435431909092\n",
      "epoch 11065: train loss: 0.11568869211343827, test loss: 0.262743884896097\n",
      "epoch 11066: train loss: 0.1156860107789688, test loss: 0.2627442266986049\n",
      "epoch 11067: train loss: 0.11568332977694093, test loss: 0.26274456859840734\n",
      "epoch 11068: train loss: 0.11568064910728079, test loss: 0.2627449105954791\n",
      "epoch 11069: train loss: 0.11567796876991457, test loss: 0.2627452526897949\n",
      "epoch 11070: train loss: 0.11567528876476837, test loss: 0.2627455948813292\n",
      "epoch 11071: train loss: 0.11567260909176838, test loss: 0.26274593717005695\n",
      "epoch 11072: train loss: 0.11566992975084084, test loss: 0.26274627955595264\n",
      "epoch 11073: train loss: 0.11566725074191198, test loss: 0.262746622038991\n",
      "epoch 11074: train loss: 0.11566457206490803, test loss: 0.26274696461914687\n",
      "epoch 11075: train loss: 0.11566189371975528, test loss: 0.2627473072963947\n",
      "epoch 11076: train loss: 0.11565921570638003, test loss: 0.2627476500707095\n",
      "epoch 11077: train loss: 0.11565653802470863, test loss: 0.2627479929420657\n",
      "epoch 11078: train loss: 0.1156538606746674, test loss: 0.2627483359104382\n",
      "epoch 11079: train loss: 0.11565118365618271, test loss: 0.2627486789758018\n",
      "epoch 11080: train loss: 0.11564850696918098, test loss: 0.2627490221381311\n",
      "epoch 11081: train loss: 0.1156458306135886, test loss: 0.262749365397401\n",
      "epoch 11082: train loss: 0.11564315458933203, test loss: 0.2627497087535861\n",
      "epoch 11083: train loss: 0.11564047889633773, test loss: 0.26275005220666137\n",
      "epoch 11084: train loss: 0.11563780353453218, test loss: 0.26275039575660136\n",
      "epoch 11085: train loss: 0.11563512850384189, test loss: 0.262750739403381\n",
      "epoch 11086: train loss: 0.11563245380419339, test loss: 0.2627510831469751\n",
      "epoch 11087: train loss: 0.11562977943551328, test loss: 0.2627514269873583\n",
      "epoch 11088: train loss: 0.11562710539772805, test loss: 0.2627517709245057\n",
      "epoch 11089: train loss: 0.1156244316907644, test loss: 0.2627521149583918\n",
      "epoch 11090: train loss: 0.1156217583145489, test loss: 0.26275245908899153\n",
      "epoch 11091: train loss: 0.11561908526900819, test loss: 0.26275280331627987\n",
      "epoch 11092: train loss: 0.11561641255406899, test loss: 0.26275314764023155\n",
      "epoch 11093: train loss: 0.11561374016965796, test loss: 0.2627534920608213\n",
      "epoch 11094: train loss: 0.1156110681157018, test loss: 0.26275383657802415\n",
      "epoch 11095: train loss: 0.11560839639212733, test loss: 0.26275418119181493\n",
      "epoch 11096: train loss: 0.11560572499886122, test loss: 0.2627545259021684\n",
      "epoch 11097: train loss: 0.11560305393583033, test loss: 0.2627548707090596\n",
      "epoch 11098: train loss: 0.1156003832029614, test loss: 0.26275521561246334\n",
      "epoch 11099: train loss: 0.11559771280018129, test loss: 0.2627555606123544\n",
      "epoch 11100: train loss: 0.11559504272741691, test loss: 0.26275590570870794\n",
      "epoch 11101: train loss: 0.11559237298459508, test loss: 0.26275625090149857\n",
      "epoch 11102: train loss: 0.11558970357164272, test loss: 0.26275659619070135\n",
      "epoch 11103: train loss: 0.11558703448848673, test loss: 0.2627569415762913\n",
      "epoch 11104: train loss: 0.11558436573505411, test loss: 0.2627572870582432\n",
      "epoch 11105: train loss: 0.1155816973112718, test loss: 0.262757632636532\n",
      "epoch 11106: train loss: 0.11557902921706674, test loss: 0.2627579783111328\n",
      "epoch 11107: train loss: 0.11557636145236604, test loss: 0.2627583240820204\n",
      "epoch 11108: train loss: 0.11557369401709672, test loss: 0.26275866994916974\n",
      "epoch 11109: train loss: 0.11557102691118576, test loss: 0.2627590159125559\n",
      "epoch 11110: train loss: 0.11556836013456034, test loss: 0.2627593619721537\n",
      "epoch 11111: train loss: 0.11556569368714756, test loss: 0.26275970812793836\n",
      "epoch 11112: train loss: 0.11556302756887447, test loss: 0.2627600543798847\n",
      "epoch 11113: train loss: 0.11556036177966833, test loss: 0.26276040072796775\n",
      "epoch 11114: train loss: 0.11555769631945624, test loss: 0.2627607471721625\n",
      "epoch 11115: train loss: 0.11555503118816542, test loss: 0.2627610937124441\n",
      "epoch 11116: train loss: 0.11555236638572311, test loss: 0.2627614403487874\n",
      "epoch 11117: train loss: 0.11554970191205653, test loss: 0.26276178708116743\n",
      "epoch 11118: train loss: 0.11554703776709298, test loss: 0.2627621339095593\n",
      "epoch 11119: train loss: 0.11554437395075973, test loss: 0.26276248083393816\n",
      "epoch 11120: train loss: 0.11554171046298407, test loss: 0.2627628278542789\n",
      "epoch 11121: train loss: 0.11553904730369335, test loss: 0.2627631749705565\n",
      "epoch 11122: train loss: 0.11553638447281496, test loss: 0.2627635221827463\n",
      "epoch 11123: train loss: 0.11553372197027628, test loss: 0.2627638694908232\n",
      "epoch 11124: train loss: 0.11553105979600467, test loss: 0.2627642168947623\n",
      "epoch 11125: train loss: 0.11552839794992759, test loss: 0.26276456439453866\n",
      "epoch 11126: train loss: 0.11552573643197245, test loss: 0.26276491199012747\n",
      "epoch 11127: train loss: 0.11552307524206679, test loss: 0.2627652596815037\n",
      "epoch 11128: train loss: 0.11552041438013808, test loss: 0.26276560746864264\n",
      "epoch 11129: train loss: 0.11551775384611379, test loss: 0.26276595535151925\n",
      "epoch 11130: train loss: 0.11551509363992152, test loss: 0.2627663033301087\n",
      "epoch 11131: train loss: 0.11551243376148883, test loss: 0.26276665140438626\n",
      "epoch 11132: train loss: 0.11550977421074325, test loss: 0.2627669995743268\n",
      "epoch 11133: train loss: 0.11550711498761246, test loss: 0.26276734783990574\n",
      "epoch 11134: train loss: 0.11550445609202402, test loss: 0.2627676962010982\n",
      "epoch 11135: train loss: 0.11550179752390566, test loss: 0.2627680446578791\n",
      "epoch 11136: train loss: 0.11549913928318499, test loss: 0.2627683932102239\n",
      "epoch 11137: train loss: 0.11549648136978975, test loss: 0.26276874185810756\n",
      "epoch 11138: train loss: 0.11549382378364767, test loss: 0.26276909060150544\n",
      "epoch 11139: train loss: 0.11549116652468644, test loss: 0.26276943944039266\n",
      "epoch 11140: train loss: 0.11548850959283388, test loss: 0.2627697883747444\n",
      "epoch 11141: train loss: 0.11548585298801775, test loss: 0.2627701374045359\n",
      "epoch 11142: train loss: 0.11548319671016591, test loss: 0.2627704865297424\n",
      "epoch 11143: train loss: 0.11548054075920615, test loss: 0.2627708357503391\n",
      "epoch 11144: train loss: 0.11547788513506632, test loss: 0.2627711850663012\n",
      "epoch 11145: train loss: 0.1154752298376743, test loss: 0.26277153447760404\n",
      "epoch 11146: train loss: 0.11547257486695804, test loss: 0.26277188398422274\n",
      "epoch 11147: train loss: 0.11546992022284541, test loss: 0.2627722335861326\n",
      "epoch 11148: train loss: 0.1154672659052644, test loss: 0.26277258328330894\n",
      "epoch 11149: train loss: 0.11546461191414296, test loss: 0.26277293307572697\n",
      "epoch 11150: train loss: 0.1154619582494091, test loss: 0.262773282963362\n",
      "epoch 11151: train loss: 0.11545930491099078, test loss: 0.2627736329461893\n",
      "epoch 11152: train loss: 0.1154566518988161, test loss: 0.2627739830241842\n",
      "epoch 11153: train loss: 0.11545399921281313, test loss: 0.2627743331973218\n",
      "epoch 11154: train loss: 0.11545134685290988, test loss: 0.2627746834655777\n",
      "epoch 11155: train loss: 0.11544869481903451, test loss: 0.26277503382892703\n",
      "epoch 11156: train loss: 0.11544604311111513, test loss: 0.2627753842873452\n",
      "epoch 11157: train loss: 0.11544339172907989, test loss: 0.26277573484080746\n",
      "epoch 11158: train loss: 0.11544074067285698, test loss: 0.26277608548928927\n",
      "epoch 11159: train loss: 0.11543808994237455, test loss: 0.2627764362327658\n",
      "epoch 11160: train loss: 0.11543543953756089, test loss: 0.26277678707121255\n",
      "epoch 11161: train loss: 0.11543278945834419, test loss: 0.2627771380046048\n",
      "epoch 11162: train loss: 0.11543013970465271, test loss: 0.2627774890329179\n",
      "epoch 11163: train loss: 0.11542749027641473, test loss: 0.2627778401561274\n",
      "epoch 11164: train loss: 0.1154248411735586, test loss: 0.26277819137420844\n",
      "epoch 11165: train loss: 0.11542219239601259, test loss: 0.2627785426871365\n",
      "epoch 11166: train loss: 0.1154195439437051, test loss: 0.2627788940948871\n",
      "epoch 11167: train loss: 0.1154168958165645, test loss: 0.2627792455974355\n",
      "epoch 11168: train loss: 0.11541424801451917, test loss: 0.2627795971947571\n",
      "epoch 11169: train loss: 0.11541160053749751, test loss: 0.2627799488868273\n",
      "epoch 11170: train loss: 0.115408953385428, test loss: 0.26278030067362174\n",
      "epoch 11171: train loss: 0.11540630655823908, test loss: 0.2627806525551156\n",
      "epoch 11172: train loss: 0.11540366005585927, test loss: 0.2627810045312845\n",
      "epoch 11173: train loss: 0.11540101387821702, test loss: 0.2627813566021037\n",
      "epoch 11174: train loss: 0.1153983680252409, test loss: 0.26278170876754886\n",
      "epoch 11175: train loss: 0.11539572249685942, test loss: 0.2627820610275954\n",
      "epoch 11176: train loss: 0.11539307729300123, test loss: 0.26278241338221864\n",
      "epoch 11177: train loss: 0.11539043241359488, test loss: 0.2627827658313942\n",
      "epoch 11178: train loss: 0.11538778785856899, test loss: 0.26278311837509755\n",
      "epoch 11179: train loss: 0.11538514362785218, test loss: 0.2627834710133041\n",
      "epoch 11180: train loss: 0.11538249972137317, test loss: 0.26278382374598946\n",
      "epoch 11181: train loss: 0.1153798561390606, test loss: 0.2627841765731291\n",
      "epoch 11182: train loss: 0.1153772128808432, test loss: 0.26278452949469844\n",
      "epoch 11183: train loss: 0.11537456994664971, test loss: 0.2627848825106731\n",
      "epoch 11184: train loss: 0.11537192733640886, test loss: 0.2627852356210286\n",
      "epoch 11185: train loss: 0.11536928505004941, test loss: 0.26278558882574055\n",
      "epoch 11186: train loss: 0.1153666430875002, test loss: 0.2627859421247843\n",
      "epoch 11187: train loss: 0.11536400144869002, test loss: 0.2627862955181356\n",
      "epoch 11188: train loss: 0.11536136013354774, test loss: 0.26278664900576987\n",
      "epoch 11189: train loss: 0.11535871914200216, test loss: 0.2627870025876627\n",
      "epoch 11190: train loss: 0.11535607847398226, test loss: 0.2627873562637898\n",
      "epoch 11191: train loss: 0.11535343812941684, test loss: 0.2627877100341266\n",
      "epoch 11192: train loss: 0.11535079810823494, test loss: 0.26278806389864867\n",
      "epoch 11193: train loss: 0.11534815841036541, test loss: 0.2627884178573317\n",
      "epoch 11194: train loss: 0.11534551903573732, test loss: 0.2627887719101512\n",
      "epoch 11195: train loss: 0.1153428799842796, test loss: 0.26278912605708293\n",
      "epoch 11196: train loss: 0.1153402412559213, test loss: 0.26278948029810234\n",
      "epoch 11197: train loss: 0.11533760285059146, test loss: 0.26278983463318517\n",
      "epoch 11198: train loss: 0.1153349647682191, test loss: 0.26279018906230706\n",
      "epoch 11199: train loss: 0.11533232700873333, test loss: 0.26279054358544357\n",
      "epoch 11200: train loss: 0.11532968957206328, test loss: 0.26279089820257034\n",
      "epoch 11201: train loss: 0.11532705245813808, test loss: 0.2627912529136631\n",
      "epoch 11202: train loss: 0.11532441566688682, test loss: 0.2627916077186975\n",
      "epoch 11203: train loss: 0.11532177919823874, test loss: 0.26279196261764914\n",
      "epoch 11204: train loss: 0.11531914305212303, test loss: 0.26279231761049376\n",
      "epoch 11205: train loss: 0.11531650722846885, test loss: 0.2627926726972071\n",
      "epoch 11206: train loss: 0.11531387172720549, test loss: 0.26279302787776465\n",
      "epoch 11207: train loss: 0.11531123654826221, test loss: 0.26279338315214223\n",
      "epoch 11208: train loss: 0.11530860169156827, test loss: 0.2627937385203156\n",
      "epoch 11209: train loss: 0.115305967157053, test loss: 0.2627940939822605\n",
      "epoch 11210: train loss: 0.11530333294464569, test loss: 0.26279444953795245\n",
      "epoch 11211: train loss: 0.11530069905427574, test loss: 0.2627948051873674\n",
      "epoch 11212: train loss: 0.11529806548587246, test loss: 0.26279516093048083\n",
      "epoch 11213: train loss: 0.11529543223936528, test loss: 0.26279551676726876\n",
      "epoch 11214: train loss: 0.11529279931468363, test loss: 0.2627958726977068\n",
      "epoch 11215: train loss: 0.1152901667117569, test loss: 0.2627962287217707\n",
      "epoch 11216: train loss: 0.1152875344305146, test loss: 0.2627965848394363\n",
      "epoch 11217: train loss: 0.11528490247088616, test loss: 0.2627969410506792\n",
      "epoch 11218: train loss: 0.11528227083280111, test loss: 0.2627972973554754\n",
      "epoch 11219: train loss: 0.11527963951618897, test loss: 0.26279765375380043\n",
      "epoch 11220: train loss: 0.11527700852097932, test loss: 0.26279801024563043\n",
      "epoch 11221: train loss: 0.11527437784710166, test loss: 0.26279836683094093\n",
      "epoch 11222: train loss: 0.11527174749448563, test loss: 0.26279872350970773\n",
      "epoch 11223: train loss: 0.11526911746306082, test loss: 0.26279908028190685\n",
      "epoch 11224: train loss: 0.11526648775275687, test loss: 0.2627994371475139\n",
      "epoch 11225: train loss: 0.11526385836350343, test loss: 0.2627997941065048\n",
      "epoch 11226: train loss: 0.1152612292952302, test loss: 0.2628001511588554\n",
      "epoch 11227: train loss: 0.11525860054786687, test loss: 0.2628005083045415\n",
      "epoch 11228: train loss: 0.11525597212134311, test loss: 0.262800865543539\n",
      "epoch 11229: train loss: 0.11525334401558876, test loss: 0.2628012228758237\n",
      "epoch 11230: train loss: 0.1152507162305335, test loss: 0.2628015803013715\n",
      "epoch 11231: train loss: 0.11524808876610715, test loss: 0.26280193782015837\n",
      "epoch 11232: train loss: 0.1152454616222395, test loss: 0.26280229543215994\n",
      "epoch 11233: train loss: 0.11524283479886042, test loss: 0.2628026531373523\n",
      "epoch 11234: train loss: 0.11524020829589972, test loss: 0.2628030109357114\n",
      "epoch 11235: train loss: 0.1152375821132873, test loss: 0.2628033688272129\n",
      "epoch 11236: train loss: 0.11523495625095304, test loss: 0.2628037268118329\n",
      "epoch 11237: train loss: 0.11523233070882685, test loss: 0.2628040848895472\n",
      "epoch 11238: train loss: 0.11522970548683871, test loss: 0.2628044430603318\n",
      "epoch 11239: train loss: 0.11522708058491851, test loss: 0.2628048013241627\n",
      "epoch 11240: train loss: 0.1152244560029963, test loss: 0.26280515968101564\n",
      "epoch 11241: train loss: 0.11522183174100206, test loss: 0.2628055181308667\n",
      "epoch 11242: train loss: 0.1152192077988658, test loss: 0.2628058766736919\n",
      "epoch 11243: train loss: 0.11521658417651758, test loss: 0.262806235309467\n",
      "epoch 11244: train loss: 0.11521396087388744, test loss: 0.2628065940381681\n",
      "epoch 11245: train loss: 0.11521133789090554, test loss: 0.2628069528597712\n",
      "epoch 11246: train loss: 0.11520871522750191, test loss: 0.2628073117742521\n",
      "epoch 11247: train loss: 0.11520609288360674, test loss: 0.26280767078158696\n",
      "epoch 11248: train loss: 0.11520347085915017, test loss: 0.2628080298817518\n",
      "epoch 11249: train loss: 0.11520084915406238, test loss: 0.2628083890747224\n",
      "epoch 11250: train loss: 0.11519822776827354, test loss: 0.2628087483604751\n",
      "epoch 11251: train loss: 0.11519560670171394, test loss: 0.2628091077389857\n",
      "epoch 11252: train loss: 0.11519298595431375, test loss: 0.26280946721023013\n",
      "epoch 11253: train loss: 0.11519036552600324, test loss: 0.26280982677418463\n",
      "epoch 11254: train loss: 0.11518774541671274, test loss: 0.26281018643082515\n",
      "epoch 11255: train loss: 0.11518512562637252, test loss: 0.2628105461801278\n",
      "epoch 11256: train loss: 0.11518250615491292, test loss: 0.2628109060220685\n",
      "epoch 11257: train loss: 0.11517988700226428, test loss: 0.2628112659566234\n",
      "epoch 11258: train loss: 0.11517726816835698, test loss: 0.2628116259837686\n",
      "epoch 11259: train loss: 0.11517464965312144, test loss: 0.2628119861034801\n",
      "epoch 11260: train loss: 0.115172031456488, test loss: 0.262812346315734\n",
      "epoch 11261: train loss: 0.11516941357838718, test loss: 0.26281270662050643\n",
      "epoch 11262: train loss: 0.11516679601874938, test loss: 0.26281306701777346\n",
      "epoch 11263: train loss: 0.1151641787775051, test loss: 0.26281342750751124\n",
      "epoch 11264: train loss: 0.11516156185458484, test loss: 0.2628137880896957\n",
      "epoch 11265: train loss: 0.11515894524991913, test loss: 0.2628141487643031\n",
      "epoch 11266: train loss: 0.11515632896343853, test loss: 0.2628145095313096\n",
      "epoch 11267: train loss: 0.1151537129950735, test loss: 0.2628148703906913\n",
      "epoch 11268: train loss: 0.11515109734475476, test loss: 0.2628152313424243\n",
      "epoch 11269: train loss: 0.11514848201241286, test loss: 0.2628155923864847\n",
      "epoch 11270: train loss: 0.11514586699797842, test loss: 0.26281595352284876\n",
      "epoch 11271: train loss: 0.11514325230138212, test loss: 0.26281631475149253\n",
      "epoch 11272: train loss: 0.11514063792255459, test loss: 0.26281667607239234\n",
      "epoch 11273: train loss: 0.11513802386142657, test loss: 0.26281703748552415\n",
      "epoch 11274: train loss: 0.11513541011792874, test loss: 0.26281739899086437\n",
      "epoch 11275: train loss: 0.11513279669199186, test loss: 0.26281776058838907\n",
      "epoch 11276: train loss: 0.11513018358354668, test loss: 0.2628181222780744\n",
      "epoch 11277: train loss: 0.11512757079252395, test loss: 0.2628184840598966\n",
      "epoch 11278: train loss: 0.11512495831885453, test loss: 0.2628188459338319\n",
      "epoch 11279: train loss: 0.11512234616246918, test loss: 0.2628192078998566\n",
      "epoch 11280: train loss: 0.11511973432329878, test loss: 0.26281956995794675\n",
      "epoch 11281: train loss: 0.11511712280127419, test loss: 0.26281993210807864\n",
      "epoch 11282: train loss: 0.1151145115963263, test loss: 0.2628202943502286\n",
      "epoch 11283: train loss: 0.115111900708386, test loss: 0.2628206566843727\n",
      "epoch 11284: train loss: 0.1151092901373842, test loss: 0.2628210191104874\n",
      "epoch 11285: train loss: 0.11510667988325192, test loss: 0.2628213816285488\n",
      "epoch 11286: train loss: 0.11510406994592007, test loss: 0.2628217442385333\n",
      "epoch 11287: train loss: 0.11510146032531968, test loss: 0.26282210694041697\n",
      "epoch 11288: train loss: 0.11509885102138175, test loss: 0.26282246973417633\n",
      "epoch 11289: train loss: 0.11509624203403726, test loss: 0.2628228326197875\n",
      "epoch 11290: train loss: 0.11509363336321736, test loss: 0.26282319559722683\n",
      "epoch 11291: train loss: 0.11509102500885306, test loss: 0.26282355866647067\n",
      "epoch 11292: train loss: 0.11508841697087548, test loss: 0.2628239218274952\n",
      "epoch 11293: train loss: 0.11508580924921578, test loss: 0.26282428508027694\n",
      "epoch 11294: train loss: 0.11508320184380504, test loss: 0.262824648424792\n",
      "epoch 11295: train loss: 0.11508059475457444, test loss: 0.26282501186101676\n",
      "epoch 11296: train loss: 0.11507798798145517, test loss: 0.2628253753889277\n",
      "epoch 11297: train loss: 0.11507538152437846, test loss: 0.26282573900850104\n",
      "epoch 11298: train loss: 0.1150727753832755, test loss: 0.2628261027197131\n",
      "epoch 11299: train loss: 0.11507016955807753, test loss: 0.2628264665225403\n",
      "epoch 11300: train loss: 0.11506756404871588, test loss: 0.26282683041695903\n",
      "epoch 11301: train loss: 0.11506495885512176, test loss: 0.26282719440294555\n",
      "epoch 11302: train loss: 0.11506235397722653, test loss: 0.26282755848047645\n",
      "epoch 11303: train loss: 0.11505974941496153, test loss: 0.26282792264952787\n",
      "epoch 11304: train loss: 0.11505714516825806, test loss: 0.26282828691007637\n",
      "epoch 11305: train loss: 0.11505454123704756, test loss: 0.26282865126209826\n",
      "epoch 11306: train loss: 0.11505193762126137, test loss: 0.26282901570557\n",
      "epoch 11307: train loss: 0.11504933432083098, test loss: 0.2628293802404681\n",
      "epoch 11308: train loss: 0.11504673133568774, test loss: 0.26282974486676874\n",
      "epoch 11309: train loss: 0.11504412866576316, test loss: 0.2628301095844484\n",
      "epoch 11310: train loss: 0.11504152631098868, test loss: 0.2628304743934838\n",
      "epoch 11311: train loss: 0.11503892427129588, test loss: 0.26283083929385104\n",
      "epoch 11312: train loss: 0.1150363225466162, test loss: 0.26283120428552675\n",
      "epoch 11313: train loss: 0.11503372113688122, test loss: 0.26283156936848734\n",
      "epoch 11314: train loss: 0.11503112004202251, test loss: 0.2628319345427093\n",
      "epoch 11315: train loss: 0.11502851926197163, test loss: 0.2628322998081691\n",
      "epoch 11316: train loss: 0.1150259187966602, test loss: 0.2628326651648431\n",
      "epoch 11317: train loss: 0.11502331864601988, test loss: 0.262833030612708\n",
      "epoch 11318: train loss: 0.11502071880998227, test loss: 0.26283339615174006\n",
      "epoch 11319: train loss: 0.11501811928847908, test loss: 0.262833761781916\n",
      "epoch 11320: train loss: 0.11501552008144193, test loss: 0.2628341275032121\n",
      "epoch 11321: train loss: 0.11501292118880262, test loss: 0.26283449331560504\n",
      "epoch 11322: train loss: 0.11501032261049282, test loss: 0.26283485921907135\n",
      "epoch 11323: train loss: 0.11500772434644432, test loss: 0.2628352252135875\n",
      "epoch 11324: train loss: 0.11500512639658889, test loss: 0.2628355912991299\n",
      "epoch 11325: train loss: 0.11500252876085831, test loss: 0.26283595747567534\n",
      "epoch 11326: train loss: 0.11499993143918441, test loss: 0.2628363237432001\n",
      "epoch 11327: train loss: 0.11499733443149904, test loss: 0.262836690101681\n",
      "epoch 11328: train loss: 0.114994737737734, test loss: 0.26283705655109446\n",
      "epoch 11329: train loss: 0.11499214135782124, test loss: 0.262837423091417\n",
      "epoch 11330: train loss: 0.11498954529169263, test loss: 0.2628377897226252\n",
      "epoch 11331: train loss: 0.11498694953928007, test loss: 0.26283815644469594\n",
      "epoch 11332: train loss: 0.11498435410051554, test loss: 0.26283852325760537\n",
      "epoch 11333: train loss: 0.114981758975331, test loss: 0.26283889016133033\n",
      "epoch 11334: train loss: 0.11497916416365841, test loss: 0.2628392571558474\n",
      "epoch 11335: train loss: 0.11497656966542978, test loss: 0.2628396242411331\n",
      "epoch 11336: train loss: 0.11497397548057715, test loss: 0.2628399914171642\n",
      "epoch 11337: train loss: 0.11497138160903259, test loss: 0.2628403586839172\n",
      "epoch 11338: train loss: 0.1149687880507281, test loss: 0.2628407260413688\n",
      "epoch 11339: train loss: 0.11496619480559583, test loss: 0.2628410934894956\n",
      "epoch 11340: train loss: 0.11496360187356786, test loss: 0.26284146102827427\n",
      "epoch 11341: train loss: 0.11496100925457632, test loss: 0.26284182865768146\n",
      "epoch 11342: train loss: 0.11495841694855337, test loss: 0.26284219637769385\n",
      "epoch 11343: train loss: 0.11495582495543118, test loss: 0.26284256418828794\n",
      "epoch 11344: train loss: 0.11495323327514197, test loss: 0.26284293208944065\n",
      "epoch 11345: train loss: 0.11495064190761789, test loss: 0.26284330008112855\n",
      "epoch 11346: train loss: 0.11494805085279122, test loss: 0.26284366816332827\n",
      "epoch 11347: train loss: 0.1149454601105942, test loss: 0.26284403633601655\n",
      "epoch 11348: train loss: 0.11494286968095914, test loss: 0.26284440459917013\n",
      "epoch 11349: train loss: 0.11494027956381828, test loss: 0.26284477295276576\n",
      "epoch 11350: train loss: 0.11493768975910397, test loss: 0.2628451413967799\n",
      "epoch 11351: train loss: 0.11493510026674858, test loss: 0.26284550993118955\n",
      "epoch 11352: train loss: 0.11493251108668438, test loss: 0.26284587855597136\n",
      "epoch 11353: train loss: 0.11492992221884384, test loss: 0.262846247271102\n",
      "epoch 11354: train loss: 0.11492733366315931, test loss: 0.2628466160765583\n",
      "epoch 11355: train loss: 0.11492474541956327, test loss: 0.26284698497231695\n",
      "epoch 11356: train loss: 0.11492215748798806, test loss: 0.2628473539583547\n",
      "epoch 11357: train loss: 0.11491956986836623, test loss: 0.2628477230346483\n",
      "epoch 11358: train loss: 0.11491698256063022, test loss: 0.26284809220117467\n",
      "epoch 11359: train loss: 0.11491439556471256, test loss: 0.26284846145791035\n",
      "epoch 11360: train loss: 0.11491180888054575, test loss: 0.26284883080483235\n",
      "epoch 11361: train loss: 0.11490922250806238, test loss: 0.26284920024191727\n",
      "epoch 11362: train loss: 0.11490663644719497, test loss: 0.26284956976914203\n",
      "epoch 11363: train loss: 0.11490405069787611, test loss: 0.2628499393864833\n",
      "epoch 11364: train loss: 0.11490146526003843, test loss: 0.26285030909391816\n",
      "epoch 11365: train loss: 0.11489888013361454, test loss: 0.26285067889142316\n",
      "epoch 11366: train loss: 0.11489629531853712, test loss: 0.26285104877897525\n",
      "epoch 11367: train loss: 0.11489371081473879, test loss: 0.2628514187565511\n",
      "epoch 11368: train loss: 0.11489112662215227, test loss: 0.26285178882412785\n",
      "epoch 11369: train loss: 0.1148885427407103, test loss: 0.2628521589816821\n",
      "epoch 11370: train loss: 0.11488595917034551, test loss: 0.2628525292291908\n",
      "epoch 11371: train loss: 0.11488337591099079, test loss: 0.2628528995666307\n",
      "epoch 11372: train loss: 0.11488079296257883, test loss: 0.2628532699939788\n",
      "epoch 11373: train loss: 0.11487821032504239, test loss: 0.26285364051121196\n",
      "epoch 11374: train loss: 0.11487562799831437, test loss: 0.26285401111830703\n",
      "epoch 11375: train loss: 0.11487304598232757, test loss: 0.26285438181524085\n",
      "epoch 11376: train loss: 0.11487046427701482, test loss: 0.26285475260199037\n",
      "epoch 11377: train loss: 0.11486788288230904, test loss: 0.2628551234785324\n",
      "epoch 11378: train loss: 0.1148653017981431, test loss: 0.2628554944448441\n",
      "epoch 11379: train loss: 0.11486272102444989, test loss: 0.262855865500902\n",
      "epoch 11380: train loss: 0.1148601405611624, test loss: 0.26285623664668334\n",
      "epoch 11381: train loss: 0.11485756040821354, test loss: 0.2628566078821649\n",
      "epoch 11382: train loss: 0.11485498056553631, test loss: 0.26285697920732365\n",
      "epoch 11383: train loss: 0.11485240103306373, test loss: 0.26285735062213655\n",
      "epoch 11384: train loss: 0.11484982181072877, test loss: 0.26285772212658054\n",
      "epoch 11385: train loss: 0.11484724289846451, test loss: 0.26285809372063246\n",
      "epoch 11386: train loss: 0.11484466429620403, test loss: 0.26285846540426944\n",
      "epoch 11387: train loss: 0.11484208600388034, test loss: 0.26285883717746844\n",
      "epoch 11388: train loss: 0.11483950802142658, test loss: 0.2628592090402063\n",
      "epoch 11389: train loss: 0.1148369303487759, test loss: 0.2628595809924601\n",
      "epoch 11390: train loss: 0.11483435298586137, test loss: 0.26285995303420684\n",
      "epoch 11391: train loss: 0.1148317759326162, test loss: 0.26286032516542346\n",
      "epoch 11392: train loss: 0.1148291991889736, test loss: 0.26286069738608697\n",
      "epoch 11393: train loss: 0.11482662275486671, test loss: 0.2628610696961744\n",
      "epoch 11394: train loss: 0.1148240466302288, test loss: 0.26286144209566276\n",
      "epoch 11395: train loss: 0.11482147081499308, test loss: 0.26286181458452906\n",
      "epoch 11396: train loss: 0.11481889530909284, test loss: 0.26286218716275045\n",
      "epoch 11397: train loss: 0.11481632011246137, test loss: 0.2628625598303037\n",
      "epoch 11398: train loss: 0.11481374522503195, test loss: 0.2628629325871661\n",
      "epoch 11399: train loss: 0.11481117064673793, test loss: 0.26286330543331465\n",
      "epoch 11400: train loss: 0.11480859637751262, test loss: 0.26286367836872626\n",
      "epoch 11401: train loss: 0.11480602241728943, test loss: 0.2628640513933782\n",
      "epoch 11402: train loss: 0.11480344876600172, test loss: 0.2628644245072475\n",
      "epoch 11403: train loss: 0.11480087542358292, test loss: 0.262864797710311\n",
      "epoch 11404: train loss: 0.11479830238996641, test loss: 0.26286517100254614\n",
      "epoch 11405: train loss: 0.11479572966508568, test loss: 0.26286554438392973\n",
      "epoch 11406: train loss: 0.11479315724887418, test loss: 0.262865917854439\n",
      "epoch 11407: train loss: 0.11479058514126543, test loss: 0.2628662914140511\n",
      "epoch 11408: train loss: 0.11478801334219288, test loss: 0.26286666506274303\n",
      "epoch 11409: train loss: 0.11478544185159013, test loss: 0.2628670388004919\n",
      "epoch 11410: train loss: 0.11478287066939066, test loss: 0.262867412627275\n",
      "epoch 11411: train loss: 0.11478029979552806, test loss: 0.26286778654306925\n",
      "epoch 11412: train loss: 0.11477772922993595, test loss: 0.2628681605478519\n",
      "epoch 11413: train loss: 0.11477515897254792, test loss: 0.2628685346416001\n",
      "epoch 11414: train loss: 0.11477258902329761, test loss: 0.2628689088242909\n",
      "epoch 11415: train loss: 0.11477001938211866, test loss: 0.2628692830959017\n",
      "epoch 11416: train loss: 0.11476745004894472, test loss: 0.26286965745640933\n",
      "epoch 11417: train loss: 0.1147648810237095, test loss: 0.2628700319057913\n",
      "epoch 11418: train loss: 0.11476231230634673, test loss: 0.26287040644402443\n",
      "epoch 11419: train loss: 0.11475974389679011, test loss: 0.2628707810710863\n",
      "epoch 11420: train loss: 0.11475717579497342, test loss: 0.2628711557869538\n",
      "epoch 11421: train loss: 0.11475460800083041, test loss: 0.26287153059160423\n",
      "epoch 11422: train loss: 0.11475204051429488, test loss: 0.2628719054850148\n",
      "epoch 11423: train loss: 0.11474947333530064, test loss: 0.2628722804671628\n",
      "epoch 11424: train loss: 0.11474690646378152, test loss: 0.26287265553802536\n",
      "epoch 11425: train loss: 0.1147443398996714, test loss: 0.26287303069757967\n",
      "epoch 11426: train loss: 0.11474177364290411, test loss: 0.26287340594580294\n",
      "epoch 11427: train loss: 0.11473920769341354, test loss: 0.2628737812826726\n",
      "epoch 11428: train loss: 0.11473664205113365, test loss: 0.2628741567081657\n",
      "epoch 11429: train loss: 0.11473407671599835, test loss: 0.26287453222225965\n",
      "epoch 11430: train loss: 0.1147315116879416, test loss: 0.26287490782493156\n",
      "epoch 11431: train loss: 0.11472894696689735, test loss: 0.26287528351615874\n",
      "epoch 11432: train loss: 0.1147263825527996, test loss: 0.2628756592959185\n",
      "epoch 11433: train loss: 0.11472381844558238, test loss: 0.26287603516418817\n",
      "epoch 11434: train loss: 0.11472125464517972, test loss: 0.26287641112094484\n",
      "epoch 11435: train loss: 0.11471869115152566, test loss: 0.262876787166166\n",
      "epoch 11436: train loss: 0.11471612796455428, test loss: 0.262877163299829\n",
      "epoch 11437: train loss: 0.11471356508419969, test loss: 0.2628775395219109\n",
      "epoch 11438: train loss: 0.11471100251039598, test loss: 0.2628779158323892\n",
      "epoch 11439: train loss: 0.11470844024307729, test loss: 0.2628782922312411\n",
      "epoch 11440: train loss: 0.11470587828217776, test loss: 0.26287866871844406\n",
      "epoch 11441: train loss: 0.11470331662763161, test loss: 0.2628790452939753\n",
      "epoch 11442: train loss: 0.11470075527937301, test loss: 0.2628794219578122\n",
      "epoch 11443: train loss: 0.11469819423733617, test loss: 0.2628797987099321\n",
      "epoch 11444: train loss: 0.11469563350145531, test loss: 0.2628801755503124\n",
      "epoch 11445: train loss: 0.11469307307166471, test loss: 0.2628805524789304\n",
      "epoch 11446: train loss: 0.1146905129478986, test loss: 0.2628809294957636\n",
      "epoch 11447: train loss: 0.11468795313009134, test loss: 0.2628813066007891\n",
      "epoch 11448: train loss: 0.11468539361817719, test loss: 0.2628816837939846\n",
      "epoch 11449: train loss: 0.11468283441209053, test loss: 0.26288206107532713\n",
      "epoch 11450: train loss: 0.11468027551176566, test loss: 0.26288243844479436\n",
      "epoch 11451: train loss: 0.11467771691713699, test loss: 0.26288281590236373\n",
      "epoch 11452: train loss: 0.1146751586281389, test loss: 0.2628831934480124\n",
      "epoch 11453: train loss: 0.11467260064470583, test loss: 0.26288357108171795\n",
      "epoch 11454: train loss: 0.11467004296677219, test loss: 0.2628839488034578\n",
      "epoch 11455: train loss: 0.11466748559427241, test loss: 0.2628843266132093\n",
      "epoch 11456: train loss: 0.11466492852714101, test loss: 0.2628847045109499\n",
      "epoch 11457: train loss: 0.11466237176531249, test loss: 0.2628850824966571\n",
      "epoch 11458: train loss: 0.11465981530872131, test loss: 0.2628854605703083\n",
      "epoch 11459: train loss: 0.11465725915730203, test loss: 0.2628858387318809\n",
      "epoch 11460: train loss: 0.11465470331098922, test loss: 0.26288621698135256\n",
      "epoch 11461: train loss: 0.11465214776971745, test loss: 0.2628865953187005\n",
      "epoch 11462: train loss: 0.11464959253342126, test loss: 0.2628869737439024\n",
      "epoch 11463: train loss: 0.11464703760203535, test loss: 0.2628873522569355\n",
      "epoch 11464: train loss: 0.11464448297549427, test loss: 0.26288773085777756\n",
      "epoch 11465: train loss: 0.11464192865373274, test loss: 0.2628881095464058\n",
      "epoch 11466: train loss: 0.1146393746366854, test loss: 0.26288848832279793\n",
      "epoch 11467: train loss: 0.11463682092428691, test loss: 0.26288886718693144\n",
      "epoch 11468: train loss: 0.11463426751647203, test loss: 0.26288924613878367\n",
      "epoch 11469: train loss: 0.11463171441317549, test loss: 0.26288962517833236\n",
      "epoch 11470: train loss: 0.11462916161433202, test loss: 0.26289000430555487\n",
      "epoch 11471: train loss: 0.11462660911987642, test loss: 0.2628903835204289\n",
      "epoch 11472: train loss: 0.11462405692974345, test loss: 0.2628907628229317\n",
      "epoch 11473: train loss: 0.11462150504386791, test loss: 0.2628911422130412\n",
      "epoch 11474: train loss: 0.11461895346218469, test loss: 0.2628915216907346\n",
      "epoch 11475: train loss: 0.11461640218462858, test loss: 0.2628919012559897\n",
      "epoch 11476: train loss: 0.11461385121113452, test loss: 0.26289228090878397\n",
      "epoch 11477: train loss: 0.11461130054163732, test loss: 0.262892660649095\n",
      "epoch 11478: train loss: 0.11460875017607193, test loss: 0.2628930404769003\n",
      "epoch 11479: train loss: 0.11460620011437328, test loss: 0.2628934203921776\n",
      "epoch 11480: train loss: 0.11460365035647632, test loss: 0.26289380039490434\n",
      "epoch 11481: train loss: 0.11460110090231602, test loss: 0.2628941804850582\n",
      "epoch 11482: train loss: 0.11459855175182736, test loss: 0.2628945606626168\n",
      "epoch 11483: train loss: 0.11459600290494532, test loss: 0.2628949409275577\n",
      "epoch 11484: train loss: 0.11459345436160501, test loss: 0.2628953212798585\n",
      "epoch 11485: train loss: 0.11459090612174137, test loss: 0.262895701719497\n",
      "epoch 11486: train loss: 0.11458835818528959, test loss: 0.2628960822464505\n",
      "epoch 11487: train loss: 0.11458581055218467, test loss: 0.26289646286069696\n",
      "epoch 11488: train loss: 0.11458326322236172, test loss: 0.26289684356221393\n",
      "epoch 11489: train loss: 0.11458071619575588, test loss: 0.26289722435097895\n",
      "epoch 11490: train loss: 0.11457816947230234, test loss: 0.26289760522696975\n",
      "epoch 11491: train loss: 0.11457562305193622, test loss: 0.2628979861901641\n",
      "epoch 11492: train loss: 0.1145730769345927, test loss: 0.2628983672405395\n",
      "epoch 11493: train loss: 0.114570531120207, test loss: 0.2628987483780737\n",
      "epoch 11494: train loss: 0.11456798560871437, test loss: 0.26289912960274436\n",
      "epoch 11495: train loss: 0.11456544040005001, test loss: 0.2628995109145293\n",
      "epoch 11496: train loss: 0.11456289549414922, test loss: 0.26289989231340605\n",
      "epoch 11497: train loss: 0.11456035089094727, test loss: 0.2629002737993524\n",
      "epoch 11498: train loss: 0.11455780659037941, test loss: 0.26290065537234597\n",
      "epoch 11499: train loss: 0.11455526259238108, test loss: 0.26290103703236456\n",
      "epoch 11500: train loss: 0.11455271889688752, test loss: 0.26290141877938594\n",
      "epoch 11501: train loss: 0.11455017550383413, test loss: 0.26290180061338775\n",
      "epoch 11502: train loss: 0.1145476324131563, test loss: 0.2629021825343478\n",
      "epoch 11503: train loss: 0.11454508962478942, test loss: 0.26290256454224376\n",
      "epoch 11504: train loss: 0.11454254713866892, test loss: 0.2629029466370533\n",
      "epoch 11505: train loss: 0.11454000495473021, test loss: 0.26290332881875433\n",
      "epoch 11506: train loss: 0.11453746307290878, test loss: 0.26290371108732463\n",
      "epoch 11507: train loss: 0.11453492149314012, test loss: 0.26290409344274196\n",
      "epoch 11508: train loss: 0.1145323802153597, test loss: 0.2629044758849839\n",
      "epoch 11509: train loss: 0.11452983923950302, test loss: 0.2629048584140284\n",
      "epoch 11510: train loss: 0.11452729856550567, test loss: 0.2629052410298533\n",
      "epoch 11511: train loss: 0.11452475819330321, test loss: 0.26290562373243626\n",
      "epoch 11512: train loss: 0.11452221812283117, test loss: 0.26290600652175516\n",
      "epoch 11513: train loss: 0.11451967835402513, test loss: 0.26290638939778777\n",
      "epoch 11514: train loss: 0.11451713888682079, test loss: 0.26290677236051185\n",
      "epoch 11515: train loss: 0.11451459972115373, test loss: 0.2629071554099054\n",
      "epoch 11516: train loss: 0.1145120608569596, test loss: 0.2629075385459461\n",
      "epoch 11517: train loss: 0.11450952229417409, test loss: 0.2629079217686119\n",
      "epoch 11518: train loss: 0.11450698403273289, test loss: 0.26290830507788043\n",
      "epoch 11519: train loss: 0.11450444607257172, test loss: 0.2629086884737297\n",
      "epoch 11520: train loss: 0.1145019084136263, test loss: 0.2629090719561376\n",
      "epoch 11521: train loss: 0.11449937105583238, test loss: 0.2629094555250819\n",
      "epoch 11522: train loss: 0.11449683399912576, test loss: 0.26290983918054045\n",
      "epoch 11523: train loss: 0.1144942972434422, test loss: 0.2629102229224912\n",
      "epoch 11524: train loss: 0.11449176078871752, test loss: 0.262910606750912\n",
      "epoch 11525: train loss: 0.11448922463488753, test loss: 0.2629109906657807\n",
      "epoch 11526: train loss: 0.11448668878188814, test loss: 0.2629113746670752\n",
      "epoch 11527: train loss: 0.11448415322965516, test loss: 0.2629117587547735\n",
      "epoch 11528: train loss: 0.11448161797812448, test loss: 0.2629121429288534\n",
      "epoch 11529: train loss: 0.11447908302723203, test loss: 0.2629125271892928\n",
      "epoch 11530: train loss: 0.1144765483769137, test loss: 0.2629129115360697\n",
      "epoch 11531: train loss: 0.1144740140271055, test loss: 0.262913295969162\n",
      "epoch 11532: train loss: 0.11447147997774337, test loss: 0.2629136804885476\n",
      "epoch 11533: train loss: 0.11446894622876325, test loss: 0.26291406509420434\n",
      "epoch 11534: train loss: 0.1144664127801012, test loss: 0.2629144497861104\n",
      "epoch 11535: train loss: 0.11446387963169319, test loss: 0.2629148345642436\n",
      "epoch 11536: train loss: 0.1144613467834753, test loss: 0.26291521942858187\n",
      "epoch 11537: train loss: 0.1144588142353836, test loss: 0.2629156043791032\n",
      "epoch 11538: train loss: 0.11445628198735415, test loss: 0.26291598941578564\n",
      "epoch 11539: train loss: 0.11445375003932305, test loss: 0.262916374538607\n",
      "epoch 11540: train loss: 0.11445121839122642, test loss: 0.2629167597475454\n",
      "epoch 11541: train loss: 0.11444868704300042, test loss: 0.26291714504257885\n",
      "epoch 11542: train loss: 0.11444615599458118, test loss: 0.2629175304236851\n",
      "epoch 11543: train loss: 0.11444362524590486, test loss: 0.2629179158908425\n",
      "epoch 11544: train loss: 0.1144410947969077, test loss: 0.26291830144402883\n",
      "epoch 11545: train loss: 0.1144385646475259, test loss: 0.2629186870832222\n",
      "epoch 11546: train loss: 0.11443603479769571, test loss: 0.26291907280840054\n",
      "epoch 11547: train loss: 0.11443350524735335, test loss: 0.262919458619542\n",
      "epoch 11548: train loss: 0.11443097599643513, test loss: 0.2629198445166246\n",
      "epoch 11549: train loss: 0.11442844704487733, test loss: 0.2629202304996263\n",
      "epoch 11550: train loss: 0.11442591839261623, test loss: 0.2629206165685251\n",
      "epoch 11551: train loss: 0.11442339003958821, test loss: 0.2629210027232992\n",
      "epoch 11552: train loss: 0.11442086198572955, test loss: 0.2629213889639266\n",
      "epoch 11553: train loss: 0.11441833423097673, test loss: 0.26292177529038535\n",
      "epoch 11554: train loss: 0.11441580677526604, test loss: 0.26292216170265353\n",
      "epoch 11555: train loss: 0.11441327961853393, test loss: 0.26292254820070926\n",
      "epoch 11556: train loss: 0.11441075276071683, test loss: 0.26292293478453055\n",
      "epoch 11557: train loss: 0.11440822620175113, test loss: 0.2629233214540956\n",
      "epoch 11558: train loss: 0.1144056999415734, test loss: 0.2629237082093823\n",
      "epoch 11559: train loss: 0.11440317398012002, test loss: 0.26292409505036907\n",
      "epoch 11560: train loss: 0.11440064831732757, test loss: 0.2629244819770337\n",
      "epoch 11561: train loss: 0.11439812295313252, test loss: 0.2629248689893546\n",
      "epoch 11562: train loss: 0.11439559788747143, test loss: 0.2629252560873096\n",
      "epoch 11563: train loss: 0.11439307312028088, test loss: 0.2629256432708771\n",
      "epoch 11564: train loss: 0.11439054865149742, test loss: 0.26292603054003505\n",
      "epoch 11565: train loss: 0.11438802448105767, test loss: 0.2629264178947617\n",
      "epoch 11566: train loss: 0.11438550060889824, test loss: 0.26292680533503515\n",
      "epoch 11567: train loss: 0.11438297703495576, test loss: 0.26292719286083355\n",
      "epoch 11568: train loss: 0.1143804537591669, test loss: 0.26292758047213505\n",
      "epoch 11569: train loss: 0.11437793078146831, test loss: 0.2629279681689179\n",
      "epoch 11570: train loss: 0.11437540810179671, test loss: 0.26292835595116026\n",
      "epoch 11571: train loss: 0.11437288572008882, test loss: 0.2629287438188402\n",
      "epoch 11572: train loss: 0.11437036363628131, test loss: 0.26292913177193605\n",
      "epoch 11573: train loss: 0.11436784185031103, test loss: 0.26292951981042584\n",
      "epoch 11574: train loss: 0.11436532036211469, test loss: 0.26292990793428794\n",
      "epoch 11575: train loss: 0.11436279917162909, test loss: 0.2629302961435005\n",
      "epoch 11576: train loss: 0.11436027827879101, test loss: 0.2629306844380417\n",
      "epoch 11577: train loss: 0.11435775768353733, test loss: 0.26293107281788974\n",
      "epoch 11578: train loss: 0.11435523738580483, test loss: 0.26293146128302297\n",
      "epoch 11579: train loss: 0.11435271738553049, test loss: 0.2629318498334195\n",
      "epoch 11580: train loss: 0.11435019768265105, test loss: 0.2629322384690576\n",
      "epoch 11581: train loss: 0.11434767827710351, test loss: 0.2629326271899155\n",
      "epoch 11582: train loss: 0.11434515916882476, test loss: 0.2629330159959715\n",
      "epoch 11583: train loss: 0.11434264035775175, test loss: 0.26293340488720385\n",
      "epoch 11584: train loss: 0.11434012184382145, test loss: 0.2629337938635908\n",
      "epoch 11585: train loss: 0.1143376036269708, test loss: 0.2629341829251105\n",
      "epoch 11586: train loss: 0.11433508570713684, test loss: 0.2629345720717415\n",
      "epoch 11587: train loss: 0.11433256808425657, test loss: 0.26293496130346183\n",
      "epoch 11588: train loss: 0.11433005075826702, test loss: 0.2629353506202499\n",
      "epoch 11589: train loss: 0.11432753372910526, test loss: 0.26293574002208403\n",
      "epoch 11590: train loss: 0.11432501699670833, test loss: 0.2629361295089425\n",
      "epoch 11591: train loss: 0.11432250056101338, test loss: 0.2629365190808035\n",
      "epoch 11592: train loss: 0.11431998442195748, test loss: 0.2629369087376455\n",
      "epoch 11593: train loss: 0.11431746857947775, test loss: 0.2629372984794467\n",
      "epoch 11594: train loss: 0.1143149530335114, test loss: 0.2629376883061856\n",
      "epoch 11595: train loss: 0.11431243778399551, test loss: 0.2629380782178404\n",
      "epoch 11596: train loss: 0.11430992283086734, test loss: 0.2629384682143894\n",
      "epoch 11597: train loss: 0.11430740817406405, test loss: 0.262938858295811\n",
      "epoch 11598: train loss: 0.11430489381352292, test loss: 0.26293924846208355\n",
      "epoch 11599: train loss: 0.11430237974918113, test loss: 0.2629396387131855\n",
      "epoch 11600: train loss: 0.11429986598097595, test loss: 0.2629400290490951\n",
      "epoch 11601: train loss: 0.11429735250884471, test loss: 0.2629404194697907\n",
      "epoch 11602: train loss: 0.11429483933272469, test loss: 0.26294080997525077\n",
      "epoch 11603: train loss: 0.11429232645255315, test loss: 0.2629412005654537\n",
      "epoch 11604: train loss: 0.11428981386826752, test loss: 0.26294159124037786\n",
      "epoch 11605: train loss: 0.11428730157980509, test loss: 0.2629419820000015\n",
      "epoch 11606: train loss: 0.1142847895871033, test loss: 0.26294237284430333\n",
      "epoch 11607: train loss: 0.11428227789009945, test loss: 0.2629427637732615\n",
      "epoch 11608: train loss: 0.11427976648873102, test loss: 0.26294315478685454\n",
      "epoch 11609: train loss: 0.11427725538293544, test loss: 0.26294354588506075\n",
      "epoch 11610: train loss: 0.11427474457265013, test loss: 0.26294393706785874\n",
      "epoch 11611: train loss: 0.11427223405781259, test loss: 0.26294432833522674\n",
      "epoch 11612: train loss: 0.11426972383836027, test loss: 0.2629447196871434\n",
      "epoch 11613: train loss: 0.11426721391423073, test loss: 0.26294511112358704\n",
      "epoch 11614: train loss: 0.11426470428536145, test loss: 0.26294550264453614\n",
      "epoch 11615: train loss: 0.11426219495169, test loss: 0.2629458942499692\n",
      "epoch 11616: train loss: 0.11425968591315393, test loss: 0.2629462859398647\n",
      "epoch 11617: train loss: 0.11425717716969079, test loss: 0.26294667771420094\n",
      "epoch 11618: train loss: 0.11425466872123824, test loss: 0.2629470695729566\n",
      "epoch 11619: train loss: 0.11425216056773385, test loss: 0.26294746151611004\n",
      "epoch 11620: train loss: 0.1142496527091153, test loss: 0.26294785354363986\n",
      "epoch 11621: train loss: 0.11424714514532021, test loss: 0.26294824565552444\n",
      "epoch 11622: train loss: 0.11424463787628629, test loss: 0.2629486378517424\n",
      "epoch 11623: train loss: 0.11424213090195118, test loss: 0.2629490301322721\n",
      "epoch 11624: train loss: 0.11423962422225264, test loss: 0.26294942249709213\n",
      "epoch 11625: train loss: 0.1142371178371284, test loss: 0.26294981494618114\n",
      "epoch 11626: train loss: 0.11423461174651617, test loss: 0.2629502074795175\n",
      "epoch 11627: train loss: 0.11423210595035375, test loss: 0.26295060009707977\n",
      "epoch 11628: train loss: 0.11422960044857891, test loss: 0.26295099279884654\n",
      "epoch 11629: train loss: 0.11422709524112948, test loss: 0.2629513855847963\n",
      "epoch 11630: train loss: 0.11422459032794326, test loss: 0.26295177845490764\n",
      "epoch 11631: train loss: 0.1142220857089581, test loss: 0.2629521714091591\n",
      "epoch 11632: train loss: 0.11421958138411185, test loss: 0.2629525644475293\n",
      "epoch 11633: train loss: 0.11421707735334241, test loss: 0.26295295756999676\n",
      "epoch 11634: train loss: 0.11421457361658767, test loss: 0.26295335077654003\n",
      "epoch 11635: train loss: 0.11421207017378557, test loss: 0.26295374406713773\n",
      "epoch 11636: train loss: 0.11420956702487399, test loss: 0.26295413744176854\n",
      "epoch 11637: train loss: 0.1142070641697909, test loss: 0.2629545309004109\n",
      "epoch 11638: train loss: 0.11420456160847432, test loss: 0.2629549244430436\n",
      "epoch 11639: train loss: 0.1142020593408622, test loss: 0.26295531806964495\n",
      "epoch 11640: train loss: 0.11419955736689257, test loss: 0.2629557117801939\n",
      "epoch 11641: train loss: 0.11419705568650343, test loss: 0.2629561055746688\n",
      "epoch 11642: train loss: 0.11419455429963282, test loss: 0.2629564994530485\n",
      "epoch 11643: train loss: 0.11419205320621886, test loss: 0.26295689341531153\n",
      "epoch 11644: train loss: 0.11418955240619959, test loss: 0.26295728746143654\n",
      "epoch 11645: train loss: 0.11418705189951311, test loss: 0.26295768159140215\n",
      "epoch 11646: train loss: 0.11418455168609758, test loss: 0.26295807580518704\n",
      "epoch 11647: train loss: 0.11418205176589108, test loss: 0.26295847010276985\n",
      "epoch 11648: train loss: 0.11417955213883182, test loss: 0.26295886448412925\n",
      "epoch 11649: train loss: 0.11417705280485795, test loss: 0.26295925894924393\n",
      "epoch 11650: train loss: 0.1141745537639077, test loss: 0.2629596534980925\n",
      "epoch 11651: train loss: 0.11417205501591923, test loss: 0.2629600481306538\n",
      "epoch 11652: train loss: 0.11416955656083079, test loss: 0.2629604428469065\n",
      "epoch 11653: train loss: 0.11416705839858063, test loss: 0.26296083764682915\n",
      "epoch 11654: train loss: 0.11416456052910703, test loss: 0.2629612325304005\n",
      "epoch 11655: train loss: 0.11416206295234826, test loss: 0.26296162749759927\n",
      "epoch 11656: train loss: 0.11415956566824265, test loss: 0.2629620225484042\n",
      "epoch 11657: train loss: 0.11415706867672851, test loss: 0.262962417682794\n",
      "epoch 11658: train loss: 0.11415457197774417, test loss: 0.26296281290074747\n",
      "epoch 11659: train loss: 0.11415207557122801, test loss: 0.26296320820224317\n",
      "epoch 11660: train loss: 0.11414957945711839, test loss: 0.26296360358726\n",
      "epoch 11661: train loss: 0.11414708363535374, test loss: 0.26296399905577666\n",
      "epoch 11662: train loss: 0.11414458810587243, test loss: 0.2629643946077719\n",
      "epoch 11663: train loss: 0.11414209286861293, test loss: 0.26296479024322433\n",
      "epoch 11664: train loss: 0.11413959792351366, test loss: 0.26296518596211294\n",
      "epoch 11665: train loss: 0.11413710327051313, test loss: 0.2629655817644165\n",
      "epoch 11666: train loss: 0.1141346089095498, test loss: 0.26296597765011354\n",
      "epoch 11667: train loss: 0.1141321148405622, test loss: 0.2629663736191831\n",
      "epoch 11668: train loss: 0.11412962106348883, test loss: 0.2629667696716038\n",
      "epoch 11669: train loss: 0.11412712757826825, test loss: 0.2629671658073546\n",
      "epoch 11670: train loss: 0.11412463438483901, test loss: 0.26296756202641414\n",
      "epoch 11671: train loss: 0.11412214148313972, test loss: 0.26296795832876135\n",
      "epoch 11672: train loss: 0.11411964887310891, test loss: 0.2629683547143749\n",
      "epoch 11673: train loss: 0.11411715655468528, test loss: 0.2629687511832337\n",
      "epoch 11674: train loss: 0.11411466452780744, test loss: 0.26296914773531666\n",
      "epoch 11675: train loss: 0.11411217279241398, test loss: 0.2629695443706025\n",
      "epoch 11676: train loss: 0.1141096813484437, test loss: 0.26296994108907\n",
      "epoch 11677: train loss: 0.11410719019583518, test loss: 0.26297033789069824\n",
      "epoch 11678: train loss: 0.11410469933452717, test loss: 0.2629707347754658\n",
      "epoch 11679: train loss: 0.1141022087644584, test loss: 0.2629711317433517\n",
      "epoch 11680: train loss: 0.11409971848556762, test loss: 0.26297152879433466\n",
      "epoch 11681: train loss: 0.11409722849779355, test loss: 0.2629719259283938\n",
      "epoch 11682: train loss: 0.11409473880107501, test loss: 0.2629723231455078\n",
      "epoch 11683: train loss: 0.11409224939535083, test loss: 0.26297272044565545\n",
      "epoch 11684: train loss: 0.11408976028055974, test loss: 0.26297311782881594\n",
      "epoch 11685: train loss: 0.11408727145664066, test loss: 0.2629735152949679\n",
      "epoch 11686: train loss: 0.11408478292353241, test loss: 0.2629739128440903\n",
      "epoch 11687: train loss: 0.11408229468117387, test loss: 0.2629743104761621\n",
      "epoch 11688: train loss: 0.11407980672950391, test loss: 0.2629747081911622\n",
      "epoch 11689: train loss: 0.11407731906846146, test loss: 0.2629751059890695\n",
      "epoch 11690: train loss: 0.11407483169798543, test loss: 0.2629755038698629\n",
      "epoch 11691: train loss: 0.11407234461801481, test loss: 0.2629759018335213\n",
      "epoch 11692: train loss: 0.11406985782848852, test loss: 0.2629762998800238\n",
      "epoch 11693: train loss: 0.11406737132934555, test loss: 0.26297669800934914\n",
      "epoch 11694: train loss: 0.1140648851205249, test loss: 0.26297709622147636\n",
      "epoch 11695: train loss: 0.11406239920196562, test loss: 0.2629774945163844\n",
      "epoch 11696: train loss: 0.11405991357360669, test loss: 0.26297789289405227\n",
      "epoch 11697: train loss: 0.1140574282353872, test loss: 0.2629782913544588\n",
      "epoch 11698: train loss: 0.11405494318724624, test loss: 0.2629786898975832\n",
      "epoch 11699: train loss: 0.11405245842912287, test loss: 0.2629790885234043\n",
      "epoch 11700: train loss: 0.11404997396095616, test loss: 0.262979487231901\n",
      "epoch 11701: train loss: 0.11404748978268535, test loss: 0.2629798860230524\n",
      "epoch 11702: train loss: 0.11404500589424947, test loss: 0.2629802848968375\n",
      "epoch 11703: train loss: 0.11404252229558776, test loss: 0.26298068385323525\n",
      "epoch 11704: train loss: 0.11404003898663934, test loss: 0.26298108289222477\n",
      "epoch 11705: train loss: 0.11403755596734348, test loss: 0.2629814820137849\n",
      "epoch 11706: train loss: 0.11403507323763934, test loss: 0.26298188121789484\n",
      "epoch 11707: train loss: 0.11403259079746618, test loss: 0.2629822805045335\n",
      "epoch 11708: train loss: 0.11403010864676323, test loss: 0.2629826798736799\n",
      "epoch 11709: train loss: 0.1140276267854698, test loss: 0.2629830793253132\n",
      "epoch 11710: train loss: 0.11402514521352515, test loss: 0.2629834788594123\n",
      "epoch 11711: train loss: 0.11402266393086861, test loss: 0.2629838784759564\n",
      "epoch 11712: train loss: 0.1140201829374395, test loss: 0.2629842781749245\n",
      "epoch 11713: train loss: 0.11401770223317712, test loss: 0.26298467795629554\n",
      "epoch 11714: train loss: 0.11401522181802091, test loss: 0.26298507782004876\n",
      "epoch 11715: train loss: 0.1140127416919102, test loss: 0.26298547776616327\n",
      "epoch 11716: train loss: 0.11401026185478438, test loss: 0.26298587779461785\n",
      "epoch 11717: train loss: 0.1140077823065829, test loss: 0.2629862779053919\n",
      "epoch 11718: train loss: 0.11400530304724515, test loss: 0.2629866780984644\n",
      "epoch 11719: train loss: 0.11400282407671064, test loss: 0.2629870783738144\n",
      "epoch 11720: train loss: 0.11400034539491877, test loss: 0.26298747873142114\n",
      "epoch 11721: train loss: 0.1139978670018091, test loss: 0.2629878791712636\n",
      "epoch 11722: train loss: 0.11399538889732108, test loss: 0.26298827969332095\n",
      "epoch 11723: train loss: 0.11399291108139425, test loss: 0.2629886802975723\n",
      "epoch 11724: train loss: 0.11399043355396817, test loss: 0.26298908098399687\n",
      "epoch 11725: train loss: 0.11398795631498239, test loss: 0.26298948175257364\n",
      "epoch 11726: train loss: 0.11398547936437647, test loss: 0.2629898826032818\n",
      "epoch 11727: train loss: 0.11398300270209002, test loss: 0.2629902835361006\n",
      "epoch 11728: train loss: 0.11398052632806266, test loss: 0.2629906845510091\n",
      "epoch 11729: train loss: 0.11397805024223398, test loss: 0.2629910856479865\n",
      "epoch 11730: train loss: 0.1139755744445437, test loss: 0.2629914868270119\n",
      "epoch 11731: train loss: 0.11397309893493145, test loss: 0.2629918880880646\n",
      "epoch 11732: train loss: 0.1139706237133369, test loss: 0.2629922894311236\n",
      "epoch 11733: train loss: 0.11396814877969977, test loss: 0.26299269085616833\n",
      "epoch 11734: train loss: 0.11396567413395978, test loss: 0.2629930923631777\n",
      "epoch 11735: train loss: 0.11396319977605664, test loss: 0.2629934939521311\n",
      "epoch 11736: train loss: 0.11396072570593013, test loss: 0.2629938956230076\n",
      "epoch 11737: train loss: 0.11395825192352006, test loss: 0.26299429737578656\n",
      "epoch 11738: train loss: 0.1139557784287662, test loss: 0.2629946992104471\n",
      "epoch 11739: train loss: 0.11395330522160829, test loss: 0.26299510112696844\n",
      "epoch 11740: train loss: 0.11395083230198626, test loss: 0.26299550312532977\n",
      "epoch 11741: train loss: 0.1139483596698399, test loss: 0.26299590520551047\n",
      "epoch 11742: train loss: 0.11394588732510909, test loss: 0.2629963073674897\n",
      "epoch 11743: train loss: 0.1139434152677337, test loss: 0.2629967096112466\n",
      "epoch 11744: train loss: 0.11394094349765364, test loss: 0.26299711193676045\n",
      "epoch 11745: train loss: 0.11393847201480885, test loss: 0.26299751434401064\n",
      "epoch 11746: train loss: 0.1139360008191392, test loss: 0.26299791683297635\n",
      "epoch 11747: train loss: 0.11393352991058467, test loss: 0.2629983194036368\n",
      "epoch 11748: train loss: 0.11393105928908526, test loss: 0.2629987220559714\n",
      "epoch 11749: train loss: 0.11392858895458094, test loss: 0.2629991247899593\n",
      "epoch 11750: train loss: 0.11392611890701171, test loss: 0.26299952760557976\n",
      "epoch 11751: train loss: 0.1139236491463176, test loss: 0.26299993050281223\n",
      "epoch 11752: train loss: 0.11392117967243864, test loss: 0.26300033348163593\n",
      "epoch 11753: train loss: 0.1139187104853149, test loss: 0.26300073654203016\n",
      "epoch 11754: train loss: 0.11391624158488646, test loss: 0.2630011396839742\n",
      "epoch 11755: train loss: 0.1139137729710934, test loss: 0.26300154290744737\n",
      "epoch 11756: train loss: 0.11391130464387586, test loss: 0.263001946212429\n",
      "epoch 11757: train loss: 0.11390883660317393, test loss: 0.2630023495988985\n",
      "epoch 11758: train loss: 0.11390636884892778, test loss: 0.26300275306683507\n",
      "epoch 11759: train loss: 0.1139039013810776, test loss: 0.26300315661621815\n",
      "epoch 11760: train loss: 0.11390143419956353, test loss: 0.263003560247027\n",
      "epoch 11761: train loss: 0.11389896730432579, test loss: 0.263003963959241\n",
      "epoch 11762: train loss: 0.11389650069530458, test loss: 0.2630043677528396\n",
      "epoch 11763: train loss: 0.11389403437244017, test loss: 0.26300477162780206\n",
      "epoch 11764: train loss: 0.1138915683356728, test loss: 0.2630051755841078\n",
      "epoch 11765: train loss: 0.11388910258494274, test loss: 0.26300557962173615\n",
      "epoch 11766: train loss: 0.11388663712019026, test loss: 0.2630059837406665\n",
      "epoch 11767: train loss: 0.11388417194135571, test loss: 0.26300638794087833\n",
      "epoch 11768: train loss: 0.1138817070483794, test loss: 0.26300679222235085\n",
      "epoch 11769: train loss: 0.11387924244120161, test loss: 0.2630071965850636\n",
      "epoch 11770: train loss: 0.11387677811976277, test loss: 0.26300760102899595\n",
      "epoch 11771: train loss: 0.11387431408400324, test loss: 0.2630080055541274\n",
      "epoch 11772: train loss: 0.11387185033386342, test loss: 0.2630084101604372\n",
      "epoch 11773: train loss: 0.11386938686928373, test loss: 0.2630088148479049\n",
      "epoch 11774: train loss: 0.11386692369020457, test loss: 0.2630092196165098\n",
      "epoch 11775: train loss: 0.1138644607965664, test loss: 0.26300962446623144\n",
      "epoch 11776: train loss: 0.11386199818830971, test loss: 0.2630100293970493\n",
      "epoch 11777: train loss: 0.11385953586537494, test loss: 0.2630104344089427\n",
      "epoch 11778: train loss: 0.11385707382770263, test loss: 0.26301083950189114\n",
      "epoch 11779: train loss: 0.11385461207523327, test loss: 0.26301124467587406\n",
      "epoch 11780: train loss: 0.11385215060790742, test loss: 0.263011649930871\n",
      "epoch 11781: train loss: 0.11384968942566562, test loss: 0.2630120552668614\n",
      "epoch 11782: train loss: 0.11384722852844842, test loss: 0.26301246068382467\n",
      "epoch 11783: train loss: 0.11384476791619648, test loss: 0.2630128661817404\n",
      "epoch 11784: train loss: 0.11384230758885028, test loss: 0.26301327176058803\n",
      "epoch 11785: train loss: 0.11383984754635054, test loss: 0.263013677420347\n",
      "epoch 11786: train loss: 0.11383738778863794, test loss: 0.2630140831609969\n",
      "epoch 11787: train loss: 0.11383492831565302, test loss: 0.26301448898251717\n",
      "epoch 11788: train loss: 0.11383246912733654, test loss: 0.26301489488488733\n",
      "epoch 11789: train loss: 0.11383001022362917, test loss: 0.26301530086808694\n",
      "epoch 11790: train loss: 0.11382755160447161, test loss: 0.26301570693209547\n",
      "epoch 11791: train loss: 0.1138250932698046, test loss: 0.2630161130768925\n",
      "epoch 11792: train loss: 0.11382263521956888, test loss: 0.26301651930245745\n",
      "epoch 11793: train loss: 0.11382017745370523, test loss: 0.26301692560877\n",
      "epoch 11794: train loss: 0.11381771997215441, test loss: 0.2630173319958097\n",
      "epoch 11795: train loss: 0.11381526277485725, test loss: 0.2630177384635559\n",
      "epoch 11796: train loss: 0.11381280586175452, test loss: 0.2630181450119885\n",
      "epoch 11797: train loss: 0.11381034923278709, test loss: 0.2630185516410867\n",
      "epoch 11798: train loss: 0.11380789288789575, test loss: 0.2630189583508303\n",
      "epoch 11799: train loss: 0.11380543682702146, test loss: 0.2630193651411988\n",
      "epoch 11800: train loss: 0.11380298105010507, test loss: 0.26301977201217186\n",
      "epoch 11801: train loss: 0.11380052555708745, test loss: 0.2630201789637289\n",
      "epoch 11802: train loss: 0.11379807034790955, test loss: 0.26302058599584976\n",
      "epoch 11803: train loss: 0.11379561542251232, test loss: 0.26302099310851373\n",
      "epoch 11804: train loss: 0.11379316078083666, test loss: 0.2630214003017006\n",
      "epoch 11805: train loss: 0.11379070642282363, test loss: 0.26302180757539007\n",
      "epoch 11806: train loss: 0.11378825234841415, test loss: 0.2630222149295616\n",
      "epoch 11807: train loss: 0.11378579855754924, test loss: 0.2630226223641949\n",
      "epoch 11808: train loss: 0.11378334505016995, test loss: 0.2630230298792695\n",
      "epoch 11809: train loss: 0.11378089182621728, test loss: 0.26302343747476514\n",
      "epoch 11810: train loss: 0.11377843888563233, test loss: 0.26302384515066135\n",
      "epoch 11811: train loss: 0.11377598622835616, test loss: 0.263024252906938\n",
      "epoch 11812: train loss: 0.11377353385432988, test loss: 0.26302466074357445\n",
      "epoch 11813: train loss: 0.1137710817634946, test loss: 0.2630250686605505\n",
      "epoch 11814: train loss: 0.11376862995579139, test loss: 0.2630254766578458\n",
      "epoch 11815: train loss: 0.11376617843116148, test loss: 0.26302588473544014\n",
      "epoch 11816: train loss: 0.11376372718954599, test loss: 0.2630262928933129\n",
      "epoch 11817: train loss: 0.11376127623088611, test loss: 0.2630267011314441\n",
      "epoch 11818: train loss: 0.11375882555512304, test loss: 0.2630271094498131\n",
      "epoch 11819: train loss: 0.113756375162198, test loss: 0.2630275178484\n",
      "epoch 11820: train loss: 0.1137539250520522, test loss: 0.26302792632718414\n",
      "epoch 11821: train loss: 0.1137514752246269, test loss: 0.26302833488614535\n",
      "epoch 11822: train loss: 0.11374902567986339, test loss: 0.2630287435252633\n",
      "epoch 11823: train loss: 0.11374657641770294, test loss: 0.2630291522445178\n",
      "epoch 11824: train loss: 0.11374412743808687, test loss: 0.26302956104388847\n",
      "epoch 11825: train loss: 0.11374167874095646, test loss: 0.26302996992335514\n",
      "epoch 11826: train loss: 0.11373923032625306, test loss: 0.26303037888289743\n",
      "epoch 11827: train loss: 0.11373678219391803, test loss: 0.2630307879224951\n",
      "epoch 11828: train loss: 0.11373433434389274, test loss: 0.26303119704212813\n",
      "epoch 11829: train loss: 0.1137318867761186, test loss: 0.26303160624177585\n",
      "epoch 11830: train loss: 0.11372943949053697, test loss: 0.2630320155214183\n",
      "epoch 11831: train loss: 0.11372699248708931, test loss: 0.26303242488103523\n",
      "epoch 11832: train loss: 0.11372454576571706, test loss: 0.26303283432060637\n",
      "epoch 11833: train loss: 0.11372209932636165, test loss: 0.2630332438401115\n",
      "epoch 11834: train loss: 0.11371965316896461, test loss: 0.2630336534395303\n",
      "epoch 11835: train loss: 0.11371720729346733, test loss: 0.26303406311884264\n",
      "epoch 11836: train loss: 0.11371476169981143, test loss: 0.26303447287802845\n",
      "epoch 11837: train loss: 0.11371231638793837, test loss: 0.26303488271706726\n",
      "epoch 11838: train loss: 0.11370987135778972, test loss: 0.263035292635939\n",
      "epoch 11839: train loss: 0.11370742660930701, test loss: 0.26303570263462356\n",
      "epoch 11840: train loss: 0.11370498214243184, test loss: 0.2630361127131006\n",
      "epoch 11841: train loss: 0.11370253795710582, test loss: 0.2630365228713501\n",
      "epoch 11842: train loss: 0.11370009405327053, test loss: 0.26303693310935183\n",
      "epoch 11843: train loss: 0.11369765043086762, test loss: 0.26303734342708546\n",
      "epoch 11844: train loss: 0.11369520708983874, test loss: 0.26303775382453104\n",
      "epoch 11845: train loss: 0.11369276403012554, test loss: 0.2630381643016684\n",
      "epoch 11846: train loss: 0.11369032125166971, test loss: 0.26303857485847726\n",
      "epoch 11847: train loss: 0.11368787875441294, test loss: 0.26303898549493754\n",
      "epoch 11848: train loss: 0.11368543653829695, test loss: 0.26303939621102906\n",
      "epoch 11849: train loss: 0.11368299460326345, test loss: 0.26303980700673185\n",
      "epoch 11850: train loss: 0.11368055294925422, test loss: 0.26304021788202564\n",
      "epoch 11851: train loss: 0.11367811157621102, test loss: 0.2630406288368903\n",
      "epoch 11852: train loss: 0.11367567048407562, test loss: 0.2630410398713057\n",
      "epoch 11853: train loss: 0.11367322967278984, test loss: 0.26304145098525183\n",
      "epoch 11854: train loss: 0.11367078914229545, test loss: 0.2630418621787085\n",
      "epoch 11855: train loss: 0.11366834889253435, test loss: 0.2630422734516557\n",
      "epoch 11856: train loss: 0.11366590892344836, test loss: 0.26304268480407317\n",
      "epoch 11857: train loss: 0.11366346923497933, test loss: 0.2630430962359411\n",
      "epoch 11858: train loss: 0.11366102982706919, test loss: 0.26304350774723917\n",
      "epoch 11859: train loss: 0.11365859069965978, test loss: 0.26304391933794724\n",
      "epoch 11860: train loss: 0.1136561518526931, test loss: 0.26304433100804553\n",
      "epoch 11861: train loss: 0.11365371328611101, test loss: 0.2630447427575138\n",
      "epoch 11862: train loss: 0.11365127499985551, test loss: 0.263045154586332\n",
      "epoch 11863: train loss: 0.11364883699386855, test loss: 0.2630455664944801\n",
      "epoch 11864: train loss: 0.11364639926809214, test loss: 0.26304597848193806\n",
      "epoch 11865: train loss: 0.11364396182246826, test loss: 0.2630463905486858\n",
      "epoch 11866: train loss: 0.11364152465693894, test loss: 0.26304680269470326\n",
      "epoch 11867: train loss: 0.11363908777144623, test loss: 0.2630472149199705\n",
      "epoch 11868: train loss: 0.11363665116593218, test loss: 0.26304762722446745\n",
      "epoch 11869: train loss: 0.11363421484033885, test loss: 0.2630480396081742\n",
      "epoch 11870: train loss: 0.11363177879460834, test loss: 0.2630484520710705\n",
      "epoch 11871: train loss: 0.11362934302868276, test loss: 0.2630488646131365\n",
      "epoch 11872: train loss: 0.1136269075425042, test loss: 0.2630492772343522\n",
      "epoch 11873: train loss: 0.11362447233601486, test loss: 0.2630496899346976\n",
      "epoch 11874: train loss: 0.11362203740915687, test loss: 0.26305010271415263\n",
      "epoch 11875: train loss: 0.1136196027618724, test loss: 0.2630505155726974\n",
      "epoch 11876: train loss: 0.11361716839410363, test loss: 0.2630509285103118\n",
      "epoch 11877: train loss: 0.11361473430579277, test loss: 0.26305134152697607\n",
      "epoch 11878: train loss: 0.11361230049688208, test loss: 0.26305175462267016\n",
      "epoch 11879: train loss: 0.11360986696731377, test loss: 0.26305216779737406\n",
      "epoch 11880: train loss: 0.11360743371703011, test loss: 0.2630525810510677\n",
      "epoch 11881: train loss: 0.11360500074597336, test loss: 0.2630529943837314\n",
      "epoch 11882: train loss: 0.11360256805408586, test loss: 0.26305340779534503\n",
      "epoch 11883: train loss: 0.11360013564130987, test loss: 0.2630538212858887\n",
      "epoch 11884: train loss: 0.1135977035075877, test loss: 0.26305423485534246\n",
      "epoch 11885: train loss: 0.11359527165286178, test loss: 0.26305464850368643\n",
      "epoch 11886: train loss: 0.1135928400770744, test loss: 0.26305506223090064\n",
      "epoch 11887: train loss: 0.11359040878016793, test loss: 0.2630554760369652\n",
      "epoch 11888: train loss: 0.11358797776208483, test loss: 0.26305588992186024\n",
      "epoch 11889: train loss: 0.11358554702276744, test loss: 0.2630563038855657\n",
      "epoch 11890: train loss: 0.11358311656215822, test loss: 0.26305671792806185\n",
      "epoch 11891: train loss: 0.11358068638019962, test loss: 0.2630571320493288\n",
      "epoch 11892: train loss: 0.11357825647683409, test loss: 0.2630575462493465\n",
      "epoch 11893: train loss: 0.1135758268520041, test loss: 0.2630579605280952\n",
      "epoch 11894: train loss: 0.11357339750565217, test loss: 0.26305837488555506\n",
      "epoch 11895: train loss: 0.1135709684377208, test loss: 0.263058789321706\n",
      "epoch 11896: train loss: 0.11356853964815251, test loss: 0.2630592038365284\n",
      "epoch 11897: train loss: 0.11356611113688986, test loss: 0.2630596184300023\n",
      "epoch 11898: train loss: 0.11356368290387542, test loss: 0.26306003310210785\n",
      "epoch 11899: train loss: 0.11356125494905171, test loss: 0.2630604478528251\n",
      "epoch 11900: train loss: 0.11355882727236143, test loss: 0.2630608626821344\n",
      "epoch 11901: train loss: 0.1135563998737471, test loss: 0.26306127759001574\n",
      "epoch 11902: train loss: 0.11355397275315138, test loss: 0.26306169257644946\n",
      "epoch 11903: train loss: 0.11355154591051693, test loss: 0.26306210764141563\n",
      "epoch 11904: train loss: 0.1135491193457864, test loss: 0.2630625227848945\n",
      "epoch 11905: train loss: 0.11354669305890244, test loss: 0.2630629380068661\n",
      "epoch 11906: train loss: 0.11354426704980783, test loss: 0.26306335330731073\n",
      "epoch 11907: train loss: 0.11354184131844523, test loss: 0.26306376868620857\n",
      "epoch 11908: train loss: 0.11353941586475734, test loss: 0.26306418414353994\n",
      "epoch 11909: train loss: 0.11353699068868695, test loss: 0.26306459967928486\n",
      "epoch 11910: train loss: 0.11353456579017682, test loss: 0.26306501529342363\n",
      "epoch 11911: train loss: 0.11353214116916971, test loss: 0.2630654309859365\n",
      "epoch 11912: train loss: 0.11352971682560843, test loss: 0.2630658467568036\n",
      "epoch 11913: train loss: 0.11352729275943581, test loss: 0.2630662626060053\n",
      "epoch 11914: train loss: 0.11352486897059463, test loss: 0.2630666785335217\n",
      "epoch 11915: train loss: 0.11352244545902782, test loss: 0.26306709453933325\n",
      "epoch 11916: train loss: 0.11352002222467818, test loss: 0.26306751062341993\n",
      "epoch 11917: train loss: 0.11351759926748856, test loss: 0.26306792678576213\n",
      "epoch 11918: train loss: 0.11351517658740193, test loss: 0.2630683430263401\n",
      "epoch 11919: train loss: 0.11351275418436117, test loss: 0.26306875934513413\n",
      "epoch 11920: train loss: 0.11351033205830924, test loss: 0.26306917574212446\n",
      "epoch 11921: train loss: 0.11350791020918903, test loss: 0.2630695922172913\n",
      "epoch 11922: train loss: 0.11350548863694356, test loss: 0.2630700087706151\n",
      "epoch 11923: train loss: 0.11350306734151575, test loss: 0.26307042540207604\n",
      "epoch 11924: train loss: 0.11350064632284866, test loss: 0.2630708421116544\n",
      "epoch 11925: train loss: 0.11349822558088526, test loss: 0.26307125889933053\n",
      "epoch 11926: train loss: 0.11349580511556862, test loss: 0.26307167576508467\n",
      "epoch 11927: train loss: 0.11349338492684176, test loss: 0.2630720927088972\n",
      "epoch 11928: train loss: 0.11349096501464771, test loss: 0.2630725097307484\n",
      "epoch 11929: train loss: 0.11348854537892962, test loss: 0.26307292683061867\n",
      "epoch 11930: train loss: 0.11348612601963054, test loss: 0.26307334400848814\n",
      "epoch 11931: train loss: 0.11348370693669359, test loss: 0.2630737612643373\n",
      "epoch 11932: train loss: 0.11348128813006192, test loss: 0.26307417859814647\n",
      "epoch 11933: train loss: 0.11347886959967866, test loss: 0.263074596009896\n",
      "epoch 11934: train loss: 0.11347645134548695, test loss: 0.26307501349956625\n",
      "epoch 11935: train loss: 0.11347403336743003, test loss: 0.26307543106713743\n",
      "epoch 11936: train loss: 0.11347161566545103, test loss: 0.2630758487125901\n",
      "epoch 11937: train loss: 0.11346919823949321, test loss: 0.26307626643590454\n",
      "epoch 11938: train loss: 0.11346678108949977, test loss: 0.2630766842370611\n",
      "epoch 11939: train loss: 0.11346436421541398, test loss: 0.26307710211604013\n",
      "epoch 11940: train loss: 0.11346194761717909, test loss: 0.2630775200728221\n",
      "epoch 11941: train loss: 0.11345953129473836, test loss: 0.2630779381073874\n",
      "epoch 11942: train loss: 0.1134571152480351, test loss: 0.2630783562197164\n",
      "epoch 11943: train loss: 0.11345469947701264, test loss: 0.2630787744097895\n",
      "epoch 11944: train loss: 0.11345228398161429, test loss: 0.263079192677587\n",
      "epoch 11945: train loss: 0.1134498687617834, test loss: 0.2630796110230894\n",
      "epoch 11946: train loss: 0.11344745381746332, test loss: 0.2630800294462772\n",
      "epoch 11947: train loss: 0.11344503914859744, test loss: 0.2630804479471307\n",
      "epoch 11948: train loss: 0.11344262475512915, test loss: 0.2630808665256304\n",
      "epoch 11949: train loss: 0.11344021063700187, test loss: 0.26308128518175666\n",
      "epoch 11950: train loss: 0.11343779679415901, test loss: 0.26308170391548996\n",
      "epoch 11951: train loss: 0.11343538322654399, test loss: 0.26308212272681075\n",
      "epoch 11952: train loss: 0.11343296993410036, test loss: 0.2630825416156995\n",
      "epoch 11953: train loss: 0.11343055691677151, test loss: 0.26308296058213665\n",
      "epoch 11954: train loss: 0.11342814417450095, test loss: 0.2630833796261026\n",
      "epoch 11955: train loss: 0.1134257317072322, test loss: 0.2630837987475779\n",
      "epoch 11956: train loss: 0.1134233195149088, test loss: 0.2630842179465429\n",
      "epoch 11957: train loss: 0.11342090759747427, test loss: 0.2630846372229783\n",
      "epoch 11958: train loss: 0.11341849595487219, test loss: 0.26308505657686443\n",
      "epoch 11959: train loss: 0.11341608458704609, test loss: 0.2630854760081817\n",
      "epoch 11960: train loss: 0.11341367349393963, test loss: 0.2630858955169108\n",
      "epoch 11961: train loss: 0.11341126267549637, test loss: 0.2630863151030321\n",
      "epoch 11962: train loss: 0.11340885213165994, test loss: 0.2630867347665261\n",
      "epoch 11963: train loss: 0.11340644186237397, test loss: 0.2630871545073734\n",
      "epoch 11964: train loss: 0.11340403186758216, test loss: 0.2630875743255545\n",
      "epoch 11965: train loss: 0.11340162214722813, test loss: 0.26308799422104984\n",
      "epoch 11966: train loss: 0.11339921270125561, test loss: 0.26308841419384\n",
      "epoch 11967: train loss: 0.11339680352960829, test loss: 0.2630888342439055\n",
      "epoch 11968: train loss: 0.11339439463222992, test loss: 0.2630892543712269\n",
      "epoch 11969: train loss: 0.11339198600906418, test loss: 0.2630896745757847\n",
      "epoch 11970: train loss: 0.11338957766005488, test loss: 0.2630900948575595\n",
      "epoch 11971: train loss: 0.11338716958514575, test loss: 0.2630905152165319\n",
      "epoch 11972: train loss: 0.1133847617842806, test loss: 0.26309093565268227\n",
      "epoch 11973: train loss: 0.11338235425740324, test loss: 0.26309135616599133\n",
      "epoch 11974: train loss: 0.1133799470044575, test loss: 0.26309177675643963\n",
      "epoch 11975: train loss: 0.11337754002538716, test loss: 0.26309219742400786\n",
      "epoch 11976: train loss: 0.11337513332013614, test loss: 0.2630926181686764\n",
      "epoch 11977: train loss: 0.11337272688864829, test loss: 0.2630930389904259\n",
      "epoch 11978: train loss: 0.11337032073086746, test loss: 0.26309345988923705\n",
      "epoch 11979: train loss: 0.11336791484673757, test loss: 0.2630938808650902\n",
      "epoch 11980: train loss: 0.11336550923620259, test loss: 0.26309430191796634\n",
      "epoch 11981: train loss: 0.11336310389920641, test loss: 0.26309472304784576\n",
      "epoch 11982: train loss: 0.11336069883569294, test loss: 0.2630951442547092\n",
      "epoch 11983: train loss: 0.11335829404560624, test loss: 0.2630955655385373\n",
      "epoch 11984: train loss: 0.11335588952889018, test loss: 0.2630959868993106\n",
      "epoch 11985: train loss: 0.11335348528548889, test loss: 0.26309640833700976\n",
      "epoch 11986: train loss: 0.1133510813153463, test loss: 0.26309682985161553\n",
      "epoch 11987: train loss: 0.11334867761840645, test loss: 0.2630972514431084\n",
      "epoch 11988: train loss: 0.11334627419461339, test loss: 0.2630976731114692\n",
      "epoch 11989: train loss: 0.11334387104391123, test loss: 0.26309809485667834\n",
      "epoch 11990: train loss: 0.11334146816624396, test loss: 0.2630985166787167\n",
      "epoch 11991: train loss: 0.11333906556155578, test loss: 0.2630989385775648\n",
      "epoch 11992: train loss: 0.11333666322979073, test loss: 0.2630993605532034\n",
      "epoch 11993: train loss: 0.113334261170893, test loss: 0.26309978260561306\n",
      "epoch 11994: train loss: 0.11333185938480667, test loss: 0.26310020473477463\n",
      "epoch 11995: train loss: 0.11332945787147594, test loss: 0.26310062694066866\n",
      "epoch 11996: train loss: 0.11332705663084497, test loss: 0.263101049223276\n",
      "epoch 11997: train loss: 0.11332465566285797, test loss: 0.2631014715825771\n",
      "epoch 11998: train loss: 0.11332225496745915, test loss: 0.26310189401855283\n",
      "epoch 11999: train loss: 0.11331985454459276, test loss: 0.26310231653118393\n",
      "epoch 12000: train loss: 0.11331745439420296, test loss: 0.26310273912045107\n",
      "epoch 12001: train loss: 0.11331505451623408, test loss: 0.2631031617863349\n",
      "epoch 12002: train loss: 0.11331265491063038, test loss: 0.26310358452881627\n",
      "epoch 12003: train loss: 0.11331025557733615, test loss: 0.26310400734787576\n",
      "epoch 12004: train loss: 0.1133078565162957, test loss: 0.2631044302434942\n",
      "epoch 12005: train loss: 0.11330545772745335, test loss: 0.26310485321565236\n",
      "epoch 12006: train loss: 0.11330305921075343, test loss: 0.263105276264331\n",
      "epoch 12007: train loss: 0.11330066096614033, test loss: 0.2631056993895107\n",
      "epoch 12008: train loss: 0.11329826299355837, test loss: 0.2631061225911725\n",
      "epoch 12009: train loss: 0.11329586529295199, test loss: 0.2631065458692969\n",
      "epoch 12010: train loss: 0.11329346786426556, test loss: 0.2631069692238648\n",
      "epoch 12011: train loss: 0.11329107070744347, test loss: 0.2631073926548569\n",
      "epoch 12012: train loss: 0.11328867382243021, test loss: 0.26310781616225404\n",
      "epoch 12013: train loss: 0.11328627720917027, test loss: 0.263108239746037\n",
      "epoch 12014: train loss: 0.11328388086760802, test loss: 0.2631086634061866\n",
      "epoch 12015: train loss: 0.11328148479768801, test loss: 0.2631090871426835\n",
      "epoch 12016: train loss: 0.11327908899935471, test loss: 0.2631095109555087\n",
      "epoch 12017: train loss: 0.11327669347255263, test loss: 0.2631099348446429\n",
      "epoch 12018: train loss: 0.11327429821722633, test loss: 0.2631103588100669\n",
      "epoch 12019: train loss: 0.11327190323332037, test loss: 0.26311078285176154\n",
      "epoch 12020: train loss: 0.11326950852077929, test loss: 0.2631112069697077\n",
      "epoch 12021: train loss: 0.11326711407954765, test loss: 0.2631116311638861\n",
      "epoch 12022: train loss: 0.11326471990957007, test loss: 0.2631120554342775\n",
      "epoch 12023: train loss: 0.11326232601079118, test loss: 0.26311247978086305\n",
      "epoch 12024: train loss: 0.11325993238315558, test loss: 0.2631129042036233\n",
      "epoch 12025: train loss: 0.11325753902660793, test loss: 0.26311332870253934\n",
      "epoch 12026: train loss: 0.1132551459410929, test loss: 0.2631137532775918\n",
      "epoch 12027: train loss: 0.11325275312655514, test loss: 0.2631141779287616\n",
      "epoch 12028: train loss: 0.11325036058293936, test loss: 0.26311460265602976\n",
      "epoch 12029: train loss: 0.11324796831019027, test loss: 0.2631150274593769\n",
      "epoch 12030: train loss: 0.11324557630825258, test loss: 0.26311545233878414\n",
      "epoch 12031: train loss: 0.11324318457707104, test loss: 0.2631158772942322\n",
      "epoch 12032: train loss: 0.11324079311659045, test loss: 0.26311630232570205\n",
      "epoch 12033: train loss: 0.1132384019267555, test loss: 0.2631167274331745\n",
      "epoch 12034: train loss: 0.11323601100751102, test loss: 0.2631171526166306\n",
      "epoch 12035: train loss: 0.11323362035880181, test loss: 0.26311757787605117\n",
      "epoch 12036: train loss: 0.11323122998057272, test loss: 0.26311800321141704\n",
      "epoch 12037: train loss: 0.11322883987276854, test loss: 0.26311842862270923\n",
      "epoch 12038: train loss: 0.11322645003533414, test loss: 0.26311885410990865\n",
      "epoch 12039: train loss: 0.11322406046821444, test loss: 0.2631192796729962\n",
      "epoch 12040: train loss: 0.11322167117135423, test loss: 0.2631197053119528\n",
      "epoch 12041: train loss: 0.11321928214469847, test loss: 0.2631201310267594\n",
      "epoch 12042: train loss: 0.11321689338819205, test loss: 0.26312055681739693\n",
      "epoch 12043: train loss: 0.11321450490177991, test loss: 0.2631209826838464\n",
      "epoch 12044: train loss: 0.11321211668540702, test loss: 0.2631214086260888\n",
      "epoch 12045: train loss: 0.11320972873901831, test loss: 0.2631218346441048\n",
      "epoch 12046: train loss: 0.11320734106255881, test loss: 0.2631222607378757\n",
      "epoch 12047: train loss: 0.11320495365597345, test loss: 0.2631226869073823\n",
      "epoch 12048: train loss: 0.11320256651920728, test loss: 0.2631231131526056\n",
      "epoch 12049: train loss: 0.11320017965220533, test loss: 0.2631235394735267\n",
      "epoch 12050: train loss: 0.11319779305491261, test loss: 0.2631239658701264\n",
      "epoch 12051: train loss: 0.1131954067272742, test loss: 0.26312439234238577\n",
      "epoch 12052: train loss: 0.11319302066923521, test loss: 0.26312481889028577\n",
      "epoch 12053: train loss: 0.11319063488074069, test loss: 0.26312524551380745\n",
      "epoch 12054: train loss: 0.11318824936173573, test loss: 0.2631256722129318\n",
      "epoch 12055: train loss: 0.11318586411216551, test loss: 0.2631260989876399\n",
      "epoch 12056: train loss: 0.11318347913197511, test loss: 0.26312652583791263\n",
      "epoch 12057: train loss: 0.11318109442110974, test loss: 0.2631269527637311\n",
      "epoch 12058: train loss: 0.1131787099795145, test loss: 0.2631273797650764\n",
      "epoch 12059: train loss: 0.11317632580713463, test loss: 0.26312780684192943\n",
      "epoch 12060: train loss: 0.11317394190391529, test loss: 0.2631282339942713\n",
      "epoch 12061: train loss: 0.11317155826980177, test loss: 0.26312866122208306\n",
      "epoch 12062: train loss: 0.11316917490473923, test loss: 0.26312908852534567\n",
      "epoch 12063: train loss: 0.11316679180867295, test loss: 0.26312951590404027\n",
      "epoch 12064: train loss: 0.1131644089815482, test loss: 0.26312994335814804\n",
      "epoch 12065: train loss: 0.11316202642331025, test loss: 0.26313037088764984\n",
      "epoch 12066: train loss: 0.11315964413390436, test loss: 0.2631307984925268\n",
      "epoch 12067: train loss: 0.11315726211327591, test loss: 0.26313122617275997\n",
      "epoch 12068: train loss: 0.11315488036137018, test loss: 0.26313165392833043\n",
      "epoch 12069: train loss: 0.11315249887813256, test loss: 0.2631320817592194\n",
      "epoch 12070: train loss: 0.11315011766350835, test loss: 0.2631325096654079\n",
      "epoch 12071: train loss: 0.11314773671744296, test loss: 0.26313293764687695\n",
      "epoch 12072: train loss: 0.11314535603988178, test loss: 0.26313336570360774\n",
      "epoch 12073: train loss: 0.1131429756307702, test loss: 0.26313379383558133\n",
      "epoch 12074: train loss: 0.11314059549005367, test loss: 0.2631342220427789\n",
      "epoch 12075: train loss: 0.1131382156176776, test loss: 0.26313465032518146\n",
      "epoch 12076: train loss: 0.11313583601358744, test loss: 0.2631350786827703\n",
      "epoch 12077: train loss: 0.11313345667772869, test loss: 0.26313550711552636\n",
      "epoch 12078: train loss: 0.11313107761004682, test loss: 0.26313593562343085\n",
      "epoch 12079: train loss: 0.11312869881048732, test loss: 0.263136364206465\n",
      "epoch 12080: train loss: 0.11312632027899572, test loss: 0.2631367928646099\n",
      "epoch 12081: train loss: 0.11312394201551754, test loss: 0.2631372215978466\n",
      "epoch 12082: train loss: 0.11312156401999834, test loss: 0.26313765040615633\n",
      "epoch 12083: train loss: 0.11311918629238368, test loss: 0.2631380792895203\n",
      "epoch 12084: train loss: 0.11311680883261914, test loss: 0.2631385082479196\n",
      "epoch 12085: train loss: 0.11311443164065031, test loss: 0.2631389372813355\n",
      "epoch 12086: train loss: 0.1131120547164228, test loss: 0.2631393663897491\n",
      "epoch 12087: train loss: 0.11310967805988222, test loss: 0.2631397955731416\n",
      "epoch 12088: train loss: 0.11310730167097426, test loss: 0.26314022483149413\n",
      "epoch 12089: train loss: 0.11310492554964453, test loss: 0.263140654164788\n",
      "epoch 12090: train loss: 0.11310254969583873, test loss: 0.2631410835730043\n",
      "epoch 12091: train loss: 0.11310017410950252, test loss: 0.2631415130561243\n",
      "epoch 12092: train loss: 0.11309779879058163, test loss: 0.2631419426141292\n",
      "epoch 12093: train loss: 0.11309542373902179, test loss: 0.26314237224700016\n",
      "epoch 12094: train loss: 0.11309304895476868, test loss: 0.2631428019547185\n",
      "epoch 12095: train loss: 0.11309067443776812, test loss: 0.2631432317372654\n",
      "epoch 12096: train loss: 0.11308830018796584, test loss: 0.26314366159462205\n",
      "epoch 12097: train loss: 0.11308592620530762, test loss: 0.2631440915267697\n",
      "epoch 12098: train loss: 0.11308355248973927, test loss: 0.2631445215336896\n",
      "epoch 12099: train loss: 0.11308117904120658, test loss: 0.26314495161536317\n",
      "epoch 12100: train loss: 0.11307880585965542, test loss: 0.2631453817717713\n",
      "epoch 12101: train loss: 0.11307643294503159, test loss: 0.26314581200289555\n",
      "epoch 12102: train loss: 0.11307406029728098, test loss: 0.263146242308717\n",
      "epoch 12103: train loss: 0.11307168791634944, test loss: 0.2631466726892171\n",
      "epoch 12104: train loss: 0.11306931580218292, test loss: 0.2631471031443769\n",
      "epoch 12105: train loss: 0.11306694395472724, test loss: 0.2631475336741778\n",
      "epoch 12106: train loss: 0.11306457237392838, test loss: 0.26314796427860115\n",
      "epoch 12107: train loss: 0.11306220105973228, test loss: 0.2631483949576282\n",
      "epoch 12108: train loss: 0.11305983001208485, test loss: 0.2631488257112401\n",
      "epoch 12109: train loss: 0.11305745923093212, test loss: 0.2631492565394183\n",
      "epoch 12110: train loss: 0.11305508871622, test loss: 0.26314968744214406\n",
      "epoch 12111: train loss: 0.11305271846789455, test loss: 0.2631501184193987\n",
      "epoch 12112: train loss: 0.11305034848590177, test loss: 0.26315054947116356\n",
      "epoch 12113: train loss: 0.11304797877018767, test loss: 0.26315098059741987\n",
      "epoch 12114: train loss: 0.11304560932069833, test loss: 0.2631514117981491\n",
      "epoch 12115: train loss: 0.11304324013737979, test loss: 0.2631518430733324\n",
      "epoch 12116: train loss: 0.11304087122017811, test loss: 0.2631522744229512\n",
      "epoch 12117: train loss: 0.11303850256903943, test loss: 0.26315270584698697\n",
      "epoch 12118: train loss: 0.11303613418390981, test loss: 0.2631531373454208\n",
      "epoch 12119: train loss: 0.1130337660647354, test loss: 0.2631535689182342\n",
      "epoch 12120: train loss: 0.11303139821146231, test loss: 0.2631540005654085\n",
      "epoch 12121: train loss: 0.11302903062403676, test loss: 0.2631544322869251\n",
      "epoch 12122: train loss: 0.11302666330240484, test loss: 0.2631548640827652\n",
      "epoch 12123: train loss: 0.11302429624651282, test loss: 0.26315529595291043\n",
      "epoch 12124: train loss: 0.1130219294563068, test loss: 0.263155727897342\n",
      "epoch 12125: train loss: 0.11301956293173308, test loss: 0.26315615991604124\n",
      "epoch 12126: train loss: 0.11301719667273787, test loss: 0.2631565920089897\n",
      "epoch 12127: train loss: 0.11301483067926739, test loss: 0.26315702417616876\n",
      "epoch 12128: train loss: 0.11301246495126793, test loss: 0.26315745641755967\n",
      "epoch 12129: train loss: 0.11301009948868575, test loss: 0.26315788873314394\n",
      "epoch 12130: train loss: 0.11300773429146715, test loss: 0.263158321122903\n",
      "epoch 12131: train loss: 0.11300536935955847, test loss: 0.26315875358681823\n",
      "epoch 12132: train loss: 0.11300300469290599, test loss: 0.2631591861248711\n",
      "epoch 12133: train loss: 0.11300064029145604, test loss: 0.26315961873704286\n",
      "epoch 12134: train loss: 0.11299827615515504, test loss: 0.2631600514233151\n",
      "epoch 12135: train loss: 0.11299591228394927, test loss: 0.26316048418366916\n",
      "epoch 12136: train loss: 0.1129935486777852, test loss: 0.26316091701808664\n",
      "epoch 12137: train loss: 0.11299118533660918, test loss: 0.2631613499265489\n",
      "epoch 12138: train loss: 0.11298882226036767, test loss: 0.2631617829090373\n",
      "epoch 12139: train loss: 0.11298645944900704, test loss: 0.26316221596553335\n",
      "epoch 12140: train loss: 0.11298409690247378, test loss: 0.2631626490960186\n",
      "epoch 12141: train loss: 0.11298173462071434, test loss: 0.2631630823004744\n",
      "epoch 12142: train loss: 0.11297937260367523, test loss: 0.2631635155788823\n",
      "epoch 12143: train loss: 0.11297701085130289, test loss: 0.2631639489312237\n",
      "epoch 12144: train loss: 0.11297464936354384, test loss: 0.2631643823574802\n",
      "epoch 12145: train loss: 0.11297228814034461, test loss: 0.2631648158576332\n",
      "epoch 12146: train loss: 0.11296992718165175, test loss: 0.2631652494316642\n",
      "epoch 12147: train loss: 0.1129675664874118, test loss: 0.2631656830795546\n",
      "epoch 12148: train loss: 0.11296520605757138, test loss: 0.26316611680128615\n",
      "epoch 12149: train loss: 0.11296284589207702, test loss: 0.2631665505968402\n",
      "epoch 12150: train loss: 0.11296048599087528, test loss: 0.2631669844661982\n",
      "epoch 12151: train loss: 0.11295812635391288, test loss: 0.2631674184093418\n",
      "epoch 12152: train loss: 0.11295576698113637, test loss: 0.2631678524262525\n",
      "epoch 12153: train loss: 0.11295340787249243, test loss: 0.26316828651691176\n",
      "epoch 12154: train loss: 0.11295104902792773, test loss: 0.26316872068130115\n",
      "epoch 12155: train loss: 0.11294869044738892, test loss: 0.26316915491940235\n",
      "epoch 12156: train loss: 0.1129463321308227, test loss: 0.2631695892311966\n",
      "epoch 12157: train loss: 0.11294397407817577, test loss: 0.2631700236166657\n",
      "epoch 12158: train loss: 0.11294161628939488, test loss: 0.26317045807579115\n",
      "epoch 12159: train loss: 0.11293925876442673, test loss: 0.2631708926085544\n",
      "epoch 12160: train loss: 0.11293690150321811, test loss: 0.26317132721493713\n",
      "epoch 12161: train loss: 0.11293454450571573, test loss: 0.2631717618949209\n",
      "epoch 12162: train loss: 0.11293218777186645, test loss: 0.26317219664848723\n",
      "epoch 12163: train loss: 0.11292983130161702, test loss: 0.2631726314756178\n",
      "epoch 12164: train loss: 0.11292747509491427, test loss: 0.2631730663762941\n",
      "epoch 12165: train loss: 0.112925119151705, test loss: 0.2631735013504978\n",
      "epoch 12166: train loss: 0.1129227634719361, test loss: 0.26317393639821035\n",
      "epoch 12167: train loss: 0.11292040805555441, test loss: 0.2631743715194135\n",
      "epoch 12168: train loss: 0.11291805290250677, test loss: 0.26317480671408877\n",
      "epoch 12169: train loss: 0.11291569801274011, test loss: 0.2631752419822178\n",
      "epoch 12170: train loss: 0.11291334338620132, test loss: 0.26317567732378233\n",
      "epoch 12171: train loss: 0.11291098902283732, test loss: 0.2631761127387638\n",
      "epoch 12172: train loss: 0.11290863492259504, test loss: 0.2631765482271439\n",
      "epoch 12173: train loss: 0.11290628108542147, test loss: 0.2631769837889043\n",
      "epoch 12174: train loss: 0.1129039275112635, test loss: 0.26317741942402656\n",
      "epoch 12175: train loss: 0.11290157420006817, test loss: 0.26317785513249237\n",
      "epoch 12176: train loss: 0.1128992211517825, test loss: 0.26317829091428335\n",
      "epoch 12177: train loss: 0.1128968683663534, test loss: 0.26317872676938125\n",
      "epoch 12178: train loss: 0.11289451584372795, test loss: 0.2631791626977676\n",
      "epoch 12179: train loss: 0.11289216358385325, test loss: 0.26317959869942414\n",
      "epoch 12180: train loss: 0.11288981158667627, test loss: 0.2631800347743325\n",
      "epoch 12181: train loss: 0.11288745985214409, test loss: 0.2631804709224743\n",
      "epoch 12182: train loss: 0.11288510838020384, test loss: 0.2631809071438314\n",
      "epoch 12183: train loss: 0.11288275717080261, test loss: 0.2631813434383853\n",
      "epoch 12184: train loss: 0.11288040622388747, test loss: 0.26318177980611784\n",
      "epoch 12185: train loss: 0.11287805553940561, test loss: 0.2631822162470106\n",
      "epoch 12186: train loss: 0.11287570511730415, test loss: 0.26318265276104524\n",
      "epoch 12187: train loss: 0.11287335495753027, test loss: 0.2631830893482036\n",
      "epoch 12188: train loss: 0.1128710050600311, test loss: 0.2631835260084674\n",
      "epoch 12189: train loss: 0.11286865542475388, test loss: 0.26318396274181816\n",
      "epoch 12190: train loss: 0.11286630605164578, test loss: 0.26318439954823775\n",
      "epoch 12191: train loss: 0.11286395694065408, test loss: 0.26318483642770796\n",
      "epoch 12192: train loss: 0.11286160809172595, test loss: 0.26318527338021025\n",
      "epoch 12193: train loss: 0.11285925950480866, test loss: 0.2631857104057267\n",
      "epoch 12194: train loss: 0.1128569111798495, test loss: 0.26318614750423874\n",
      "epoch 12195: train loss: 0.11285456311679574, test loss: 0.2631865846757283\n",
      "epoch 12196: train loss: 0.11285221531559468, test loss: 0.26318702192017707\n",
      "epoch 12197: train loss: 0.11284986777619363, test loss: 0.26318745923756687\n",
      "epoch 12198: train loss: 0.11284752049853992, test loss: 0.26318789662787945\n",
      "epoch 12199: train loss: 0.11284517348258087, test loss: 0.26318833409109643\n",
      "epoch 12200: train loss: 0.11284282672826386, test loss: 0.2631887716271997\n",
      "epoch 12201: train loss: 0.11284048023553625, test loss: 0.26318920923617106\n",
      "epoch 12202: train loss: 0.11283813400434543, test loss: 0.26318964691799224\n",
      "epoch 12203: train loss: 0.11283578803463883, test loss: 0.26319008467264504\n",
      "epoch 12204: train loss: 0.1128334423263638, test loss: 0.26319052250011127\n",
      "epoch 12205: train loss: 0.11283109687946787, test loss: 0.2631909604003727\n",
      "epoch 12206: train loss: 0.1128287516938984, test loss: 0.2631913983734111\n",
      "epoch 12207: train loss: 0.11282640676960287, test loss: 0.26319183641920835\n",
      "epoch 12208: train loss: 0.11282406210652876, test loss: 0.26319227453774624\n",
      "epoch 12209: train loss: 0.11282171770462361, test loss: 0.2631927127290065\n",
      "epoch 12210: train loss: 0.1128193735638349, test loss: 0.2631931509929711\n",
      "epoch 12211: train loss: 0.11281702968411007, test loss: 0.2631935893296217\n",
      "epoch 12212: train loss: 0.11281468606539677, test loss: 0.26319402773894024\n",
      "epoch 12213: train loss: 0.11281234270764251, test loss: 0.26319446622090864\n",
      "epoch 12214: train loss: 0.11280999961079484, test loss: 0.26319490477550855\n",
      "epoch 12215: train loss: 0.11280765677480138, test loss: 0.26319534340272194\n",
      "epoch 12216: train loss: 0.1128053141996097, test loss: 0.2631957821025306\n",
      "epoch 12217: train loss: 0.11280297188516739, test loss: 0.26319622087491645\n",
      "epoch 12218: train loss: 0.1128006298314221, test loss: 0.2631966597198613\n",
      "epoch 12219: train loss: 0.1127982880383215, test loss: 0.26319709863734697\n",
      "epoch 12220: train loss: 0.11279594650581318, test loss: 0.2631975376273554\n",
      "epoch 12221: train loss: 0.11279360523384487, test loss: 0.2631979766898686\n",
      "epoch 12222: train loss: 0.11279126422236424, test loss: 0.26319841582486825\n",
      "epoch 12223: train loss: 0.11278892347131897, test loss: 0.2631988550323362\n",
      "epoch 12224: train loss: 0.11278658298065679, test loss: 0.26319929431225453\n",
      "epoch 12225: train loss: 0.11278424275032543, test loss: 0.26319973366460503\n",
      "epoch 12226: train loss: 0.11278190278027264, test loss: 0.26320017308936966\n",
      "epoch 12227: train loss: 0.11277956307044618, test loss: 0.2632006125865303\n",
      "epoch 12228: train loss: 0.11277722362079379, test loss: 0.26320105215606887\n",
      "epoch 12229: train loss: 0.11277488443126331, test loss: 0.2632014917979672\n",
      "epoch 12230: train loss: 0.1127725455018025, test loss: 0.26320193151220733\n",
      "epoch 12231: train loss: 0.11277020683235925, test loss: 0.26320237129877116\n",
      "epoch 12232: train loss: 0.11276786842288133, test loss: 0.26320281115764055\n",
      "epoch 12233: train loss: 0.11276553027331661, test loss: 0.26320325108879755\n",
      "epoch 12234: train loss: 0.11276319238361293, test loss: 0.263203691092224\n",
      "epoch 12235: train loss: 0.1127608547537182, test loss: 0.263204131167902\n",
      "epoch 12236: train loss: 0.11275851738358032, test loss: 0.26320457131581326\n",
      "epoch 12237: train loss: 0.11275618027314716, test loss: 0.26320501153593995\n",
      "epoch 12238: train loss: 0.11275384342236666, test loss: 0.263205451828264\n",
      "epoch 12239: train loss: 0.11275150683118679, test loss: 0.26320589219276724\n",
      "epoch 12240: train loss: 0.11274917049955543, test loss: 0.2632063326294318\n",
      "epoch 12241: train loss: 0.11274683442742063, test loss: 0.26320677313823965\n",
      "epoch 12242: train loss: 0.11274449861473032, test loss: 0.26320721371917266\n",
      "epoch 12243: train loss: 0.11274216306143252, test loss: 0.26320765437221294\n",
      "epoch 12244: train loss: 0.11273982776747524, test loss: 0.26320809509734233\n",
      "epoch 12245: train loss: 0.11273749273280646, test loss: 0.263208535894543\n",
      "epoch 12246: train loss: 0.1127351579573743, test loss: 0.26320897676379684\n",
      "epoch 12247: train loss: 0.11273282344112678, test loss: 0.2632094177050859\n",
      "epoch 12248: train loss: 0.11273048918401195, test loss: 0.2632098587183922\n",
      "epoch 12249: train loss: 0.11272815518597792, test loss: 0.2632102998036977\n",
      "epoch 12250: train loss: 0.11272582144697278, test loss: 0.2632107409609845\n",
      "epoch 12251: train loss: 0.11272348796694465, test loss: 0.2632111821902346\n",
      "epoch 12252: train loss: 0.11272115474584166, test loss: 0.26321162349143007\n",
      "epoch 12253: train loss: 0.11271882178361194, test loss: 0.2632120648645529\n",
      "epoch 12254: train loss: 0.11271648908020367, test loss: 0.2632125063095851\n",
      "epoch 12255: train loss: 0.11271415663556503, test loss: 0.26321294782650884\n",
      "epoch 12256: train loss: 0.11271182444964419, test loss: 0.26321338941530603\n",
      "epoch 12257: train loss: 0.11270949252238932, test loss: 0.2632138310759588\n",
      "epoch 12258: train loss: 0.11270716085374871, test loss: 0.2632142728084492\n",
      "epoch 12259: train loss: 0.11270482944367055, test loss: 0.26321471461275936\n",
      "epoch 12260: train loss: 0.1127024982921031, test loss: 0.2632151564888712\n",
      "epoch 12261: train loss: 0.1127001673989946, test loss: 0.26321559843676695\n",
      "epoch 12262: train loss: 0.11269783676429336, test loss: 0.2632160404564287\n",
      "epoch 12263: train loss: 0.11269550638794767, test loss: 0.26321648254783836\n",
      "epoch 12264: train loss: 0.11269317626990581, test loss: 0.26321692471097824\n",
      "epoch 12265: train loss: 0.11269084641011609, test loss: 0.26321736694583037\n",
      "epoch 12266: train loss: 0.1126885168085269, test loss: 0.2632178092523768\n",
      "epoch 12267: train loss: 0.11268618746508652, test loss: 0.2632182516305996\n",
      "epoch 12268: train loss: 0.11268385837974336, test loss: 0.26321869408048104\n",
      "epoch 12269: train loss: 0.11268152955244581, test loss: 0.2632191366020031\n",
      "epoch 12270: train loss: 0.11267920098314221, test loss: 0.26321957919514793\n",
      "epoch 12271: train loss: 0.11267687267178104, test loss: 0.2632200218598977\n",
      "epoch 12272: train loss: 0.11267454461831065, test loss: 0.2632204645962345\n",
      "epoch 12273: train loss: 0.11267221682267953, test loss: 0.2632209074041405\n",
      "epoch 12274: train loss: 0.11266988928483611, test loss: 0.26322135028359783\n",
      "epoch 12275: train loss: 0.11266756200472886, test loss: 0.26322179323458866\n",
      "epoch 12276: train loss: 0.11266523498230625, test loss: 0.2632222362570951\n",
      "epoch 12277: train loss: 0.11266290821751682, test loss: 0.26322267935109944\n",
      "epoch 12278: train loss: 0.11266058171030903, test loss: 0.26322312251658353\n",
      "epoch 12279: train loss: 0.11265825546063142, test loss: 0.2632235657535299\n",
      "epoch 12280: train loss: 0.11265592946843253, test loss: 0.2632240090619205\n",
      "epoch 12281: train loss: 0.11265360373366093, test loss: 0.2632244524417376\n",
      "epoch 12282: train loss: 0.11265127825626518, test loss: 0.26322489589296333\n",
      "epoch 12283: train loss: 0.11264895303619384, test loss: 0.26322533941558\n",
      "epoch 12284: train loss: 0.11264662807339555, test loss: 0.2632257830095696\n",
      "epoch 12285: train loss: 0.11264430336781889, test loss: 0.26322622667491447\n",
      "epoch 12286: train loss: 0.11264197891941251, test loss: 0.2632266704115968\n",
      "epoch 12287: train loss: 0.11263965472812501, test loss: 0.2632271142195988\n",
      "epoch 12288: train loss: 0.11263733079390509, test loss: 0.2632275580989026\n",
      "epoch 12289: train loss: 0.11263500711670141, test loss: 0.2632280020494905\n",
      "epoch 12290: train loss: 0.11263268369646263, test loss: 0.2632284460713447\n",
      "epoch 12291: train loss: 0.11263036053313748, test loss: 0.2632288901644474\n",
      "epoch 12292: train loss: 0.11262803762667467, test loss: 0.2632293343287809\n",
      "epoch 12293: train loss: 0.1126257149770229, test loss: 0.2632297785643274\n",
      "epoch 12294: train loss: 0.11262339258413098, test loss: 0.2632302228710692\n",
      "epoch 12295: train loss: 0.11262107044794757, test loss: 0.26323066724898836\n",
      "epoch 12296: train loss: 0.11261874856842154, test loss: 0.2632311116980673\n",
      "epoch 12297: train loss: 0.11261642694550161, test loss: 0.26323155621828825\n",
      "epoch 12298: train loss: 0.11261410557913659, test loss: 0.26323200080963355\n",
      "epoch 12299: train loss: 0.11261178446927535, test loss: 0.26323244547208524\n",
      "epoch 12300: train loss: 0.11260946361586666, test loss: 0.2632328902056258\n",
      "epoch 12301: train loss: 0.11260714301885935, test loss: 0.2632333350102374\n",
      "epoch 12302: train loss: 0.11260482267820233, test loss: 0.26323377988590235\n",
      "epoch 12303: train loss: 0.11260250259384445, test loss: 0.263234224832603\n",
      "epoch 12304: train loss: 0.11260018276573462, test loss: 0.26323466985032157\n",
      "epoch 12305: train loss: 0.11259786319382173, test loss: 0.26323511493904034\n",
      "epoch 12306: train loss: 0.11259554387805465, test loss: 0.26323556009874166\n",
      "epoch 12307: train loss: 0.1125932248183824, test loss: 0.2632360053294079\n",
      "epoch 12308: train loss: 0.11259090601475384, test loss: 0.2632364506310212\n",
      "epoch 12309: train loss: 0.11258858746711799, test loss: 0.2632368960035639\n",
      "epoch 12310: train loss: 0.11258626917542378, test loss: 0.26323734144701855\n",
      "epoch 12311: train loss: 0.11258395113962022, test loss: 0.2632377869613672\n",
      "epoch 12312: train loss: 0.11258163335965633, test loss: 0.2632382325465923\n",
      "epoch 12313: train loss: 0.1125793158354811, test loss: 0.2632386782026762\n",
      "epoch 12314: train loss: 0.11257699856704358, test loss: 0.2632391239296013\n",
      "epoch 12315: train loss: 0.11257468155429279, test loss: 0.26323956972734985\n",
      "epoch 12316: train loss: 0.11257236479717783, test loss: 0.26324001559590415\n",
      "epoch 12317: train loss: 0.11257004829564775, test loss: 0.2632404615352466\n",
      "epoch 12318: train loss: 0.11256773204965163, test loss: 0.26324090754535967\n",
      "epoch 12319: train loss: 0.11256541605913858, test loss: 0.26324135362622564\n",
      "epoch 12320: train loss: 0.11256310032405775, test loss: 0.26324179977782686\n",
      "epoch 12321: train loss: 0.11256078484435825, test loss: 0.26324224600014573\n",
      "epoch 12322: train loss: 0.11255846961998922, test loss: 0.2632426922931646\n",
      "epoch 12323: train loss: 0.11255615465089981, test loss: 0.2632431386568659\n",
      "epoch 12324: train loss: 0.11255383993703921, test loss: 0.263243585091232\n",
      "epoch 12325: train loss: 0.1125515254783566, test loss: 0.2632440315962453\n",
      "epoch 12326: train loss: 0.11254921127480123, test loss: 0.2632444781718883\n",
      "epoch 12327: train loss: 0.11254689732632228, test loss: 0.2632449248181431\n",
      "epoch 12328: train loss: 0.11254458363286897, test loss: 0.26324537153499245\n",
      "epoch 12329: train loss: 0.11254227019439057, test loss: 0.26324581832241856\n",
      "epoch 12330: train loss: 0.11253995701083631, test loss: 0.26324626518040395\n",
      "epoch 12331: train loss: 0.11253764408215552, test loss: 0.26324671210893097\n",
      "epoch 12332: train loss: 0.11253533140829745, test loss: 0.26324715910798213\n",
      "epoch 12333: train loss: 0.1125330189892114, test loss: 0.26324760617753984\n",
      "epoch 12334: train loss: 0.11253070682484673, test loss: 0.26324805331758644\n",
      "epoch 12335: train loss: 0.11252839491515272, test loss: 0.26324850052810456\n",
      "epoch 12336: train loss: 0.11252608326007878, test loss: 0.26324894780907643\n",
      "epoch 12337: train loss: 0.1125237718595742, test loss: 0.2632493951604847\n",
      "epoch 12338: train loss: 0.1125214607135884, test loss: 0.2632498425823117\n",
      "epoch 12339: train loss: 0.11251914982207076, test loss: 0.2632502900745399\n",
      "epoch 12340: train loss: 0.11251683918497067, test loss: 0.26325073763715184\n",
      "epoch 12341: train loss: 0.11251452880223756, test loss: 0.26325118527013\n",
      "epoch 12342: train loss: 0.11251221867382089, test loss: 0.2632516329734568\n",
      "epoch 12343: train loss: 0.1125099087996701, test loss: 0.2632520807471147\n",
      "epoch 12344: train loss: 0.11250759917973457, test loss: 0.26325252859108617\n",
      "epoch 12345: train loss: 0.11250528981396389, test loss: 0.26325297650535384\n",
      "epoch 12346: train loss: 0.11250298070230746, test loss: 0.2632534244899001\n",
      "epoch 12347: train loss: 0.11250067184471484, test loss: 0.2632538725447075\n",
      "epoch 12348: train loss: 0.11249836324113552, test loss: 0.2632543206697584\n",
      "epoch 12349: train loss: 0.11249605489151908, test loss: 0.2632547688650356\n",
      "epoch 12350: train loss: 0.11249374679581498, test loss: 0.2632552171305214\n",
      "epoch 12351: train loss: 0.11249143895397282, test loss: 0.26325566546619833\n",
      "epoch 12352: train loss: 0.11248913136594223, test loss: 0.26325611387204895\n",
      "epoch 12353: train loss: 0.11248682403167272, test loss: 0.26325656234805583\n",
      "epoch 12354: train loss: 0.11248451695111393, test loss: 0.2632570108942015\n",
      "epoch 12355: train loss: 0.11248221012421546, test loss: 0.2632574595104684\n",
      "epoch 12356: train loss: 0.112479903550927, test loss: 0.2632579081968392\n",
      "epoch 12357: train loss: 0.11247759723119811, test loss: 0.26325835695329647\n",
      "epoch 12358: train loss: 0.11247529116497851, test loss: 0.2632588057798225\n",
      "epoch 12359: train loss: 0.11247298535221784, test loss: 0.26325925467640027\n",
      "epoch 12360: train loss: 0.1124706797928658, test loss: 0.26325970364301193\n",
      "epoch 12361: train loss: 0.1124683744868721, test loss: 0.2632601526796403\n",
      "epoch 12362: train loss: 0.11246606943418644, test loss: 0.263260601786268\n",
      "epoch 12363: train loss: 0.11246376463475859, test loss: 0.2632610509628774\n",
      "epoch 12364: train loss: 0.11246146008853825, test loss: 0.26326150020945116\n",
      "epoch 12365: train loss: 0.11245915579547516, test loss: 0.263261949525972\n",
      "epoch 12366: train loss: 0.11245685175551916, test loss: 0.2632623989124223\n",
      "epoch 12367: train loss: 0.11245454796861999, test loss: 0.2632628483687849\n",
      "epoch 12368: train loss: 0.11245224443472748, test loss: 0.26326329789504216\n",
      "epoch 12369: train loss: 0.11244994115379142, test loss: 0.2632637474911768\n",
      "epoch 12370: train loss: 0.11244763812576167, test loss: 0.2632641971571715\n",
      "epoch 12371: train loss: 0.11244533535058804, test loss: 0.26326464689300877\n",
      "epoch 12372: train loss: 0.11244303282822038, test loss: 0.26326509669867126\n",
      "epoch 12373: train loss: 0.1124407305586086, test loss: 0.26326554657414164\n",
      "epoch 12374: train loss: 0.11243842854170256, test loss: 0.26326599651940247\n",
      "epoch 12375: train loss: 0.11243612677745218, test loss: 0.26326644653443654\n",
      "epoch 12376: train loss: 0.11243382526580739, test loss: 0.2632668966192262\n",
      "epoch 12377: train loss: 0.11243152400671803, test loss: 0.26326734677375446\n",
      "epoch 12378: train loss: 0.11242922300013414, test loss: 0.2632677969980037\n",
      "epoch 12379: train loss: 0.11242692224600564, test loss: 0.2632682472919567\n",
      "epoch 12380: train loss: 0.11242462174428247, test loss: 0.26326869765559613\n",
      "epoch 12381: train loss: 0.11242232149491466, test loss: 0.26326914808890456\n",
      "epoch 12382: train loss: 0.11242002149785219, test loss: 0.2632695985918646\n",
      "epoch 12383: train loss: 0.11241772175304504, test loss: 0.26327004916445923\n",
      "epoch 12384: train loss: 0.11241542226044332, test loss: 0.26327049980667083\n",
      "epoch 12385: train loss: 0.112413123019997, test loss: 0.26327095051848226\n",
      "epoch 12386: train loss: 0.11241082403165613, test loss: 0.26327140129987614\n",
      "epoch 12387: train loss: 0.11240852529537083, test loss: 0.2632718521508351\n",
      "epoch 12388: train loss: 0.11240622681109112, test loss: 0.26327230307134203\n",
      "epoch 12389: train loss: 0.11240392857876717, test loss: 0.26327275406137934\n",
      "epoch 12390: train loss: 0.11240163059834903, test loss: 0.2632732051209301\n",
      "epoch 12391: train loss: 0.11239933286978683, test loss: 0.26327365624997673\n",
      "epoch 12392: train loss: 0.11239703539303074, test loss: 0.26327410744850205\n",
      "epoch 12393: train loss: 0.11239473816803087, test loss: 0.26327455871648886\n",
      "epoch 12394: train loss: 0.11239244119473744, test loss: 0.26327501005391973\n",
      "epoch 12395: train loss: 0.11239014447310058, test loss: 0.2632754614607775\n",
      "epoch 12396: train loss: 0.11238784800307051, test loss: 0.2632759129370449\n",
      "epoch 12397: train loss: 0.11238555178459743, test loss: 0.26327636448270475\n",
      "epoch 12398: train loss: 0.1123832558176316, test loss: 0.26327681609773956\n",
      "epoch 12399: train loss: 0.11238096010212317, test loss: 0.26327726778213223\n",
      "epoch 12400: train loss: 0.11237866463802244, test loss: 0.26327771953586565\n",
      "epoch 12401: train loss: 0.1123763694252797, test loss: 0.26327817135892234\n",
      "epoch 12402: train loss: 0.11237407446384522, test loss: 0.2632786232512852\n",
      "epoch 12403: train loss: 0.11237177975366923, test loss: 0.263279075212937\n",
      "epoch 12404: train loss: 0.11236948529470211, test loss: 0.26327952724386045\n",
      "epoch 12405: train loss: 0.11236719108689414, test loss: 0.2632799793440384\n",
      "epoch 12406: train loss: 0.11236489713019565, test loss: 0.2632804315134536\n",
      "epoch 12407: train loss: 0.112362603424557, test loss: 0.2632808837520889\n",
      "epoch 12408: train loss: 0.11236030996992859, test loss: 0.26328133605992693\n",
      "epoch 12409: train loss: 0.11235801676626074, test loss: 0.2632817884369507\n",
      "epoch 12410: train loss: 0.11235572381350384, test loss: 0.26328224088314284\n",
      "epoch 12411: train loss: 0.1123534311116083, test loss: 0.26328269339848626\n",
      "epoch 12412: train loss: 0.11235113866052457, test loss: 0.2632831459829637\n",
      "epoch 12413: train loss: 0.11234884646020304, test loss: 0.26328359863655815\n",
      "epoch 12414: train loss: 0.11234655451059418, test loss: 0.2632840513592522\n",
      "epoch 12415: train loss: 0.11234426281164843, test loss: 0.26328450415102883\n",
      "epoch 12416: train loss: 0.11234197136331626, test loss: 0.2632849570118708\n",
      "epoch 12417: train loss: 0.11233968016554821, test loss: 0.2632854099417611\n",
      "epoch 12418: train loss: 0.11233738921829473, test loss: 0.2632858629406823\n",
      "epoch 12419: train loss: 0.11233509852150632, test loss: 0.2632863160086174\n",
      "epoch 12420: train loss: 0.11233280807513353, test loss: 0.2632867691455493\n",
      "epoch 12421: train loss: 0.11233051787912693, test loss: 0.2632872223514609\n",
      "epoch 12422: train loss: 0.11232822793343701, test loss: 0.2632876756263348\n",
      "epoch 12423: train loss: 0.11232593823801439, test loss: 0.2632881289701541\n",
      "epoch 12424: train loss: 0.11232364879280965, test loss: 0.26328858238290165\n",
      "epoch 12425: train loss: 0.11232135959777337, test loss: 0.2632890358645602\n",
      "epoch 12426: train loss: 0.11231907065285615, test loss: 0.26328948941511277\n",
      "epoch 12427: train loss: 0.11231678195800864, test loss: 0.2632899430345422\n",
      "epoch 12428: train loss: 0.1123144935131815, test loss: 0.2632903967228313\n",
      "epoch 12429: train loss: 0.11231220531832532, test loss: 0.26329085047996303\n",
      "epoch 12430: train loss: 0.11230991737339081, test loss: 0.2632913043059203\n",
      "epoch 12431: train loss: 0.11230762967832864, test loss: 0.263291758200686\n",
      "epoch 12432: train loss: 0.11230534223308951, test loss: 0.26329221216424303\n",
      "epoch 12433: train loss: 0.1123030550376241, test loss: 0.2632926661965743\n",
      "epoch 12434: train loss: 0.11230076809188315, test loss: 0.2632931202976628\n",
      "epoch 12435: train loss: 0.1122984813958174, test loss: 0.26329357446749135\n",
      "epoch 12436: train loss: 0.11229619494937758, test loss: 0.2632940287060429\n",
      "epoch 12437: train loss: 0.1122939087525145, test loss: 0.26329448301330044\n",
      "epoch 12438: train loss: 0.1122916228051789, test loss: 0.2632949373892468\n",
      "epoch 12439: train loss: 0.11228933710732153, test loss: 0.26329539183386513\n",
      "epoch 12440: train loss: 0.11228705165889327, test loss: 0.2632958463471381\n",
      "epoch 12441: train loss: 0.11228476645984489, test loss: 0.2632963009290489\n",
      "epoch 12442: train loss: 0.11228248151012725, test loss: 0.26329675557958027\n",
      "epoch 12443: train loss: 0.11228019680969116, test loss: 0.2632972102987154\n",
      "epoch 12444: train loss: 0.1122779123584875, test loss: 0.263297665086437\n",
      "epoch 12445: train loss: 0.11227562815646715, test loss: 0.2632981199427283\n",
      "epoch 12446: train loss: 0.11227334420358098, test loss: 0.2632985748675721\n",
      "epoch 12447: train loss: 0.1122710604997799, test loss: 0.26329902986095144\n",
      "epoch 12448: train loss: 0.11226877704501483, test loss: 0.2632994849228492\n",
      "epoch 12449: train loss: 0.11226649383923666, test loss: 0.2632999400532487\n",
      "epoch 12450: train loss: 0.11226421088239638, test loss: 0.2633003952521324\n",
      "epoch 12451: train loss: 0.11226192817444491, test loss: 0.2633008505194838\n",
      "epoch 12452: train loss: 0.11225964571533321, test loss: 0.26330130585528566\n",
      "epoch 12453: train loss: 0.11225736350501231, test loss: 0.26330176125952093\n",
      "epoch 12454: train loss: 0.1122550815434332, test loss: 0.2633022167321728\n",
      "epoch 12455: train loss: 0.11225279983054684, test loss: 0.2633026722732242\n",
      "epoch 12456: train loss: 0.11225051836630426, test loss: 0.26330312788265803\n",
      "epoch 12457: train loss: 0.11224823715065653, test loss: 0.26330358356045763\n",
      "epoch 12458: train loss: 0.11224595618355471, test loss: 0.2633040393066058\n",
      "epoch 12459: train loss: 0.11224367546494982, test loss: 0.26330449512108545\n",
      "epoch 12460: train loss: 0.11224139499479296, test loss: 0.2633049510038799\n",
      "epoch 12461: train loss: 0.1122391147730352, test loss: 0.26330540695497207\n",
      "epoch 12462: train loss: 0.11223683479962769, test loss: 0.263305862974345\n",
      "epoch 12463: train loss: 0.11223455507452151, test loss: 0.26330631906198176\n",
      "epoch 12464: train loss: 0.1122322755976678, test loss: 0.2633067752178655\n",
      "epoch 12465: train loss: 0.11222999636901773, test loss: 0.26330723144197904\n",
      "epoch 12466: train loss: 0.11222771738852241, test loss: 0.2633076877343057\n",
      "epoch 12467: train loss: 0.11222543865613308, test loss: 0.2633081440948284\n",
      "epoch 12468: train loss: 0.11222316017180085, test loss: 0.26330860052353033\n",
      "epoch 12469: train loss: 0.11222088193547698, test loss: 0.2633090570203944\n",
      "epoch 12470: train loss: 0.11221860394711268, test loss: 0.2633095135854039\n",
      "epoch 12471: train loss: 0.11221632620665911, test loss: 0.26330997021854186\n",
      "epoch 12472: train loss: 0.11221404871406762, test loss: 0.2633104269197913\n",
      "epoch 12473: train loss: 0.11221177146928937, test loss: 0.26331088368913536\n",
      "epoch 12474: train loss: 0.11220949447227568, test loss: 0.2633113405265572\n",
      "epoch 12475: train loss: 0.11220721772297777, test loss: 0.2633117974320398\n",
      "epoch 12476: train loss: 0.11220494122134703, test loss: 0.2633122544055664\n",
      "epoch 12477: train loss: 0.11220266496733469, test loss: 0.26331271144712\n",
      "epoch 12478: train loss: 0.11220038896089213, test loss: 0.26331316855668385\n",
      "epoch 12479: train loss: 0.11219811320197064, test loss: 0.2633136257342411\n",
      "epoch 12480: train loss: 0.11219583769052159, test loss: 0.26331408297977466\n",
      "epoch 12481: train loss: 0.11219356242649635, test loss: 0.26331454029326795\n",
      "epoch 12482: train loss: 0.11219128740984625, test loss: 0.2633149976747039\n",
      "epoch 12483: train loss: 0.11218901264052276, test loss: 0.2633154551240658\n",
      "epoch 12484: train loss: 0.11218673811847721, test loss: 0.2633159126413367\n",
      "epoch 12485: train loss: 0.11218446384366107, test loss: 0.26331637022649973\n",
      "epoch 12486: train loss: 0.11218218981602572, test loss: 0.2633168278795382\n",
      "epoch 12487: train loss: 0.11217991603552263, test loss: 0.26331728560043516\n",
      "epoch 12488: train loss: 0.1121776425021033, test loss: 0.2633177433891738\n",
      "epoch 12489: train loss: 0.11217536921571913, test loss: 0.2633182012457373\n",
      "epoch 12490: train loss: 0.11217309617632164, test loss: 0.2633186591701089\n",
      "epoch 12491: train loss: 0.11217082338386235, test loss: 0.26331911716227163\n",
      "epoch 12492: train loss: 0.11216855083829269, test loss: 0.26331957522220883\n",
      "epoch 12493: train loss: 0.11216627853956428, test loss: 0.26332003334990356\n",
      "epoch 12494: train loss: 0.1121640064876286, test loss: 0.2633204915453392\n",
      "epoch 12495: train loss: 0.11216173468243724, test loss: 0.26332094980849885\n",
      "epoch 12496: train loss: 0.11215946312394175, test loss: 0.26332140813936566\n",
      "epoch 12497: train loss: 0.11215719181209369, test loss: 0.2633218665379229\n",
      "epoch 12498: train loss: 0.11215492074684466, test loss: 0.2633223250041538\n",
      "epoch 12499: train loss: 0.11215264992814628, test loss: 0.2633227835380416\n",
      "epoch 12500: train loss: 0.11215037935595015, test loss: 0.2633232421395694\n",
      "epoch 12501: train loss: 0.11214810903020794, test loss: 0.2633237008087206\n",
      "epoch 12502: train loss: 0.11214583895087125, test loss: 0.26332415954547833\n",
      "epoch 12503: train loss: 0.11214356911789179, test loss: 0.26332461834982585\n",
      "epoch 12504: train loss: 0.11214129953122119, test loss: 0.2633250772217465\n",
      "epoch 12505: train loss: 0.11213903019081116, test loss: 0.26332553616122323\n",
      "epoch 12506: train loss: 0.11213676109661337, test loss: 0.2633259951682396\n",
      "epoch 12507: train loss: 0.11213449224857958, test loss: 0.2633264542427788\n",
      "epoch 12508: train loss: 0.11213222364666148, test loss: 0.26332691338482406\n",
      "epoch 12509: train loss: 0.11212995529081084, test loss: 0.2633273725943586\n",
      "epoch 12510: train loss: 0.11212768718097937, test loss: 0.26332783187136577\n",
      "epoch 12511: train loss: 0.11212541931711888, test loss: 0.26332829121582874\n",
      "epoch 12512: train loss: 0.11212315169918112, test loss: 0.263328750627731\n",
      "epoch 12513: train loss: 0.11212088432711789, test loss: 0.2633292101070556\n",
      "epoch 12514: train loss: 0.11211861720088104, test loss: 0.26332966965378596\n",
      "epoch 12515: train loss: 0.11211635032042232, test loss: 0.26333012926790533\n",
      "epoch 12516: train loss: 0.11211408368569362, test loss: 0.263330588949397\n",
      "epoch 12517: train loss: 0.11211181729664675, test loss: 0.2633310486982443\n",
      "epoch 12518: train loss: 0.11210955115323358, test loss: 0.2633315085144305\n",
      "epoch 12519: train loss: 0.112107285255406, test loss: 0.26333196839793904\n",
      "epoch 12520: train loss: 0.1121050196031159, test loss: 0.2633324283487531\n",
      "epoch 12521: train loss: 0.11210275419631516, test loss: 0.26333288836685603\n",
      "epoch 12522: train loss: 0.11210048903495572, test loss: 0.2633333484522312\n",
      "epoch 12523: train loss: 0.11209822411898948, test loss: 0.26333380860486194\n",
      "epoch 12524: train loss: 0.11209595944836838, test loss: 0.2633342688247315\n",
      "epoch 12525: train loss: 0.1120936950230444, test loss: 0.26333472911182326\n",
      "epoch 12526: train loss: 0.11209143084296949, test loss: 0.26333518946612067\n",
      "epoch 12527: train loss: 0.11208916690809563, test loss: 0.26333564988760694\n",
      "epoch 12528: train loss: 0.11208690321837483, test loss: 0.2633361103762655\n",
      "epoch 12529: train loss: 0.11208463977375907, test loss: 0.2633365709320796\n",
      "epoch 12530: train loss: 0.11208237657420039, test loss: 0.26333703155503274\n",
      "epoch 12531: train loss: 0.11208011361965081, test loss: 0.2633374922451083\n",
      "epoch 12532: train loss: 0.11207785091006241, test loss: 0.2633379530022895\n",
      "epoch 12533: train loss: 0.1120755884453872, test loss: 0.2633384138265598\n",
      "epoch 12534: train loss: 0.11207332622557731, test loss: 0.2633388747179027\n",
      "epoch 12535: train loss: 0.11207106425058477, test loss: 0.2633393356763014\n",
      "epoch 12536: train loss: 0.11206880252036175, test loss: 0.2633397967017393\n",
      "epoch 12537: train loss: 0.11206654103486029, test loss: 0.26334025779419995\n",
      "epoch 12538: train loss: 0.11206427979403255, test loss: 0.26334071895366656\n",
      "epoch 12539: train loss: 0.11206201879783068, test loss: 0.2633411801801227\n",
      "epoch 12540: train loss: 0.11205975804620684, test loss: 0.26334164147355177\n",
      "epoch 12541: train loss: 0.11205749753911316, test loss: 0.263342102833937\n",
      "epoch 12542: train loss: 0.11205523727650185, test loss: 0.263342564261262\n",
      "epoch 12543: train loss: 0.11205297725832508, test loss: 0.26334302575551005\n",
      "epoch 12544: train loss: 0.11205071748453507, test loss: 0.2633434873166648\n",
      "epoch 12545: train loss: 0.11204845795508406, test loss: 0.2633439489447094\n",
      "epoch 12546: train loss: 0.11204619866992427, test loss: 0.2633444106396274\n",
      "epoch 12547: train loss: 0.11204393962900791, test loss: 0.26334487240140236\n",
      "epoch 12548: train loss: 0.11204168083228726, test loss: 0.2633453342300175\n",
      "epoch 12549: train loss: 0.11203942227971464, test loss: 0.26334579612545644\n",
      "epoch 12550: train loss: 0.11203716397124229, test loss: 0.2633462580877026\n",
      "epoch 12551: train loss: 0.1120349059068225, test loss: 0.26334672011673943\n",
      "epoch 12552: train loss: 0.1120326480864076, test loss: 0.2633471822125504\n",
      "epoch 12553: train loss: 0.11203039050994992, test loss: 0.26334764437511893\n",
      "epoch 12554: train loss: 0.1120281331774018, test loss: 0.26334810660442853\n",
      "epoch 12555: train loss: 0.11202587608871559, test loss: 0.26334856890046265\n",
      "epoch 12556: train loss: 0.11202361924384363, test loss: 0.26334903126320486\n",
      "epoch 12557: train loss: 0.11202136264273832, test loss: 0.26334949369263855\n",
      "epoch 12558: train loss: 0.11201910628535204, test loss: 0.26334995618874724\n",
      "epoch 12559: train loss: 0.11201685017163722, test loss: 0.2633504187515144\n",
      "epoch 12560: train loss: 0.11201459430154627, test loss: 0.26335088138092355\n",
      "epoch 12561: train loss: 0.11201233867503159, test loss: 0.2633513440769582\n",
      "epoch 12562: train loss: 0.11201008329204566, test loss: 0.2633518068396019\n",
      "epoch 12563: train loss: 0.11200782815254094, test loss: 0.26335226966883807\n",
      "epoch 12564: train loss: 0.11200557325646984, test loss: 0.26335273256465025\n",
      "epoch 12565: train loss: 0.11200331860378493, test loss: 0.263353195527022\n",
      "epoch 12566: train loss: 0.11200106419443864, test loss: 0.26335365855593684\n",
      "epoch 12567: train loss: 0.11199881002838351, test loss: 0.26335412165137834\n",
      "epoch 12568: train loss: 0.11199655610557206, test loss: 0.26335458481333\n",
      "epoch 12569: train loss: 0.11199430242595682, test loss: 0.26335504804177534\n",
      "epoch 12570: train loss: 0.11199204898949035, test loss: 0.2633555113366979\n",
      "epoch 12571: train loss: 0.11198979579612517, test loss: 0.2633559746980812\n",
      "epoch 12572: train loss: 0.11198754284581394, test loss: 0.26335643812590886\n",
      "epoch 12573: train loss: 0.11198529013850918, test loss: 0.2633569016201644\n",
      "epoch 12574: train loss: 0.11198303767416348, test loss: 0.26335736518083136\n",
      "epoch 12575: train loss: 0.11198078545272953, test loss: 0.26335782880789343\n",
      "epoch 12576: train loss: 0.1119785334741599, test loss: 0.263358292501334\n",
      "epoch 12577: train loss: 0.1119762817384072, test loss: 0.2633587562611369\n",
      "epoch 12578: train loss: 0.11197403024542417, test loss: 0.2633592200872854\n",
      "epoch 12579: train loss: 0.11197177899516342, test loss: 0.2633596839797633\n",
      "epoch 12580: train loss: 0.11196952798757766, test loss: 0.26336014793855406\n",
      "epoch 12581: train loss: 0.11196727722261954, test loss: 0.2633606119636414\n",
      "epoch 12582: train loss: 0.1119650267002418, test loss: 0.26336107605500886\n",
      "epoch 12583: train loss: 0.11196277642039716, test loss: 0.26336154021264\n",
      "epoch 12584: train loss: 0.11196052638303833, test loss: 0.26336200443651847\n",
      "epoch 12585: train loss: 0.11195827658811808, test loss: 0.2633624687266279\n",
      "epoch 12586: train loss: 0.11195602703558916, test loss: 0.2633629330829519\n",
      "epoch 12587: train loss: 0.11195377772540434, test loss: 0.26336339750547405\n",
      "epoch 12588: train loss: 0.11195152865751641, test loss: 0.26336386199417805\n",
      "epoch 12589: train loss: 0.11194927983187813, test loss: 0.26336432654904735\n",
      "epoch 12590: train loss: 0.11194703124844238, test loss: 0.26336479117006584\n",
      "epoch 12591: train loss: 0.11194478290716195, test loss: 0.263365255857217\n",
      "epoch 12592: train loss: 0.11194253480798963, test loss: 0.26336572061048447\n",
      "epoch 12593: train loss: 0.11194028695087833, test loss: 0.263366185429852\n",
      "epoch 12594: train loss: 0.11193803933578089, test loss: 0.26336665031530315\n",
      "epoch 12595: train loss: 0.1119357919626502, test loss: 0.2633671152668215\n",
      "epoch 12596: train loss: 0.11193354483143914, test loss: 0.2633675802843909\n",
      "epoch 12597: train loss: 0.11193129794210059, test loss: 0.2633680453679949\n",
      "epoch 12598: train loss: 0.11192905129458751, test loss: 0.2633685105176173\n",
      "epoch 12599: train loss: 0.1119268048888528, test loss: 0.26336897573324153\n",
      "epoch 12600: train loss: 0.1119245587248494, test loss: 0.2633694410148515\n",
      "epoch 12601: train loss: 0.11192231280253026, test loss: 0.26336990636243074\n",
      "epoch 12602: train loss: 0.11192006712184835, test loss: 0.26337037177596306\n",
      "epoch 12603: train loss: 0.11191782168275663, test loss: 0.26337083725543203\n",
      "epoch 12604: train loss: 0.11191557648520814, test loss: 0.2633713028008215\n",
      "epoch 12605: train loss: 0.11191333152915586, test loss: 0.26337176841211496\n",
      "epoch 12606: train loss: 0.1119110868145528, test loss: 0.26337223408929633\n",
      "epoch 12607: train loss: 0.11190884234135201, test loss: 0.2633726998323492\n",
      "epoch 12608: train loss: 0.11190659810950651, test loss: 0.2633731656412573\n",
      "epoch 12609: train loss: 0.11190435411896939, test loss: 0.2633736315160044\n",
      "epoch 12610: train loss: 0.11190211036969366, test loss: 0.26337409745657425\n",
      "epoch 12611: train loss: 0.11189986686163249, test loss: 0.26337456346295035\n",
      "epoch 12612: train loss: 0.11189762359473888, test loss: 0.2633750295351166\n",
      "epoch 12613: train loss: 0.11189538056896602, test loss: 0.26337549567305685\n",
      "epoch 12614: train loss: 0.11189313778426699, test loss: 0.2633759618767546\n",
      "epoch 12615: train loss: 0.11189089524059494, test loss: 0.2633764281461938\n",
      "epoch 12616: train loss: 0.11188865293790301, test loss: 0.26337689448135804\n",
      "epoch 12617: train loss: 0.11188641087614436, test loss: 0.26337736088223124\n",
      "epoch 12618: train loss: 0.11188416905527215, test loss: 0.263377827348797\n",
      "epoch 12619: train loss: 0.1118819274752396, test loss: 0.2633782938810392\n",
      "epoch 12620: train loss: 0.11187968613599988, test loss: 0.26337876047894154\n",
      "epoch 12621: train loss: 0.11187744503750624, test loss: 0.26337922714248785\n",
      "epoch 12622: train loss: 0.11187520417971186, test loss: 0.26337969387166177\n",
      "epoch 12623: train loss: 0.11187296356256997, test loss: 0.26338016066644726\n",
      "epoch 12624: train loss: 0.11187072318603387, test loss: 0.263380627526828\n",
      "epoch 12625: train loss: 0.11186848305005681, test loss: 0.26338109445278785\n",
      "epoch 12626: train loss: 0.11186624315459202, test loss: 0.26338156144431046\n",
      "epoch 12627: train loss: 0.11186400349959286, test loss: 0.26338202850137976\n",
      "epoch 12628: train loss: 0.1118617640850126, test loss: 0.26338249562397964\n",
      "epoch 12629: train loss: 0.11185952491080452, test loss: 0.26338296281209367\n",
      "epoch 12630: train loss: 0.11185728597692199, test loss: 0.2633834300657057\n",
      "epoch 12631: train loss: 0.11185504728331834, test loss: 0.26338389738479984\n",
      "epoch 12632: train loss: 0.11185280882994693, test loss: 0.26338436476935956\n",
      "epoch 12633: train loss: 0.11185057061676112, test loss: 0.2633848322193688\n",
      "epoch 12634: train loss: 0.11184833264371428, test loss: 0.26338529973481145\n",
      "epoch 12635: train loss: 0.11184609491075982, test loss: 0.2633857673156713\n",
      "epoch 12636: train loss: 0.11184385741785113, test loss: 0.2633862349619322\n",
      "epoch 12637: train loss: 0.11184162016494163, test loss: 0.263386702673578\n",
      "epoch 12638: train loss: 0.11183938315198477, test loss: 0.26338717045059257\n",
      "epoch 12639: train loss: 0.11183714637893395, test loss: 0.2633876382929596\n",
      "epoch 12640: train loss: 0.11183490984574267, test loss: 0.2633881062006631\n",
      "epoch 12641: train loss: 0.11183267355236437, test loss: 0.263388574173687\n",
      "epoch 12642: train loss: 0.11183043749875254, test loss: 0.26338904221201503\n",
      "epoch 12643: train loss: 0.11182820168486067, test loss: 0.26338951031563107\n",
      "epoch 12644: train loss: 0.1118259661106423, test loss: 0.26338997848451895\n",
      "epoch 12645: train loss: 0.1118237307760509, test loss: 0.26339044671866274\n",
      "epoch 12646: train loss: 0.11182149568104004, test loss: 0.2633909150180461\n",
      "epoch 12647: train loss: 0.11181926082556323, test loss: 0.26339138338265317\n",
      "epoch 12648: train loss: 0.11181702620957407, test loss: 0.2633918518124675\n",
      "epoch 12649: train loss: 0.1118147918330261, test loss: 0.2633923203074733\n",
      "epoch 12650: train loss: 0.11181255769587292, test loss: 0.2633927888676542\n",
      "epoch 12651: train loss: 0.1118103237980681, test loss: 0.26339325749299436\n",
      "epoch 12652: train loss: 0.1118080901395653, test loss: 0.2633937261834776\n",
      "epoch 12653: train loss: 0.11180585672031806, test loss: 0.2633941949390877\n",
      "epoch 12654: train loss: 0.11180362354028012, test loss: 0.26339466375980874\n",
      "epoch 12655: train loss: 0.11180139059940504, test loss: 0.2633951326456246\n",
      "epoch 12656: train loss: 0.11179915789764654, test loss: 0.26339560159651915\n",
      "epoch 12657: train loss: 0.11179692543495824, test loss: 0.2633960706124764\n",
      "epoch 12658: train loss: 0.11179469321129386, test loss: 0.26339653969348026\n",
      "epoch 12659: train loss: 0.11179246122660708, test loss: 0.2633970088395146\n",
      "epoch 12660: train loss: 0.11179022948085163, test loss: 0.26339747805056346\n",
      "epoch 12661: train loss: 0.1117879979739812, test loss: 0.26339794732661076\n",
      "epoch 12662: train loss: 0.11178576670594957, test loss: 0.2633984166676404\n",
      "epoch 12663: train loss: 0.11178353567671045, test loss: 0.26339888607363643\n",
      "epoch 12664: train loss: 0.11178130488621765, test loss: 0.2633993555445827\n",
      "epoch 12665: train loss: 0.1117790743344249, test loss: 0.26339982508046333\n",
      "epoch 12666: train loss: 0.11177684402128599, test loss: 0.263400294681262\n",
      "epoch 12667: train loss: 0.11177461394675474, test loss: 0.2634007643469631\n",
      "epoch 12668: train loss: 0.11177238411078495, test loss: 0.2634012340775503\n",
      "epoch 12669: train loss: 0.11177015451333044, test loss: 0.2634017038730077\n",
      "epoch 12670: train loss: 0.11176792515434504, test loss: 0.2634021737333192\n",
      "epoch 12671: train loss: 0.11176569603378265, test loss: 0.26340264365846894\n",
      "epoch 12672: train loss: 0.11176346715159707, test loss: 0.2634031136484407\n",
      "epoch 12673: train loss: 0.11176123850774224, test loss: 0.2634035837032187\n",
      "epoch 12674: train loss: 0.11175901010217199, test loss: 0.26340405382278675\n",
      "epoch 12675: train loss: 0.11175678193484025, test loss: 0.26340452400712905\n",
      "epoch 12676: train loss: 0.1117545540057009, test loss: 0.26340499425622954\n",
      "epoch 12677: train loss: 0.11175232631470791, test loss: 0.26340546457007213\n",
      "epoch 12678: train loss: 0.11175009886181522, test loss: 0.263405934948641\n",
      "epoch 12679: train loss: 0.11174787164697675, test loss: 0.26340640539192\n",
      "epoch 12680: train loss: 0.11174564467014644, test loss: 0.2634068758998934\n",
      "epoch 12681: train loss: 0.11174341793127837, test loss: 0.26340734647254505\n",
      "epoch 12682: train loss: 0.1117411914303264, test loss: 0.26340781710985905\n",
      "epoch 12683: train loss: 0.11173896516724463, test loss: 0.26340828781181946\n",
      "epoch 12684: train loss: 0.11173673914198705, test loss: 0.2634087585784103\n",
      "epoch 12685: train loss: 0.11173451335450765, test loss: 0.26340922940961564\n",
      "epoch 12686: train loss: 0.11173228780476048, test loss: 0.2634097003054195\n",
      "epoch 12687: train loss: 0.11173006249269964, test loss: 0.2634101712658059\n",
      "epoch 12688: train loss: 0.11172783741827913, test loss: 0.26341064229075906\n",
      "epoch 12689: train loss: 0.11172561258145311, test loss: 0.26341111338026285\n",
      "epoch 12690: train loss: 0.11172338798217557, test loss: 0.26341158453430163\n",
      "epoch 12691: train loss: 0.11172116362040069, test loss: 0.26341205575285925\n",
      "epoch 12692: train loss: 0.11171893949608254, test loss: 0.2634125270359198\n",
      "epoch 12693: train loss: 0.11171671560917527, test loss: 0.26341299838346743\n",
      "epoch 12694: train loss: 0.11171449195963301, test loss: 0.2634134697954862\n",
      "epoch 12695: train loss: 0.11171226854740994, test loss: 0.2634139412719603\n",
      "epoch 12696: train loss: 0.11171004537246017, test loss: 0.26341441281287364\n",
      "epoch 12697: train loss: 0.11170782243473791, test loss: 0.2634148844182105\n",
      "epoch 12698: train loss: 0.11170559973419739, test loss: 0.2634153560879549\n",
      "epoch 12699: train loss: 0.11170337727079273, test loss: 0.26341582782209094\n",
      "epoch 12700: train loss: 0.11170115504447821, test loss: 0.26341629962060287\n",
      "epoch 12701: train loss: 0.11169893305520803, test loss: 0.2634167714834746\n",
      "epoch 12702: train loss: 0.11169671130293643, test loss: 0.26341724341069045\n",
      "epoch 12703: train loss: 0.11169448978761767, test loss: 0.26341771540223446\n",
      "epoch 12704: train loss: 0.11169226850920605, test loss: 0.26341818745809076\n",
      "epoch 12705: train loss: 0.11169004746765579, test loss: 0.2634186595782436\n",
      "epoch 12706: train loss: 0.11168782666292117, test loss: 0.2634191317626769\n",
      "epoch 12707: train loss: 0.11168560609495656, test loss: 0.26341960401137504\n",
      "epoch 12708: train loss: 0.11168338576371624, test loss: 0.263420076324322\n",
      "epoch 12709: train loss: 0.11168116566915452, test loss: 0.263420548701502\n",
      "epoch 12710: train loss: 0.11167894581122578, test loss: 0.2634210211428993\n",
      "epoch 12711: train loss: 0.11167672618988433, test loss: 0.2634214936484978\n",
      "epoch 12712: train loss: 0.11167450680508459, test loss: 0.263421966218282\n",
      "epoch 12713: train loss: 0.11167228765678087, test loss: 0.2634224388522358\n",
      "epoch 12714: train loss: 0.11167006874492763, test loss: 0.26342291155034353\n",
      "epoch 12715: train loss: 0.11166785006947923, test loss: 0.2634233843125894\n",
      "epoch 12716: train loss: 0.11166563163039007, test loss: 0.2634238571389574\n",
      "epoch 12717: train loss: 0.11166341342761461, test loss: 0.26342433002943194\n",
      "epoch 12718: train loss: 0.11166119546110728, test loss: 0.2634248029839971\n",
      "epoch 12719: train loss: 0.11165897773082258, test loss: 0.26342527600263715\n",
      "epoch 12720: train loss: 0.11165676023671485, test loss: 0.2634257490853361\n",
      "epoch 12721: train loss: 0.11165454297873868, test loss: 0.26342622223207846\n",
      "epoch 12722: train loss: 0.11165232595684856, test loss: 0.2634266954428482\n",
      "epoch 12723: train loss: 0.11165010917099892, test loss: 0.26342716871762967\n",
      "epoch 12724: train loss: 0.11164789262114433, test loss: 0.26342764205640706\n",
      "epoch 12725: train loss: 0.11164567630723927, test loss: 0.2634281154591645\n",
      "epoch 12726: train loss: 0.11164346022923834, test loss: 0.26342858892588633\n",
      "epoch 12727: train loss: 0.11164124438709606, test loss: 0.2634290624565568\n",
      "epoch 12728: train loss: 0.11163902878076697, test loss: 0.26342953605116004\n",
      "epoch 12729: train loss: 0.1116368134102057, test loss: 0.26343000970968034\n",
      "epoch 12730: train loss: 0.11163459827536679, test loss: 0.263430483432102\n",
      "epoch 12731: train loss: 0.11163238337620486, test loss: 0.26343095721840915\n",
      "epoch 12732: train loss: 0.11163016871267456, test loss: 0.2634314310685862\n",
      "epoch 12733: train loss: 0.11162795428473045, test loss: 0.2634319049826173\n",
      "epoch 12734: train loss: 0.1116257400923272, test loss: 0.2634323789604867\n",
      "epoch 12735: train loss: 0.11162352613541947, test loss: 0.2634328530021787\n",
      "epoch 12736: train loss: 0.11162131241396192, test loss: 0.2634333271076777\n",
      "epoch 12737: train loss: 0.11161909892790924, test loss: 0.26343380127696775\n",
      "epoch 12738: train loss: 0.11161688567721605, test loss: 0.2634342755100333\n",
      "epoch 12739: train loss: 0.11161467266183714, test loss: 0.2634347498068585\n",
      "epoch 12740: train loss: 0.11161245988172716, test loss: 0.26343522416742776\n",
      "epoch 12741: train loss: 0.11161024733684087, test loss: 0.2634356985917253\n",
      "epoch 12742: train loss: 0.11160803502713298, test loss: 0.26343617307973544\n",
      "epoch 12743: train loss: 0.11160582295255826, test loss: 0.26343664763144253\n",
      "epoch 12744: train loss: 0.11160361111307149, test loss: 0.2634371222468308\n",
      "epoch 12745: train loss: 0.1116013995086274, test loss: 0.2634375969258846\n",
      "epoch 12746: train loss: 0.1115991881391808, test loss: 0.2634380716685882\n",
      "epoch 12747: train loss: 0.11159697700468647, test loss: 0.2634385464749259\n",
      "epoch 12748: train loss: 0.11159476610509927, test loss: 0.26343902134488223\n",
      "epoch 12749: train loss: 0.11159255544037397, test loss: 0.26343949627844127\n",
      "epoch 12750: train loss: 0.11159034501046543, test loss: 0.26343997127558744\n",
      "epoch 12751: train loss: 0.11158813481532849, test loss: 0.2634404463363051\n",
      "epoch 12752: train loss: 0.11158592485491806, test loss: 0.26344092146057846\n",
      "epoch 12753: train loss: 0.11158371512918894, test loss: 0.2634413966483921\n",
      "epoch 12754: train loss: 0.11158150563809603, test loss: 0.2634418718997303\n",
      "epoch 12755: train loss: 0.11157929638159425, test loss: 0.26344234721457727\n",
      "epoch 12756: train loss: 0.11157708735963849, test loss: 0.2634428225929173\n",
      "epoch 12757: train loss: 0.1115748785721837, test loss: 0.26344329803473504\n",
      "epoch 12758: train loss: 0.1115726700191848, test loss: 0.26344377354001475\n",
      "epoch 12759: train loss: 0.11157046170059676, test loss: 0.2634442491087407\n",
      "epoch 12760: train loss: 0.11156825361637449, test loss: 0.26344472474089736\n",
      "epoch 12761: train loss: 0.11156604576647298, test loss: 0.26344520043646913\n",
      "epoch 12762: train loss: 0.11156383815084724, test loss: 0.26344567619544024\n",
      "epoch 12763: train loss: 0.1115616307694522, test loss: 0.2634461520177952\n",
      "epoch 12764: train loss: 0.11155942362224294, test loss: 0.26344662790351836\n",
      "epoch 12765: train loss: 0.11155721670917447, test loss: 0.26344710385259423\n",
      "epoch 12766: train loss: 0.1115550100302018, test loss: 0.26344757986500705\n",
      "epoch 12767: train loss: 0.11155280358527994, test loss: 0.2634480559407412\n",
      "epoch 12768: train loss: 0.11155059737436401, test loss: 0.26344853207978125\n",
      "epoch 12769: train loss: 0.11154839139740907, test loss: 0.2634490082821116\n",
      "epoch 12770: train loss: 0.11154618565437016, test loss: 0.26344948454771644\n",
      "epoch 12771: train loss: 0.11154398014520242, test loss: 0.26344996087658046\n",
      "epoch 12772: train loss: 0.11154177486986093, test loss: 0.26345043726868794\n",
      "epoch 12773: train loss: 0.11153956982830081, test loss: 0.26345091372402335\n",
      "epoch 12774: train loss: 0.11153736502047722, test loss: 0.2634513902425711\n",
      "epoch 12775: train loss: 0.11153516044634523, test loss: 0.26345186682431565\n",
      "epoch 12776: train loss: 0.11153295610586005, test loss: 0.26345234346924135\n",
      "epoch 12777: train loss: 0.11153075199897687, test loss: 0.2634528201773329\n",
      "epoch 12778: train loss: 0.11152854812565081, test loss: 0.2634532969485745\n",
      "epoch 12779: train loss: 0.11152634448583708, test loss: 0.2634537737829506\n",
      "epoch 12780: train loss: 0.11152414107949092, test loss: 0.2634542506804458\n",
      "epoch 12781: train loss: 0.11152193790656746, test loss: 0.2634547276410445\n",
      "epoch 12782: train loss: 0.11151973496702203, test loss: 0.26345520466473116\n",
      "epoch 12783: train loss: 0.1115175322608098, test loss: 0.26345568175149026\n",
      "epoch 12784: train loss: 0.11151532978788609, test loss: 0.2634561589013063\n",
      "epoch 12785: train loss: 0.11151312754820605, test loss: 0.2634566361141637\n",
      "epoch 12786: train loss: 0.11151092554172508, test loss: 0.26345711339004696\n",
      "epoch 12787: train loss: 0.11150872376839838, test loss: 0.2634575907289406\n",
      "epoch 12788: train loss: 0.11150652222818129, test loss: 0.26345806813082917\n",
      "epoch 12789: train loss: 0.11150432092102912, test loss: 0.263458545595697\n",
      "epoch 12790: train loss: 0.1115021198468972, test loss: 0.2634590231235287\n",
      "epoch 12791: train loss: 0.11149991900574084, test loss: 0.2634595007143087\n",
      "epoch 12792: train loss: 0.1114977183975154, test loss: 0.2634599783680216\n",
      "epoch 12793: train loss: 0.11149551802217628, test loss: 0.2634604560846519\n",
      "epoch 12794: train loss: 0.11149331787967881, test loss: 0.263460933864184\n",
      "epoch 12795: train loss: 0.11149111796997836, test loss: 0.2634614117066026\n",
      "epoch 12796: train loss: 0.1114889182930304, test loss: 0.2634618896118922\n",
      "epoch 12797: train loss: 0.11148671884879027, test loss: 0.2634623675800372\n",
      "epoch 12798: train loss: 0.11148451963721343, test loss: 0.2634628456110222\n",
      "epoch 12799: train loss: 0.11148232065825525, test loss: 0.2634633237048318\n",
      "epoch 12800: train loss: 0.11148012191187127, test loss: 0.26346380186145035\n",
      "epoch 12801: train loss: 0.11147792339801688, test loss: 0.2634642800808626\n",
      "epoch 12802: train loss: 0.11147572511664759, test loss: 0.26346475836305294\n",
      "epoch 12803: train loss: 0.11147352706771887, test loss: 0.2634652367080061\n",
      "epoch 12804: train loss: 0.11147132925118619, test loss: 0.2634657151157065\n",
      "epoch 12805: train loss: 0.11146913166700509, test loss: 0.26346619358613876\n",
      "epoch 12806: train loss: 0.11146693431513106, test loss: 0.2634666721192875\n",
      "epoch 12807: train loss: 0.11146473719551966, test loss: 0.26346715071513716\n",
      "epoch 12808: train loss: 0.11146254030812643, test loss: 0.26346762937367246\n",
      "epoch 12809: train loss: 0.11146034365290688, test loss: 0.26346810809487775\n",
      "epoch 12810: train loss: 0.11145814722981662, test loss: 0.2634685868787379\n",
      "epoch 12811: train loss: 0.1114559510388112, test loss: 0.26346906572523726\n",
      "epoch 12812: train loss: 0.11145375507984627, test loss: 0.2634695446343606\n",
      "epoch 12813: train loss: 0.11145155935287734, test loss: 0.2634700236060925\n",
      "epoch 12814: train loss: 0.11144936385786011, test loss: 0.2634705026404175\n",
      "epoch 12815: train loss: 0.11144716859475015, test loss: 0.2634709817373201\n",
      "epoch 12816: train loss: 0.11144497356350314, test loss: 0.263471460896785\n",
      "epoch 12817: train loss: 0.1114427787640747, test loss: 0.263471940118797\n",
      "epoch 12818: train loss: 0.1114405841964205, test loss: 0.2634724194033404\n",
      "epoch 12819: train loss: 0.11143838986049623, test loss: 0.2634728987504\n",
      "epoch 12820: train loss: 0.11143619575625754, test loss: 0.26347337815996036\n",
      "epoch 12821: train loss: 0.11143400188366019, test loss: 0.26347385763200615\n",
      "epoch 12822: train loss: 0.11143180824265983, test loss: 0.26347433716652197\n",
      "epoch 12823: train loss: 0.1114296148332122, test loss: 0.26347481676349255\n",
      "epoch 12824: train loss: 0.1114274216552731, test loss: 0.26347529642290246\n",
      "epoch 12825: train loss: 0.11142522870879816, test loss: 0.26347577614473633\n",
      "epoch 12826: train loss: 0.1114230359937432, test loss: 0.2634762559289788\n",
      "epoch 12827: train loss: 0.11142084351006401, test loss: 0.26347673577561453\n",
      "epoch 12828: train loss: 0.11141865125771634, test loss: 0.26347721568462823\n",
      "epoch 12829: train loss: 0.111416459236656, test loss: 0.26347769565600454\n",
      "epoch 12830: train loss: 0.1114142674468388, test loss: 0.2634781756897281\n",
      "epoch 12831: train loss: 0.11141207588822055, test loss: 0.2634786557857836\n",
      "epoch 12832: train loss: 0.11140988456075711, test loss: 0.26347913594415573\n",
      "epoch 12833: train loss: 0.11140769346440425, test loss: 0.2634796161648292\n",
      "epoch 12834: train loss: 0.1114055025991179, test loss: 0.2634800964477885\n",
      "epoch 12835: train loss: 0.11140331196485388, test loss: 0.2634805767930185\n",
      "epoch 12836: train loss: 0.11140112156156806, test loss: 0.2634810572005041\n",
      "epoch 12837: train loss: 0.11139893138921639, test loss: 0.2634815376702295\n",
      "epoch 12838: train loss: 0.11139674144775474, test loss: 0.26348201820217965\n",
      "epoch 12839: train loss: 0.111394551737139, test loss: 0.26348249879633934\n",
      "epoch 12840: train loss: 0.11139236225732514, test loss: 0.26348297945269317\n",
      "epoch 12841: train loss: 0.11139017300826907, test loss: 0.2634834601712258\n",
      "epoch 12842: train loss: 0.11138798398992673, test loss: 0.26348394095192207\n",
      "epoch 12843: train loss: 0.11138579520225411, test loss: 0.26348442179476667\n",
      "epoch 12844: train loss: 0.11138360664520716, test loss: 0.2634849026997443\n",
      "epoch 12845: train loss: 0.1113814183187419, test loss: 0.26348538366683966\n",
      "epoch 12846: train loss: 0.11137923022281428, test loss: 0.2634858646960375\n",
      "epoch 12847: train loss: 0.11137704235738034, test loss: 0.2634863457873226\n",
      "epoch 12848: train loss: 0.11137485472239611, test loss: 0.26348682694067965\n",
      "epoch 12849: train loss: 0.11137266731781757, test loss: 0.2634873081560935\n",
      "epoch 12850: train loss: 0.11137048014360083, test loss: 0.2634877894335487\n",
      "epoch 12851: train loss: 0.11136829319970194, test loss: 0.2634882707730302\n",
      "epoch 12852: train loss: 0.11136610648607694, test loss: 0.2634887521745226\n",
      "epoch 12853: train loss: 0.11136392000268187, test loss: 0.26348923363801086\n",
      "epoch 12854: train loss: 0.11136173374947292, test loss: 0.26348971516347947\n",
      "epoch 12855: train loss: 0.11135954772640615, test loss: 0.2634901967509134\n",
      "epoch 12856: train loss: 0.11135736193343768, test loss: 0.2634906784002974\n",
      "epoch 12857: train loss: 0.1113551763705236, test loss: 0.2634911601116162\n",
      "epoch 12858: train loss: 0.11135299103762009, test loss: 0.26349164188485463\n",
      "epoch 12859: train loss: 0.11135080593468331, test loss: 0.2634921237199974\n",
      "epoch 12860: train loss: 0.1113486210616694, test loss: 0.26349260561702936\n",
      "epoch 12861: train loss: 0.11134643641853455, test loss: 0.26349308757593537\n",
      "epoch 12862: train loss: 0.11134425200523491, test loss: 0.2634935695967001\n",
      "epoch 12863: train loss: 0.11134206782172676, test loss: 0.26349405167930834\n",
      "epoch 12864: train loss: 0.1113398838679662, test loss: 0.263494533823745\n",
      "epoch 12865: train loss: 0.11133770014390955, test loss: 0.26349501602999487\n",
      "epoch 12866: train loss: 0.111335516649513, test loss: 0.26349549829804275\n",
      "epoch 12867: train loss: 0.11133333338473282, test loss: 0.26349598062787344\n",
      "epoch 12868: train loss: 0.11133115034952525, test loss: 0.26349646301947177\n",
      "epoch 12869: train loss: 0.11132896754384652, test loss: 0.26349694547282254\n",
      "epoch 12870: train loss: 0.11132678496765301, test loss: 0.26349742798791065\n",
      "epoch 12871: train loss: 0.11132460262090095, test loss: 0.2634979105647208\n",
      "epoch 12872: train loss: 0.11132242050354663, test loss: 0.26349839320323815\n",
      "epoch 12873: train loss: 0.1113202386155464, test loss: 0.2634988759034471\n",
      "epoch 12874: train loss: 0.1113180569568566, test loss: 0.2634993586653328\n",
      "epoch 12875: train loss: 0.11131587552743354, test loss: 0.26349984148888006\n",
      "epoch 12876: train loss: 0.11131369432723356, test loss: 0.26350032437407367\n",
      "epoch 12877: train loss: 0.11131151335621306, test loss: 0.2635008073208985\n",
      "epoch 12878: train loss: 0.11130933261432845, test loss: 0.26350129032933933\n",
      "epoch 12879: train loss: 0.11130715210153602, test loss: 0.2635017733993813\n",
      "epoch 12880: train loss: 0.11130497181779223, test loss: 0.263502256531009\n",
      "epoch 12881: train loss: 0.11130279176305351, test loss: 0.26350273972420735\n",
      "epoch 12882: train loss: 0.11130061193727624, test loss: 0.2635032229789614\n",
      "epoch 12883: train loss: 0.11129843234041686, test loss: 0.26350370629525594\n",
      "epoch 12884: train loss: 0.11129625297243184, test loss: 0.2635041896730758\n",
      "epoch 12885: train loss: 0.11129407383327763, test loss: 0.26350467311240583\n",
      "epoch 12886: train loss: 0.11129189492291068, test loss: 0.26350515661323115\n",
      "epoch 12887: train loss: 0.11128971624128753, test loss: 0.26350564017553646\n",
      "epoch 12888: train loss: 0.11128753778836459, test loss: 0.26350612379930666\n",
      "epoch 12889: train loss: 0.11128535956409841, test loss: 0.2635066074845268\n",
      "epoch 12890: train loss: 0.11128318156844554, test loss: 0.2635070912311817\n",
      "epoch 12891: train loss: 0.11128100380136247, test loss: 0.26350757503925626\n",
      "epoch 12892: train loss: 0.1112788262628057, test loss: 0.26350805890873547\n",
      "epoch 12893: train loss: 0.11127664895273187, test loss: 0.2635085428396042\n",
      "epoch 12894: train loss: 0.11127447187109749, test loss: 0.26350902683184735\n",
      "epoch 12895: train loss: 0.11127229501785912, test loss: 0.26350951088544994\n",
      "epoch 12896: train loss: 0.11127011839297339, test loss: 0.26350999500039685\n",
      "epoch 12897: train loss: 0.1112679419963969, test loss: 0.26351047917667303\n",
      "epoch 12898: train loss: 0.11126576582808621, test loss: 0.2635109634142634\n",
      "epoch 12899: train loss: 0.111263589887998, test loss: 0.26351144771315305\n",
      "epoch 12900: train loss: 0.11126141417608888, test loss: 0.2635119320733267\n",
      "epoch 12901: train loss: 0.11125923869231549, test loss: 0.26351241649476953\n",
      "epoch 12902: train loss: 0.1112570634366345, test loss: 0.26351290097746627\n",
      "epoch 12903: train loss: 0.11125488840900256, test loss: 0.2635133855214021\n",
      "epoch 12904: train loss: 0.11125271360937634, test loss: 0.2635138701265618\n",
      "epoch 12905: train loss: 0.11125053903771259, test loss: 0.2635143547929305\n",
      "epoch 12906: train loss: 0.11124836469396797, test loss: 0.26351483952049315\n",
      "epoch 12907: train loss: 0.1112461905780992, test loss: 0.2635153243092347\n",
      "epoch 12908: train loss: 0.11124401669006302, test loss: 0.2635158091591401\n",
      "epoch 12909: train loss: 0.11124184302981614, test loss: 0.2635162940701944\n",
      "epoch 12910: train loss: 0.11123966959731536, test loss: 0.2635167790423825\n",
      "epoch 12911: train loss: 0.11123749639251738, test loss: 0.2635172640756895\n",
      "epoch 12912: train loss: 0.11123532341537903, test loss: 0.2635177491701004\n",
      "epoch 12913: train loss: 0.11123315066585705, test loss: 0.2635182343256002\n",
      "epoch 12914: train loss: 0.11123097814390828, test loss: 0.2635187195421738\n",
      "epoch 12915: train loss: 0.1112288058494895, test loss: 0.2635192048198063\n",
      "epoch 12916: train loss: 0.11122663378255755, test loss: 0.26351969015848276\n",
      "epoch 12917: train loss: 0.11122446194306924, test loss: 0.26352017555818813\n",
      "epoch 12918: train loss: 0.1112222903309814, test loss: 0.2635206610189075\n",
      "epoch 12919: train loss: 0.11122011894625095, test loss: 0.26352114654062575\n",
      "epoch 12920: train loss: 0.11121794778883469, test loss: 0.263521632123328\n",
      "epoch 12921: train loss: 0.11121577685868952, test loss: 0.2635221177669994\n",
      "epoch 12922: train loss: 0.11121360615577235, test loss: 0.26352260347162487\n",
      "epoch 12923: train loss: 0.11121143568004006, test loss: 0.26352308923718953\n",
      "epoch 12924: train loss: 0.11120926543144957, test loss: 0.2635235750636783\n",
      "epoch 12925: train loss: 0.11120709540995781, test loss: 0.26352406095107633\n",
      "epoch 12926: train loss: 0.11120492561552169, test loss: 0.2635245468993687\n",
      "epoch 12927: train loss: 0.11120275604809819, test loss: 0.2635250329085404\n",
      "epoch 12928: train loss: 0.11120058670764425, test loss: 0.26352551897857657\n",
      "epoch 12929: train loss: 0.11119841759411685, test loss: 0.2635260051094622\n",
      "epoch 12930: train loss: 0.11119624870747298, test loss: 0.26352649130118244\n",
      "epoch 12931: train loss: 0.1111940800476696, test loss: 0.26352697755372234\n",
      "epoch 12932: train loss: 0.11119191161466375, test loss: 0.26352746386706694\n",
      "epoch 12933: train loss: 0.11118974340841245, test loss: 0.2635279502412013\n",
      "epoch 12934: train loss: 0.1111875754288727, test loss: 0.2635284366761106\n",
      "epoch 12935: train loss: 0.11118540767600155, test loss: 0.26352892317177995\n",
      "epoch 12936: train loss: 0.11118324014975606, test loss: 0.26352940972819433\n",
      "epoch 12937: train loss: 0.11118107285009331, test loss: 0.26352989634533897\n",
      "epoch 12938: train loss: 0.1111789057769703, test loss: 0.2635303830231989\n",
      "epoch 12939: train loss: 0.11117673893034422, test loss: 0.26353086976175916\n",
      "epoch 12940: train loss: 0.11117457231017211, test loss: 0.26353135656100496\n",
      "epoch 12941: train loss: 0.11117240591641106, test loss: 0.2635318434209215\n",
      "epoch 12942: train loss: 0.11117023974901827, test loss: 0.26353233034149376\n",
      "epoch 12943: train loss: 0.11116807380795078, test loss: 0.2635328173227069\n",
      "epoch 12944: train loss: 0.1111659080931658, test loss: 0.26353330436454603\n",
      "epoch 12945: train loss: 0.11116374260462045, test loss: 0.2635337914669964\n",
      "epoch 12946: train loss: 0.1111615773422719, test loss: 0.2635342786300429\n",
      "epoch 12947: train loss: 0.11115941230607734, test loss: 0.26353476585367097\n",
      "epoch 12948: train loss: 0.11115724749599398, test loss: 0.2635352531378656\n",
      "epoch 12949: train loss: 0.11115508291197894, test loss: 0.26353574048261197\n",
      "epoch 12950: train loss: 0.11115291855398954, test loss: 0.26353622788789516\n",
      "epoch 12951: train loss: 0.11115075442198295, test loss: 0.2635367153537004\n",
      "epoch 12952: train loss: 0.11114859051591637, test loss: 0.26353720288001287\n",
      "epoch 12953: train loss: 0.11114642683574712, test loss: 0.2635376904668176\n",
      "epoch 12954: train loss: 0.11114426338143239, test loss: 0.26353817811409996\n",
      "epoch 12955: train loss: 0.11114210015292951, test loss: 0.26353866582184504\n",
      "epoch 12956: train loss: 0.11113993715019575, test loss: 0.26353915359003793\n",
      "epoch 12957: train loss: 0.11113777437318838, test loss: 0.2635396414186638\n",
      "epoch 12958: train loss: 0.1111356118218647, test loss: 0.2635401293077081\n",
      "epoch 12959: train loss: 0.11113344949618204, test loss: 0.2635406172571557\n",
      "epoch 12960: train loss: 0.11113128739609773, test loss: 0.26354110526699204\n",
      "epoch 12961: train loss: 0.11112912552156913, test loss: 0.2635415933372021\n",
      "epoch 12962: train loss: 0.11112696387255355, test loss: 0.26354208146777125\n",
      "epoch 12963: train loss: 0.11112480244900835, test loss: 0.2635425696586846\n",
      "epoch 12964: train loss: 0.11112264125089093, test loss: 0.2635430579099274\n",
      "epoch 12965: train loss: 0.11112048027815867, test loss: 0.2635435462214849\n",
      "epoch 12966: train loss: 0.11111831953076895, test loss: 0.2635440345933422\n",
      "epoch 12967: train loss: 0.11111615900867922, test loss: 0.2635445230254846\n",
      "epoch 12968: train loss: 0.11111399871184681, test loss: 0.26354501151789733\n",
      "epoch 12969: train loss: 0.11111183864022924, test loss: 0.2635455000705656\n",
      "epoch 12970: train loss: 0.11110967879378393, test loss: 0.2635459886834747\n",
      "epoch 12971: train loss: 0.1111075191724683, test loss: 0.2635464773566097\n",
      "epoch 12972: train loss: 0.11110535977623982, test loss: 0.263546966089956\n",
      "epoch 12973: train loss: 0.111103200605056, test loss: 0.2635474548834988\n",
      "epoch 12974: train loss: 0.11110104165887431, test loss: 0.2635479437372233\n",
      "epoch 12975: train loss: 0.11109888293765223, test loss: 0.2635484326511149\n",
      "epoch 12976: train loss: 0.1110967244413473, test loss: 0.2635489216251587\n",
      "epoch 12977: train loss: 0.11109456616991703, test loss: 0.2635494106593399\n",
      "epoch 12978: train loss: 0.11109240812331894, test loss: 0.263549899753644\n",
      "epoch 12979: train loss: 0.1110902503015106, test loss: 0.26355038890805615\n",
      "epoch 12980: train loss: 0.11108809270444954, test loss: 0.26355087812256156\n",
      "epoch 12981: train loss: 0.11108593533209332, test loss: 0.26355136739714563\n",
      "epoch 12982: train loss: 0.11108377818439952, test loss: 0.26355185673179354\n",
      "epoch 12983: train loss: 0.11108162126132576, test loss: 0.26355234612649064\n",
      "epoch 12984: train loss: 0.11107946456282963, test loss: 0.2635528355812221\n",
      "epoch 12985: train loss: 0.11107730808886872, test loss: 0.2635533250959734\n",
      "epoch 12986: train loss: 0.1110751518394007, test loss: 0.2635538146707297\n",
      "epoch 12987: train loss: 0.11107299581438312, test loss: 0.2635543043054763\n",
      "epoch 12988: train loss: 0.11107084001377371, test loss: 0.26355479400019854\n",
      "epoch 12989: train loss: 0.1110686844375301, test loss: 0.2635552837548817\n",
      "epoch 12990: train loss: 0.11106652908560993, test loss: 0.2635557735695111\n",
      "epoch 12991: train loss: 0.11106437395797092, test loss: 0.2635562634440721\n",
      "epoch 12992: train loss: 0.11106221905457074, test loss: 0.26355675337855\n",
      "epoch 12993: train loss: 0.1110600643753671, test loss: 0.26355724337293013\n",
      "epoch 12994: train loss: 0.1110579099203177, test loss: 0.26355773342719774\n",
      "epoch 12995: train loss: 0.1110557556893803, test loss: 0.2635582235413383\n",
      "epoch 12996: train loss: 0.11105360168251258, test loss: 0.2635587137153369\n",
      "epoch 12997: train loss: 0.11105144789967233, test loss: 0.26355920394917925\n",
      "epoch 12998: train loss: 0.11104929434081731, test loss: 0.2635596942428503\n",
      "epoch 12999: train loss: 0.11104714100590528, test loss: 0.2635601845963357\n",
      "epoch 13000: train loss: 0.11104498789489402, test loss: 0.2635606750096206\n",
      "epoch 13001: train loss: 0.1110428350077413, test loss: 0.2635611654826905\n",
      "epoch 13002: train loss: 0.11104068234440496, test loss: 0.26356165601553067\n",
      "epoch 13003: train loss: 0.11103852990484281, test loss: 0.26356214660812655\n",
      "epoch 13004: train loss: 0.11103637768901269, test loss: 0.26356263726046336\n",
      "epoch 13005: train loss: 0.11103422569687237, test loss: 0.26356312797252657\n",
      "epoch 13006: train loss: 0.1110320739283798, test loss: 0.26356361874430156\n",
      "epoch 13007: train loss: 0.11102992238349273, test loss: 0.26356410957577375\n",
      "epoch 13008: train loss: 0.11102777106216913, test loss: 0.2635646004669284\n",
      "epoch 13009: train loss: 0.1110256199643668, test loss: 0.263565091417751\n",
      "epoch 13010: train loss: 0.11102346909004371, test loss: 0.2635655824282269\n",
      "epoch 13011: train loss: 0.11102131843915775, test loss: 0.2635660734983415\n",
      "epoch 13012: train loss: 0.11101916801166677, test loss: 0.2635665646280802\n",
      "epoch 13013: train loss: 0.11101701780752878, test loss: 0.2635670558174283\n",
      "epoch 13014: train loss: 0.11101486782670167, test loss: 0.2635675470663714\n",
      "epoch 13015: train loss: 0.11101271806914341, test loss: 0.2635680383748948\n",
      "epoch 13016: train loss: 0.11101056853481193, test loss: 0.2635685297429839\n",
      "epoch 13017: train loss: 0.11100841922366525, test loss: 0.2635690211706241\n",
      "epoch 13018: train loss: 0.11100627013566135, test loss: 0.26356951265780093\n",
      "epoch 13019: train loss: 0.11100412127075816, test loss: 0.26357000420449966\n",
      "epoch 13020: train loss: 0.11100197262891376, test loss: 0.2635704958107059\n",
      "epoch 13021: train loss: 0.11099982421008614, test loss: 0.26357098747640495\n",
      "epoch 13022: train loss: 0.11099767601423334, test loss: 0.26357147920158214\n",
      "epoch 13023: train loss: 0.11099552804131338, test loss: 0.26357197098622326\n",
      "epoch 13024: train loss: 0.11099338029128432, test loss: 0.26357246283031344\n",
      "epoch 13025: train loss: 0.11099123276410422, test loss: 0.26357295473383824\n",
      "epoch 13026: train loss: 0.11098908545973117, test loss: 0.2635734466967831\n",
      "epoch 13027: train loss: 0.11098693837812323, test loss: 0.26357393871913354\n",
      "epoch 13028: train loss: 0.11098479151923851, test loss: 0.26357443080087495\n",
      "epoch 13029: train loss: 0.11098264488303514, test loss: 0.26357492294199275\n",
      "epoch 13030: train loss: 0.11098049846947117, test loss: 0.26357541514247246\n",
      "epoch 13031: train loss: 0.11097835227850478, test loss: 0.2635759074022995\n",
      "epoch 13032: train loss: 0.1109762063100941, test loss: 0.2635763997214595\n",
      "epoch 13033: train loss: 0.11097406056419729, test loss: 0.2635768920999378\n",
      "epoch 13034: train loss: 0.11097191504077251, test loss: 0.26357738453771995\n",
      "epoch 13035: train loss: 0.11096976973977793, test loss: 0.26357787703479135\n",
      "epoch 13036: train loss: 0.11096762466117173, test loss: 0.26357836959113756\n",
      "epoch 13037: train loss: 0.11096547980491211, test loss: 0.2635788622067441\n",
      "epoch 13038: train loss: 0.11096333517095727, test loss: 0.26357935488159634\n",
      "epoch 13039: train loss: 0.11096119075926544, test loss: 0.26357984761568\n",
      "epoch 13040: train loss: 0.11095904656979486, test loss: 0.26358034040898043\n",
      "epoch 13041: train loss: 0.11095690260250375, test loss: 0.26358083326148307\n",
      "epoch 13042: train loss: 0.11095475885735037, test loss: 0.26358132617317354\n",
      "epoch 13043: train loss: 0.110952615334293, test loss: 0.26358181914403744\n",
      "epoch 13044: train loss: 0.11095047203328985, test loss: 0.26358231217406014\n",
      "epoch 13045: train loss: 0.11094832895429928, test loss: 0.26358280526322725\n",
      "epoch 13046: train loss: 0.11094618609727956, test loss: 0.2635832984115243\n",
      "epoch 13047: train loss: 0.11094404346218897, test loss: 0.26358379161893664\n",
      "epoch 13048: train loss: 0.11094190104898587, test loss: 0.26358428488545016\n",
      "epoch 13049: train loss: 0.11093975885762859, test loss: 0.26358477821105014\n",
      "epoch 13050: train loss: 0.11093761688807541, test loss: 0.2635852715957222\n",
      "epoch 13051: train loss: 0.11093547514028476, test loss: 0.26358576503945186\n",
      "epoch 13052: train loss: 0.11093333361421495, test loss: 0.26358625854222467\n",
      "epoch 13053: train loss: 0.11093119230982436, test loss: 0.26358675210402627\n",
      "epoch 13054: train loss: 0.1109290512270714, test loss: 0.26358724572484205\n",
      "epoch 13055: train loss: 0.11092691036591447, test loss: 0.2635877394046578\n",
      "epoch 13056: train loss: 0.11092476972631193, test loss: 0.2635882331434589\n",
      "epoch 13057: train loss: 0.11092262930822222, test loss: 0.263588726941231\n",
      "epoch 13058: train loss: 0.11092048911160379, test loss: 0.2635892207979597\n",
      "epoch 13059: train loss: 0.11091834913641506, test loss: 0.2635897147136306\n",
      "epoch 13060: train loss: 0.1109162093826145, test loss: 0.263590208688229\n",
      "epoch 13061: train loss: 0.11091406985016054, test loss: 0.26359070272174095\n",
      "epoch 13062: train loss: 0.11091193053901169, test loss: 0.2635911968141517\n",
      "epoch 13063: train loss: 0.11090979144912641, test loss: 0.263591690965447\n",
      "epoch 13064: train loss: 0.11090765258046321, test loss: 0.2635921851756124\n",
      "epoch 13065: train loss: 0.11090551393298057, test loss: 0.2635926794446334\n",
      "epoch 13066: train loss: 0.11090337550663705, test loss: 0.26359317377249586\n",
      "epoch 13067: train loss: 0.11090123730139116, test loss: 0.2635936681591851\n",
      "epoch 13068: train loss: 0.11089909931720139, test loss: 0.26359416260468693\n",
      "epoch 13069: train loss: 0.11089696155402638, test loss: 0.26359465710898694\n",
      "epoch 13070: train loss: 0.11089482401182461, test loss: 0.2635951516720707\n",
      "epoch 13071: train loss: 0.11089268669055471, test loss: 0.26359564629392385\n",
      "epoch 13072: train loss: 0.11089054959017525, test loss: 0.26359614097453204\n",
      "epoch 13073: train loss: 0.11088841271064478, test loss: 0.2635966357138808\n",
      "epoch 13074: train loss: 0.11088627605192197, test loss: 0.26359713051195605\n",
      "epoch 13075: train loss: 0.1108841396139654, test loss: 0.26359762536874315\n",
      "epoch 13076: train loss: 0.11088200339673372, test loss: 0.2635981202842277\n",
      "epoch 13077: train loss: 0.11087986740018554, test loss: 0.2635986152583956\n",
      "epoch 13078: train loss: 0.11087773162427955, test loss: 0.26359911029123234\n",
      "epoch 13079: train loss: 0.11087559606897436, test loss: 0.2635996053827236\n",
      "epoch 13080: train loss: 0.11087346073422868, test loss: 0.26360010053285515\n",
      "epoch 13081: train loss: 0.11087132562000114, test loss: 0.26360059574161254\n",
      "epoch 13082: train loss: 0.11086919072625051, test loss: 0.26360109100898144\n",
      "epoch 13083: train loss: 0.11086705605293545, test loss: 0.26360158633494746\n",
      "epoch 13084: train loss: 0.1108649216000147, test loss: 0.2636020817194964\n",
      "epoch 13085: train loss: 0.11086278736744694, test loss: 0.2636025771626139\n",
      "epoch 13086: train loss: 0.11086065335519095, test loss: 0.26360307266428573\n",
      "epoch 13087: train loss: 0.11085851956320543, test loss: 0.2636035682244973\n",
      "epoch 13088: train loss: 0.1108563859914492, test loss: 0.26360406384323454\n",
      "epoch 13089: train loss: 0.11085425263988098, test loss: 0.2636045595204831\n",
      "epoch 13090: train loss: 0.1108521195084596, test loss: 0.26360505525622874\n",
      "epoch 13091: train loss: 0.11084998659714382, test loss: 0.2636055510504571\n",
      "epoch 13092: train loss: 0.11084785390589243, test loss: 0.2636060469031537\n",
      "epoch 13093: train loss: 0.11084572143466427, test loss: 0.26360654281430457\n",
      "epoch 13094: train loss: 0.11084358918341819, test loss: 0.2636070387838952\n",
      "epoch 13095: train loss: 0.11084145715211294, test loss: 0.2636075348119114\n",
      "epoch 13096: train loss: 0.11083932534070744, test loss: 0.26360803089833884\n",
      "epoch 13097: train loss: 0.11083719374916053, test loss: 0.26360852704316334\n",
      "epoch 13098: train loss: 0.11083506237743107, test loss: 0.2636090232463705\n",
      "epoch 13099: train loss: 0.11083293122547792, test loss: 0.2636095195079461\n",
      "epoch 13100: train loss: 0.11083080029326002, test loss: 0.26361001582787597\n",
      "epoch 13101: train loss: 0.11082866958073623, test loss: 0.26361051220614573\n",
      "epoch 13102: train loss: 0.11082653908786551, test loss: 0.2636110086427411\n",
      "epoch 13103: train loss: 0.1108244088146067, test loss: 0.26361150513764803\n",
      "epoch 13104: train loss: 0.11082227876091881, test loss: 0.263612001690852\n",
      "epoch 13105: train loss: 0.11082014892676074, test loss: 0.26361249830233896\n",
      "epoch 13106: train loss: 0.11081801931209148, test loss: 0.26361299497209467\n",
      "epoch 13107: train loss: 0.11081588991686997, test loss: 0.26361349170010473\n",
      "epoch 13108: train loss: 0.1108137607410552, test loss: 0.263613988486355\n",
      "epoch 13109: train loss: 0.11081163178460614, test loss: 0.2636144853308313\n",
      "epoch 13110: train loss: 0.11080950304748181, test loss: 0.2636149822335193\n",
      "epoch 13111: train loss: 0.11080737452964122, test loss: 0.26361547919440487\n",
      "epoch 13112: train loss: 0.11080524623104339, test loss: 0.26361597621347377\n",
      "epoch 13113: train loss: 0.11080311815164734, test loss: 0.26361647329071175\n",
      "epoch 13114: train loss: 0.11080099029141209, test loss: 0.26361697042610466\n",
      "epoch 13115: train loss: 0.11079886265029674, test loss: 0.26361746761963817\n",
      "epoch 13116: train loss: 0.11079673522826033, test loss: 0.2636179648712982\n",
      "epoch 13117: train loss: 0.11079460802526192, test loss: 0.26361846218107055\n",
      "epoch 13118: train loss: 0.11079248104126065, test loss: 0.26361895954894093\n",
      "epoch 13119: train loss: 0.11079035427621556, test loss: 0.2636194569748952\n",
      "epoch 13120: train loss: 0.11078822773008579, test loss: 0.2636199544589191\n",
      "epoch 13121: train loss: 0.11078610140283043, test loss: 0.26362045200099865\n",
      "epoch 13122: train loss: 0.11078397529440863, test loss: 0.2636209496011195\n",
      "epoch 13123: train loss: 0.1107818494047795, test loss: 0.26362144725926745\n",
      "epoch 13124: train loss: 0.11077972373390228, test loss: 0.26362194497542846\n",
      "epoch 13125: train loss: 0.11077759828173601, test loss: 0.26362244274958824\n",
      "epoch 13126: train loss: 0.11077547304823994, test loss: 0.26362294058173275\n",
      "epoch 13127: train loss: 0.11077334803337323, test loss: 0.2636234384718476\n",
      "epoch 13128: train loss: 0.11077122323709507, test loss: 0.2636239364199189\n",
      "epoch 13129: train loss: 0.11076909865936464, test loss: 0.26362443442593225\n",
      "epoch 13130: train loss: 0.11076697430014124, test loss: 0.26362493248987373\n",
      "epoch 13131: train loss: 0.110764850159384, test loss: 0.263625430611729\n",
      "epoch 13132: train loss: 0.11076272623705222, test loss: 0.2636259287914841\n",
      "epoch 13133: train loss: 0.11076060253310512, test loss: 0.26362642702912475\n",
      "epoch 13134: train loss: 0.11075847904750198, test loss: 0.2636269253246369\n",
      "epoch 13135: train loss: 0.11075635578020203, test loss: 0.26362742367800623\n",
      "epoch 13136: train loss: 0.11075423273116458, test loss: 0.26362792208921887\n",
      "epoch 13137: train loss: 0.11075210990034894, test loss: 0.2636284205582606\n",
      "epoch 13138: train loss: 0.11074998728771433, test loss: 0.2636289190851172\n",
      "epoch 13139: train loss: 0.11074786489322015, test loss: 0.26362941766977466\n",
      "epoch 13140: train loss: 0.1107457427168257, test loss: 0.2636299163122188\n",
      "epoch 13141: train loss: 0.11074362075849027, test loss: 0.2636304150124356\n",
      "epoch 13142: train loss: 0.11074149901817329, test loss: 0.2636309137704109\n",
      "epoch 13143: train loss: 0.11073937749583403, test loss: 0.2636314125861306\n",
      "epoch 13144: train loss: 0.11073725619143189, test loss: 0.26363191145958065\n",
      "epoch 13145: train loss: 0.11073513510492625, test loss: 0.2636324103907468\n",
      "epoch 13146: train loss: 0.11073301423627652, test loss: 0.26363290937961514\n",
      "epoch 13147: train loss: 0.11073089358544203, test loss: 0.2636334084261715\n",
      "epoch 13148: train loss: 0.11072877315238225, test loss: 0.2636339075304018\n",
      "epoch 13149: train loss: 0.11072665293705657, test loss: 0.26363440669229193\n",
      "epoch 13150: train loss: 0.11072453293942443, test loss: 0.26363490591182787\n",
      "epoch 13151: train loss: 0.11072241315944531, test loss: 0.26363540518899553\n",
      "epoch 13152: train loss: 0.11072029359707859, test loss: 0.2636359045237808\n",
      "epoch 13153: train loss: 0.11071817425228375, test loss: 0.26363640391616977\n",
      "epoch 13154: train loss: 0.11071605512502032, test loss: 0.2636369033661482\n",
      "epoch 13155: train loss: 0.11071393621524772, test loss: 0.26363740287370196\n",
      "epoch 13156: train loss: 0.1107118175229255, test loss: 0.26363790243881735\n",
      "epoch 13157: train loss: 0.11070969904801309, test loss: 0.2636384020614799\n",
      "epoch 13158: train loss: 0.11070758079047005, test loss: 0.2636389017416759\n",
      "epoch 13159: train loss: 0.11070546275025592, test loss: 0.26363940147939113\n",
      "epoch 13160: train loss: 0.11070334492733024, test loss: 0.2636399012746116\n",
      "epoch 13161: train loss: 0.11070122732165252, test loss: 0.26364040112732323\n",
      "epoch 13162: train loss: 0.11069910993318233, test loss: 0.2636409010375121\n",
      "epoch 13163: train loss: 0.11069699276187923, test loss: 0.26364140100516414\n",
      "epoch 13164: train loss: 0.11069487580770287, test loss: 0.26364190103026514\n",
      "epoch 13165: train loss: 0.11069275907061277, test loss: 0.26364240111280135\n",
      "epoch 13166: train loss: 0.11069064255056853, test loss: 0.26364290125275863\n",
      "epoch 13167: train loss: 0.1106885262475298, test loss: 0.26364340145012294\n",
      "epoch 13168: train loss: 0.11068641016145618, test loss: 0.2636439017048803\n",
      "epoch 13169: train loss: 0.11068429429230733, test loss: 0.2636444020170167\n",
      "epoch 13170: train loss: 0.11068217864004282, test loss: 0.26364490238651817\n",
      "epoch 13171: train loss: 0.11068006320462238, test loss: 0.2636454028133708\n",
      "epoch 13172: train loss: 0.11067794798600566, test loss: 0.2636459032975603\n",
      "epoch 13173: train loss: 0.11067583298415233, test loss: 0.263646403839073\n",
      "epoch 13174: train loss: 0.11067371819902207, test loss: 0.2636469044378948\n",
      "epoch 13175: train loss: 0.11067160363057457, test loss: 0.2636474050940116\n",
      "epoch 13176: train loss: 0.11066948927876955, test loss: 0.2636479058074095\n",
      "epoch 13177: train loss: 0.11066737514356671, test loss: 0.26364840657807465\n",
      "epoch 13178: train loss: 0.11066526122492581, test loss: 0.26364890740599284\n",
      "epoch 13179: train loss: 0.11066314752280658, test loss: 0.2636494082911503\n",
      "epoch 13180: train loss: 0.11066103403716875, test loss: 0.263649909233533\n",
      "epoch 13181: train loss: 0.11065892076797208, test loss: 0.26365041023312696\n",
      "epoch 13182: train loss: 0.11065680771517637, test loss: 0.26365091128991824\n",
      "epoch 13183: train loss: 0.1106546948787414, test loss: 0.2636514124038929\n",
      "epoch 13184: train loss: 0.11065258225862694, test loss: 0.2636519135750369\n",
      "epoch 13185: train loss: 0.11065046985479281, test loss: 0.26365241480333645\n",
      "epoch 13186: train loss: 0.11064835766719881, test loss: 0.26365291608877756\n",
      "epoch 13187: train loss: 0.11064624569580478, test loss: 0.26365341743134624\n",
      "epoch 13188: train loss: 0.11064413394057052, test loss: 0.26365391883102846\n",
      "epoch 13189: train loss: 0.1106420224014559, test loss: 0.26365442028781044\n",
      "epoch 13190: train loss: 0.11063991107842082, test loss: 0.26365492180167827\n",
      "epoch 13191: train loss: 0.11063779997142507, test loss: 0.26365542337261794\n",
      "epoch 13192: train loss: 0.11063568908042855, test loss: 0.2636559250006156\n",
      "epoch 13193: train loss: 0.11063357840539119, test loss: 0.26365642668565714\n",
      "epoch 13194: train loss: 0.11063146794627281, test loss: 0.2636569284277289\n",
      "epoch 13195: train loss: 0.11062935770303339, test loss: 0.2636574302268169\n",
      "epoch 13196: train loss: 0.11062724767563284, test loss: 0.2636579320829073\n",
      "epoch 13197: train loss: 0.11062513786403105, test loss: 0.2636584339959859\n",
      "epoch 13198: train loss: 0.11062302826818797, test loss: 0.26365893596603907\n",
      "epoch 13199: train loss: 0.1106209188880636, test loss: 0.26365943799305286\n",
      "epoch 13200: train loss: 0.11061880972361784, test loss: 0.2636599400770134\n",
      "epoch 13201: train loss: 0.1106167007748107, test loss: 0.26366044221790674\n",
      "epoch 13202: train loss: 0.11061459204160214, test loss: 0.26366094441571897\n",
      "epoch 13203: train loss: 0.11061248352395216, test loss: 0.26366144667043634\n",
      "epoch 13204: train loss: 0.11061037522182077, test loss: 0.2636619489820449\n",
      "epoch 13205: train loss: 0.11060826713516797, test loss: 0.2636624513505307\n",
      "epoch 13206: train loss: 0.11060615926395383, test loss: 0.26366295377588\n",
      "epoch 13207: train loss: 0.11060405160813831, test loss: 0.26366345625807897\n",
      "epoch 13208: train loss: 0.1106019441676815, test loss: 0.2636639587971136\n",
      "epoch 13209: train loss: 0.11059983694254345, test loss: 0.26366446139296995\n",
      "epoch 13210: train loss: 0.11059772993268424, test loss: 0.2636649640456345\n",
      "epoch 13211: train loss: 0.11059562313806395, test loss: 0.2636654667550931\n",
      "epoch 13212: train loss: 0.11059351655864262, test loss: 0.2636659695213321\n",
      "epoch 13213: train loss: 0.11059141019438039, test loss: 0.26366647234433754\n",
      "epoch 13214: train loss: 0.11058930404523735, test loss: 0.26366697522409555\n",
      "epoch 13215: train loss: 0.11058719811117365, test loss: 0.26366747816059244\n",
      "epoch 13216: train loss: 0.11058509239214938, test loss: 0.2636679811538142\n",
      "epoch 13217: train loss: 0.1105829868881247, test loss: 0.26366848420374717\n",
      "epoch 13218: train loss: 0.11058088159905977, test loss: 0.2636689873103774\n",
      "epoch 13219: train loss: 0.1105787765249147, test loss: 0.2636694904736911\n",
      "epoch 13220: train loss: 0.11057667166564972, test loss: 0.26366999369367455\n",
      "epoch 13221: train loss: 0.110574567021225, test loss: 0.26367049697031375\n",
      "epoch 13222: train loss: 0.11057246259160072, test loss: 0.263671000303595\n",
      "epoch 13223: train loss: 0.11057035837673707, test loss: 0.26367150369350456\n",
      "epoch 13224: train loss: 0.11056825437659429, test loss: 0.26367200714002853\n",
      "epoch 13225: train loss: 0.11056615059113259, test loss: 0.26367251064315317\n",
      "epoch 13226: train loss: 0.1105640470203122, test loss: 0.2636730142028646\n",
      "epoch 13227: train loss: 0.11056194366409339, test loss: 0.2636735178191491\n",
      "epoch 13228: train loss: 0.11055984052243638, test loss: 0.2636740214919928\n",
      "epoch 13229: train loss: 0.11055773759530144, test loss: 0.26367452522138196\n",
      "epoch 13230: train loss: 0.11055563488264886, test loss: 0.26367502900730294\n",
      "epoch 13231: train loss: 0.11055353238443892, test loss: 0.2636755328497417\n",
      "epoch 13232: train loss: 0.11055143010063194, test loss: 0.2636760367486847\n",
      "epoch 13233: train loss: 0.11054932803118817, test loss: 0.263676540704118\n",
      "epoch 13234: train loss: 0.11054722617606799, test loss: 0.26367704471602793\n",
      "epoch 13235: train loss: 0.1105451245352317, test loss: 0.2636775487844007\n",
      "epoch 13236: train loss: 0.11054302310863964, test loss: 0.2636780529092225\n",
      "epoch 13237: train loss: 0.11054092189625216, test loss: 0.2636785570904797\n",
      "epoch 13238: train loss: 0.11053882089802958, test loss: 0.2636790613281585\n",
      "epoch 13239: train loss: 0.11053672011393234, test loss: 0.26367956562224504\n",
      "epoch 13240: train loss: 0.11053461954392078, test loss: 0.26368006997272564\n",
      "epoch 13241: train loss: 0.11053251918795529, test loss: 0.26368057437958664\n",
      "epoch 13242: train loss: 0.11053041904599628, test loss: 0.26368107884281416\n",
      "epoch 13243: train loss: 0.11052831911800416, test loss: 0.26368158336239467\n",
      "epoch 13244: train loss: 0.11052621940393934, test loss: 0.26368208793831427\n",
      "epoch 13245: train loss: 0.11052411990376226, test loss: 0.26368259257055926\n",
      "epoch 13246: train loss: 0.11052202061743337, test loss: 0.26368309725911593\n",
      "epoch 13247: train loss: 0.11051992154491312, test loss: 0.26368360200397056\n",
      "epoch 13248: train loss: 0.11051782268616193, test loss: 0.26368410680510945\n",
      "epoch 13249: train loss: 0.11051572404114035, test loss: 0.26368461166251883\n",
      "epoch 13250: train loss: 0.1105136256098088, test loss: 0.26368511657618515\n",
      "epoch 13251: train loss: 0.11051152739212781, test loss: 0.2636856215460945\n",
      "epoch 13252: train loss: 0.1105094293880579, test loss: 0.2636861265722333\n",
      "epoch 13253: train loss: 0.1105073315975595, test loss: 0.26368663165458783\n",
      "epoch 13254: train loss: 0.11050523402059323, test loss: 0.26368713679314437\n",
      "epoch 13255: train loss: 0.11050313665711958, test loss: 0.2636876419878893\n",
      "epoch 13256: train loss: 0.11050103950709907, test loss: 0.26368814723880885\n",
      "epoch 13257: train loss: 0.11049894257049232, test loss: 0.2636886525458893\n",
      "epoch 13258: train loss: 0.11049684584725986, test loss: 0.2636891579091171\n",
      "epoch 13259: train loss: 0.11049474933736228, test loss: 0.26368966332847854\n",
      "epoch 13260: train loss: 0.11049265304076011, test loss: 0.2636901688039599\n",
      "epoch 13261: train loss: 0.11049055695741405, test loss: 0.26369067433554755\n",
      "epoch 13262: train loss: 0.11048846108728462, test loss: 0.26369117992322777\n",
      "epoch 13263: train loss: 0.11048636543033247, test loss: 0.26369168556698697\n",
      "epoch 13264: train loss: 0.11048426998651824, test loss: 0.2636921912668114\n",
      "epoch 13265: train loss: 0.11048217475580255, test loss: 0.26369269702268755\n",
      "epoch 13266: train loss: 0.11048007973814605, test loss: 0.26369320283460157\n",
      "epoch 13267: train loss: 0.11047798493350941, test loss: 0.26369370870253994\n",
      "epoch 13268: train loss: 0.11047589034185329, test loss: 0.26369421462648907\n",
      "epoch 13269: train loss: 0.11047379596313839, test loss: 0.26369472060643523\n",
      "epoch 13270: train loss: 0.11047170179732536, test loss: 0.26369522664236483\n",
      "epoch 13271: train loss: 0.11046960784437491, test loss: 0.2636957327342641\n",
      "epoch 13272: train loss: 0.1104675141042478, test loss: 0.2636962388821196\n",
      "epoch 13273: train loss: 0.1104654205769047, test loss: 0.26369674508591756\n",
      "epoch 13274: train loss: 0.11046332726230634, test loss: 0.26369725134564453\n",
      "epoch 13275: train loss: 0.11046123416041351, test loss: 0.26369775766128667\n",
      "epoch 13276: train loss: 0.11045914127118688, test loss: 0.2636982640328306\n",
      "epoch 13277: train loss: 0.11045704859458728, test loss: 0.26369877046026247\n",
      "epoch 13278: train loss: 0.11045495613057545, test loss: 0.26369927694356887\n",
      "epoch 13279: train loss: 0.11045286387911223, test loss: 0.26369978348273604\n",
      "epoch 13280: train loss: 0.11045077184015831, test loss: 0.26370029007775053\n",
      "epoch 13281: train loss: 0.11044868001367458, test loss: 0.2637007967285986\n",
      "epoch 13282: train loss: 0.11044658839962179, test loss: 0.26370130343526677\n",
      "epoch 13283: train loss: 0.11044449699796084, test loss: 0.2637018101977414\n",
      "epoch 13284: train loss: 0.1104424058086525, test loss: 0.26370231701600894\n",
      "epoch 13285: train loss: 0.11044031483165762, test loss: 0.2637028238900557\n",
      "epoch 13286: train loss: 0.11043822406693707, test loss: 0.26370333081986813\n",
      "epoch 13287: train loss: 0.11043613351445174, test loss: 0.26370383780543283\n",
      "epoch 13288: train loss: 0.11043404317416243, test loss: 0.26370434484673605\n",
      "epoch 13289: train loss: 0.11043195304603007, test loss: 0.2637048519437642\n",
      "epoch 13290: train loss: 0.11042986313001558, test loss: 0.2637053590965039\n",
      "epoch 13291: train loss: 0.11042777342607984, test loss: 0.26370586630494136\n",
      "epoch 13292: train loss: 0.11042568393418374, test loss: 0.26370637356906323\n",
      "epoch 13293: train loss: 0.11042359465428825, test loss: 0.2637068808888558\n",
      "epoch 13294: train loss: 0.11042150558635429, test loss: 0.2637073882643056\n",
      "epoch 13295: train loss: 0.1104194167303428, test loss: 0.2637078956953991\n",
      "epoch 13296: train loss: 0.11041732808621471, test loss: 0.2637084031821226\n",
      "epoch 13297: train loss: 0.11041523965393103, test loss: 0.2637089107244628\n",
      "epoch 13298: train loss: 0.11041315143345275, test loss: 0.26370941832240596\n",
      "epoch 13299: train loss: 0.11041106342474077, test loss: 0.2637099259759387\n",
      "epoch 13300: train loss: 0.11040897562775619, test loss: 0.2637104336850474\n",
      "epoch 13301: train loss: 0.11040688804245996, test loss: 0.2637109414497185\n",
      "epoch 13302: train loss: 0.1104048006688131, test loss: 0.26371144926993856\n",
      "epoch 13303: train loss: 0.11040271350677663, test loss: 0.263711957145694\n",
      "epoch 13304: train loss: 0.11040062655631164, test loss: 0.26371246507697144\n",
      "epoch 13305: train loss: 0.11039853981737911, test loss: 0.26371297306375724\n",
      "epoch 13306: train loss: 0.11039645328994013, test loss: 0.2637134811060378\n",
      "epoch 13307: train loss: 0.1103943669739558, test loss: 0.2637139892037999\n",
      "epoch 13308: train loss: 0.11039228086938714, test loss: 0.26371449735702973\n",
      "epoch 13309: train loss: 0.11039019497619523, test loss: 0.26371500556571403\n",
      "epoch 13310: train loss: 0.11038810929434124, test loss: 0.2637155138298392\n",
      "epoch 13311: train loss: 0.11038602382378626, test loss: 0.26371602214939177\n",
      "epoch 13312: train loss: 0.11038393856449134, test loss: 0.26371653052435823\n",
      "epoch 13313: train loss: 0.11038185351641769, test loss: 0.2637170389547251\n",
      "epoch 13314: train loss: 0.1103797686795264, test loss: 0.2637175474404789\n",
      "epoch 13315: train loss: 0.11037768405377864, test loss: 0.2637180559816062\n",
      "epoch 13316: train loss: 0.11037559963913558, test loss: 0.26371856457809345\n",
      "epoch 13317: train loss: 0.11037351543555836, test loss: 0.2637190732299272\n",
      "epoch 13318: train loss: 0.11037143144300816, test loss: 0.2637195819370941\n",
      "epoch 13319: train loss: 0.11036934766144622, test loss: 0.2637200906995805\n",
      "epoch 13320: train loss: 0.11036726409083368, test loss: 0.2637205995173732\n",
      "epoch 13321: train loss: 0.11036518073113175, test loss: 0.2637211083904585\n",
      "epoch 13322: train loss: 0.11036309758230171, test loss: 0.26372161731882304\n",
      "epoch 13323: train loss: 0.11036101464430473, test loss: 0.2637221263024533\n",
      "epoch 13324: train loss: 0.11035893191710208, test loss: 0.26372263534133594\n",
      "epoch 13325: train loss: 0.11035684940065504, test loss: 0.26372314443545747\n",
      "epoch 13326: train loss: 0.1103547670949248, test loss: 0.2637236535848045\n",
      "epoch 13327: train loss: 0.11035268499987268, test loss: 0.26372416278936356\n",
      "epoch 13328: train loss: 0.11035060311545991, test loss: 0.2637246720491212\n",
      "epoch 13329: train loss: 0.11034852144164783, test loss: 0.263725181364064\n",
      "epoch 13330: train loss: 0.11034643997839776, test loss: 0.2637256907341786\n",
      "epoch 13331: train loss: 0.11034435872567094, test loss: 0.26372620015945153\n",
      "epoch 13332: train loss: 0.11034227768342873, test loss: 0.26372670963986933\n",
      "epoch 13333: train loss: 0.11034019685163247, test loss: 0.26372721917541864\n",
      "epoch 13334: train loss: 0.11033811623024349, test loss: 0.26372772876608613\n",
      "epoch 13335: train loss: 0.11033603581922315, test loss: 0.2637282384118582\n",
      "epoch 13336: train loss: 0.11033395561853278, test loss: 0.26372874811272157\n",
      "epoch 13337: train loss: 0.11033187562813378, test loss: 0.2637292578686629\n",
      "epoch 13338: train loss: 0.11032979584798755, test loss: 0.26372976767966866\n",
      "epoch 13339: train loss: 0.11032771627805542, test loss: 0.26373027754572553\n",
      "epoch 13340: train loss: 0.11032563691829886, test loss: 0.2637307874668202\n",
      "epoch 13341: train loss: 0.1103235577686792, test loss: 0.26373129744293905\n",
      "epoch 13342: train loss: 0.11032147882915795, test loss: 0.26373180747406894\n",
      "epoch 13343: train loss: 0.1103194000996965, test loss: 0.2637323175601963\n",
      "epoch 13344: train loss: 0.11031732158025628, test loss: 0.26373282770130807\n",
      "epoch 13345: train loss: 0.11031524327079875, test loss: 0.2637333378973905\n",
      "epoch 13346: train loss: 0.1103131651712854, test loss: 0.2637338481484304\n",
      "epoch 13347: train loss: 0.11031108728167766, test loss: 0.2637343584544145\n",
      "epoch 13348: train loss: 0.11030900960193703, test loss: 0.2637348688153293\n",
      "epoch 13349: train loss: 0.110306932132025, test loss: 0.26373537923116147\n",
      "epoch 13350: train loss: 0.11030485487190309, test loss: 0.2637358897018976\n",
      "epoch 13351: train loss: 0.11030277782153278, test loss: 0.2637364002275245\n",
      "epoch 13352: train loss: 0.11030070098087559, test loss: 0.2637369108080287\n",
      "epoch 13353: train loss: 0.11029862434989308, test loss: 0.26373742144339696\n",
      "epoch 13354: train loss: 0.11029654792854679, test loss: 0.26373793213361574\n",
      "epoch 13355: train loss: 0.11029447171679824, test loss: 0.263738442878672\n",
      "epoch 13356: train loss: 0.11029239571460903, test loss: 0.2637389536785521\n",
      "epoch 13357: train loss: 0.1102903199219407, test loss: 0.26373946453324293\n",
      "epoch 13358: train loss: 0.11028824433875485, test loss: 0.2637399754427311\n",
      "epoch 13359: train loss: 0.11028616896501309, test loss: 0.2637404864070032\n",
      "epoch 13360: train loss: 0.11028409380067697, test loss: 0.2637409974260461\n",
      "epoch 13361: train loss: 0.11028201884570812, test loss: 0.2637415084998464\n",
      "epoch 13362: train loss: 0.1102799441000682, test loss: 0.2637420196283907\n",
      "epoch 13363: train loss: 0.11027786956371878, test loss: 0.26374253081166577\n",
      "epoch 13364: train loss: 0.11027579523662155, test loss: 0.26374304204965837\n",
      "epoch 13365: train loss: 0.11027372111873815, test loss: 0.26374355334235505\n",
      "epoch 13366: train loss: 0.11027164721003023, test loss: 0.26374406468974265\n",
      "epoch 13367: train loss: 0.11026957351045948, test loss: 0.26374457609180785\n",
      "epoch 13368: train loss: 0.11026750001998757, test loss: 0.2637450875485372\n",
      "epoch 13369: train loss: 0.11026542673857617, test loss: 0.2637455990599177\n",
      "epoch 13370: train loss: 0.11026335366618702, test loss: 0.2637461106259358\n",
      "epoch 13371: train loss: 0.1102612808027818, test loss: 0.26374662224657835\n",
      "epoch 13372: train loss: 0.11025920814832228, test loss: 0.2637471339218321\n",
      "epoch 13373: train loss: 0.1102571357027701, test loss: 0.2637476456516837\n",
      "epoch 13374: train loss: 0.11025506346608709, test loss: 0.26374815743611985\n",
      "epoch 13375: train loss: 0.11025299143823496, test loss: 0.2637486692751274\n",
      "epoch 13376: train loss: 0.11025091961917546, test loss: 0.26374918116869306\n",
      "epoch 13377: train loss: 0.1102488480088704, test loss: 0.2637496931168036\n",
      "epoch 13378: train loss: 0.11024677660728154, test loss: 0.2637502051194456\n",
      "epoch 13379: train loss: 0.11024470541437065, test loss: 0.26375071717660603\n",
      "epoch 13380: train loss: 0.11024263443009957, test loss: 0.26375122928827144\n",
      "epoch 13381: train loss: 0.1102405636544301, test loss: 0.26375174145442876\n",
      "epoch 13382: train loss: 0.11023849308732402, test loss: 0.2637522536750646\n",
      "epoch 13383: train loss: 0.11023642272874319, test loss: 0.2637527659501659\n",
      "epoch 13384: train loss: 0.11023435257864947, test loss: 0.2637532782797192\n",
      "epoch 13385: train loss: 0.11023228263700467, test loss: 0.26375379066371146\n",
      "epoch 13386: train loss: 0.1102302129037707, test loss: 0.2637543031021294\n",
      "epoch 13387: train loss: 0.11022814337890936, test loss: 0.26375481559495967\n",
      "epoch 13388: train loss: 0.11022607406238259, test loss: 0.2637553281421892\n",
      "epoch 13389: train loss: 0.11022400495415229, test loss: 0.26375584074380476\n",
      "epoch 13390: train loss: 0.11022193605418028, test loss: 0.2637563533997931\n",
      "epoch 13391: train loss: 0.11021986736242853, test loss: 0.263756866110141\n",
      "epoch 13392: train loss: 0.11021779887885895, test loss: 0.2637573788748353\n",
      "epoch 13393: train loss: 0.11021573060343347, test loss: 0.26375789169386266\n",
      "epoch 13394: train loss: 0.11021366253611403, test loss: 0.26375840456721006\n",
      "epoch 13395: train loss: 0.11021159467686255, test loss: 0.2637589174948643\n",
      "epoch 13396: train loss: 0.11020952702564105, test loss: 0.26375943047681205\n",
      "epoch 13397: train loss: 0.11020745958241142, test loss: 0.26375994351304016\n",
      "epoch 13398: train loss: 0.11020539234713571, test loss: 0.26376045660353553\n",
      "epoch 13399: train loss: 0.11020332531977585, test loss: 0.26376096974828495\n",
      "epoch 13400: train loss: 0.11020125850029387, test loss: 0.26376148294727514\n",
      "epoch 13401: train loss: 0.11019919188865178, test loss: 0.263761996200493\n",
      "epoch 13402: train loss: 0.11019712548481159, test loss: 0.2637625095079254\n",
      "epoch 13403: train loss: 0.11019505928873535, test loss: 0.26376302286955905\n",
      "epoch 13404: train loss: 0.11019299330038504, test loss: 0.26376353628538096\n",
      "epoch 13405: train loss: 0.11019092751972276, test loss: 0.2637640497553778\n",
      "epoch 13406: train loss: 0.11018886194671054, test loss: 0.26376456327953657\n",
      "epoch 13407: train loss: 0.11018679658131045, test loss: 0.26376507685784395\n",
      "epoch 13408: train loss: 0.11018473142348459, test loss: 0.26376559049028686\n",
      "epoch 13409: train loss: 0.110182666473195, test loss: 0.2637661041768522\n",
      "epoch 13410: train loss: 0.11018060173040384, test loss: 0.26376661791752676\n",
      "epoch 13411: train loss: 0.11017853719507316, test loss: 0.2637671317122974\n",
      "epoch 13412: train loss: 0.1101764728671651, test loss: 0.263767645561151\n",
      "epoch 13413: train loss: 0.11017440874664178, test loss: 0.26376815946407445\n",
      "epoch 13414: train loss: 0.11017234483346534, test loss: 0.2637686734210547\n",
      "epoch 13415: train loss: 0.11017028112759794, test loss: 0.26376918743207833\n",
      "epoch 13416: train loss: 0.1101682176290017, test loss: 0.2637697014971326\n",
      "epoch 13417: train loss: 0.11016615433763882, test loss: 0.263770215616204\n",
      "epoch 13418: train loss: 0.11016409125347142, test loss: 0.2637707297892798\n",
      "epoch 13419: train loss: 0.11016202837646176, test loss: 0.26377124401634666\n",
      "epoch 13420: train loss: 0.11015996570657201, test loss: 0.26377175829739147\n",
      "epoch 13421: train loss: 0.1101579032437643, test loss: 0.2637722726324012\n",
      "epoch 13422: train loss: 0.11015584098800098, test loss: 0.26377278702136264\n",
      "epoch 13423: train loss: 0.11015377893924415, test loss: 0.2637733014642628\n",
      "epoch 13424: train loss: 0.11015171709745608, test loss: 0.26377381596108856\n",
      "epoch 13425: train loss: 0.11014965546259906, test loss: 0.26377433051182686\n",
      "epoch 13426: train loss: 0.1101475940346353, test loss: 0.2637748451164644\n",
      "epoch 13427: train loss: 0.11014553281352704, test loss: 0.2637753597749884\n",
      "epoch 13428: train loss: 0.11014347179923661, test loss: 0.26377587448738554\n",
      "epoch 13429: train loss: 0.11014141099172627, test loss: 0.2637763892536429\n",
      "epoch 13430: train loss: 0.11013935039095828, test loss: 0.2637769040737474\n",
      "epoch 13431: train loss: 0.11013728999689498, test loss: 0.2637774189476858\n",
      "epoch 13432: train loss: 0.11013522980949868, test loss: 0.2637779338754451\n",
      "epoch 13433: train loss: 0.11013316982873167, test loss: 0.2637784488570124\n",
      "epoch 13434: train loss: 0.11013111005455631, test loss: 0.26377896389237443\n",
      "epoch 13435: train loss: 0.11012905048693494, test loss: 0.2637794789815182\n",
      "epoch 13436: train loss: 0.1101269911258299, test loss: 0.26377999412443065\n",
      "epoch 13437: train loss: 0.11012493197120353, test loss: 0.2637805093210988\n",
      "epoch 13438: train loss: 0.11012287302301825, test loss: 0.26378102457150954\n",
      "epoch 13439: train loss: 0.11012081428123638, test loss: 0.2637815398756499\n",
      "epoch 13440: train loss: 0.11011875574582035, test loss: 0.2637820552335066\n",
      "epoch 13441: train loss: 0.11011669741673258, test loss: 0.26378257064506694\n",
      "epoch 13442: train loss: 0.11011463929393539, test loss: 0.26378308611031764\n",
      "epoch 13443: train loss: 0.1101125813773913, test loss: 0.26378360162924563\n",
      "epoch 13444: train loss: 0.11011052366706267, test loss: 0.2637841172018382\n",
      "epoch 13445: train loss: 0.110108466162912, test loss: 0.263784632828082\n",
      "epoch 13446: train loss: 0.11010640886490165, test loss: 0.26378514850796414\n",
      "epoch 13447: train loss: 0.11010435177299417, test loss: 0.26378566424147165\n",
      "epoch 13448: train loss: 0.11010229488715194, test loss: 0.26378618002859144\n",
      "epoch 13449: train loss: 0.1101002382073375, test loss: 0.2637866958693105\n",
      "epoch 13450: train loss: 0.11009818173351331, test loss: 0.2637872117636158\n",
      "epoch 13451: train loss: 0.11009612546564189, test loss: 0.2637877277114945\n",
      "epoch 13452: train loss: 0.11009406940368571, test loss: 0.26378824371293336\n",
      "epoch 13453: train loss: 0.11009201354760728, test loss: 0.26378875976791943\n",
      "epoch 13454: train loss: 0.11008995789736917, test loss: 0.2637892758764399\n",
      "epoch 13455: train loss: 0.11008790245293389, test loss: 0.2637897920384817\n",
      "epoch 13456: train loss: 0.11008584721426397, test loss: 0.26379030825403177\n",
      "epoch 13457: train loss: 0.110083792181322, test loss: 0.2637908245230771\n",
      "epoch 13458: train loss: 0.11008173735407052, test loss: 0.26379134084560485\n",
      "epoch 13459: train loss: 0.1100796827324721, test loss: 0.26379185722160187\n",
      "epoch 13460: train loss: 0.11007762831648932, test loss: 0.26379237365105535\n",
      "epoch 13461: train loss: 0.11007557410608479, test loss: 0.26379289013395224\n",
      "epoch 13462: train loss: 0.11007352010122108, test loss: 0.26379340667027956\n",
      "epoch 13463: train loss: 0.11007146630186085, test loss: 0.26379392326002443\n",
      "epoch 13464: train loss: 0.11006941270796668, test loss: 0.26379443990317386\n",
      "epoch 13465: train loss: 0.11006735931950123, test loss: 0.2637949565997148\n",
      "epoch 13466: train loss: 0.11006530613642712, test loss: 0.2637954733496343\n",
      "epoch 13467: train loss: 0.110063253158707, test loss: 0.26379599015291955\n",
      "epoch 13468: train loss: 0.11006120038630353, test loss: 0.2637965070095575\n",
      "epoch 13469: train loss: 0.11005914781917943, test loss: 0.26379702391953525\n",
      "epoch 13470: train loss: 0.11005709545729729, test loss: 0.2637975408828398\n",
      "epoch 13471: train loss: 0.11005504330061987, test loss: 0.2637980578994583\n",
      "epoch 13472: train loss: 0.11005299134910983, test loss: 0.26379857496937775\n",
      "epoch 13473: train loss: 0.11005093960272991, test loss: 0.26379909209258523\n",
      "epoch 13474: train loss: 0.11004888806144278, test loss: 0.2637996092690678\n",
      "epoch 13475: train loss: 0.11004683672521119, test loss: 0.26380012649881257\n",
      "epoch 13476: train loss: 0.1100447855939979, test loss: 0.2638006437818067\n",
      "epoch 13477: train loss: 0.11004273466776565, test loss: 0.2638011611180371\n",
      "epoch 13478: train loss: 0.11004068394647717, test loss: 0.263801678507491\n",
      "epoch 13479: train loss: 0.11003863343009523, test loss: 0.26380219595015536\n",
      "epoch 13480: train loss: 0.11003658311858265, test loss: 0.26380271344601736\n",
      "epoch 13481: train loss: 0.11003453301190215, test loss: 0.26380323099506414\n",
      "epoch 13482: train loss: 0.11003248311001655, test loss: 0.26380374859728273\n",
      "epoch 13483: train loss: 0.11003043341288869, test loss: 0.26380426625266024\n",
      "epoch 13484: train loss: 0.11002838392048131, test loss: 0.26380478396118373\n",
      "epoch 13485: train loss: 0.11002633463275732, test loss: 0.26380530172284045\n",
      "epoch 13486: train loss: 0.11002428554967945, test loss: 0.2638058195376174\n",
      "epoch 13487: train loss: 0.11002223667121065, test loss: 0.26380633740550175\n",
      "epoch 13488: train loss: 0.11002018799731371, test loss: 0.26380685532648057\n",
      "epoch 13489: train loss: 0.11001813952795148, test loss: 0.26380737330054094\n",
      "epoch 13490: train loss: 0.11001609126308687, test loss: 0.2638078913276702\n",
      "epoch 13491: train loss: 0.11001404320268275, test loss: 0.2638084094078553\n",
      "epoch 13492: train loss: 0.11001199534670199, test loss: 0.2638089275410834\n",
      "epoch 13493: train loss: 0.11000994769510751, test loss: 0.2638094457273417\n",
      "epoch 13494: train loss: 0.1100079002478622, test loss: 0.2638099639666172\n",
      "epoch 13495: train loss: 0.11000585300492899, test loss: 0.2638104822588972\n",
      "epoch 13496: train loss: 0.11000380596627082, test loss: 0.2638110006041688\n",
      "epoch 13497: train loss: 0.1100017591318506, test loss: 0.2638115190024191\n",
      "epoch 13498: train loss: 0.10999971250163129, test loss: 0.26381203745363535\n",
      "epoch 13499: train loss: 0.10999766607557587, test loss: 0.2638125559578046\n",
      "epoch 13500: train loss: 0.10999561985364728, test loss: 0.2638130745149141\n",
      "epoch 13501: train loss: 0.1099935738358085, test loss: 0.26381359312495095\n",
      "epoch 13502: train loss: 0.10999152802202254, test loss: 0.2638141117879023\n",
      "epoch 13503: train loss: 0.10998948241225232, test loss: 0.26381463050375537\n",
      "epoch 13504: train loss: 0.10998743700646094, test loss: 0.26381514927249733\n",
      "epoch 13505: train loss: 0.10998539180461135, test loss: 0.26381566809411544\n",
      "epoch 13506: train loss: 0.10998334680666659, test loss: 0.26381618696859677\n",
      "epoch 13507: train loss: 0.1099813020125897, test loss: 0.2638167058959285\n",
      "epoch 13508: train loss: 0.10997925742234373, test loss: 0.26381722487609777\n",
      "epoch 13509: train loss: 0.1099772130358917, test loss: 0.263817743909092\n",
      "epoch 13510: train loss: 0.10997516885319669, test loss: 0.26381826299489813\n",
      "epoch 13511: train loss: 0.10997312487422178, test loss: 0.2638187821335035\n",
      "epoch 13512: train loss: 0.10997108109893006, test loss: 0.2638193013248953\n",
      "epoch 13513: train loss: 0.10996903752728457, test loss: 0.26381982056906067\n",
      "epoch 13514: train loss: 0.10996699415924847, test loss: 0.2638203398659868\n",
      "epoch 13515: train loss: 0.1099649509947848, test loss: 0.263820859215661\n",
      "epoch 13516: train loss: 0.10996290803385676, test loss: 0.26382137861807053\n",
      "epoch 13517: train loss: 0.10996086527642741, test loss: 0.26382189807320244\n",
      "epoch 13518: train loss: 0.10995882272245992, test loss: 0.263822417581044\n",
      "epoch 13519: train loss: 0.10995678037191743, test loss: 0.2638229371415825\n",
      "epoch 13520: train loss: 0.10995473822476308, test loss: 0.26382345675480523\n",
      "epoch 13521: train loss: 0.10995269628096006, test loss: 0.26382397642069916\n",
      "epoch 13522: train loss: 0.10995065454047154, test loss: 0.26382449613925174\n",
      "epoch 13523: train loss: 0.1099486130032607, test loss: 0.2638250159104502\n",
      "epoch 13524: train loss: 0.10994657166929073, test loss: 0.26382553573428175\n",
      "epoch 13525: train loss: 0.10994453053852485, test loss: 0.26382605561073347\n",
      "epoch 13526: train loss: 0.10994248961092623, test loss: 0.26382657553979294\n",
      "epoch 13527: train loss: 0.10994044888645814, test loss: 0.2638270955214471\n",
      "epoch 13528: train loss: 0.1099384083650838, test loss: 0.2638276155556834\n",
      "epoch 13529: train loss: 0.1099363680467664, test loss: 0.263828135642489\n",
      "epoch 13530: train loss: 0.1099343279314693, test loss: 0.2638286557818512\n",
      "epoch 13531: train loss: 0.10993228801915565, test loss: 0.2638291759737572\n",
      "epoch 13532: train loss: 0.10993024830978877, test loss: 0.26382969621819446\n",
      "epoch 13533: train loss: 0.10992820880333193, test loss: 0.26383021651515004\n",
      "epoch 13534: train loss: 0.10992616949974841, test loss: 0.2638307368646113\n",
      "epoch 13535: train loss: 0.10992413039900155, test loss: 0.2638312572665654\n",
      "epoch 13536: train loss: 0.10992209150105463, test loss: 0.26383177772099986\n",
      "epoch 13537: train loss: 0.10992005280587094, test loss: 0.2638322982279018\n",
      "epoch 13538: train loss: 0.10991801431341379, test loss: 0.2638328187872585\n",
      "epoch 13539: train loss: 0.1099159760236466, test loss: 0.26383333939905734\n",
      "epoch 13540: train loss: 0.10991393793653266, test loss: 0.2638338600632855\n",
      "epoch 13541: train loss: 0.10991190005203533, test loss: 0.2638343807799304\n",
      "epoch 13542: train loss: 0.10990986237011796, test loss: 0.26383490154897926\n",
      "epoch 13543: train loss: 0.10990782489074397, test loss: 0.26383542237041946\n",
      "epoch 13544: train loss: 0.10990578761387666, test loss: 0.2638359432442381\n",
      "epoch 13545: train loss: 0.1099037505394795, test loss: 0.2638364641704227\n",
      "epoch 13546: train loss: 0.10990171366751586, test loss: 0.2638369851489606\n",
      "epoch 13547: train loss: 0.10989967699794914, test loss: 0.263837506179839\n",
      "epoch 13548: train loss: 0.10989764053074279, test loss: 0.26383802726304517\n",
      "epoch 13549: train loss: 0.1098956042658602, test loss: 0.2638385483985666\n",
      "epoch 13550: train loss: 0.10989356820326485, test loss: 0.26383906958639053\n",
      "epoch 13551: train loss: 0.10989153234292011, test loss: 0.26383959082650427\n",
      "epoch 13552: train loss: 0.10988949668478956, test loss: 0.26384011211889513\n",
      "epoch 13553: train loss: 0.10988746122883658, test loss: 0.2638406334635506\n",
      "epoch 13554: train loss: 0.10988542597502465, test loss: 0.2638411548604579\n",
      "epoch 13555: train loss: 0.10988339092331728, test loss: 0.2638416763096043\n",
      "epoch 13556: train loss: 0.10988135607367795, test loss: 0.2638421978109773\n",
      "epoch 13557: train loss: 0.10987932142607018, test loss: 0.26384271936456416\n",
      "epoch 13558: train loss: 0.10987728698045746, test loss: 0.26384324097035233\n",
      "epoch 13559: train loss: 0.10987525273680333, test loss: 0.263843762628329\n",
      "epoch 13560: train loss: 0.10987321869507131, test loss: 0.26384428433848156\n",
      "epoch 13561: train loss: 0.10987118485522496, test loss: 0.26384480610079764\n",
      "epoch 13562: train loss: 0.1098691512172278, test loss: 0.26384532791526427\n",
      "epoch 13563: train loss: 0.10986711778104344, test loss: 0.263845849781869\n",
      "epoch 13564: train loss: 0.10986508454663539, test loss: 0.2638463717005991\n",
      "epoch 13565: train loss: 0.10986305151396726, test loss: 0.26384689367144204\n",
      "epoch 13566: train loss: 0.10986101868300263, test loss: 0.2638474156943852\n",
      "epoch 13567: train loss: 0.10985898605370512, test loss: 0.2638479377694159\n",
      "epoch 13568: train loss: 0.1098569536260383, test loss: 0.26384845989652156\n",
      "epoch 13569: train loss: 0.1098549213999658, test loss: 0.2638489820756896\n",
      "epoch 13570: train loss: 0.10985288937545125, test loss: 0.26384950430690723\n",
      "epoch 13571: train loss: 0.10985085755245827, test loss: 0.2638500265901621\n",
      "epoch 13572: train loss: 0.10984882593095055, test loss: 0.26385054892544146\n",
      "epoch 13573: train loss: 0.10984679451089167, test loss: 0.2638510713127328\n",
      "epoch 13574: train loss: 0.10984476329224536, test loss: 0.2638515937520235\n",
      "epoch 13575: train loss: 0.10984273227497526, test loss: 0.26385211624330085\n",
      "epoch 13576: train loss: 0.10984070145904504, test loss: 0.26385263878655235\n",
      "epoch 13577: train loss: 0.1098386708444184, test loss: 0.2638531613817655\n",
      "epoch 13578: train loss: 0.10983664043105908, test loss: 0.2638536840289276\n",
      "epoch 13579: train loss: 0.1098346102189307, test loss: 0.2638542067280261\n",
      "epoch 13580: train loss: 0.10983258020799706, test loss: 0.26385472947904853\n",
      "epoch 13581: train loss: 0.10983055039822184, test loss: 0.26385525228198214\n",
      "epoch 13582: train loss: 0.10982852078956881, test loss: 0.2638557751368145\n",
      "epoch 13583: train loss: 0.10982649138200169, test loss: 0.26385629804353294\n",
      "epoch 13584: train loss: 0.10982446217548425, test loss: 0.26385682100212504\n",
      "epoch 13585: train loss: 0.10982243316998025, test loss: 0.26385734401257815\n",
      "epoch 13586: train loss: 0.10982040436545346, test loss: 0.2638578670748797\n",
      "epoch 13587: train loss: 0.10981837576186765, test loss: 0.2638583901890171\n",
      "epoch 13588: train loss: 0.10981634735918665, test loss: 0.2638589133549779\n",
      "epoch 13589: train loss: 0.1098143191573742, test loss: 0.2638594365727495\n",
      "epoch 13590: train loss: 0.10981229115639418, test loss: 0.2638599598423194\n",
      "epoch 13591: train loss: 0.10981026335621039, test loss: 0.26386048316367505\n",
      "epoch 13592: train loss: 0.10980823575678661, test loss: 0.26386100653680383\n",
      "epoch 13593: train loss: 0.10980620835808674, test loss: 0.2638615299616934\n",
      "epoch 13594: train loss: 0.1098041811600746, test loss: 0.2638620534383309\n",
      "epoch 13595: train loss: 0.10980215416271402, test loss: 0.2638625769667042\n",
      "epoch 13596: train loss: 0.10980012736596892, test loss: 0.26386310054680057\n",
      "epoch 13597: train loss: 0.10979810076980318, test loss: 0.26386362417860737\n",
      "epoch 13598: train loss: 0.10979607437418061, test loss: 0.26386414786211243\n",
      "epoch 13599: train loss: 0.10979404817906517, test loss: 0.26386467159730287\n",
      "epoch 13600: train loss: 0.10979202218442072, test loss: 0.26386519538416636\n",
      "epoch 13601: train loss: 0.10978999639021123, test loss: 0.2638657192226905\n",
      "epoch 13602: train loss: 0.10978797079640054, test loss: 0.2638662431128626\n",
      "epoch 13603: train loss: 0.10978594540295264, test loss: 0.26386676705467016\n",
      "epoch 13604: train loss: 0.10978392020983146, test loss: 0.26386729104810075\n",
      "epoch 13605: train loss: 0.10978189521700095, test loss: 0.2638678150931419\n",
      "epoch 13606: train loss: 0.10977987042442505, test loss: 0.26386833918978103\n",
      "epoch 13607: train loss: 0.10977784583206775, test loss: 0.26386886333800585\n",
      "epoch 13608: train loss: 0.109775821439893, test loss: 0.2638693875378037\n",
      "epoch 13609: train loss: 0.10977379724786482, test loss: 0.26386991178916214\n",
      "epoch 13610: train loss: 0.10977177325594717, test loss: 0.2638704360920686\n",
      "epoch 13611: train loss: 0.10976974946410407, test loss: 0.2638709604465108\n",
      "epoch 13612: train loss: 0.10976772587229953, test loss: 0.26387148485247613\n",
      "epoch 13613: train loss: 0.10976570248049759, test loss: 0.2638720093099522\n",
      "epoch 13614: train loss: 0.10976367928866226, test loss: 0.2638725338189265\n",
      "epoch 13615: train loss: 0.10976165629675759, test loss: 0.26387305837938657\n",
      "epoch 13616: train loss: 0.10975963350474763, test loss: 0.2638735829913199\n",
      "epoch 13617: train loss: 0.1097576109125964, test loss: 0.26387410765471414\n",
      "epoch 13618: train loss: 0.10975558852026804, test loss: 0.2638746323695568\n",
      "epoch 13619: train loss: 0.10975356632772656, test loss: 0.2638751571358355\n",
      "epoch 13620: train loss: 0.10975154433493611, test loss: 0.26387568195353756\n",
      "epoch 13621: train loss: 0.10974952254186075, test loss: 0.2638762068226508\n",
      "epoch 13622: train loss: 0.10974750094846457, test loss: 0.2638767317431627\n",
      "epoch 13623: train loss: 0.1097454795547117, test loss: 0.26387725671506074\n",
      "epoch 13624: train loss: 0.10974345836056626, test loss: 0.26387778173833254\n",
      "epoch 13625: train loss: 0.1097414373659924, test loss: 0.2638783068129657\n",
      "epoch 13626: train loss: 0.10973941657095423, test loss: 0.2638788319389478\n",
      "epoch 13627: train loss: 0.10973739597541594, test loss: 0.26387935711626637\n",
      "epoch 13628: train loss: 0.10973537557934167, test loss: 0.2638798823449091\n",
      "epoch 13629: train loss: 0.10973335538269557, test loss: 0.2638804076248634\n",
      "epoch 13630: train loss: 0.10973133538544184, test loss: 0.26388093295611703\n",
      "epoch 13631: train loss: 0.10972931558754467, test loss: 0.26388145833865745\n",
      "epoch 13632: train loss: 0.10972729598896824, test loss: 0.2638819837724723\n",
      "epoch 13633: train loss: 0.10972527658967676, test loss: 0.2638825092575492\n",
      "epoch 13634: train loss: 0.10972325738963447, test loss: 0.26388303479387565\n",
      "epoch 13635: train loss: 0.10972123838880553, test loss: 0.26388356038143945\n",
      "epoch 13636: train loss: 0.10971921958715423, test loss: 0.263884086020228\n",
      "epoch 13637: train loss: 0.10971720098464481, test loss: 0.2638846117102291\n",
      "epoch 13638: train loss: 0.10971518258124148, test loss: 0.26388513745143016\n",
      "epoch 13639: train loss: 0.10971316437690853, test loss: 0.26388566324381896\n",
      "epoch 13640: train loss: 0.10971114637161022, test loss: 0.26388618908738304\n",
      "epoch 13641: train loss: 0.10970912856531084, test loss: 0.26388671498211\n",
      "epoch 13642: train loss: 0.10970711095797465, test loss: 0.26388724092798754\n",
      "epoch 13643: train loss: 0.10970509354956601, test loss: 0.26388776692500326\n",
      "epoch 13644: train loss: 0.10970307634004917, test loss: 0.2638882929731448\n",
      "epoch 13645: train loss: 0.10970105932938841, test loss: 0.2638888190723998\n",
      "epoch 13646: train loss: 0.10969904251754813, test loss: 0.26388934522275587\n",
      "epoch 13647: train loss: 0.10969702590449261, test loss: 0.2638898714242006\n",
      "epoch 13648: train loss: 0.10969500949018623, test loss: 0.2638903976767218\n",
      "epoch 13649: train loss: 0.10969299327459332, test loss: 0.263890923980307\n",
      "epoch 13650: train loss: 0.10969097725767822, test loss: 0.26389145033494377\n",
      "epoch 13651: train loss: 0.10968896143940535, test loss: 0.26389197674061987\n",
      "epoch 13652: train loss: 0.10968694581973903, test loss: 0.26389250319732305\n",
      "epoch 13653: train loss: 0.1096849303986437, test loss: 0.26389302970504086\n",
      "epoch 13654: train loss: 0.10968291517608371, test loss: 0.263893556263761\n",
      "epoch 13655: train loss: 0.1096809001520235, test loss: 0.263894082873471\n",
      "epoch 13656: train loss: 0.10967888532642746, test loss: 0.2638946095341587\n",
      "epoch 13657: train loss: 0.10967687069926005, test loss: 0.2638951362458117\n",
      "epoch 13658: train loss: 0.10967485627048562, test loss: 0.26389566300841777\n",
      "epoch 13659: train loss: 0.10967284204006869, test loss: 0.26389618982196444\n",
      "epoch 13660: train loss: 0.10967082800797372, test loss: 0.2638967166864395\n",
      "epoch 13661: train loss: 0.10966881417416509, test loss: 0.2638972436018306\n",
      "epoch 13662: train loss: 0.10966680053860733, test loss: 0.2638977705681254\n",
      "epoch 13663: train loss: 0.10966478710126487, test loss: 0.26389829758531164\n",
      "epoch 13664: train loss: 0.10966277386210228, test loss: 0.263898824653377\n",
      "epoch 13665: train loss: 0.10966076082108397, test loss: 0.2638993517723092\n",
      "epoch 13666: train loss: 0.10965874797817449, test loss: 0.263899878942096\n",
      "epoch 13667: train loss: 0.10965673533333835, test loss: 0.2639004061627249\n",
      "epoch 13668: train loss: 0.10965472288654002, test loss: 0.26390093343418386\n",
      "epoch 13669: train loss: 0.1096527106377441, test loss: 0.26390146075646037\n",
      "epoch 13670: train loss: 0.1096506985869151, test loss: 0.2639019881295423\n",
      "epoch 13671: train loss: 0.10964868673401756, test loss: 0.2639025155534173\n",
      "epoch 13672: train loss: 0.10964667507901607, test loss: 0.2639030430280731\n",
      "epoch 13673: train loss: 0.10964466362187517, test loss: 0.2639035705534974\n",
      "epoch 13674: train loss: 0.10964265236255943, test loss: 0.263904098129678\n",
      "epoch 13675: train loss: 0.10964064130103346, test loss: 0.26390462575660256\n",
      "epoch 13676: train loss: 0.10963863043726185, test loss: 0.2639051534342589\n",
      "epoch 13677: train loss: 0.10963661977120917, test loss: 0.2639056811626347\n",
      "epoch 13678: train loss: 0.10963460930284011, test loss: 0.2639062089417176\n",
      "epoch 13679: train loss: 0.10963259903211918, test loss: 0.26390673677149556\n",
      "epoch 13680: train loss: 0.10963058895901112, test loss: 0.26390726465195613\n",
      "epoch 13681: train loss: 0.1096285790834805, test loss: 0.26390779258308716\n",
      "epoch 13682: train loss: 0.10962656940549198, test loss: 0.2639083205648764\n",
      "epoch 13683: train loss: 0.10962455992501025, test loss: 0.26390884859731156\n",
      "epoch 13684: train loss: 0.1096225506419999, test loss: 0.2639093766803805\n",
      "epoch 13685: train loss: 0.1096205415564257, test loss: 0.2639099048140709\n",
      "epoch 13686: train loss: 0.10961853266825226, test loss: 0.2639104329983705\n",
      "epoch 13687: train loss: 0.10961652397744431, test loss: 0.26391096123326707\n",
      "epoch 13688: train loss: 0.10961451548396656, test loss: 0.2639114895187485\n",
      "epoch 13689: train loss: 0.1096125071877837, test loss: 0.2639120178548025\n",
      "epoch 13690: train loss: 0.10961049908886046, test loss: 0.26391254624141675\n",
      "epoch 13691: train loss: 0.10960849118716155, test loss: 0.26391307467857916\n",
      "epoch 13692: train loss: 0.10960648348265174, test loss: 0.2639136031662775\n",
      "epoch 13693: train loss: 0.10960447597529573, test loss: 0.26391413170449946\n",
      "epoch 13694: train loss: 0.10960246866505831, test loss: 0.2639146602932329\n",
      "epoch 13695: train loss: 0.10960046155190425, test loss: 0.2639151889324657\n",
      "epoch 13696: train loss: 0.1095984546357983, test loss: 0.26391571762218546\n",
      "epoch 13697: train loss: 0.10959644791670527, test loss: 0.26391624636238015\n",
      "epoch 13698: train loss: 0.10959444139458992, test loss: 0.26391677515303763\n",
      "epoch 13699: train loss: 0.10959243506941707, test loss: 0.2639173039941454\n",
      "epoch 13700: train loss: 0.10959042894115151, test loss: 0.26391783288569154\n",
      "epoch 13701: train loss: 0.10958842300975807, test loss: 0.2639183618276638\n",
      "epoch 13702: train loss: 0.1095864172752016, test loss: 0.26391889082004993\n",
      "epoch 13703: train loss: 0.10958441173744689, test loss: 0.26391941986283785\n",
      "epoch 13704: train loss: 0.10958240639645883, test loss: 0.2639199489560153\n",
      "epoch 13705: train loss: 0.10958040125220224, test loss: 0.26392047809957014\n",
      "epoch 13706: train loss: 0.109578396304642, test loss: 0.26392100729349016\n",
      "epoch 13707: train loss: 0.10957639155374296, test loss: 0.2639215365377633\n",
      "epoch 13708: train loss: 0.10957438699947004, test loss: 0.2639220658323774\n",
      "epoch 13709: train loss: 0.10957238264178809, test loss: 0.26392259517732003\n",
      "epoch 13710: train loss: 0.10957037848066203, test loss: 0.2639231245725794\n",
      "epoch 13711: train loss: 0.10956837451605678, test loss: 0.26392365401814305\n",
      "epoch 13712: train loss: 0.10956637074793721, test loss: 0.26392418351399904\n",
      "epoch 13713: train loss: 0.10956436717626827, test loss: 0.26392471306013504\n",
      "epoch 13714: train loss: 0.1095623638010149, test loss: 0.26392524265653905\n",
      "epoch 13715: train loss: 0.10956036062214203, test loss: 0.26392577230319886\n",
      "epoch 13716: train loss: 0.10955835763961463, test loss: 0.2639263020001024\n",
      "epoch 13717: train loss: 0.10955635485339765, test loss: 0.2639268317472375\n",
      "epoch 13718: train loss: 0.10955435226345607, test loss: 0.2639273615445919\n",
      "epoch 13719: train loss: 0.10955234986975483, test loss: 0.2639278913921536\n",
      "epoch 13720: train loss: 0.10955034767225896, test loss: 0.26392842128991056\n",
      "epoch 13721: train loss: 0.10954834567093343, test loss: 0.2639289512378504\n",
      "epoch 13722: train loss: 0.10954634386574325, test loss: 0.26392948123596127\n",
      "epoch 13723: train loss: 0.10954434225665341, test loss: 0.2639300112842308\n",
      "epoch 13724: train loss: 0.109542340843629, test loss: 0.2639305413826471\n",
      "epoch 13725: train loss: 0.10954033962663498, test loss: 0.26393107153119794\n",
      "epoch 13726: train loss: 0.10953833860563643, test loss: 0.26393160172987123\n",
      "epoch 13727: train loss: 0.1095363377805984, test loss: 0.2639321319786548\n",
      "epoch 13728: train loss: 0.10953433715148592, test loss: 0.2639326622775367\n",
      "epoch 13729: train loss: 0.10953233671826407, test loss: 0.2639331926265047\n",
      "epoch 13730: train loss: 0.10953033648089792, test loss: 0.2639337230255468\n",
      "epoch 13731: train loss: 0.10952833643935261, test loss: 0.2639342534746508\n",
      "epoch 13732: train loss: 0.10952633659359311, test loss: 0.2639347839738047\n",
      "epoch 13733: train loss: 0.10952433694358465, test loss: 0.26393531452299634\n",
      "epoch 13734: train loss: 0.10952233748929224, test loss: 0.2639358451222137\n",
      "epoch 13735: train loss: 0.10952033823068107, test loss: 0.2639363757714447\n",
      "epoch 13736: train loss: 0.10951833916771622, test loss: 0.26393690647067714\n",
      "epoch 13737: train loss: 0.10951634030036289, test loss: 0.26393743721989915\n",
      "epoch 13738: train loss: 0.10951434162858616, test loss: 0.26393796801909847\n",
      "epoch 13739: train loss: 0.10951234315235121, test loss: 0.26393849886826315\n",
      "epoch 13740: train loss: 0.10951034487162321, test loss: 0.2639390297673811\n",
      "epoch 13741: train loss: 0.10950834678636731, test loss: 0.26393956071644026\n",
      "epoch 13742: train loss: 0.10950634889654873, test loss: 0.2639400917154285\n",
      "epoch 13743: train loss: 0.1095043512021326, test loss: 0.26394062276433383\n",
      "epoch 13744: train loss: 0.1095023537030842, test loss: 0.26394115386314426\n",
      "epoch 13745: train loss: 0.10950035639936867, test loss: 0.26394168501184756\n",
      "epoch 13746: train loss: 0.10949835929095122, test loss: 0.2639422162104319\n",
      "epoch 13747: train loss: 0.10949636237779714, test loss: 0.2639427474588851\n",
      "epoch 13748: train loss: 0.10949436565987161, test loss: 0.26394327875719514\n",
      "epoch 13749: train loss: 0.1094923691371399, test loss: 0.26394381010535\n",
      "epoch 13750: train loss: 0.10949037280956724, test loss: 0.26394434150333757\n",
      "epoch 13751: train loss: 0.10948837667711889, test loss: 0.263944872951146\n",
      "epoch 13752: train loss: 0.10948638073976015, test loss: 0.26394540444876297\n",
      "epoch 13753: train loss: 0.10948438499745626, test loss: 0.2639459359961769\n",
      "epoch 13754: train loss: 0.10948238945017252, test loss: 0.2639464675933753\n",
      "epoch 13755: train loss: 0.10948039409787423, test loss: 0.2639469992403464\n",
      "epoch 13756: train loss: 0.1094783989405267, test loss: 0.2639475309370782\n",
      "epoch 13757: train loss: 0.10947640397809522, test loss: 0.2639480626835586\n",
      "epoch 13758: train loss: 0.10947440921054513, test loss: 0.2639485944797756\n",
      "epoch 13759: train loss: 0.10947241463784174, test loss: 0.26394912632571715\n",
      "epoch 13760: train loss: 0.10947042025995042, test loss: 0.2639496582213714\n",
      "epoch 13761: train loss: 0.10946842607683649, test loss: 0.2639501901667262\n",
      "epoch 13762: train loss: 0.1094664320884653, test loss: 0.2639507221617697\n",
      "epoch 13763: train loss: 0.10946443829480228, test loss: 0.2639512542064898\n",
      "epoch 13764: train loss: 0.10946244469581273, test loss: 0.2639517863008745\n",
      "epoch 13765: train loss: 0.10946045129146205, test loss: 0.26395231844491185\n",
      "epoch 13766: train loss: 0.10945845808171564, test loss: 0.26395285063858975\n",
      "epoch 13767: train loss: 0.1094564650665389, test loss: 0.2639533828818965\n",
      "epoch 13768: train loss: 0.10945447224589724, test loss: 0.2639539151748198\n",
      "epoch 13769: train loss: 0.10945247961975607, test loss: 0.2639544475173479\n",
      "epoch 13770: train loss: 0.10945048718808081, test loss: 0.2639549799094688\n",
      "epoch 13771: train loss: 0.10944849495083693, test loss: 0.26395551235117043\n",
      "epoch 13772: train loss: 0.10944650290798982, test loss: 0.26395604484244084\n",
      "epoch 13773: train loss: 0.10944451105950499, test loss: 0.2639565773832681\n",
      "epoch 13774: train loss: 0.10944251940534787, test loss: 0.26395710997364025\n",
      "epoch 13775: train loss: 0.10944052794548391, test loss: 0.26395764261354543\n",
      "epoch 13776: train loss: 0.1094385366798786, test loss: 0.26395817530297144\n",
      "epoch 13777: train loss: 0.10943654560849747, test loss: 0.26395870804190663\n",
      "epoch 13778: train loss: 0.10943455473130595, test loss: 0.26395924083033884\n",
      "epoch 13779: train loss: 0.10943256404826959, test loss: 0.2639597736682562\n",
      "epoch 13780: train loss: 0.10943057355935389, test loss: 0.2639603065556467\n",
      "epoch 13781: train loss: 0.10942858326452437, test loss: 0.26396083949249854\n",
      "epoch 13782: train loss: 0.10942659316374655, test loss: 0.2639613724787997\n",
      "epoch 13783: train loss: 0.10942460325698597, test loss: 0.26396190551453813\n",
      "epoch 13784: train loss: 0.10942261354420825, test loss: 0.26396243859970214\n",
      "epoch 13785: train loss: 0.10942062402537882, test loss: 0.26396297173427963\n",
      "epoch 13786: train loss: 0.10941863470046334, test loss: 0.26396350491825876\n",
      "epoch 13787: train loss: 0.10941664556942733, test loss: 0.2639640381516276\n",
      "epoch 13788: train loss: 0.10941465663223643, test loss: 0.2639645714343742\n",
      "epoch 13789: train loss: 0.10941266788885617, test loss: 0.26396510476648666\n",
      "epoch 13790: train loss: 0.10941067933925215, test loss: 0.2639656381479531\n",
      "epoch 13791: train loss: 0.10940869098339004, test loss: 0.2639661715787615\n",
      "epoch 13792: train loss: 0.10940670282123541, test loss: 0.26396670505890013\n",
      "epoch 13793: train loss: 0.10940471485275392, test loss: 0.26396723858835697\n",
      "epoch 13794: train loss: 0.10940272707791115, test loss: 0.26396777216712014\n",
      "epoch 13795: train loss: 0.10940073949667278, test loss: 0.26396830579517777\n",
      "epoch 13796: train loss: 0.10939875210900446, test loss: 0.2639688394725179\n",
      "epoch 13797: train loss: 0.10939676491487185, test loss: 0.26396937319912867\n",
      "epoch 13798: train loss: 0.10939477791424061, test loss: 0.2639699069749984\n",
      "epoch 13799: train loss: 0.10939279110707643, test loss: 0.2639704408001148\n",
      "epoch 13800: train loss: 0.10939080449334497, test loss: 0.26397097467446634\n",
      "epoch 13801: train loss: 0.10938881807301197, test loss: 0.26397150859804097\n",
      "epoch 13802: train loss: 0.10938683184604309, test loss: 0.2639720425708269\n",
      "epoch 13803: train loss: 0.10938484581240406, test loss: 0.26397257659281226\n",
      "epoch 13804: train loss: 0.10938285997206058, test loss: 0.263973110663985\n",
      "epoch 13805: train loss: 0.10938087432497844, test loss: 0.26397364478433344\n",
      "epoch 13806: train loss: 0.10937888887112329, test loss: 0.2639741789538458\n",
      "epoch 13807: train loss: 0.10937690361046098, test loss: 0.26397471317251\n",
      "epoch 13808: train loss: 0.10937491854295715, test loss: 0.2639752474403142\n",
      "epoch 13809: train loss: 0.10937293366857766, test loss: 0.26397578175724673\n",
      "epoch 13810: train loss: 0.10937094898728822, test loss: 0.26397631612329564\n",
      "epoch 13811: train loss: 0.10936896449905464, test loss: 0.2639768505384491\n",
      "epoch 13812: train loss: 0.10936698020384272, test loss: 0.2639773850026952\n",
      "epoch 13813: train loss: 0.1093649961016182, test loss: 0.2639779195160222\n",
      "epoch 13814: train loss: 0.10936301219234698, test loss: 0.26397845407841813\n",
      "epoch 13815: train loss: 0.1093610284759948, test loss: 0.2639789886898713\n",
      "epoch 13816: train loss: 0.10935904495252752, test loss: 0.2639795233503698\n",
      "epoch 13817: train loss: 0.10935706162191094, test loss: 0.2639800580599018\n",
      "epoch 13818: train loss: 0.10935507848411094, test loss: 0.26398059281845554\n",
      "epoch 13819: train loss: 0.10935309553909335, test loss: 0.26398112762601905\n",
      "epoch 13820: train loss: 0.10935111278682402, test loss: 0.2639816624825807\n",
      "epoch 13821: train loss: 0.10934913022726882, test loss: 0.26398219738812856\n",
      "epoch 13822: train loss: 0.10934714786039365, test loss: 0.26398273234265085\n",
      "epoch 13823: train loss: 0.10934516568616436, test loss: 0.2639832673461357\n",
      "epoch 13824: train loss: 0.10934318370454686, test loss: 0.26398380239857133\n",
      "epoch 13825: train loss: 0.10934120191550704, test loss: 0.26398433749994604\n",
      "epoch 13826: train loss: 0.10933922031901083, test loss: 0.263984872650248\n",
      "epoch 13827: train loss: 0.10933723891502412, test loss: 0.2639854078494653\n",
      "epoch 13828: train loss: 0.10933525770351288, test loss: 0.26398594309758605\n",
      "epoch 13829: train loss: 0.10933327668444301, test loss: 0.26398647839459877\n",
      "epoch 13830: train loss: 0.10933129585778044, test loss: 0.26398701374049144\n",
      "epoch 13831: train loss: 0.10932931522349117, test loss: 0.2639875491352524\n",
      "epoch 13832: train loss: 0.10932733478154112, test loss: 0.26398808457886985\n",
      "epoch 13833: train loss: 0.10932535453189628, test loss: 0.2639886200713319\n",
      "epoch 13834: train loss: 0.1093233744745226, test loss: 0.26398915561262687\n",
      "epoch 13835: train loss: 0.10932139460938613, test loss: 0.26398969120274296\n",
      "epoch 13836: train loss: 0.1093194149364528, test loss: 0.2639902268416684\n",
      "epoch 13837: train loss: 0.10931743545568864, test loss: 0.2639907625293914\n",
      "epoch 13838: train loss: 0.10931545616705965, test loss: 0.26399129826590023\n",
      "epoch 13839: train loss: 0.10931347707053188, test loss: 0.26399183405118315\n",
      "epoch 13840: train loss: 0.10931149816607132, test loss: 0.2639923698852283\n",
      "epoch 13841: train loss: 0.10930951945364402, test loss: 0.2639929057680241\n",
      "epoch 13842: train loss: 0.10930754093321603, test loss: 0.26399344169955863\n",
      "epoch 13843: train loss: 0.10930556260475342, test loss: 0.26399397767982025\n",
      "epoch 13844: train loss: 0.10930358446822225, test loss: 0.26399451370879706\n",
      "epoch 13845: train loss: 0.10930160652358857, test loss: 0.2639950497864774\n",
      "epoch 13846: train loss: 0.10929962877081847, test loss: 0.26399558591284966\n",
      "epoch 13847: train loss: 0.10929765120987804, test loss: 0.26399612208790196\n",
      "epoch 13848: train loss: 0.1092956738407334, test loss: 0.2639966583116226\n",
      "epoch 13849: train loss: 0.10929369666335059, test loss: 0.26399719458399984\n",
      "epoch 13850: train loss: 0.10929171967769578, test loss: 0.2639977309050219\n",
      "epoch 13851: train loss: 0.10928974288373511, test loss: 0.26399826727467723\n",
      "epoch 13852: train loss: 0.10928776628143463, test loss: 0.26399880369295387\n",
      "epoch 13853: train loss: 0.10928578987076058, test loss: 0.2639993401598403\n",
      "epoch 13854: train loss: 0.10928381365167905, test loss: 0.26399987667532465\n",
      "epoch 13855: train loss: 0.10928183762415618, test loss: 0.2640004132393953\n",
      "epoch 13856: train loss: 0.10927986178815818, test loss: 0.26400094985204053\n",
      "epoch 13857: train loss: 0.10927788614365118, test loss: 0.2640014865132486\n",
      "epoch 13858: train loss: 0.10927591069060143, test loss: 0.2640020232230079\n",
      "epoch 13859: train loss: 0.10927393542897501, test loss: 0.2640025599813065\n",
      "epoch 13860: train loss: 0.10927196035873823, test loss: 0.26400309678813305\n",
      "epoch 13861: train loss: 0.10926998547985725, test loss: 0.26400363364347557\n",
      "epoch 13862: train loss: 0.10926801079229827, test loss: 0.2640041705473224\n",
      "epoch 13863: train loss: 0.10926603629602755, test loss: 0.264004707499662\n",
      "epoch 13864: train loss: 0.10926406199101128, test loss: 0.26400524450048246\n",
      "epoch 13865: train loss: 0.10926208787721574, test loss: 0.26400578154977233\n",
      "epoch 13866: train loss: 0.10926011395460718, test loss: 0.2640063186475198\n",
      "epoch 13867: train loss: 0.10925814022315183, test loss: 0.2640068557937132\n",
      "epoch 13868: train loss: 0.10925616668281596, test loss: 0.2640073929883409\n",
      "epoch 13869: train loss: 0.10925419333356588, test loss: 0.26400793023139113\n",
      "epoch 13870: train loss: 0.10925222017536783, test loss: 0.26400846752285234\n",
      "epoch 13871: train loss: 0.10925024720818813, test loss: 0.26400900486271284\n",
      "epoch 13872: train loss: 0.10924827443199306, test loss: 0.26400954225096085\n",
      "epoch 13873: train loss: 0.10924630184674892, test loss: 0.2640100796875849\n",
      "epoch 13874: train loss: 0.10924432945242209, test loss: 0.2640106171725732\n",
      "epoch 13875: train loss: 0.10924235724897884, test loss: 0.26401115470591413\n",
      "epoch 13876: train loss: 0.10924038523638548, test loss: 0.26401169228759613\n",
      "epoch 13877: train loss: 0.10923841341460841, test loss: 0.2640122299176073\n",
      "epoch 13878: train loss: 0.10923644178361397, test loss: 0.26401276759593634\n",
      "epoch 13879: train loss: 0.10923447034336851, test loss: 0.2640133053225712\n",
      "epoch 13880: train loss: 0.10923249909383838, test loss: 0.2640138430975007\n",
      "epoch 13881: train loss: 0.10923052803498998, test loss: 0.2640143809207129\n",
      "epoch 13882: train loss: 0.10922855716678967, test loss: 0.2640149187921962\n",
      "epoch 13883: train loss: 0.10922658648920386, test loss: 0.264015456711939\n",
      "epoch 13884: train loss: 0.10922461600219897, test loss: 0.26401599467992976\n",
      "epoch 13885: train loss: 0.10922264570574136, test loss: 0.26401653269615655\n",
      "epoch 13886: train loss: 0.10922067559979749, test loss: 0.26401707076060826\n",
      "epoch 13887: train loss: 0.10921870568433377, test loss: 0.26401760887327286\n",
      "epoch 13888: train loss: 0.10921673595931669, test loss: 0.2640181470341389\n",
      "epoch 13889: train loss: 0.10921476642471256, test loss: 0.26401868524319466\n",
      "epoch 13890: train loss: 0.10921279708048796, test loss: 0.26401922350042867\n",
      "epoch 13891: train loss: 0.10921082792660929, test loss: 0.2640197618058292\n",
      "epoch 13892: train loss: 0.10920885896304304, test loss: 0.26402030015938477\n",
      "epoch 13893: train loss: 0.10920689018975566, test loss: 0.2640208385610837\n",
      "epoch 13894: train loss: 0.10920492160671369, test loss: 0.2640213770109144\n",
      "epoch 13895: train loss: 0.10920295321388356, test loss: 0.2640219155088653\n",
      "epoch 13896: train loss: 0.10920098501123178, test loss: 0.26402245405492475\n",
      "epoch 13897: train loss: 0.1091990169987249, test loss: 0.26402299264908125\n",
      "epoch 13898: train loss: 0.10919704917632941, test loss: 0.26402353129132305\n",
      "epoch 13899: train loss: 0.10919508154401186, test loss: 0.26402406998163874\n",
      "epoch 13900: train loss: 0.10919311410173874, test loss: 0.2640246087200167\n",
      "epoch 13901: train loss: 0.10919114684947666, test loss: 0.26402514750644535\n",
      "epoch 13902: train loss: 0.10918917978719209, test loss: 0.2640256863409131\n",
      "epoch 13903: train loss: 0.10918721291485166, test loss: 0.26402622522340835\n",
      "epoch 13904: train loss: 0.10918524623242191, test loss: 0.2640267641539197\n",
      "epoch 13905: train loss: 0.10918327973986944, test loss: 0.2640273031324353\n",
      "epoch 13906: train loss: 0.10918131343716077, test loss: 0.2640278421589437\n",
      "epoch 13907: train loss: 0.10917934732426257, test loss: 0.26402838123343353\n",
      "epoch 13908: train loss: 0.10917738140114143, test loss: 0.26402892035589304\n",
      "epoch 13909: train loss: 0.1091754156677639, test loss: 0.26402945952631063\n",
      "epoch 13910: train loss: 0.10917345012409665, test loss: 0.2640299987446749\n",
      "epoch 13911: train loss: 0.10917148477010631, test loss: 0.2640305380109742\n",
      "epoch 13912: train loss: 0.10916951960575949, test loss: 0.2640310773251971\n",
      "epoch 13913: train loss: 0.10916755463102285, test loss: 0.264031616687332\n",
      "epoch 13914: train loss: 0.10916558984586304, test loss: 0.2640321560973673\n",
      "epoch 13915: train loss: 0.1091636252502467, test loss: 0.2640326955552916\n",
      "epoch 13916: train loss: 0.10916166084414054, test loss: 0.2640332350610931\n",
      "epoch 13917: train loss: 0.10915969662751122, test loss: 0.26403377461476063\n",
      "epoch 13918: train loss: 0.10915773260032541, test loss: 0.26403431421628243\n",
      "epoch 13919: train loss: 0.10915576876254977, test loss: 0.264034853865647\n",
      "epoch 13920: train loss: 0.1091538051141511, test loss: 0.2640353935628429\n",
      "epoch 13921: train loss: 0.10915184165509605, test loss: 0.2640359333078585\n",
      "epoch 13922: train loss: 0.1091498783853513, test loss: 0.26403647310068246\n",
      "epoch 13923: train loss: 0.10914791530488366, test loss: 0.26403701294130316\n",
      "epoch 13924: train loss: 0.10914595241365978, test loss: 0.264037552829709\n",
      "epoch 13925: train loss: 0.10914398971164647, test loss: 0.2640380927658886\n",
      "epoch 13926: train loss: 0.10914202719881048, test loss: 0.26403863274983036\n",
      "epoch 13927: train loss: 0.10914006487511851, test loss: 0.2640391727815229\n",
      "epoch 13928: train loss: 0.1091381027405374, test loss: 0.2640397128609547\n",
      "epoch 13929: train loss: 0.10913614079503389, test loss: 0.26404025298811423\n",
      "epoch 13930: train loss: 0.10913417903857475, test loss: 0.26404079316298995\n",
      "epoch 13931: train loss: 0.1091322174711268, test loss: 0.26404133338557045\n",
      "epoch 13932: train loss: 0.10913025609265681, test loss: 0.26404187365584425\n",
      "epoch 13933: train loss: 0.10912829490313164, test loss: 0.2640424139737998\n",
      "epoch 13934: train loss: 0.10912633390251805, test loss: 0.26404295433942565\n",
      "epoch 13935: train loss: 0.10912437309078292, test loss: 0.26404349475271033\n",
      "epoch 13936: train loss: 0.10912241246789306, test loss: 0.26404403521364234\n",
      "epoch 13937: train loss: 0.10912045203381529, test loss: 0.2640445757222103\n",
      "epoch 13938: train loss: 0.1091184917885165, test loss: 0.2640451162784026\n",
      "epoch 13939: train loss: 0.10911653173196352, test loss: 0.26404565688220794\n",
      "epoch 13940: train loss: 0.10911457186412324, test loss: 0.26404619753361475\n",
      "epoch 13941: train loss: 0.1091126121849625, test loss: 0.26404673823261154\n",
      "epoch 13942: train loss: 0.10911065269444822, test loss: 0.2640472789791869\n",
      "epoch 13943: train loss: 0.1091086933925473, test loss: 0.2640478197733294\n",
      "epoch 13944: train loss: 0.1091067342792266, test loss: 0.2640483606150276\n",
      "epoch 13945: train loss: 0.10910477535445307, test loss: 0.26404890150427\n",
      "epoch 13946: train loss: 0.10910281661819356, test loss: 0.2640494424410452\n",
      "epoch 13947: train loss: 0.10910085807041509, test loss: 0.26404998342534175\n",
      "epoch 13948: train loss: 0.1090988997110845, test loss: 0.2640505244571482\n",
      "epoch 13949: train loss: 0.10909694154016879, test loss: 0.2640510655364532\n",
      "epoch 13950: train loss: 0.1090949835576349, test loss: 0.2640516066632451\n",
      "epoch 13951: train loss: 0.10909302576344977, test loss: 0.2640521478375126\n",
      "epoch 13952: train loss: 0.1090910681575804, test loss: 0.2640526890592443\n",
      "epoch 13953: train loss: 0.10908911073999374, test loss: 0.2640532303284288\n",
      "epoch 13954: train loss: 0.10908715351065673, test loss: 0.2640537716450546\n",
      "epoch 13955: train loss: 0.10908519646953643, test loss: 0.2640543130091103\n",
      "epoch 13956: train loss: 0.10908323961659985, test loss: 0.2640548544205846\n",
      "epoch 13957: train loss: 0.10908128295181392, test loss: 0.26405539587946586\n",
      "epoch 13958: train loss: 0.1090793264751457, test loss: 0.26405593738574284\n",
      "epoch 13959: train loss: 0.10907737018656224, test loss: 0.2640564789394041\n",
      "epoch 13960: train loss: 0.10907541408603048, test loss: 0.2640570205404382\n",
      "epoch 13961: train loss: 0.10907345817351757, test loss: 0.26405756218883386\n",
      "epoch 13962: train loss: 0.1090715024489905, test loss: 0.2640581038845795\n",
      "epoch 13963: train loss: 0.10906954691241637, test loss: 0.26405864562766385\n",
      "epoch 13964: train loss: 0.10906759156376214, test loss: 0.26405918741807555\n",
      "epoch 13965: train loss: 0.10906563640299499, test loss: 0.26405972925580307\n",
      "epoch 13966: train loss: 0.10906368143008198, test loss: 0.2640602711408351\n",
      "epoch 13967: train loss: 0.10906172664499014, test loss: 0.2640608130731603\n",
      "epoch 13968: train loss: 0.10905977204768665, test loss: 0.2640613550527672\n",
      "epoch 13969: train loss: 0.10905781763813856, test loss: 0.2640618970796444\n",
      "epoch 13970: train loss: 0.10905586341631297, test loss: 0.26406243915378064\n",
      "epoch 13971: train loss: 0.10905390938217707, test loss: 0.2640629812751646\n",
      "epoch 13972: train loss: 0.10905195553569794, test loss: 0.2640635234437847\n",
      "epoch 13973: train loss: 0.10905000187684273, test loss: 0.26406406565962975\n",
      "epoch 13974: train loss: 0.10904804840557857, test loss: 0.2640646079226883\n",
      "epoch 13975: train loss: 0.10904609512187262, test loss: 0.2640651502329491\n",
      "epoch 13976: train loss: 0.1090441420256921, test loss: 0.2640656925904005\n",
      "epoch 13977: train loss: 0.10904218911700408, test loss: 0.26406623499503157\n",
      "epoch 13978: train loss: 0.10904023639577581, test loss: 0.2640667774468306\n",
      "epoch 13979: train loss: 0.10903828386197445, test loss: 0.26406731994578647\n",
      "epoch 13980: train loss: 0.1090363315155672, test loss: 0.26406786249188774\n",
      "epoch 13981: train loss: 0.10903437935652127, test loss: 0.2640684050851231\n",
      "epoch 13982: train loss: 0.10903242738480388, test loss: 0.2640689477254811\n",
      "epoch 13983: train loss: 0.1090304756003822, test loss: 0.26406949041295064\n",
      "epoch 13984: train loss: 0.10902852400322355, test loss: 0.2640700331475201\n",
      "epoch 13985: train loss: 0.10902657259329507, test loss: 0.26407057592917826\n",
      "epoch 13986: train loss: 0.10902462137056404, test loss: 0.2640711187579139\n",
      "epoch 13987: train loss: 0.10902267033499773, test loss: 0.2640716616337156\n",
      "epoch 13988: train loss: 0.10902071948656339, test loss: 0.26407220455657204\n",
      "epoch 13989: train loss: 0.10901876882522826, test loss: 0.2640727475264719\n",
      "epoch 13990: train loss: 0.10901681835095968, test loss: 0.2640732905434039\n",
      "epoch 13991: train loss: 0.10901486806372486, test loss: 0.2640738336073567\n",
      "epoch 13992: train loss: 0.10901291796349113, test loss: 0.264074376718319\n",
      "epoch 13993: train loss: 0.1090109680502258, test loss: 0.2640749198762794\n",
      "epoch 13994: train loss: 0.10900901832389616, test loss: 0.2640754630812268\n",
      "epoch 13995: train loss: 0.10900706878446952, test loss: 0.2640760063331497\n",
      "epoch 13996: train loss: 0.10900511943191322, test loss: 0.2640765496320369\n",
      "epoch 13997: train loss: 0.10900317026619462, test loss: 0.264077092977877\n",
      "epoch 13998: train loss: 0.10900122128728101, test loss: 0.2640776363706589\n",
      "epoch 13999: train loss: 0.1089992724951398, test loss: 0.2640781798103711\n",
      "epoch 14000: train loss: 0.10899732388973823, test loss: 0.2640787232970025\n",
      "epoch 14001: train loss: 0.1089953754710438, test loss: 0.26407926683054167\n",
      "epoch 14002: train loss: 0.10899342723902382, test loss: 0.26407981041097733\n",
      "epoch 14003: train loss: 0.1089914791936457, test loss: 0.26408035403829827\n",
      "epoch 14004: train loss: 0.10898953133487677, test loss: 0.2640808977124933\n",
      "epoch 14005: train loss: 0.10898758366268448, test loss: 0.26408144143355095\n",
      "epoch 14006: train loss: 0.10898563617703623, test loss: 0.2640819852014601\n",
      "epoch 14007: train loss: 0.10898368887789943, test loss: 0.2640825290162093\n",
      "epoch 14008: train loss: 0.10898174176524149, test loss: 0.2640830728777875\n",
      "epoch 14009: train loss: 0.10897979483902986, test loss: 0.26408361678618336\n",
      "epoch 14010: train loss: 0.10897784809923194, test loss: 0.2640841607413856\n",
      "epoch 14011: train loss: 0.10897590154581524, test loss: 0.26408470474338286\n",
      "epoch 14012: train loss: 0.10897395517874717, test loss: 0.2640852487921641\n",
      "epoch 14013: train loss: 0.10897200899799518, test loss: 0.264085792887718\n",
      "epoch 14014: train loss: 0.1089700630035268, test loss: 0.2640863370300332\n",
      "epoch 14015: train loss: 0.10896811719530941, test loss: 0.26408688121909857\n",
      "epoch 14016: train loss: 0.10896617157331061, test loss: 0.2640874254549028\n",
      "epoch 14017: train loss: 0.10896422613749782, test loss: 0.26408796973743476\n",
      "epoch 14018: train loss: 0.10896228088783856, test loss: 0.2640885140666831\n",
      "epoch 14019: train loss: 0.10896033582430034, test loss: 0.2640890584426366\n",
      "epoch 14020: train loss: 0.1089583909468507, test loss: 0.264089602865284\n",
      "epoch 14021: train loss: 0.10895644625545713, test loss: 0.2640901473346142\n",
      "epoch 14022: train loss: 0.1089545017500872, test loss: 0.26409069185061596\n",
      "epoch 14023: train loss: 0.1089525574307084, test loss: 0.26409123641327786\n",
      "epoch 14024: train loss: 0.10895061329728835, test loss: 0.2640917810225889\n",
      "epoch 14025: train loss: 0.10894866934979457, test loss: 0.2640923256785378\n",
      "epoch 14026: train loss: 0.10894672558819461, test loss: 0.2640928703811133\n",
      "epoch 14027: train loss: 0.1089447820124561, test loss: 0.2640934151303042\n",
      "epoch 14028: train loss: 0.10894283862254658, test loss: 0.26409395992609935\n",
      "epoch 14029: train loss: 0.10894089541843366, test loss: 0.26409450476848756\n",
      "epoch 14030: train loss: 0.1089389524000849, test loss: 0.2640950496574574\n",
      "epoch 14031: train loss: 0.10893700956746798, test loss: 0.264095594592998\n",
      "epoch 14032: train loss: 0.10893506692055045, test loss: 0.264096139575098\n",
      "epoch 14033: train loss: 0.10893312445929995, test loss: 0.26409668460374613\n",
      "epoch 14034: train loss: 0.10893118218368411, test loss: 0.2640972296789313\n",
      "epoch 14035: train loss: 0.10892924009367061, test loss: 0.2640977748006424\n",
      "epoch 14036: train loss: 0.10892729818922701, test loss: 0.26409831996886807\n",
      "epoch 14037: train loss: 0.10892535647032109, test loss: 0.26409886518359726\n",
      "epoch 14038: train loss: 0.10892341493692037, test loss: 0.26409941044481866\n",
      "epoch 14039: train loss: 0.10892147358899262, test loss: 0.26409995575252126\n",
      "epoch 14040: train loss: 0.10891953242650548, test loss: 0.2641005011066938\n",
      "epoch 14041: train loss: 0.10891759144942664, test loss: 0.2641010465073251\n",
      "epoch 14042: train loss: 0.10891565065772385, test loss: 0.2641015919544039\n",
      "epoch 14043: train loss: 0.10891371005136471, test loss: 0.26410213744791927\n",
      "epoch 14044: train loss: 0.10891176963031703, test loss: 0.2641026829878599\n",
      "epoch 14045: train loss: 0.10890982939454844, test loss: 0.2641032285742146\n",
      "epoch 14046: train loss: 0.10890788934402673, test loss: 0.2641037742069723\n",
      "epoch 14047: train loss: 0.10890594947871964, test loss: 0.26410431988612176\n",
      "epoch 14048: train loss: 0.10890400979859485, test loss: 0.2641048656116519\n",
      "epoch 14049: train loss: 0.10890207030362017, test loss: 0.2641054113835516\n",
      "epoch 14050: train loss: 0.10890013099376335, test loss: 0.26410595720180957\n",
      "epoch 14051: train loss: 0.10889819186899213, test loss: 0.2641065030664148\n",
      "epoch 14052: train loss: 0.10889625292927428, test loss: 0.26410704897735615\n",
      "epoch 14053: train loss: 0.10889431417457762, test loss: 0.2641075949346223\n",
      "epoch 14054: train loss: 0.10889237560486993, test loss: 0.2641081409382025\n",
      "epoch 14055: train loss: 0.10889043722011901, test loss: 0.2641086869880852\n",
      "epoch 14056: train loss: 0.10888849902029263, test loss: 0.26410923308425954\n",
      "epoch 14057: train loss: 0.10888656100535865, test loss: 0.2641097792267142\n",
      "epoch 14058: train loss: 0.10888462317528486, test loss: 0.2641103254154383\n",
      "epoch 14059: train loss: 0.10888268553003912, test loss: 0.26411087165042046\n",
      "epoch 14060: train loss: 0.10888074806958924, test loss: 0.2641114179316497\n",
      "epoch 14061: train loss: 0.10887881079390307, test loss: 0.26411196425911493\n",
      "epoch 14062: train loss: 0.1088768737029485, test loss: 0.2641125106328051\n",
      "epoch 14063: train loss: 0.10887493679669336, test loss: 0.26411305705270893\n",
      "epoch 14064: train loss: 0.10887300007510552, test loss: 0.26411360351881535\n",
      "epoch 14065: train loss: 0.10887106353815285, test loss: 0.26411415003111327\n",
      "epoch 14066: train loss: 0.10886912718580327, test loss: 0.2641146965895917\n",
      "epoch 14067: train loss: 0.10886719101802463, test loss: 0.26411524319423935\n",
      "epoch 14068: train loss: 0.10886525503478488, test loss: 0.2641157898450453\n",
      "epoch 14069: train loss: 0.10886331923605187, test loss: 0.2641163365419984\n",
      "epoch 14070: train loss: 0.10886138362179358, test loss: 0.26411688328508753\n",
      "epoch 14071: train loss: 0.10885944819197788, test loss: 0.2641174300743016\n",
      "epoch 14072: train loss: 0.10885751294657275, test loss: 0.26411797690962957\n",
      "epoch 14073: train loss: 0.1088555778855461, test loss: 0.2641185237910604\n",
      "epoch 14074: train loss: 0.1088536430088659, test loss: 0.2641190707185829\n",
      "epoch 14075: train loss: 0.10885170831650007, test loss: 0.26411961769218606\n",
      "epoch 14076: train loss: 0.10884977380841661, test loss: 0.26412016471185884\n",
      "epoch 14077: train loss: 0.1088478394845835, test loss: 0.2641207117775901\n",
      "epoch 14078: train loss: 0.10884590534496871, test loss: 0.26412125888936877\n",
      "epoch 14079: train loss: 0.10884397138954022, test loss: 0.2641218060471839\n",
      "epoch 14080: train loss: 0.10884203761826601, test loss: 0.26412235325102423\n",
      "epoch 14081: train loss: 0.10884010403111413, test loss: 0.2641229005008789\n",
      "epoch 14082: train loss: 0.10883817062805252, test loss: 0.2641234477967368\n",
      "epoch 14083: train loss: 0.1088362374090493, test loss: 0.2641239951385868\n",
      "epoch 14084: train loss: 0.10883430437407242, test loss: 0.264124542526418\n",
      "epoch 14085: train loss: 0.10883237152308993, test loss: 0.2641250899602192\n",
      "epoch 14086: train loss: 0.10883043885606991, test loss: 0.2641256374399794\n",
      "epoch 14087: train loss: 0.10882850637298037, test loss: 0.2641261849656875\n",
      "epoch 14088: train loss: 0.10882657407378937, test loss: 0.26412673253733265\n",
      "epoch 14089: train loss: 0.10882464195846503, test loss: 0.2641272801549037\n",
      "epoch 14090: train loss: 0.10882271002697536, test loss: 0.2641278278183896\n",
      "epoch 14091: train loss: 0.10882077827928846, test loss: 0.2641283755277793\n",
      "epoch 14092: train loss: 0.10881884671537247, test loss: 0.26412892328306187\n",
      "epoch 14093: train loss: 0.1088169153351954, test loss: 0.2641294710842262\n",
      "epoch 14094: train loss: 0.10881498413872542, test loss: 0.2641300189312613\n",
      "epoch 14095: train loss: 0.10881305312593068, test loss: 0.26413056682415614\n",
      "epoch 14096: train loss: 0.1088111222967792, test loss: 0.2641311147628997\n",
      "epoch 14097: train loss: 0.10880919165123919, test loss: 0.264131662747481\n",
      "epoch 14098: train loss: 0.10880726118927875, test loss: 0.26413221077788895\n",
      "epoch 14099: train loss: 0.10880533091086601, test loss: 0.2641327588541127\n",
      "epoch 14100: train loss: 0.1088034008159692, test loss: 0.26413330697614107\n",
      "epoch 14101: train loss: 0.1088014709045564, test loss: 0.26413385514396315\n",
      "epoch 14102: train loss: 0.10879954117659583, test loss: 0.26413440335756794\n",
      "epoch 14103: train loss: 0.10879761163205563, test loss: 0.26413495161694445\n",
      "epoch 14104: train loss: 0.10879568227090403, test loss: 0.26413549992208163\n",
      "epoch 14105: train loss: 0.10879375309310918, test loss: 0.26413604827296855\n",
      "epoch 14106: train loss: 0.1087918240986393, test loss: 0.26413659666959416\n",
      "epoch 14107: train loss: 0.10878989528746259, test loss: 0.26413714511194747\n",
      "epoch 14108: train loss: 0.10878796665954728, test loss: 0.26413769360001765\n",
      "epoch 14109: train loss: 0.10878603821486159, test loss: 0.26413824213379355\n",
      "epoch 14110: train loss: 0.10878410995337374, test loss: 0.2641387907132643\n",
      "epoch 14111: train loss: 0.10878218187505198, test loss: 0.2641393393384188\n",
      "epoch 14112: train loss: 0.10878025397986456, test loss: 0.2641398880092462\n",
      "epoch 14113: train loss: 0.10877832626777971, test loss: 0.2641404367257355\n",
      "epoch 14114: train loss: 0.10877639873876575, test loss: 0.26414098548787573\n",
      "epoch 14115: train loss: 0.10877447139279088, test loss: 0.26414153429565584\n",
      "epoch 14116: train loss: 0.10877254422982341, test loss: 0.264142083149065\n",
      "epoch 14117: train loss: 0.10877061724983164, test loss: 0.2641426320480922\n",
      "epoch 14118: train loss: 0.10876869045278387, test loss: 0.26414318099272655\n",
      "epoch 14119: train loss: 0.10876676383864839, test loss: 0.26414372998295704\n",
      "epoch 14120: train loss: 0.1087648374073935, test loss: 0.26414427901877263\n",
      "epoch 14121: train loss: 0.10876291115898748, test loss: 0.26414482810016254\n",
      "epoch 14122: train loss: 0.10876098509339877, test loss: 0.2641453772271156\n",
      "epoch 14123: train loss: 0.10875905921059559, test loss: 0.26414592639962114\n",
      "epoch 14124: train loss: 0.10875713351054633, test loss: 0.26414647561766813\n",
      "epoch 14125: train loss: 0.10875520799321935, test loss: 0.26414702488124553\n",
      "epoch 14126: train loss: 0.10875328265858294, test loss: 0.2641475741903425\n",
      "epoch 14127: train loss: 0.10875135750660556, test loss: 0.26414812354494804\n",
      "epoch 14128: train loss: 0.10874943253725552, test loss: 0.26414867294505134\n",
      "epoch 14129: train loss: 0.10874750775050124, test loss: 0.2641492223906415\n",
      "epoch 14130: train loss: 0.10874558314631105, test loss: 0.2641497718817074\n",
      "epoch 14131: train loss: 0.1087436587246534, test loss: 0.26415032141823824\n",
      "epoch 14132: train loss: 0.10874173448549666, test loss: 0.26415087100022316\n",
      "epoch 14133: train loss: 0.10873981042880924, test loss: 0.26415142062765107\n",
      "epoch 14134: train loss: 0.10873788655455961, test loss: 0.26415197030051124\n",
      "epoch 14135: train loss: 0.10873596286271614, test loss: 0.2641525200187927\n",
      "epoch 14136: train loss: 0.10873403935324726, test loss: 0.2641530697824845\n",
      "epoch 14137: train loss: 0.10873211602612144, test loss: 0.26415361959157574\n",
      "epoch 14138: train loss: 0.10873019288130716, test loss: 0.26415416944605563\n",
      "epoch 14139: train loss: 0.10872826991877281, test loss: 0.26415471934591317\n",
      "epoch 14140: train loss: 0.10872634713848689, test loss: 0.2641552692911375\n",
      "epoch 14141: train loss: 0.10872442454041785, test loss: 0.2641558192817178\n",
      "epoch 14142: train loss: 0.10872250212453423, test loss: 0.26415636931764297\n",
      "epoch 14143: train loss: 0.10872057989080446, test loss: 0.2641569193989024\n",
      "epoch 14144: train loss: 0.10871865783919706, test loss: 0.26415746952548497\n",
      "epoch 14145: train loss: 0.10871673596968051, test loss: 0.2641580196973799\n",
      "epoch 14146: train loss: 0.10871481428222335, test loss: 0.2641585699145764\n",
      "epoch 14147: train loss: 0.10871289277679409, test loss: 0.26415912017706344\n",
      "epoch 14148: train loss: 0.10871097145336125, test loss: 0.26415967048483024\n",
      "epoch 14149: train loss: 0.10870905031189337, test loss: 0.2641602208378659\n",
      "epoch 14150: train loss: 0.108707129352359, test loss: 0.2641607712361596\n",
      "epoch 14151: train loss: 0.1087052085747267, test loss: 0.2641613216797004\n",
      "epoch 14152: train loss: 0.10870328797896497, test loss: 0.26416187216847753\n",
      "epoch 14153: train loss: 0.10870136756504245, test loss: 0.26416242270248\n",
      "epoch 14154: train loss: 0.10869944733292765, test loss: 0.26416297328169713\n",
      "epoch 14155: train loss: 0.10869752728258922, test loss: 0.26416352390611786\n",
      "epoch 14156: train loss: 0.1086956074139957, test loss: 0.2641640745757315\n",
      "epoch 14157: train loss: 0.10869368772711567, test loss: 0.26416462529052714\n",
      "epoch 14158: train loss: 0.10869176822191776, test loss: 0.264165176050494\n",
      "epoch 14159: train loss: 0.1086898488983706, test loss: 0.26416572685562123\n",
      "epoch 14160: train loss: 0.10868792975644279, test loss: 0.26416627770589785\n",
      "epoch 14161: train loss: 0.10868601079610295, test loss: 0.2641668286013131\n",
      "epoch 14162: train loss: 0.10868409201731973, test loss: 0.26416737954185626\n",
      "epoch 14163: train loss: 0.10868217342006176, test loss: 0.26416793052751636\n",
      "epoch 14164: train loss: 0.1086802550042977, test loss: 0.2641684815582827\n",
      "epoch 14165: train loss: 0.1086783367699962, test loss: 0.2641690326341443\n",
      "epoch 14166: train loss: 0.10867641871712592, test loss: 0.2641695837550904\n",
      "epoch 14167: train loss: 0.10867450084565557, test loss: 0.2641701349211102\n",
      "epoch 14168: train loss: 0.10867258315555377, test loss: 0.26417068613219297\n",
      "epoch 14169: train loss: 0.10867066564678925, test loss: 0.26417123738832776\n",
      "epoch 14170: train loss: 0.10866874831933071, test loss: 0.26417178868950375\n",
      "epoch 14171: train loss: 0.10866683117314682, test loss: 0.26417234003571016\n",
      "epoch 14172: train loss: 0.10866491420820633, test loss: 0.26417289142693634\n",
      "epoch 14173: train loss: 0.10866299742447794, test loss: 0.26417344286317124\n",
      "epoch 14174: train loss: 0.10866108082193039, test loss: 0.26417399434440425\n",
      "epoch 14175: train loss: 0.10865916440053237, test loss: 0.2641745458706245\n",
      "epoch 14176: train loss: 0.10865724816025267, test loss: 0.2641750974418211\n",
      "epoch 14177: train loss: 0.10865533210106004, test loss: 0.2641756490579835\n",
      "epoch 14178: train loss: 0.10865341622292324, test loss: 0.26417620071910064\n",
      "epoch 14179: train loss: 0.10865150052581099, test loss: 0.2641767524251619\n",
      "epoch 14180: train loss: 0.10864958500969209, test loss: 0.26417730417615637\n",
      "epoch 14181: train loss: 0.10864766967453533, test loss: 0.26417785597207344\n",
      "epoch 14182: train loss: 0.10864575452030949, test loss: 0.26417840781290214\n",
      "epoch 14183: train loss: 0.10864383954698335, test loss: 0.264178959698632\n",
      "epoch 14184: train loss: 0.10864192475452572, test loss: 0.26417951162925185\n",
      "epoch 14185: train loss: 0.10864001014290547, test loss: 0.26418006360475116\n",
      "epoch 14186: train loss: 0.10863809571209133, test loss: 0.2641806156251191\n",
      "epoch 14187: train loss: 0.10863618146205216, test loss: 0.264181167690345\n",
      "epoch 14188: train loss: 0.1086342673927568, test loss: 0.2641817198004179\n",
      "epoch 14189: train loss: 0.10863235350417408, test loss: 0.2641822719553272\n",
      "epoch 14190: train loss: 0.10863043979627289, test loss: 0.2641828241550621\n",
      "epoch 14191: train loss: 0.10862852626902203, test loss: 0.2641833763996118\n",
      "epoch 14192: train loss: 0.1086266129223904, test loss: 0.2641839286889657\n",
      "epoch 14193: train loss: 0.10862469975634685, test loss: 0.2641844810231128\n",
      "epoch 14194: train loss: 0.10862278677086028, test loss: 0.2641850334020426\n",
      "epoch 14195: train loss: 0.10862087396589956, test loss: 0.26418558582574425\n",
      "epoch 14196: train loss: 0.10861896134143359, test loss: 0.264186138294207\n",
      "epoch 14197: train loss: 0.1086170488974313, test loss: 0.2641866908074201\n",
      "epoch 14198: train loss: 0.10861513663386155, test loss: 0.26418724336537286\n",
      "epoch 14199: train loss: 0.10861322455069328, test loss: 0.26418779596805453\n",
      "epoch 14200: train loss: 0.10861131264789542, test loss: 0.26418834861545437\n",
      "epoch 14201: train loss: 0.10860940092543689, test loss: 0.2641889013075617\n",
      "epoch 14202: train loss: 0.10860748938328665, test loss: 0.2641894540443657\n",
      "epoch 14203: train loss: 0.10860557802141363, test loss: 0.2641900068258557\n",
      "epoch 14204: train loss: 0.10860366683978678, test loss: 0.26419055965202104\n",
      "epoch 14205: train loss: 0.10860175583837511, test loss: 0.2641911125228509\n",
      "epoch 14206: train loss: 0.10859984501714756, test loss: 0.2641916654383346\n",
      "epoch 14207: train loss: 0.10859793437607308, test loss: 0.26419221839846146\n",
      "epoch 14208: train loss: 0.10859602391512066, test loss: 0.2641927714032208\n",
      "epoch 14209: train loss: 0.10859411363425935, test loss: 0.26419332445260185\n",
      "epoch 14210: train loss: 0.10859220353345812, test loss: 0.2641938775465939\n",
      "epoch 14211: train loss: 0.10859029361268593, test loss: 0.2641944306851863\n",
      "epoch 14212: train loss: 0.10858838387191186, test loss: 0.2641949838683683\n",
      "epoch 14213: train loss: 0.10858647431110495, test loss: 0.26419553709612925\n",
      "epoch 14214: train loss: 0.10858456493023415, test loss: 0.2641960903684584\n",
      "epoch 14215: train loss: 0.10858265572926853, test loss: 0.26419664368534523\n",
      "epoch 14216: train loss: 0.10858074670817718, test loss: 0.2641971970467789\n",
      "epoch 14217: train loss: 0.10857883786692911, test loss: 0.2641977504527487\n",
      "epoch 14218: train loss: 0.1085769292054934, test loss: 0.264198303903244\n",
      "epoch 14219: train loss: 0.10857502072383912, test loss: 0.26419885739825416\n",
      "epoch 14220: train loss: 0.10857311242193532, test loss: 0.26419941093776844\n",
      "epoch 14221: train loss: 0.10857120429975112, test loss: 0.2641999645217762\n",
      "epoch 14222: train loss: 0.10856929635725558, test loss: 0.2642005181502668\n",
      "epoch 14223: train loss: 0.1085673885944178, test loss: 0.2642010718232294\n",
      "epoch 14224: train loss: 0.10856548101120694, test loss: 0.26420162554065363\n",
      "epoch 14225: train loss: 0.10856357360759204, test loss: 0.2642021793025286\n",
      "epoch 14226: train loss: 0.10856166638354228, test loss: 0.2642027331088438\n",
      "epoch 14227: train loss: 0.10855975933902676, test loss: 0.26420328695958845\n",
      "epoch 14228: train loss: 0.10855785247401462, test loss: 0.2642038408547519\n",
      "epoch 14229: train loss: 0.10855594578847498, test loss: 0.26420439479432356\n",
      "epoch 14230: train loss: 0.10855403928237706, test loss: 0.26420494877829276\n",
      "epoch 14231: train loss: 0.10855213295568994, test loss: 0.2642055028066488\n",
      "epoch 14232: train loss: 0.10855022680838285, test loss: 0.26420605687938115\n",
      "epoch 14233: train loss: 0.10854832084042493, test loss: 0.26420661099647913\n",
      "epoch 14234: train loss: 0.10854641505178537, test loss: 0.26420716515793197\n",
      "epoch 14235: train loss: 0.10854450944243337, test loss: 0.2642077193637293\n",
      "epoch 14236: train loss: 0.10854260401233809, test loss: 0.26420827361386023\n",
      "epoch 14237: train loss: 0.10854069876146877, test loss: 0.2642088279083143\n",
      "epoch 14238: train loss: 0.10853879368979463, test loss: 0.26420938224708074\n",
      "epoch 14239: train loss: 0.10853688879728488, test loss: 0.26420993663014897\n",
      "epoch 14240: train loss: 0.1085349840839087, test loss: 0.2642104910575085\n",
      "epoch 14241: train loss: 0.10853307954963537, test loss: 0.2642110455291485\n",
      "epoch 14242: train loss: 0.10853117519443416, test loss: 0.26421160004505856\n",
      "epoch 14243: train loss: 0.10852927101827425, test loss: 0.2642121546052279\n",
      "epoch 14244: train loss: 0.10852736702112495, test loss: 0.2642127092096459\n",
      "epoch 14245: train loss: 0.10852546320295549, test loss: 0.2642132638583022\n",
      "epoch 14246: train loss: 0.1085235595637352, test loss: 0.2642138185511859\n",
      "epoch 14247: train loss: 0.10852165610343327, test loss: 0.26421437328828656\n",
      "epoch 14248: train loss: 0.10851975282201906, test loss: 0.26421492806959357\n",
      "epoch 14249: train loss: 0.10851784971946181, test loss: 0.2642154828950962\n",
      "epoch 14250: train loss: 0.10851594679573086, test loss: 0.2642160377647841\n",
      "epoch 14251: train loss: 0.10851404405079552, test loss: 0.2642165926786464\n",
      "epoch 14252: train loss: 0.10851214148462511, test loss: 0.2642171476366726\n",
      "epoch 14253: train loss: 0.1085102390971889, test loss: 0.2642177026388523\n",
      "epoch 14254: train loss: 0.10850833688845628, test loss: 0.2642182576851747\n",
      "epoch 14255: train loss: 0.10850643485839657, test loss: 0.26421881277562925\n",
      "epoch 14256: train loss: 0.10850453300697913, test loss: 0.2642193679102054\n",
      "epoch 14257: train loss: 0.10850263133417329, test loss: 0.26421992308889264\n",
      "epoch 14258: train loss: 0.10850072983994842, test loss: 0.26422047831168033\n",
      "epoch 14259: train loss: 0.10849882852427388, test loss: 0.2642210335785578\n",
      "epoch 14260: train loss: 0.10849692738711908, test loss: 0.2642215888895147\n",
      "epoch 14261: train loss: 0.10849502642845338, test loss: 0.26422214424454027\n",
      "epoch 14262: train loss: 0.10849312564824615, test loss: 0.26422269964362416\n",
      "epoch 14263: train loss: 0.10849122504646683, test loss: 0.26422325508675554\n",
      "epoch 14264: train loss: 0.10848932462308479, test loss: 0.264223810573924\n",
      "epoch 14265: train loss: 0.10848742437806948, test loss: 0.264224366105119\n",
      "epoch 14266: train loss: 0.1084855243113903, test loss: 0.26422492168033\n",
      "epoch 14267: train loss: 0.10848362442301668, test loss: 0.2642254772995464\n",
      "epoch 14268: train loss: 0.10848172471291806, test loss: 0.26422603296275754\n",
      "epoch 14269: train loss: 0.10847982518106386, test loss: 0.26422658866995313\n",
      "epoch 14270: train loss: 0.10847792582742358, test loss: 0.26422714442112244\n",
      "epoch 14271: train loss: 0.10847602665196662, test loss: 0.26422770021625497\n",
      "epoch 14272: train loss: 0.1084741276546625, test loss: 0.26422825605534017\n",
      "epoch 14273: train loss: 0.10847222883548063, test loss: 0.2642288119383676\n",
      "epoch 14274: train loss: 0.10847033019439056, test loss: 0.2642293678653267\n",
      "epoch 14275: train loss: 0.10846843173136175, test loss: 0.26422992383620675\n",
      "epoch 14276: train loss: 0.10846653344636369, test loss: 0.26423047985099746\n",
      "epoch 14277: train loss: 0.10846463533936586, test loss: 0.2642310359096882\n",
      "epoch 14278: train loss: 0.1084627374103378, test loss: 0.2642315920122684\n",
      "epoch 14279: train loss: 0.10846083965924905, test loss: 0.2642321481587278\n",
      "epoch 14280: train loss: 0.10845894208606907, test loss: 0.2642327043490556\n",
      "epoch 14281: train loss: 0.10845704469076743, test loss: 0.26423326058324137\n",
      "epoch 14282: train loss: 0.10845514747331368, test loss: 0.2642338168612746\n",
      "epoch 14283: train loss: 0.10845325043367737, test loss: 0.26423437318314486\n",
      "epoch 14284: train loss: 0.10845135357182802, test loss: 0.26423492954884154\n",
      "epoch 14285: train loss: 0.10844945688773523, test loss: 0.2642354859583541\n",
      "epoch 14286: train loss: 0.10844756038136852, test loss: 0.2642360424116722\n",
      "epoch 14287: train loss: 0.10844566405269752, test loss: 0.26423659890878526\n",
      "epoch 14288: train loss: 0.10844376790169179, test loss: 0.26423715544968274\n",
      "epoch 14289: train loss: 0.10844187192832092, test loss: 0.26423771203435426\n",
      "epoch 14290: train loss: 0.10843997613255452, test loss: 0.26423826866278927\n",
      "epoch 14291: train loss: 0.10843808051436218, test loss: 0.26423882533497717\n",
      "epoch 14292: train loss: 0.10843618507371353, test loss: 0.2642393820509077\n",
      "epoch 14293: train loss: 0.10843428981057818, test loss: 0.2642399388105702\n",
      "epoch 14294: train loss: 0.10843239472492573, test loss: 0.2642404956139543\n",
      "epoch 14295: train loss: 0.1084304998167259, test loss: 0.26424105246104934\n",
      "epoch 14296: train loss: 0.10842860508594825, test loss: 0.26424160935184515\n",
      "epoch 14297: train loss: 0.10842671053256246, test loss: 0.264242166286331\n",
      "epoch 14298: train loss: 0.1084248161565382, test loss: 0.2642427232644966\n",
      "epoch 14299: train loss: 0.10842292195784513, test loss: 0.26424328028633126\n",
      "epoch 14300: train loss: 0.10842102793645289, test loss: 0.2642438373518248\n",
      "epoch 14301: train loss: 0.1084191340923312, test loss: 0.2642443944609666\n",
      "epoch 14302: train loss: 0.10841724042544972, test loss: 0.26424495161374617\n",
      "epoch 14303: train loss: 0.1084153469357782, test loss: 0.2642455088101531\n",
      "epoch 14304: train loss: 0.10841345362328626, test loss: 0.26424606605017686\n",
      "epoch 14305: train loss: 0.10841156048794366, test loss: 0.2642466233338072\n",
      "epoch 14306: train loss: 0.10840966752972009, test loss: 0.26424718066103353\n",
      "epoch 14307: train loss: 0.1084077747485853, test loss: 0.26424773803184537\n",
      "epoch 14308: train loss: 0.10840588214450904, test loss: 0.2642482954462324\n",
      "epoch 14309: train loss: 0.10840398971746097, test loss: 0.26424885290418415\n",
      "epoch 14310: train loss: 0.10840209746741093, test loss: 0.26424941040569006\n",
      "epoch 14311: train loss: 0.1084002053943286, test loss: 0.26424996795073985\n",
      "epoch 14312: train loss: 0.1083983134981838, test loss: 0.264250525539323\n",
      "epoch 14313: train loss: 0.1083964217789462, test loss: 0.2642510831714291\n",
      "epoch 14314: train loss: 0.10839453023658571, test loss: 0.2642516408470477\n",
      "epoch 14315: train loss: 0.10839263887107205, test loss: 0.26425219856616844\n",
      "epoch 14316: train loss: 0.10839074768237497, test loss: 0.2642527563287809\n",
      "epoch 14317: train loss: 0.1083888566704643, test loss: 0.2642533141348746\n",
      "epoch 14318: train loss: 0.10838696583530988, test loss: 0.26425387198443917\n",
      "epoch 14319: train loss: 0.10838507517688147, test loss: 0.2642544298774641\n",
      "epoch 14320: train loss: 0.1083831846951489, test loss: 0.26425498781393914\n",
      "epoch 14321: train loss: 0.10838129439008201, test loss: 0.26425554579385374\n",
      "epoch 14322: train loss: 0.10837940426165064, test loss: 0.26425610381719755\n",
      "epoch 14323: train loss: 0.10837751430982462, test loss: 0.2642566618839602\n",
      "epoch 14324: train loss: 0.10837562453457376, test loss: 0.26425721999413126\n",
      "epoch 14325: train loss: 0.10837373493586801, test loss: 0.26425777814770035\n",
      "epoch 14326: train loss: 0.10837184551367716, test loss: 0.26425833634465706\n",
      "epoch 14327: train loss: 0.10836995626797108, test loss: 0.264258894584991\n",
      "epoch 14328: train loss: 0.10836806719871968, test loss: 0.2642594528686917\n",
      "epoch 14329: train loss: 0.10836617830589283, test loss: 0.264260011195749\n",
      "epoch 14330: train loss: 0.10836428958946044, test loss: 0.2642605695661522\n",
      "epoch 14331: train loss: 0.10836240104939236, test loss: 0.26426112797989115\n",
      "epoch 14332: train loss: 0.10836051268565854, test loss: 0.2642616864369554\n",
      "epoch 14333: train loss: 0.10835862449822889, test loss: 0.26426224493733463\n",
      "epoch 14334: train loss: 0.10835673648707331, test loss: 0.2642628034810183\n",
      "epoch 14335: train loss: 0.10835484865216177, test loss: 0.2642633620679963\n",
      "epoch 14336: train loss: 0.10835296099346416, test loss: 0.26426392069825794\n",
      "epoch 14337: train loss: 0.10835107351095044, test loss: 0.2642644793717932\n",
      "epoch 14338: train loss: 0.10834918620459055, test loss: 0.2642650380885915\n",
      "epoch 14339: train loss: 0.10834729907435449, test loss: 0.2642655968486424\n",
      "epoch 14340: train loss: 0.10834541212021218, test loss: 0.2642661556519358\n",
      "epoch 14341: train loss: 0.10834352534213362, test loss: 0.2642667144984611\n",
      "epoch 14342: train loss: 0.10834163874008876, test loss: 0.26426727338820816\n",
      "epoch 14343: train loss: 0.1083397523140476, test loss: 0.2642678323211664\n",
      "epoch 14344: train loss: 0.10833786606398016, test loss: 0.26426839129732566\n",
      "epoch 14345: train loss: 0.10833597998985642, test loss: 0.26426895031667563\n",
      "epoch 14346: train loss: 0.10833409409164638, test loss: 0.2642695093792058\n",
      "epoch 14347: train loss: 0.10833220836932007, test loss: 0.26427006848490575\n",
      "epoch 14348: train loss: 0.10833032282284752, test loss: 0.26427062763376546\n",
      "epoch 14349: train loss: 0.10832843745219868, test loss: 0.2642711868257744\n",
      "epoch 14350: train loss: 0.10832655225734371, test loss: 0.26427174606092224\n",
      "epoch 14351: train loss: 0.10832466723825263, test loss: 0.2642723053391986\n",
      "epoch 14352: train loss: 0.10832278239489541, test loss: 0.26427286466059335\n",
      "epoch 14353: train loss: 0.10832089772724217, test loss: 0.26427342402509596\n",
      "epoch 14354: train loss: 0.10831901323526298, test loss: 0.2642739834326962\n",
      "epoch 14355: train loss: 0.10831712891892789, test loss: 0.2642745428833838\n",
      "epoch 14356: train loss: 0.108315244778207, test loss: 0.26427510237714835\n",
      "epoch 14357: train loss: 0.1083133608130704, test loss: 0.2642756619139795\n",
      "epoch 14358: train loss: 0.10831147702348817, test loss: 0.2642762214938671\n",
      "epoch 14359: train loss: 0.10830959340943042, test loss: 0.26427678111680064\n",
      "epoch 14360: train loss: 0.10830770997086724, test loss: 0.26427734078277\n",
      "epoch 14361: train loss: 0.10830582670776877, test loss: 0.2642779004917648\n",
      "epoch 14362: train loss: 0.10830394362010516, test loss: 0.2642784602437747\n",
      "epoch 14363: train loss: 0.10830206070784648, test loss: 0.2642790200387894\n",
      "epoch 14364: train loss: 0.10830017797096293, test loss: 0.2642795798767987\n",
      "epoch 14365: train loss: 0.10829829540942461, test loss: 0.26428013975779224\n",
      "epoch 14366: train loss: 0.1082964130232017, test loss: 0.26428069968175977\n",
      "epoch 14367: train loss: 0.10829453081226433, test loss: 0.2642812596486908\n",
      "epoch 14368: train loss: 0.10829264877658268, test loss: 0.26428181965857545\n",
      "epoch 14369: train loss: 0.10829076691612696, test loss: 0.264282379711403\n",
      "epoch 14370: train loss: 0.10828888523086733, test loss: 0.2642829398071635\n",
      "epoch 14371: train loss: 0.10828700372077395, test loss: 0.26428349994584643\n",
      "epoch 14372: train loss: 0.10828512238581706, test loss: 0.2642840601274417\n",
      "epoch 14373: train loss: 0.10828324122596683, test loss: 0.2642846203519389\n",
      "epoch 14374: train loss: 0.10828136024119349, test loss: 0.2642851806193278\n",
      "epoch 14375: train loss: 0.10827947943146725, test loss: 0.2642857409295982\n",
      "epoch 14376: train loss: 0.10827759879675834, test loss: 0.2642863012827398\n",
      "epoch 14377: train loss: 0.10827571833703697, test loss: 0.26428686167874227\n",
      "epoch 14378: train loss: 0.10827383805227343, test loss: 0.2642874221175954\n",
      "epoch 14379: train loss: 0.10827195794243792, test loss: 0.26428798259928904\n",
      "epoch 14380: train loss: 0.10827007800750074, test loss: 0.26428854312381267\n",
      "epoch 14381: train loss: 0.10826819824743207, test loss: 0.2642891036911563\n",
      "epoch 14382: train loss: 0.10826631866220227, test loss: 0.2642896643013095\n",
      "epoch 14383: train loss: 0.10826443925178157, test loss: 0.2642902249542622\n",
      "epoch 14384: train loss: 0.10826256001614024, test loss: 0.264290785650004\n",
      "epoch 14385: train loss: 0.1082606809552486, test loss: 0.2642913463885247\n",
      "epoch 14386: train loss: 0.10825880206907693, test loss: 0.26429190716981404\n",
      "epoch 14387: train loss: 0.10825692335759556, test loss: 0.26429246799386186\n",
      "epoch 14388: train loss: 0.10825504482077473, test loss: 0.2642930288606579\n",
      "epoch 14389: train loss: 0.10825316645858482, test loss: 0.2642935897701919\n",
      "epoch 14390: train loss: 0.10825128827099618, test loss: 0.2642941507224536\n",
      "epoch 14391: train loss: 0.1082494102579791, test loss: 0.26429471171743285\n",
      "epoch 14392: train loss: 0.1082475324195039, test loss: 0.2642952727551194\n",
      "epoch 14393: train loss: 0.10824565475554097, test loss: 0.2642958338355029\n",
      "epoch 14394: train loss: 0.10824377726606065, test loss: 0.26429639495857343\n",
      "epoch 14395: train loss: 0.10824189995103328, test loss: 0.26429695612432047\n",
      "epoch 14396: train loss: 0.10824002281042926, test loss: 0.26429751733273393\n",
      "epoch 14397: train loss: 0.10823814584421892, test loss: 0.26429807858380355\n",
      "epoch 14398: train loss: 0.10823626905237273, test loss: 0.2642986398775193\n",
      "epoch 14399: train loss: 0.10823439243486101, test loss: 0.2642992012138708\n",
      "epoch 14400: train loss: 0.10823251599165415, test loss: 0.2642997625928478\n",
      "epoch 14401: train loss: 0.10823063972272258, test loss: 0.26430032401444026\n",
      "epoch 14402: train loss: 0.10822876362803673, test loss: 0.26430088547863784\n",
      "epoch 14403: train loss: 0.10822688770756697, test loss: 0.2643014469854304\n",
      "epoch 14404: train loss: 0.10822501196128377, test loss: 0.26430200853480784\n",
      "epoch 14405: train loss: 0.10822313638915752, test loss: 0.2643025701267598\n",
      "epoch 14406: train loss: 0.10822126099115872, test loss: 0.2643031317612763\n",
      "epoch 14407: train loss: 0.10821938576725774, test loss: 0.2643036934383469\n",
      "epoch 14408: train loss: 0.1082175107174251, test loss: 0.2643042551579616\n",
      "epoch 14409: train loss: 0.10821563584163124, test loss: 0.2643048169201101\n",
      "epoch 14410: train loss: 0.10821376113984661, test loss: 0.2643053787247824\n",
      "epoch 14411: train loss: 0.10821188661204167, test loss: 0.26430594057196816\n",
      "epoch 14412: train loss: 0.108210012258187, test loss: 0.2643065024616572\n",
      "epoch 14413: train loss: 0.10820813807825297, test loss: 0.26430706439383944\n",
      "epoch 14414: train loss: 0.10820626407221015, test loss: 0.26430762636850474\n",
      "epoch 14415: train loss: 0.10820439024002901, test loss: 0.2643081883856428\n",
      "epoch 14416: train loss: 0.10820251658168006, test loss: 0.2643087504452435\n",
      "epoch 14417: train loss: 0.10820064309713384, test loss: 0.26430931254729684\n",
      "epoch 14418: train loss: 0.10819876978636088, test loss: 0.2643098746917924\n",
      "epoch 14419: train loss: 0.10819689664933169, test loss: 0.26431043687872025\n",
      "epoch 14420: train loss: 0.1081950236860168, test loss: 0.26431099910807004\n",
      "epoch 14421: train loss: 0.10819315089638679, test loss: 0.2643115613798318\n",
      "epoch 14422: train loss: 0.10819127828041218, test loss: 0.26431212369399526\n",
      "epoch 14423: train loss: 0.10818940583806355, test loss: 0.2643126860505503\n",
      "epoch 14424: train loss: 0.10818753356931148, test loss: 0.2643132484494869\n",
      "epoch 14425: train loss: 0.10818566147412652, test loss: 0.26431381089079475\n",
      "epoch 14426: train loss: 0.10818378955247923, test loss: 0.26431437337446373\n",
      "epoch 14427: train loss: 0.10818191780434028, test loss: 0.2643149359004838\n",
      "epoch 14428: train loss: 0.10818004622968017, test loss: 0.2643154984688448\n",
      "epoch 14429: train loss: 0.10817817482846954, test loss: 0.26431606107953654\n",
      "epoch 14430: train loss: 0.10817630360067905, test loss: 0.264316623732549\n",
      "epoch 14431: train loss: 0.10817443254627927, test loss: 0.2643171864278719\n",
      "epoch 14432: train loss: 0.10817256166524082, test loss: 0.2643177491654952\n",
      "epoch 14433: train loss: 0.10817069095753432, test loss: 0.2643183119454089\n",
      "epoch 14434: train loss: 0.10816882042313043, test loss: 0.2643188747676027\n",
      "epoch 14435: train loss: 0.10816695006199982, test loss: 0.26431943763206645\n",
      "epoch 14436: train loss: 0.1081650798741131, test loss: 0.2643200005387903\n",
      "epoch 14437: train loss: 0.10816320985944096, test loss: 0.26432056348776384\n",
      "epoch 14438: train loss: 0.10816134001795401, test loss: 0.26432112647897715\n",
      "epoch 14439: train loss: 0.10815947034962299, test loss: 0.26432168951242013\n",
      "epoch 14440: train loss: 0.10815760085441856, test loss: 0.2643222525880826\n",
      "epoch 14441: train loss: 0.10815573153231138, test loss: 0.2643228157059545\n",
      "epoch 14442: train loss: 0.1081538623832722, test loss: 0.2643233788660256\n",
      "epoch 14443: train loss: 0.10815199340727165, test loss: 0.264323942068286\n",
      "epoch 14444: train loss: 0.1081501246042805, test loss: 0.2643245053127256\n",
      "epoch 14445: train loss: 0.10814825597426946, test loss: 0.26432506859933413\n",
      "epoch 14446: train loss: 0.1081463875172092, test loss: 0.26432563192810166\n",
      "epoch 14447: train loss: 0.10814451923307052, test loss: 0.2643261952990181\n",
      "epoch 14448: train loss: 0.10814265112182408, test loss: 0.26432675871207323\n",
      "epoch 14449: train loss: 0.10814078318344071, test loss: 0.2643273221672572\n",
      "epoch 14450: train loss: 0.1081389154178911, test loss: 0.26432788566455967\n",
      "epoch 14451: train loss: 0.10813704782514602, test loss: 0.26432844920397075\n",
      "epoch 14452: train loss: 0.10813518040517624, test loss: 0.2643290127854803\n",
      "epoch 14453: train loss: 0.10813331315795253, test loss: 0.2643295764090783\n",
      "epoch 14454: train loss: 0.10813144608344566, test loss: 0.26433014007475464\n",
      "epoch 14455: train loss: 0.10812957918162645, test loss: 0.26433070378249923\n",
      "epoch 14456: train loss: 0.10812771245246566, test loss: 0.264331267532302\n",
      "epoch 14457: train loss: 0.10812584589593408, test loss: 0.264331831324153\n",
      "epoch 14458: train loss: 0.10812397951200257, test loss: 0.26433239515804213\n",
      "epoch 14459: train loss: 0.10812211330064186, test loss: 0.2643329590339592\n",
      "epoch 14460: train loss: 0.10812024726182287, test loss: 0.2643335229518943\n",
      "epoch 14461: train loss: 0.10811838139551636, test loss: 0.26433408691183746\n",
      "epoch 14462: train loss: 0.10811651570169317, test loss: 0.2643346509137784\n",
      "epoch 14463: train loss: 0.10811465018032418, test loss: 0.2643352149577073\n",
      "epoch 14464: train loss: 0.10811278483138022, test loss: 0.26433577904361377\n",
      "epoch 14465: train loss: 0.1081109196548321, test loss: 0.26433634317148824\n",
      "epoch 14466: train loss: 0.10810905465065077, test loss: 0.26433690734132037\n",
      "epoch 14467: train loss: 0.10810718981880704, test loss: 0.26433747155310017\n",
      "epoch 14468: train loss: 0.10810532515927179, test loss: 0.26433803580681775\n",
      "epoch 14469: train loss: 0.10810346067201591, test loss: 0.2643386001024629\n",
      "epoch 14470: train loss: 0.1081015963570103, test loss: 0.2643391644400257\n",
      "epoch 14471: train loss: 0.10809973221422585, test loss: 0.264339728819496\n",
      "epoch 14472: train loss: 0.10809786824363349, test loss: 0.26434029324086394\n",
      "epoch 14473: train loss: 0.10809600444520408, test loss: 0.2643408577041194\n",
      "epoch 14474: train loss: 0.10809414081890857, test loss: 0.2643414222092525\n",
      "epoch 14475: train loss: 0.1080922773647179, test loss: 0.264341986756253\n",
      "epoch 14476: train loss: 0.10809041408260295, test loss: 0.26434255134511103\n",
      "epoch 14477: train loss: 0.10808855097253475, test loss: 0.26434311597581656\n",
      "epoch 14478: train loss: 0.10808668803448417, test loss: 0.26434368064835956\n",
      "epoch 14479: train loss: 0.10808482526842217, test loss: 0.26434424536273005\n",
      "epoch 14480: train loss: 0.10808296267431976, test loss: 0.264344810118918\n",
      "epoch 14481: train loss: 0.10808110025214784, test loss: 0.26434537491691346\n",
      "epoch 14482: train loss: 0.10807923800187744, test loss: 0.2643459397567064\n",
      "epoch 14483: train loss: 0.10807737592347952, test loss: 0.2643465046382868\n",
      "epoch 14484: train loss: 0.10807551401692503, test loss: 0.2643470695616448\n",
      "epoch 14485: train loss: 0.10807365228218506, test loss: 0.26434763452677024\n",
      "epoch 14486: train loss: 0.10807179071923052, test loss: 0.2643481995336532\n",
      "epoch 14487: train loss: 0.10806992932803247, test loss: 0.2643487645822837\n",
      "epoch 14488: train loss: 0.10806806810856191, test loss: 0.2643493296726517\n",
      "epoch 14489: train loss: 0.10806620706078984, test loss: 0.2643498948047473\n",
      "epoch 14490: train loss: 0.10806434618468735, test loss: 0.2643504599785606\n",
      "epoch 14491: train loss: 0.10806248548022541, test loss: 0.2643510251940814\n",
      "epoch 14492: train loss: 0.10806062494737514, test loss: 0.2643515904512999\n",
      "epoch 14493: train loss: 0.10805876458610748, test loss: 0.26435215575020604\n",
      "epoch 14494: train loss: 0.10805690439639362, test loss: 0.26435272109078994\n",
      "epoch 14495: train loss: 0.10805504437820453, test loss: 0.26435328647304157\n",
      "epoch 14496: train loss: 0.10805318453151129, test loss: 0.264353851896951\n",
      "epoch 14497: train loss: 0.10805132485628502, test loss: 0.2643544173625083\n",
      "epoch 14498: train loss: 0.10804946535249677, test loss: 0.2643549828697035\n",
      "epoch 14499: train loss: 0.10804760602011765, test loss: 0.26435554841852643\n",
      "epoch 14500: train loss: 0.10804574685911877, test loss: 0.26435611400896747\n",
      "epoch 14501: train loss: 0.10804388786947122, test loss: 0.2643566796410165\n",
      "epoch 14502: train loss: 0.10804202905114611, test loss: 0.2643572453146636\n",
      "epoch 14503: train loss: 0.10804017040411455, test loss: 0.2643578110298988\n",
      "epoch 14504: train loss: 0.10803831192834769, test loss: 0.2643583767867122\n",
      "epoch 14505: train loss: 0.10803645362381666, test loss: 0.26435894258509385\n",
      "epoch 14506: train loss: 0.1080345954904926, test loss: 0.26435950842503375\n",
      "epoch 14507: train loss: 0.10803273752834665, test loss: 0.2643600743065221\n",
      "epoch 14508: train loss: 0.10803087973734997, test loss: 0.2643606402295488\n",
      "epoch 14509: train loss: 0.1080290221174737, test loss: 0.2643612061941041\n",
      "epoch 14510: train loss: 0.10802716466868904, test loss: 0.26436177220017787\n",
      "epoch 14511: train loss: 0.10802530739096715, test loss: 0.26436233824776034\n",
      "epoch 14512: train loss: 0.10802345028427922, test loss: 0.2643629043368415\n",
      "epoch 14513: train loss: 0.10802159334859644, test loss: 0.2643634704674116\n",
      "epoch 14514: train loss: 0.10801973658388998, test loss: 0.2643640366394605\n",
      "epoch 14515: train loss: 0.10801787999013107, test loss: 0.26436460285297836\n",
      "epoch 14516: train loss: 0.10801602356729091, test loss: 0.26436516910795527\n",
      "epoch 14517: train loss: 0.10801416731534072, test loss: 0.2643657354043814\n",
      "epoch 14518: train loss: 0.10801231123425174, test loss: 0.2643663017422468\n",
      "epoch 14519: train loss: 0.10801045532399517, test loss: 0.2643668681215415\n",
      "epoch 14520: train loss: 0.10800859958454226, test loss: 0.2643674345422556\n",
      "epoch 14521: train loss: 0.10800674401586424, test loss: 0.2643680010043792\n",
      "epoch 14522: train loss: 0.10800488861793239, test loss: 0.26436856750790255\n",
      "epoch 14523: train loss: 0.10800303339071794, test loss: 0.26436913405281565\n",
      "epoch 14524: train loss: 0.10800117833419218, test loss: 0.2643697006391085\n",
      "epoch 14525: train loss: 0.10799932344832637, test loss: 0.2643702672667714\n",
      "epoch 14526: train loss: 0.10799746873309175, test loss: 0.2643708339357943\n",
      "epoch 14527: train loss: 0.10799561418845968, test loss: 0.26437140064616743\n",
      "epoch 14528: train loss: 0.10799375981440139, test loss: 0.2643719673978808\n",
      "epoch 14529: train loss: 0.10799190561088821, test loss: 0.26437253419092477\n",
      "epoch 14530: train loss: 0.10799005157789143, test loss: 0.2643731010252892\n",
      "epoch 14531: train loss: 0.10798819771538237, test loss: 0.2643736679009642\n",
      "epoch 14532: train loss: 0.10798634402333239, test loss: 0.26437423481794003\n",
      "epoch 14533: train loss: 0.10798449050171273, test loss: 0.26437480177620687\n",
      "epoch 14534: train loss: 0.10798263715049479, test loss: 0.26437536877575474\n",
      "epoch 14535: train loss: 0.10798078396964989, test loss: 0.26437593581657376\n",
      "epoch 14536: train loss: 0.10797893095914937, test loss: 0.26437650289865416\n",
      "epoch 14537: train loss: 0.10797707811896458, test loss: 0.264377070021986\n",
      "epoch 14538: train loss: 0.10797522544906689, test loss: 0.2643776371865594\n",
      "epoch 14539: train loss: 0.1079733729494277, test loss: 0.2643782043923646\n",
      "epoch 14540: train loss: 0.10797152062001832, test loss: 0.2643787716393917\n",
      "epoch 14541: train loss: 0.10796966846081019, test loss: 0.2643793389276309\n",
      "epoch 14542: train loss: 0.10796781647177464, test loss: 0.2643799062570722\n",
      "epoch 14543: train loss: 0.10796596465288308, test loss: 0.26438047362770595\n",
      "epoch 14544: train loss: 0.10796411300410697, test loss: 0.26438104103952215\n",
      "epoch 14545: train loss: 0.10796226152541767, test loss: 0.264381608492511\n",
      "epoch 14546: train loss: 0.10796041021678658, test loss: 0.26438217598666275\n",
      "epoch 14547: train loss: 0.10795855907818515, test loss: 0.26438274352196744\n",
      "epoch 14548: train loss: 0.10795670810958478, test loss: 0.26438331109841534\n",
      "epoch 14549: train loss: 0.10795485731095696, test loss: 0.26438387871599645\n",
      "epoch 14550: train loss: 0.10795300668227306, test loss: 0.26438444637470115\n",
      "epoch 14551: train loss: 0.1079511562235046, test loss: 0.26438501407451953\n",
      "epoch 14552: train loss: 0.10794930593462297, test loss: 0.26438558181544175\n",
      "epoch 14553: train loss: 0.10794745581559968, test loss: 0.26438614959745793\n",
      "epoch 14554: train loss: 0.10794560586640621, test loss: 0.26438671742055836\n",
      "epoch 14555: train loss: 0.10794375608701401, test loss: 0.2643872852847332\n",
      "epoch 14556: train loss: 0.10794190647739456, test loss: 0.2643878531899726\n",
      "epoch 14557: train loss: 0.10794005703751934, test loss: 0.26438842113626676\n",
      "epoch 14558: train loss: 0.10793820776735986, test loss: 0.26438898912360587\n",
      "epoch 14559: train loss: 0.10793635866688764, test loss: 0.26438955715198015\n",
      "epoch 14560: train loss: 0.10793450973607417, test loss: 0.2643901252213797\n",
      "epoch 14561: train loss: 0.10793266097489095, test loss: 0.2643906933317948\n",
      "epoch 14562: train loss: 0.10793081238330955, test loss: 0.2643912614832157\n",
      "epoch 14563: train loss: 0.10792896396130146, test loss: 0.26439182967563246\n",
      "epoch 14564: train loss: 0.10792711570883828, test loss: 0.2643923979090355\n",
      "epoch 14565: train loss: 0.10792526762589146, test loss: 0.26439296618341485\n",
      "epoch 14566: train loss: 0.10792341971243262, test loss: 0.2643935344987607\n",
      "epoch 14567: train loss: 0.1079215719684333, test loss: 0.2643941028550633\n",
      "epoch 14568: train loss: 0.10791972439386503, test loss: 0.264394671252313\n",
      "epoch 14569: train loss: 0.10791787698869947, test loss: 0.26439523969049983\n",
      "epoch 14570: train loss: 0.1079160297529081, test loss: 0.26439580816961405\n",
      "epoch 14571: train loss: 0.10791418268646254, test loss: 0.264396376689646\n",
      "epoch 14572: train loss: 0.10791233578933442, test loss: 0.26439694525058577\n",
      "epoch 14573: train loss: 0.1079104890614953, test loss: 0.26439751385242366\n",
      "epoch 14574: train loss: 0.10790864250291676, test loss: 0.2643980824951499\n",
      "epoch 14575: train loss: 0.1079067961135705, test loss: 0.26439865117875466\n",
      "epoch 14576: train loss: 0.10790494989342803, test loss: 0.2643992199032283\n",
      "epoch 14577: train loss: 0.10790310384246105, test loss: 0.2643997886685609\n",
      "epoch 14578: train loss: 0.10790125796064115, test loss: 0.2644003574747428\n",
      "epoch 14579: train loss: 0.10789941224794002, test loss: 0.2644009263217641\n",
      "epoch 14580: train loss: 0.10789756670432926, test loss: 0.2644014952096153\n",
      "epoch 14581: train loss: 0.10789572132978051, test loss: 0.2644020641382865\n",
      "epoch 14582: train loss: 0.1078938761242655, test loss: 0.26440263310776785\n",
      "epoch 14583: train loss: 0.10789203108775584, test loss: 0.26440320211804974\n",
      "epoch 14584: train loss: 0.1078901862202232, test loss: 0.26440377116912245\n",
      "epoch 14585: train loss: 0.10788834152163929, test loss: 0.26440434026097615\n",
      "epoch 14586: train loss: 0.10788649699197576, test loss: 0.264404909393601\n",
      "epoch 14587: train loss: 0.10788465263120432, test loss: 0.2644054785669875\n",
      "epoch 14588: train loss: 0.1078828084392967, test loss: 0.2644060477811257\n",
      "epoch 14589: train loss: 0.10788096441622458, test loss: 0.2644066170360061\n",
      "epoch 14590: train loss: 0.10787912056195964, test loss: 0.2644071863316188\n",
      "epoch 14591: train loss: 0.10787727687647365, test loss: 0.26440775566795405\n",
      "epoch 14592: train loss: 0.10787543335973833, test loss: 0.26440832504500217\n",
      "epoch 14593: train loss: 0.10787359001172536, test loss: 0.26440889446275334\n",
      "epoch 14594: train loss: 0.10787174683240658, test loss: 0.26440946392119813\n",
      "epoch 14595: train loss: 0.10786990382175363, test loss: 0.26441003342032654\n",
      "epoch 14596: train loss: 0.1078680609797383, test loss: 0.2644106029601289\n",
      "epoch 14597: train loss: 0.1078662183063324, test loss: 0.26441117254059554\n",
      "epoch 14598: train loss: 0.10786437580150764, test loss: 0.26441174216171676\n",
      "epoch 14599: train loss: 0.10786253346523582, test loss: 0.26441231182348285\n",
      "epoch 14600: train loss: 0.10786069129748871, test loss: 0.26441288152588405\n",
      "epoch 14601: train loss: 0.10785884929823808, test loss: 0.26441345126891064\n",
      "epoch 14602: train loss: 0.10785700746745576, test loss: 0.264414021052553\n",
      "epoch 14603: train loss: 0.10785516580511352, test loss: 0.2644145908768014\n",
      "epoch 14604: train loss: 0.1078533243111832, test loss: 0.2644151607416462\n",
      "epoch 14605: train loss: 0.10785148298563658, test loss: 0.26441573064707763\n",
      "epoch 14606: train loss: 0.1078496418284455, test loss: 0.26441630059308596\n",
      "epoch 14607: train loss: 0.10784780083958179, test loss: 0.2644168705796615\n",
      "epoch 14608: train loss: 0.10784596001901725, test loss: 0.2644174406067946\n",
      "epoch 14609: train loss: 0.10784411936672374, test loss: 0.26441801067447573\n",
      "epoch 14610: train loss: 0.10784227888267313, test loss: 0.2644185807826949\n",
      "epoch 14611: train loss: 0.10784043856683724, test loss: 0.2644191509314426\n",
      "epoch 14612: train loss: 0.10783859841918796, test loss: 0.26441972112070916\n",
      "epoch 14613: train loss: 0.10783675843969713, test loss: 0.26442029135048484\n",
      "epoch 14614: train loss: 0.10783491862833663, test loss: 0.2644208616207601\n",
      "epoch 14615: train loss: 0.10783307898507836, test loss: 0.2644214319315251\n",
      "epoch 14616: train loss: 0.1078312395098942, test loss: 0.2644220022827703\n",
      "epoch 14617: train loss: 0.10782940020275604, test loss: 0.26442257267448593\n",
      "epoch 14618: train loss: 0.10782756106363577, test loss: 0.2644231431066623\n",
      "epoch 14619: train loss: 0.10782572209250532, test loss: 0.26442371357928995\n",
      "epoch 14620: train loss: 0.10782388328933656, test loss: 0.264424284092359\n",
      "epoch 14621: train loss: 0.10782204465410146, test loss: 0.2644248546458599\n",
      "epoch 14622: train loss: 0.10782020618677195, test loss: 0.264425425239783\n",
      "epoch 14623: train loss: 0.1078183678873199, test loss: 0.2644259958741187\n",
      "epoch 14624: train loss: 0.10781652975571733, test loss: 0.26442656654885716\n",
      "epoch 14625: train loss: 0.10781469179193612, test loss: 0.26442713726398887\n",
      "epoch 14626: train loss: 0.10781285399594825, test loss: 0.2644277080195042\n",
      "epoch 14627: train loss: 0.10781101636772572, test loss: 0.2644282788153935\n",
      "epoch 14628: train loss: 0.10780917890724044, test loss: 0.264428849651647\n",
      "epoch 14629: train loss: 0.1078073416144644, test loss: 0.26442942052825524\n",
      "epoch 14630: train loss: 0.10780550448936957, test loss: 0.2644299914452084\n",
      "epoch 14631: train loss: 0.107803667531928, test loss: 0.264430562402497\n",
      "epoch 14632: train loss: 0.1078018307421116, test loss: 0.2644311334001114\n",
      "epoch 14633: train loss: 0.10779999411989241, test loss: 0.26443170443804187\n",
      "epoch 14634: train loss: 0.10779815766524244, test loss: 0.2644322755162788\n",
      "epoch 14635: train loss: 0.10779632137813369, test loss: 0.2644328466348128\n",
      "epoch 14636: train loss: 0.10779448525853817, test loss: 0.2644334177936339\n",
      "epoch 14637: train loss: 0.10779264930642794, test loss: 0.2644339889927326\n",
      "epoch 14638: train loss: 0.10779081352177503, test loss: 0.2644345602320995\n",
      "epoch 14639: train loss: 0.10778897790455144, test loss: 0.2644351315117246\n",
      "epoch 14640: train loss: 0.10778714245472926, test loss: 0.26443570283159873\n",
      "epoch 14641: train loss: 0.10778530717228055, test loss: 0.2644362741917118\n",
      "epoch 14642: train loss: 0.10778347205717732, test loss: 0.2644368455920546\n",
      "epoch 14643: train loss: 0.10778163710939166, test loss: 0.26443741703261736\n",
      "epoch 14644: train loss: 0.10777980232889565, test loss: 0.26443798851339045\n",
      "epoch 14645: train loss: 0.10777796771566141, test loss: 0.2644385600343644\n",
      "epoch 14646: train loss: 0.10777613326966094, test loss: 0.2644391315955294\n",
      "epoch 14647: train loss: 0.10777429899086638, test loss: 0.264439703196876\n",
      "epoch 14648: train loss: 0.10777246487924984, test loss: 0.26444027483839466\n",
      "epoch 14649: train loss: 0.10777063093478338, test loss: 0.26444084652007566\n",
      "epoch 14650: train loss: 0.10776879715743917, test loss: 0.26444141824190953\n",
      "epoch 14651: train loss: 0.10776696354718933, test loss: 0.2644419900038865\n",
      "epoch 14652: train loss: 0.10776513010400592, test loss: 0.2644425618059972\n",
      "epoch 14653: train loss: 0.10776329682786112, test loss: 0.26444313364823196\n",
      "epoch 14654: train loss: 0.1077614637187271, test loss: 0.26444370553058116\n",
      "epoch 14655: train loss: 0.10775963077657594, test loss: 0.2644442774530352\n",
      "epoch 14656: train loss: 0.10775779800137983, test loss: 0.2644448494155847\n",
      "epoch 14657: train loss: 0.1077559653931109, test loss: 0.26444542141821986\n",
      "epoch 14658: train loss: 0.10775413295174135, test loss: 0.2644459934609313\n",
      "epoch 14659: train loss: 0.10775230067724334, test loss: 0.2644465655437093\n",
      "epoch 14660: train loss: 0.10775046856958903, test loss: 0.2644471376665443\n",
      "epoch 14661: train loss: 0.10774863662875064, test loss: 0.26444770982942684\n",
      "epoch 14662: train loss: 0.10774680485470035, test loss: 0.2644482820323473\n",
      "epoch 14663: train loss: 0.10774497324741038, test loss: 0.2644488542752962\n",
      "epoch 14664: train loss: 0.10774314180685286, test loss: 0.2644494265582638\n",
      "epoch 14665: train loss: 0.10774131053300007, test loss: 0.2644499988812407\n",
      "epoch 14666: train loss: 0.1077394794258242, test loss: 0.26445057124421745\n",
      "epoch 14667: train loss: 0.10773764848529747, test loss: 0.2644511436471843\n",
      "epoch 14668: train loss: 0.10773581771139215, test loss: 0.26445171609013174\n",
      "epoch 14669: train loss: 0.10773398710408046, test loss: 0.26445228857305025\n",
      "epoch 14670: train loss: 0.10773215666333462, test loss: 0.26445286109593025\n",
      "epoch 14671: train loss: 0.10773032638912688, test loss: 0.2644534336587624\n",
      "epoch 14672: train loss: 0.10772849628142957, test loss: 0.26445400626153687\n",
      "epoch 14673: train loss: 0.10772666634021485, test loss: 0.26445457890424434\n",
      "epoch 14674: train loss: 0.10772483656545505, test loss: 0.2644551515868752\n",
      "epoch 14675: train loss: 0.10772300695712246, test loss: 0.26445572430942\n",
      "epoch 14676: train loss: 0.10772117751518932, test loss: 0.264456297071869\n",
      "epoch 14677: train loss: 0.10771934823962795, test loss: 0.2644568698742129\n",
      "epoch 14678: train loss: 0.10771751913041065, test loss: 0.2644574427164421\n",
      "epoch 14679: train loss: 0.10771569018750972, test loss: 0.2644580155985471\n",
      "epoch 14680: train loss: 0.10771386141089744, test loss: 0.2644585885205183\n",
      "epoch 14681: train loss: 0.10771203280054616, test loss: 0.2644591614823463\n",
      "epoch 14682: train loss: 0.10771020435642821, test loss: 0.2644597344840215\n",
      "epoch 14683: train loss: 0.10770837607851586, test loss: 0.2644603075255344\n",
      "epoch 14684: train loss: 0.10770654796678154, test loss: 0.26446088060687556\n",
      "epoch 14685: train loss: 0.10770472002119749, test loss: 0.2644614537280354\n",
      "epoch 14686: train loss: 0.10770289224173617, test loss: 0.26446202688900444\n",
      "epoch 14687: train loss: 0.10770106462836983, test loss: 0.2644626000897732\n",
      "epoch 14688: train loss: 0.10769923718107088, test loss: 0.2644631733303322\n",
      "epoch 14689: train loss: 0.10769740989981173, test loss: 0.2644637466106719\n",
      "epoch 14690: train loss: 0.10769558278456469, test loss: 0.26446431993078273\n",
      "epoch 14691: train loss: 0.10769375583530216, test loss: 0.26446489329065537\n",
      "epoch 14692: train loss: 0.10769192905199652, test loss: 0.26446546669028015\n",
      "epoch 14693: train loss: 0.10769010243462022, test loss: 0.26446604012964786\n",
      "epoch 14694: train loss: 0.10768827598314559, test loss: 0.2644666136087487\n",
      "epoch 14695: train loss: 0.10768644969754508, test loss: 0.2644671871275734\n",
      "epoch 14696: train loss: 0.10768462357779111, test loss: 0.2644677606861123\n",
      "epoch 14697: train loss: 0.10768279762385605, test loss: 0.264468334284356\n",
      "epoch 14698: train loss: 0.10768097183571239, test loss: 0.26446890792229516\n",
      "epoch 14699: train loss: 0.10767914621333251, test loss: 0.26446948159992023\n",
      "epoch 14700: train loss: 0.1076773207566889, test loss: 0.26447005531722156\n",
      "epoch 14701: train loss: 0.10767549546575399, test loss: 0.26447062907418994\n",
      "epoch 14702: train loss: 0.1076736703405002, test loss: 0.26447120287081577\n",
      "epoch 14703: train loss: 0.10767184538090004, test loss: 0.2644717767070896\n",
      "epoch 14704: train loss: 0.10767002058692593, test loss: 0.264472350583002\n",
      "epoch 14705: train loss: 0.10766819595855039, test loss: 0.2644729244985434\n",
      "epoch 14706: train loss: 0.10766637149574584, test loss: 0.26447349845370455\n",
      "epoch 14707: train loss: 0.10766454719848487, test loss: 0.26447407244847587\n",
      "epoch 14708: train loss: 0.10766272306673985, test loss: 0.26447464648284785\n",
      "epoch 14709: train loss: 0.10766089910048336, test loss: 0.2644752205568111\n",
      "epoch 14710: train loss: 0.10765907529968785, test loss: 0.2644757946703563\n",
      "epoch 14711: train loss: 0.10765725166432588, test loss: 0.2644763688234738\n",
      "epoch 14712: train loss: 0.10765542819436995, test loss: 0.26447694301615426\n",
      "epoch 14713: train loss: 0.10765360488979259, test loss: 0.26447751724838825\n",
      "epoch 14714: train loss: 0.10765178175056633, test loss: 0.26447809152016627\n",
      "epoch 14715: train loss: 0.10764995877666368, test loss: 0.26447866583147894\n",
      "epoch 14716: train loss: 0.10764813596805724, test loss: 0.2644792401823167\n",
      "epoch 14717: train loss: 0.10764631332471954, test loss: 0.26447981457267045\n",
      "epoch 14718: train loss: 0.10764449084662309, test loss: 0.2644803890025304\n",
      "epoch 14719: train loss: 0.1076426685337405, test loss: 0.2644809634718872\n",
      "epoch 14720: train loss: 0.10764084638604432, test loss: 0.26448153798073165\n",
      "epoch 14721: train loss: 0.10763902440350717, test loss: 0.26448211252905407\n",
      "epoch 14722: train loss: 0.10763720258610159, test loss: 0.2644826871168452\n",
      "epoch 14723: train loss: 0.10763538093380016, test loss: 0.26448326174409553\n",
      "epoch 14724: train loss: 0.10763355944657549, test loss: 0.26448383641079565\n",
      "epoch 14725: train loss: 0.10763173812440023, test loss: 0.2644844111169362\n",
      "epoch 14726: train loss: 0.10762991696724694, test loss: 0.2644849858625078\n",
      "epoch 14727: train loss: 0.1076280959750882, test loss: 0.2644855606475009\n",
      "epoch 14728: train loss: 0.1076262751478967, test loss: 0.2644861354719062\n",
      "epoch 14729: train loss: 0.10762445448564506, test loss: 0.2644867103357143\n",
      "epoch 14730: train loss: 0.10762263398830588, test loss: 0.2644872852389158\n",
      "epoch 14731: train loss: 0.10762081365585181, test loss: 0.2644878601815014\n",
      "epoch 14732: train loss: 0.10761899348825554, test loss: 0.2644884351634613\n",
      "epoch 14733: train loss: 0.10761717348548969, test loss: 0.2644890101847866\n",
      "epoch 14734: train loss: 0.1076153536475269, test loss: 0.2644895852454677\n",
      "epoch 14735: train loss: 0.10761353397433986, test loss: 0.2644901603454951\n",
      "epoch 14736: train loss: 0.10761171446590126, test loss: 0.26449073548485963\n",
      "epoch 14737: train loss: 0.10760989512218375, test loss: 0.2644913106635518\n",
      "epoch 14738: train loss: 0.10760807594316002, test loss: 0.2644918858815622\n",
      "epoch 14739: train loss: 0.10760625692880278, test loss: 0.2644924611388814\n",
      "epoch 14740: train loss: 0.10760443807908474, test loss: 0.26449303643550026\n",
      "epoch 14741: train loss: 0.10760261939397857, test loss: 0.26449361177140923\n",
      "epoch 14742: train loss: 0.10760080087345698, test loss: 0.26449418714659884\n",
      "epoch 14743: train loss: 0.10759898251749273, test loss: 0.26449476256105986\n",
      "epoch 14744: train loss: 0.10759716432605852, test loss: 0.264495338014783\n",
      "epoch 14745: train loss: 0.10759534629912708, test loss: 0.2644959135077587\n",
      "epoch 14746: train loss: 0.10759352843667112, test loss: 0.26449648903997774\n",
      "epoch 14747: train loss: 0.10759171073866348, test loss: 0.26449706461143074\n",
      "epoch 14748: train loss: 0.10758989320507678, test loss: 0.2644976402221082\n",
      "epoch 14749: train loss: 0.10758807583588387, test loss: 0.26449821587200095\n",
      "epoch 14750: train loss: 0.10758625863105749, test loss: 0.2644987915610995\n",
      "epoch 14751: train loss: 0.1075844415905704, test loss: 0.2644993672893946\n",
      "epoch 14752: train loss: 0.10758262471439536, test loss: 0.26449994305687685\n",
      "epoch 14753: train loss: 0.10758080800250522, test loss: 0.2645005188635369\n",
      "epoch 14754: train loss: 0.10757899145487265, test loss: 0.26450109470936545\n",
      "epoch 14755: train loss: 0.1075771750714706, test loss: 0.26450167059435314\n",
      "epoch 14756: train loss: 0.10757535885227174, test loss: 0.2645022465184905\n",
      "epoch 14757: train loss: 0.10757354279724891, test loss: 0.26450282248176843\n",
      "epoch 14758: train loss: 0.10757172690637498, test loss: 0.2645033984841775\n",
      "epoch 14759: train loss: 0.10756991117962272, test loss: 0.2645039745257082\n",
      "epoch 14760: train loss: 0.10756809561696495, test loss: 0.2645045506063514\n",
      "epoch 14761: train loss: 0.10756628021837455, test loss: 0.2645051267260978\n",
      "epoch 14762: train loss: 0.10756446498382434, test loss: 0.2645057028849378\n",
      "epoch 14763: train loss: 0.10756264991328715, test loss: 0.2645062790828624\n",
      "epoch 14764: train loss: 0.10756083500673583, test loss: 0.2645068553198621\n",
      "epoch 14765: train loss: 0.10755902026414327, test loss: 0.2645074315959277\n",
      "epoch 14766: train loss: 0.10755720568548228, test loss: 0.2645080079110497\n",
      "epoch 14767: train loss: 0.1075553912707258, test loss: 0.2645085842652189\n",
      "epoch 14768: train loss: 0.10755357701984669, test loss: 0.26450916065842606\n",
      "epoch 14769: train loss: 0.1075517629328178, test loss: 0.2645097370906618\n",
      "epoch 14770: train loss: 0.10754994900961204, test loss: 0.2645103135619166\n",
      "epoch 14771: train loss: 0.10754813525020232, test loss: 0.2645108900721816\n",
      "epoch 14772: train loss: 0.10754632165456154, test loss: 0.26451146662144714\n",
      "epoch 14773: train loss: 0.10754450822266261, test loss: 0.26451204320970406\n",
      "epoch 14774: train loss: 0.1075426949544784, test loss: 0.26451261983694296\n",
      "epoch 14775: train loss: 0.10754088184998192, test loss: 0.2645131965031546\n",
      "epoch 14776: train loss: 0.10753906890914604, test loss: 0.2645137732083298\n",
      "epoch 14777: train loss: 0.10753725613194369, test loss: 0.2645143499524591\n",
      "epoch 14778: train loss: 0.10753544351834787, test loss: 0.2645149267355333\n",
      "epoch 14779: train loss: 0.10753363106833143, test loss: 0.26451550355754305\n",
      "epoch 14780: train loss: 0.10753181878186742, test loss: 0.2645160804184792\n",
      "epoch 14781: train loss: 0.10753000665892877, test loss: 0.2645166573183323\n",
      "epoch 14782: train loss: 0.10752819469948843, test loss: 0.2645172342570931\n",
      "epoch 14783: train loss: 0.10752638290351937, test loss: 0.2645178112347524\n",
      "epoch 14784: train loss: 0.10752457127099461, test loss: 0.2645183882513009\n",
      "epoch 14785: train loss: 0.1075227598018871, test loss: 0.2645189653067293\n",
      "epoch 14786: train loss: 0.10752094849616983, test loss: 0.26451954240102826\n",
      "epoch 14787: train loss: 0.10751913735381584, test loss: 0.26452011953418875\n",
      "epoch 14788: train loss: 0.10751732637479808, test loss: 0.2645206967062013\n",
      "epoch 14789: train loss: 0.10751551555908961, test loss: 0.26452127391705665\n",
      "epoch 14790: train loss: 0.10751370490666341, test loss: 0.26452185116674554\n",
      "epoch 14791: train loss: 0.10751189441749254, test loss: 0.2645224284552588\n",
      "epoch 14792: train loss: 0.10751008409155002, test loss: 0.26452300578258714\n",
      "epoch 14793: train loss: 0.10750827392880886, test loss: 0.26452358314872126\n",
      "epoch 14794: train loss: 0.10750646392924214, test loss: 0.26452416055365185\n",
      "epoch 14795: train loss: 0.10750465409282288, test loss: 0.26452473799736986\n",
      "epoch 14796: train loss: 0.10750284441952412, test loss: 0.2645253154798659\n",
      "epoch 14797: train loss: 0.10750103490931899, test loss: 0.2645258930011306\n",
      "epoch 14798: train loss: 0.10749922556218051, test loss: 0.2645264705611551\n",
      "epoch 14799: train loss: 0.10749741637808174, test loss: 0.2645270481599297\n",
      "epoch 14800: train loss: 0.10749560735699584, test loss: 0.2645276257974455\n",
      "epoch 14801: train loss: 0.10749379849889579, test loss: 0.26452820347369316\n",
      "epoch 14802: train loss: 0.10749198980375475, test loss: 0.26452878118866335\n",
      "epoch 14803: train loss: 0.10749018127154583, test loss: 0.26452935894234697\n",
      "epoch 14804: train loss: 0.10748837290224207, test loss: 0.2645299367347347\n",
      "epoch 14805: train loss: 0.10748656469581666, test loss: 0.26453051456581733\n",
      "epoch 14806: train loss: 0.10748475665224268, test loss: 0.26453109243558565\n",
      "epoch 14807: train loss: 0.10748294877149325, test loss: 0.26453167034403047\n",
      "epoch 14808: train loss: 0.1074811410535415, test loss: 0.26453224829114247\n",
      "epoch 14809: train loss: 0.1074793334983606, test loss: 0.26453282627691255\n",
      "epoch 14810: train loss: 0.10747752610592368, test loss: 0.26453340430133143\n",
      "epoch 14811: train loss: 0.10747571887620389, test loss: 0.2645339823643899\n",
      "epoch 14812: train loss: 0.10747391180917436, test loss: 0.26453456046607865\n",
      "epoch 14813: train loss: 0.10747210490480827, test loss: 0.2645351386063887\n",
      "epoch 14814: train loss: 0.10747029816307878, test loss: 0.2645357167853107\n",
      "epoch 14815: train loss: 0.1074684915839591, test loss: 0.2645362950028354\n",
      "epoch 14816: train loss: 0.10746668516742237, test loss: 0.26453687325895375\n",
      "epoch 14817: train loss: 0.10746487891344185, test loss: 0.2645374515536563\n",
      "epoch 14818: train loss: 0.10746307282199062, test loss: 0.2645380298869341\n",
      "epoch 14819: train loss: 0.10746126689304197, test loss: 0.26453860825877784\n",
      "epoch 14820: train loss: 0.1074594611265691, test loss: 0.2645391866691783\n",
      "epoch 14821: train loss: 0.10745765552254519, test loss: 0.2645397651181265\n",
      "epoch 14822: train loss: 0.10745585008094345, test loss: 0.26454034360561296\n",
      "epoch 14823: train loss: 0.10745404480173715, test loss: 0.2645409221316285\n",
      "epoch 14824: train loss: 0.10745223968489949, test loss: 0.26454150069616417\n",
      "epoch 14825: train loss: 0.10745043473040372, test loss: 0.2645420792992106\n",
      "epoch 14826: train loss: 0.10744862993822309, test loss: 0.2645426579407588\n",
      "epoch 14827: train loss: 0.10744682530833086, test loss: 0.26454323662079937\n",
      "epoch 14828: train loss: 0.10744502084070025, test loss: 0.2645438153393232\n",
      "epoch 14829: train loss: 0.10744321653530456, test loss: 0.26454439409632124\n",
      "epoch 14830: train loss: 0.10744141239211703, test loss: 0.2645449728917842\n",
      "epoch 14831: train loss: 0.10743960841111097, test loss: 0.2645455517257029\n",
      "epoch 14832: train loss: 0.10743780459225963, test loss: 0.2645461305980682\n",
      "epoch 14833: train loss: 0.10743600093553629, test loss: 0.26454670950887094\n",
      "epoch 14834: train loss: 0.1074341974409143, test loss: 0.264547288458102\n",
      "epoch 14835: train loss: 0.1074323941083669, test loss: 0.2645478674457522\n",
      "epoch 14836: train loss: 0.10743059093786744, test loss: 0.26454844647181225\n",
      "epoch 14837: train loss: 0.1074287879293892, test loss: 0.26454902553627313\n",
      "epoch 14838: train loss: 0.10742698508290553, test loss: 0.26454960463912575\n",
      "epoch 14839: train loss: 0.10742518239838976, test loss: 0.2645501837803609\n",
      "epoch 14840: train loss: 0.10742337987581518, test loss: 0.26455076295996915\n",
      "epoch 14841: train loss: 0.10742157751515516, test loss: 0.2645513421779419\n",
      "epoch 14842: train loss: 0.10741977531638301, test loss: 0.26455192143426953\n",
      "epoch 14843: train loss: 0.10741797327947214, test loss: 0.26455250072894315\n",
      "epoch 14844: train loss: 0.10741617140439587, test loss: 0.26455308006195355\n",
      "epoch 14845: train loss: 0.10741436969112757, test loss: 0.26455365943329157\n",
      "epoch 14846: train loss: 0.10741256813964059, test loss: 0.26455423884294804\n",
      "epoch 14847: train loss: 0.10741076674990832, test loss: 0.264554818290914\n",
      "epoch 14848: train loss: 0.10740896552190417, test loss: 0.26455539777718007\n",
      "epoch 14849: train loss: 0.1074071644556015, test loss: 0.26455597730173736\n",
      "epoch 14850: train loss: 0.10740536355097369, test loss: 0.2645565568645765\n",
      "epoch 14851: train loss: 0.10740356280799417, test loss: 0.2645571364656886\n",
      "epoch 14852: train loss: 0.10740176222663632, test loss: 0.2645577161050644\n",
      "epoch 14853: train loss: 0.10739996180687356, test loss: 0.2645582957826948\n",
      "epoch 14854: train loss: 0.10739816154867933, test loss: 0.26455887549857077\n",
      "epoch 14855: train loss: 0.10739636145202704, test loss: 0.26455945525268304\n",
      "epoch 14856: train loss: 0.10739456151689013, test loss: 0.2645600350450227\n",
      "epoch 14857: train loss: 0.10739276174324201, test loss: 0.26456061487558036\n",
      "epoch 14858: train loss: 0.10739096213105614, test loss: 0.2645611947443472\n",
      "epoch 14859: train loss: 0.107389162680306, test loss: 0.264561774651314\n",
      "epoch 14860: train loss: 0.10738736339096501, test loss: 0.2645623545964715\n",
      "epoch 14861: train loss: 0.10738556426300665, test loss: 0.2645629345798108\n",
      "epoch 14862: train loss: 0.10738376529640437, test loss: 0.26456351460132277\n",
      "epoch 14863: train loss: 0.10738196649113167, test loss: 0.26456409466099823\n",
      "epoch 14864: train loss: 0.10738016784716203, test loss: 0.26456467475882817\n",
      "epoch 14865: train loss: 0.1073783693644689, test loss: 0.2645652548948034\n",
      "epoch 14866: train loss: 0.10737657104302581, test loss: 0.2645658350689149\n",
      "epoch 14867: train loss: 0.10737477288280622, test loss: 0.2645664152811536\n",
      "epoch 14868: train loss: 0.10737297488378372, test loss: 0.2645669955315105\n",
      "epoch 14869: train loss: 0.10737117704593173, test loss: 0.26456757581997625\n",
      "epoch 14870: train loss: 0.1073693793692238, test loss: 0.26456815614654194\n",
      "epoch 14871: train loss: 0.10736758185363346, test loss: 0.2645687365111985\n",
      "epoch 14872: train loss: 0.10736578449913425, test loss: 0.2645693169139368\n",
      "epoch 14873: train loss: 0.1073639873056997, test loss: 0.2645698973547478\n",
      "epoch 14874: train loss: 0.10736219027330333, test loss: 0.26457047783362236\n",
      "epoch 14875: train loss: 0.10736039340191873, test loss: 0.2645710583505515\n",
      "epoch 14876: train loss: 0.10735859669151943, test loss: 0.26457163890552615\n",
      "epoch 14877: train loss: 0.10735680014207898, test loss: 0.26457221949853715\n",
      "epoch 14878: train loss: 0.10735500375357095, test loss: 0.2645728001295755\n",
      "epoch 14879: train loss: 0.10735320752596894, test loss: 0.2645733807986322\n",
      "epoch 14880: train loss: 0.10735141145924652, test loss: 0.264573961505698\n",
      "epoch 14881: train loss: 0.1073496155533773, test loss: 0.2645745422507641\n",
      "epoch 14882: train loss: 0.10734781980833479, test loss: 0.26457512303382125\n",
      "epoch 14883: train loss: 0.10734602422409267, test loss: 0.26457570385486046\n",
      "epoch 14884: train loss: 0.1073442288006245, test loss: 0.2645762847138726\n",
      "epoch 14885: train loss: 0.10734243353790393, test loss: 0.26457686561084875\n",
      "epoch 14886: train loss: 0.10734063843590455, test loss: 0.2645774465457798\n",
      "epoch 14887: train loss: 0.10733884349459999, test loss: 0.2645780275186567\n",
      "epoch 14888: train loss: 0.10733704871396388, test loss: 0.2645786085294705\n",
      "epoch 14889: train loss: 0.10733525409396982, test loss: 0.26457918957821197\n",
      "epoch 14890: train loss: 0.10733345963459151, test loss: 0.26457977066487226\n",
      "epoch 14891: train loss: 0.10733166533580256, test loss: 0.2645803517894423\n",
      "epoch 14892: train loss: 0.10732987119757663, test loss: 0.264580932951913\n",
      "epoch 14893: train loss: 0.10732807721988738, test loss: 0.26458151415227527\n",
      "epoch 14894: train loss: 0.1073262834027085, test loss: 0.2645820953905202\n",
      "epoch 14895: train loss: 0.10732448974601363, test loss: 0.2645826766666387\n",
      "epoch 14896: train loss: 0.10732269624977647, test loss: 0.26458325798062177\n",
      "epoch 14897: train loss: 0.10732090291397067, test loss: 0.2645838393324604\n",
      "epoch 14898: train loss: 0.10731910973856996, test loss: 0.26458442072214555\n",
      "epoch 14899: train loss: 0.10731731672354802, test loss: 0.2645850021496682\n",
      "epoch 14900: train loss: 0.10731552386887856, test loss: 0.26458558361501944\n",
      "epoch 14901: train loss: 0.10731373117453527, test loss: 0.26458616511819\n",
      "epoch 14902: train loss: 0.10731193864049189, test loss: 0.2645867466591711\n",
      "epoch 14903: train loss: 0.10731014626672213, test loss: 0.2645873282379537\n",
      "epoch 14904: train loss: 0.10730835405319969, test loss: 0.2645879098545287\n",
      "epoch 14905: train loss: 0.10730656199989834, test loss: 0.2645884915088871\n",
      "epoch 14906: train loss: 0.1073047701067918, test loss: 0.26458907320102\n",
      "epoch 14907: train loss: 0.10730297837385386, test loss: 0.2645896549309184\n",
      "epoch 14908: train loss: 0.10730118680105821, test loss: 0.2645902366985732\n",
      "epoch 14909: train loss: 0.10729939538837863, test loss: 0.26459081850397537\n",
      "epoch 14910: train loss: 0.10729760413578891, test loss: 0.26459140034711615\n",
      "epoch 14911: train loss: 0.10729581304326277, test loss: 0.26459198222798636\n",
      "epoch 14912: train loss: 0.10729402211077402, test loss: 0.264592564146577\n",
      "epoch 14913: train loss: 0.10729223133829643, test loss: 0.26459314610287915\n",
      "epoch 14914: train loss: 0.10729044072580382, test loss: 0.2645937280968838\n",
      "epoch 14915: train loss: 0.10728865027326993, test loss: 0.264594310128582\n",
      "epoch 14916: train loss: 0.1072868599806686, test loss: 0.2645948921979647\n",
      "epoch 14917: train loss: 0.10728506984797363, test loss: 0.264595474305023\n",
      "epoch 14918: train loss: 0.1072832798751588, test loss: 0.2645960564497479\n",
      "epoch 14919: train loss: 0.10728149006219798, test loss: 0.2645966386321304\n",
      "epoch 14920: train loss: 0.10727970040906495, test loss: 0.2645972208521616\n",
      "epoch 14921: train loss: 0.10727791091573359, test loss: 0.2645978031098324\n",
      "epoch 14922: train loss: 0.10727612158217768, test loss: 0.264598385405134\n",
      "epoch 14923: train loss: 0.10727433240837111, test loss: 0.2645989677380572\n",
      "epoch 14924: train loss: 0.1072725433942877, test loss: 0.26459955010859326\n",
      "epoch 14925: train loss: 0.10727075453990131, test loss: 0.2646001325167332\n",
      "epoch 14926: train loss: 0.10726896584518585, test loss: 0.26460071496246795\n",
      "epoch 14927: train loss: 0.10726717731011509, test loss: 0.2646012974457887\n",
      "epoch 14928: train loss: 0.107265388934663, test loss: 0.26460187996668627\n",
      "epoch 14929: train loss: 0.10726360071880336, test loss: 0.2646024625251519\n",
      "epoch 14930: train loss: 0.10726181266251017, test loss: 0.2646030451211765\n",
      "epoch 14931: train loss: 0.10726002476575722, test loss: 0.2646036277547514\n",
      "epoch 14932: train loss: 0.10725823702851849, test loss: 0.2646042104258674\n",
      "epoch 14933: train loss: 0.10725644945076784, test loss: 0.26460479313451557\n",
      "epoch 14934: train loss: 0.10725466203247916, test loss: 0.26460537588068694\n",
      "epoch 14935: train loss: 0.10725287477362638, test loss: 0.26460595866437275\n",
      "epoch 14936: train loss: 0.10725108767418347, test loss: 0.264606541485564\n",
      "epoch 14937: train loss: 0.10724930073412431, test loss: 0.2646071243442517\n",
      "epoch 14938: train loss: 0.10724751395342284, test loss: 0.2646077072404268\n",
      "epoch 14939: train loss: 0.107245727332053, test loss: 0.2646082901740807\n",
      "epoch 14940: train loss: 0.10724394086998874, test loss: 0.26460887314520426\n",
      "epoch 14941: train loss: 0.10724215456720401, test loss: 0.26460945615378845\n",
      "epoch 14942: train loss: 0.1072403684236728, test loss: 0.26461003919982456\n",
      "epoch 14943: train loss: 0.10723858243936903, test loss: 0.2646106222833035\n",
      "epoch 14944: train loss: 0.10723679661426669, test loss: 0.2646112054042165\n",
      "epoch 14945: train loss: 0.10723501094833975, test loss: 0.26461178856255463\n",
      "epoch 14946: train loss: 0.10723322544156218, test loss: 0.2646123717583089\n",
      "epoch 14947: train loss: 0.10723144009390799, test loss: 0.26461295499147036\n",
      "epoch 14948: train loss: 0.10722965490535116, test loss: 0.2646135382620301\n",
      "epoch 14949: train loss: 0.1072278698758657, test loss: 0.26461412156997943\n",
      "epoch 14950: train loss: 0.10722608500542563, test loss: 0.2646147049153092\n",
      "epoch 14951: train loss: 0.10722430029400494, test loss: 0.2646152882980107\n",
      "epoch 14952: train loss: 0.10722251574157766, test loss: 0.26461587171807477\n",
      "epoch 14953: train loss: 0.1072207313481178, test loss: 0.2646164551754927\n",
      "epoch 14954: train loss: 0.1072189471135994, test loss: 0.2646170386702556\n",
      "epoch 14955: train loss: 0.10721716303799651, test loss: 0.2646176222023545\n",
      "epoch 14956: train loss: 0.10721537912128314, test loss: 0.2646182057717806\n",
      "epoch 14957: train loss: 0.10721359536343339, test loss: 0.2646187893785249\n",
      "epoch 14958: train loss: 0.10721181176442127, test loss: 0.2646193730225785\n",
      "epoch 14959: train loss: 0.10721002832422083, test loss: 0.2646199567039326\n",
      "epoch 14960: train loss: 0.1072082450428062, test loss: 0.2646205404225784\n",
      "epoch 14961: train loss: 0.10720646192015139, test loss: 0.26462112417850675\n",
      "epoch 14962: train loss: 0.10720467895623048, test loss: 0.26462170797170903\n",
      "epoch 14963: train loss: 0.10720289615101762, test loss: 0.2646222918021763\n",
      "epoch 14964: train loss: 0.10720111350448683, test loss: 0.26462287566989956\n",
      "epoch 14965: train loss: 0.10719933101661225, test loss: 0.26462345957487005\n",
      "epoch 14966: train loss: 0.10719754868736794, test loss: 0.2646240435170789\n",
      "epoch 14967: train loss: 0.10719576651672806, test loss: 0.26462462749651716\n",
      "epoch 14968: train loss: 0.10719398450466672, test loss: 0.26462521151317603\n",
      "epoch 14969: train loss: 0.10719220265115798, test loss: 0.26462579556704674\n",
      "epoch 14970: train loss: 0.10719042095617606, test loss: 0.26462637965812014\n",
      "epoch 14971: train loss: 0.10718863941969499, test loss: 0.2646269637863876\n",
      "epoch 14972: train loss: 0.107186858041689, test loss: 0.26462754795184035\n",
      "epoch 14973: train loss: 0.10718507682213214, test loss: 0.2646281321544693\n",
      "epoch 14974: train loss: 0.10718329576099865, test loss: 0.2646287163942656\n",
      "epoch 14975: train loss: 0.10718151485826269, test loss: 0.26462930067122065\n",
      "epoch 14976: train loss: 0.10717973411389835, test loss: 0.2646298849853253\n",
      "epoch 14977: train loss: 0.1071779535278798, test loss: 0.2646304693365709\n",
      "epoch 14978: train loss: 0.10717617310018128, test loss: 0.2646310537249486\n",
      "epoch 14979: train loss: 0.10717439283077695, test loss: 0.2646316381504495\n",
      "epoch 14980: train loss: 0.10717261271964099, test loss: 0.26463222261306474\n",
      "epoch 14981: train loss: 0.10717083276674756, test loss: 0.2646328071127856\n",
      "epoch 14982: train loss: 0.10716905297207088, test loss: 0.26463339164960303\n",
      "epoch 14983: train loss: 0.10716727333558518, test loss: 0.2646339762235083\n",
      "epoch 14984: train loss: 0.10716549385726466, test loss: 0.26463456083449266\n",
      "epoch 14985: train loss: 0.10716371453708354, test loss: 0.26463514548254724\n",
      "epoch 14986: train loss: 0.10716193537501599, test loss: 0.2646357301676632\n",
      "epoch 14987: train loss: 0.1071601563710363, test loss: 0.26463631488983164\n",
      "epoch 14988: train loss: 0.1071583775251187, test loss: 0.2646368996490438\n",
      "epoch 14989: train loss: 0.1071565988372374, test loss: 0.2646374844452909\n",
      "epoch 14990: train loss: 0.10715482030736666, test loss: 0.26463806927856404\n",
      "epoch 14991: train loss: 0.10715304193548074, test loss: 0.26463865414885446\n",
      "epoch 14992: train loss: 0.10715126372155385, test loss: 0.26463923905615333\n",
      "epoch 14993: train loss: 0.10714948566556033, test loss: 0.26463982400045194\n",
      "epoch 14994: train loss: 0.10714770776747443, test loss: 0.26464040898174135\n",
      "epoch 14995: train loss: 0.10714593002727041, test loss: 0.2646409940000126\n",
      "epoch 14996: train loss: 0.10714415244492254, test loss: 0.2646415790552572\n",
      "epoch 14997: train loss: 0.1071423750204051, test loss: 0.2646421641474662\n",
      "epoch 14998: train loss: 0.10714059775369243, test loss: 0.2646427492766308\n",
      "epoch 14999: train loss: 0.10713882064475878, test loss: 0.26464333444274224\n",
      "epoch 15000: train loss: 0.10713704369357853, test loss: 0.26464391964579154\n",
      "epoch 15001: train loss: 0.1071352669001259, test loss: 0.2646445048857701\n",
      "epoch 15002: train loss: 0.10713349026437527, test loss: 0.2646450901626693\n",
      "epoch 15003: train loss: 0.10713171378630093, test loss: 0.26464567547647994\n",
      "epoch 15004: train loss: 0.10712993746587722, test loss: 0.2646462608271934\n",
      "epoch 15005: train loss: 0.1071281613030785, test loss: 0.26464684621480106\n",
      "epoch 15006: train loss: 0.1071263852978791, test loss: 0.26464743163929383\n",
      "epoch 15007: train loss: 0.10712460945025333, test loss: 0.26464801710066316\n",
      "epoch 15008: train loss: 0.10712283376017559, test loss: 0.26464860259890016\n",
      "epoch 15009: train loss: 0.10712105822762022, test loss: 0.2646491881339962\n",
      "epoch 15010: train loss: 0.10711928285256157, test loss: 0.2646497737059423\n",
      "epoch 15011: train loss: 0.10711750763497405, test loss: 0.26465035931472974\n",
      "epoch 15012: train loss: 0.10711573257483199, test loss: 0.2646509449603498\n",
      "epoch 15013: train loss: 0.1071139576721098, test loss: 0.26465153064279373\n",
      "epoch 15014: train loss: 0.10711218292678186, test loss: 0.26465211636205277\n",
      "epoch 15015: train loss: 0.10711040833882259, test loss: 0.26465270211811814\n",
      "epoch 15016: train loss: 0.10710863390820637, test loss: 0.26465328791098097\n",
      "epoch 15017: train loss: 0.10710685963490758, test loss: 0.2646538737406326\n",
      "epoch 15018: train loss: 0.10710508551890069, test loss: 0.26465445960706435\n",
      "epoch 15019: train loss: 0.10710331156016008, test loss: 0.2646550455102672\n",
      "epoch 15020: train loss: 0.10710153775866019, test loss: 0.2646556314502327\n",
      "epoch 15021: train loss: 0.10709976411437544, test loss: 0.2646562174269518\n",
      "epoch 15022: train loss: 0.10709799062728025, test loss: 0.2646568034404161\n",
      "epoch 15023: train loss: 0.1070962172973491, test loss: 0.2646573894906165\n",
      "epoch 15024: train loss: 0.10709444412455639, test loss: 0.26465797557754445\n",
      "epoch 15025: train loss: 0.10709267110887664, test loss: 0.26465856170119123\n",
      "epoch 15026: train loss: 0.10709089825028427, test loss: 0.26465914786154804\n",
      "epoch 15027: train loss: 0.10708912554875372, test loss: 0.26465973405860604\n",
      "epoch 15028: train loss: 0.10708735300425953, test loss: 0.2646603202923567\n",
      "epoch 15029: train loss: 0.1070855806167761, test loss: 0.2646609065627911\n",
      "epoch 15030: train loss: 0.10708380838627797, test loss: 0.2646614928699006\n",
      "epoch 15031: train loss: 0.1070820363127396, test loss: 0.26466207921367646\n",
      "epoch 15032: train loss: 0.10708026439613551, test loss: 0.26466266559410995\n",
      "epoch 15033: train loss: 0.10707849263644018, test loss: 0.26466325201119234\n",
      "epoch 15034: train loss: 0.1070767210336281, test loss: 0.26466383846491487\n",
      "epoch 15035: train loss: 0.10707494958767383, test loss: 0.2646644249552689\n",
      "epoch 15036: train loss: 0.10707317829855185, test loss: 0.2646650114822456\n",
      "epoch 15037: train loss: 0.1070714071662367, test loss: 0.26466559804583634\n",
      "epoch 15038: train loss: 0.10706963619070292, test loss: 0.26466618464603237\n",
      "epoch 15039: train loss: 0.10706786537192503, test loss: 0.264666771282825\n",
      "epoch 15040: train loss: 0.10706609470987757, test loss: 0.2646673579562054\n",
      "epoch 15041: train loss: 0.1070643242045351, test loss: 0.26466794466616506\n",
      "epoch 15042: train loss: 0.10706255385587218, test loss: 0.26466853141269514\n",
      "epoch 15043: train loss: 0.10706078366386333, test loss: 0.2646691181957869\n",
      "epoch 15044: train loss: 0.10705901362848316, test loss: 0.2646697050154318\n",
      "epoch 15045: train loss: 0.10705724374970621, test loss: 0.264670291871621\n",
      "epoch 15046: train loss: 0.1070554740275071, test loss: 0.2646708787643458\n",
      "epoch 15047: train loss: 0.10705370446186038, test loss: 0.26467146569359756\n",
      "epoch 15048: train loss: 0.10705193505274063, test loss: 0.26467205265936766\n",
      "epoch 15049: train loss: 0.10705016580012248, test loss: 0.2646726396616472\n",
      "epoch 15050: train loss: 0.1070483967039805, test loss: 0.2646732267004277\n",
      "epoch 15051: train loss: 0.10704662776428929, test loss: 0.26467381377570026\n",
      "epoch 15052: train loss: 0.10704485898102352, test loss: 0.2646744008874564\n",
      "epoch 15053: train loss: 0.10704309035415772, test loss: 0.2646749880356873\n",
      "epoch 15054: train loss: 0.10704132188366661, test loss: 0.2646755752203843\n",
      "epoch 15055: train loss: 0.10703955356952473, test loss: 0.2646761624415388\n",
      "epoch 15056: train loss: 0.10703778541170678, test loss: 0.26467674969914207\n",
      "epoch 15057: train loss: 0.10703601741018737, test loss: 0.2646773369931854\n",
      "epoch 15058: train loss: 0.10703424956494115, test loss: 0.2646779243236601\n",
      "epoch 15059: train loss: 0.1070324818759428, test loss: 0.26467851169055756\n",
      "epoch 15060: train loss: 0.10703071434316694, test loss: 0.2646790990938691\n",
      "epoch 15061: train loss: 0.10702894696658827, test loss: 0.2646796865335861\n",
      "epoch 15062: train loss: 0.10702717974618142, test loss: 0.26468027400969973\n",
      "epoch 15063: train loss: 0.10702541268192112, test loss: 0.26468086152220155\n",
      "epoch 15064: train loss: 0.10702364577378201, test loss: 0.2646814490710827\n",
      "epoch 15065: train loss: 0.1070218790217388, test loss: 0.26468203665633466\n",
      "epoch 15066: train loss: 0.10702011242576619, test loss: 0.2646826242779487\n",
      "epoch 15067: train loss: 0.10701834598583883, test loss: 0.26468321193591615\n",
      "epoch 15068: train loss: 0.1070165797019315, test loss: 0.2646837996302284\n",
      "epoch 15069: train loss: 0.10701481357401885, test loss: 0.26468438736087685\n",
      "epoch 15070: train loss: 0.10701304760207563, test loss: 0.2646849751278527\n",
      "epoch 15071: train loss: 0.10701128178607655, test loss: 0.2646855629311475\n",
      "epoch 15072: train loss: 0.10700951612599631, test loss: 0.26468615077075247\n",
      "epoch 15073: train loss: 0.10700775062180973, test loss: 0.264686738646659\n",
      "epoch 15074: train loss: 0.1070059852734915, test loss: 0.26468732655885835\n",
      "epoch 15075: train loss: 0.10700422008101634, test loss: 0.264687914507342\n",
      "epoch 15076: train loss: 0.10700245504435905, test loss: 0.26468850249210146\n",
      "epoch 15077: train loss: 0.10700069016349435, test loss: 0.2646890905131278\n",
      "epoch 15078: train loss: 0.10699892543839702, test loss: 0.2646896785704125\n",
      "epoch 15079: train loss: 0.10699716086904183, test loss: 0.264690266663947\n",
      "epoch 15080: train loss: 0.10699539645540355, test loss: 0.2646908547937226\n",
      "epoch 15081: train loss: 0.10699363219745699, test loss: 0.26469144295973074\n",
      "epoch 15082: train loss: 0.10699186809517691, test loss: 0.26469203116196277\n",
      "epoch 15083: train loss: 0.1069901041485381, test loss: 0.26469261940041\n",
      "epoch 15084: train loss: 0.10698834035751535, test loss: 0.2646932076750639\n",
      "epoch 15085: train loss: 0.10698657672208352, test loss: 0.2646937959859157\n",
      "epoch 15086: train loss: 0.10698481324221736, test loss: 0.26469438433295706\n",
      "epoch 15087: train loss: 0.10698304991789169, test loss: 0.2646949727161791\n",
      "epoch 15088: train loss: 0.10698128674908138, test loss: 0.2646955611355733\n",
      "epoch 15089: train loss: 0.10697952373576118, test loss: 0.2646961495911311\n",
      "epoch 15090: train loss: 0.10697776087790603, test loss: 0.2646967380828439\n",
      "epoch 15091: train loss: 0.10697599817549067, test loss: 0.2646973266107031\n",
      "epoch 15092: train loss: 0.10697423562849, test loss: 0.2646979151746999\n",
      "epoch 15093: train loss: 0.10697247323687882, test loss: 0.264698503774826\n",
      "epoch 15094: train loss: 0.10697071100063209, test loss: 0.2646990924110726\n",
      "epoch 15095: train loss: 0.10696894891972454, test loss: 0.2646996810834312\n",
      "epoch 15096: train loss: 0.10696718699413114, test loss: 0.2647002697918932\n",
      "epoch 15097: train loss: 0.10696542522382671, test loss: 0.26470085853644987\n",
      "epoch 15098: train loss: 0.10696366360878613, test loss: 0.2647014473170928\n",
      "epoch 15099: train loss: 0.10696190214898432, test loss: 0.2647020361338134\n",
      "epoch 15100: train loss: 0.10696014084439617, test loss: 0.26470262498660285\n",
      "epoch 15101: train loss: 0.10695837969499655, test loss: 0.26470321387545276\n",
      "epoch 15102: train loss: 0.10695661870076037, test loss: 0.26470380280035455\n",
      "epoch 15103: train loss: 0.10695485786166252, test loss: 0.2647043917612997\n",
      "epoch 15104: train loss: 0.10695309717767797, test loss: 0.2647049807582794\n",
      "epoch 15105: train loss: 0.10695133664878159, test loss: 0.26470556979128523\n",
      "epoch 15106: train loss: 0.10694957627494832, test loss: 0.2647061588603086\n",
      "epoch 15107: train loss: 0.1069478160561531, test loss: 0.26470674796534105\n",
      "epoch 15108: train loss: 0.10694605599237086, test loss: 0.26470733710637384\n",
      "epoch 15109: train loss: 0.10694429608357651, test loss: 0.26470792628339845\n",
      "epoch 15110: train loss: 0.10694253632974508, test loss: 0.26470851549640634\n",
      "epoch 15111: train loss: 0.10694077673085145, test loss: 0.26470910474538895\n",
      "epoch 15112: train loss: 0.10693901728687061, test loss: 0.2647096940303377\n",
      "epoch 15113: train loss: 0.10693725799777752, test loss: 0.264710283351244\n",
      "epoch 15114: train loss: 0.10693549886354718, test loss: 0.26471087270809945\n",
      "epoch 15115: train loss: 0.10693373988415454, test loss: 0.2647114621008953\n",
      "epoch 15116: train loss: 0.10693198105957456, test loss: 0.26471205152962307\n",
      "epoch 15117: train loss: 0.10693022238978228, test loss: 0.26471264099427416\n",
      "epoch 15118: train loss: 0.10692846387475263, test loss: 0.26471323049484025\n",
      "epoch 15119: train loss: 0.1069267055144607, test loss: 0.26471382003131244\n",
      "epoch 15120: train loss: 0.10692494730888143, test loss: 0.2647144096036825\n",
      "epoch 15121: train loss: 0.10692318925798983, test loss: 0.26471499921194164\n",
      "epoch 15122: train loss: 0.10692143136176097, test loss: 0.26471558885608154\n",
      "epoch 15123: train loss: 0.1069196736201698, test loss: 0.2647161785360935\n",
      "epoch 15124: train loss: 0.10691791603319142, test loss: 0.26471676825196916\n",
      "epoch 15125: train loss: 0.10691615860080082, test loss: 0.2647173580036997\n",
      "epoch 15126: train loss: 0.10691440132297306, test loss: 0.2647179477912769\n",
      "epoch 15127: train loss: 0.10691264419968319, test loss: 0.26471853761469194\n",
      "epoch 15128: train loss: 0.10691088723090622, test loss: 0.2647191274739365\n",
      "epoch 15129: train loss: 0.10690913041661726, test loss: 0.26471971736900207\n",
      "epoch 15130: train loss: 0.10690737375679138, test loss: 0.26472030729988005\n",
      "epoch 15131: train loss: 0.10690561725140359, test loss: 0.26472089726656184\n",
      "epoch 15132: train loss: 0.10690386090042901, test loss: 0.26472148726903905\n",
      "epoch 15133: train loss: 0.10690210470384272, test loss: 0.2647220773073031\n",
      "epoch 15134: train loss: 0.10690034866161978, test loss: 0.2647226673813455\n",
      "epoch 15135: train loss: 0.10689859277373531, test loss: 0.26472325749115777\n",
      "epoch 15136: train loss: 0.10689683704016438, test loss: 0.2647238476367313\n",
      "epoch 15137: train loss: 0.10689508146088214, test loss: 0.26472443781805766\n",
      "epoch 15138: train loss: 0.10689332603586366, test loss: 0.2647250280351283\n",
      "epoch 15139: train loss: 0.10689157076508403, test loss: 0.2647256182879347\n",
      "epoch 15140: train loss: 0.10688981564851843, test loss: 0.2647262085764684\n",
      "epoch 15141: train loss: 0.10688806068614194, test loss: 0.264726798900721\n",
      "epoch 15142: train loss: 0.10688630587792973, test loss: 0.2647273892606839\n",
      "epoch 15143: train loss: 0.10688455122385694, test loss: 0.26472797965634853\n",
      "epoch 15144: train loss: 0.10688279672389864, test loss: 0.26472857008770645\n",
      "epoch 15145: train loss: 0.10688104237803009, test loss: 0.26472916055474927\n",
      "epoch 15146: train loss: 0.10687928818622636, test loss: 0.2647297510574683\n",
      "epoch 15147: train loss: 0.10687753414846263, test loss: 0.26473034159585523\n",
      "epoch 15148: train loss: 0.1068757802647141, test loss: 0.2647309321699015\n",
      "epoch 15149: train loss: 0.10687402653495588, test loss: 0.26473152277959866\n",
      "epoch 15150: train loss: 0.10687227295916321, test loss: 0.2647321134249381\n",
      "epoch 15151: train loss: 0.10687051953731125, test loss: 0.2647327041059116\n",
      "epoch 15152: train loss: 0.10686876626937515, test loss: 0.26473329482251046\n",
      "epoch 15153: train loss: 0.10686701315533018, test loss: 0.26473388557472627\n",
      "epoch 15154: train loss: 0.1068652601951515, test loss: 0.26473447636255054\n",
      "epoch 15155: train loss: 0.1068635073888143, test loss: 0.2647350671859748\n",
      "epoch 15156: train loss: 0.1068617547362938, test loss: 0.2647356580449906\n",
      "epoch 15157: train loss: 0.10686000223756521, test loss: 0.26473624893958947\n",
      "epoch 15158: train loss: 0.10685824989260378, test loss: 0.264736839869763\n",
      "epoch 15159: train loss: 0.10685649770138475, test loss: 0.2647374308355026\n",
      "epoch 15160: train loss: 0.10685474566388332, test loss: 0.26473802183679995\n",
      "epoch 15161: train loss: 0.10685299378007473, test loss: 0.2647386128736465\n",
      "epoch 15162: train loss: 0.10685124204993425, test loss: 0.26473920394603373\n",
      "epoch 15163: train loss: 0.10684949047343711, test loss: 0.26473979505395334\n",
      "epoch 15164: train loss: 0.1068477390505586, test loss: 0.26474038619739676\n",
      "epoch 15165: train loss: 0.10684598778127391, test loss: 0.26474097737635566\n",
      "epoch 15166: train loss: 0.10684423666555841, test loss: 0.2647415685908215\n",
      "epoch 15167: train loss: 0.10684248570338728, test loss: 0.26474215984078575\n",
      "epoch 15168: train loss: 0.10684073489473585, test loss: 0.26474275112624013\n",
      "epoch 15169: train loss: 0.10683898423957938, test loss: 0.2647433424471761\n",
      "epoch 15170: train loss: 0.10683723373789318, test loss: 0.26474393380358524\n",
      "epoch 15171: train loss: 0.10683548338965256, test loss: 0.2647445251954592\n",
      "epoch 15172: train loss: 0.10683373319483279, test loss: 0.2647451166227894\n",
      "epoch 15173: train loss: 0.1068319831534092, test loss: 0.26474570808556747\n",
      "epoch 15174: train loss: 0.10683023326535707, test loss: 0.264746299583785\n",
      "epoch 15175: train loss: 0.10682848353065177, test loss: 0.26474689111743355\n",
      "epoch 15176: train loss: 0.1068267339492686, test loss: 0.2647474826865046\n",
      "epoch 15177: train loss: 0.10682498452118289, test loss: 0.2647480742909898\n",
      "epoch 15178: train loss: 0.10682323524636995, test loss: 0.26474866593088087\n",
      "epoch 15179: train loss: 0.10682148612480515, test loss: 0.26474925760616913\n",
      "epoch 15180: train loss: 0.10681973715646385, test loss: 0.26474984931684625\n",
      "epoch 15181: train loss: 0.10681798834132138, test loss: 0.26475044106290385\n",
      "epoch 15182: train loss: 0.1068162396793531, test loss: 0.26475103284433354\n",
      "epoch 15183: train loss: 0.10681449117053439, test loss: 0.26475162466112684\n",
      "epoch 15184: train loss: 0.10681274281484061, test loss: 0.26475221651327535\n",
      "epoch 15185: train loss: 0.10681099461224713, test loss: 0.2647528084007707\n",
      "epoch 15186: train loss: 0.10680924656272936, test loss: 0.26475340032360445\n",
      "epoch 15187: train loss: 0.10680749866626262, test loss: 0.26475399228176816\n",
      "epoch 15188: train loss: 0.10680575092282237, test loss: 0.26475458427525345\n",
      "epoch 15189: train loss: 0.10680400333238396, test loss: 0.264755176304052\n",
      "epoch 15190: train loss: 0.10680225589492284, test loss: 0.26475576836815534\n",
      "epoch 15191: train loss: 0.10680050861041439, test loss: 0.264756360467555\n",
      "epoch 15192: train loss: 0.10679876147883402, test loss: 0.2647569526022427\n",
      "epoch 15193: train loss: 0.10679701450015716, test loss: 0.2647575447722101\n",
      "epoch 15194: train loss: 0.10679526767435926, test loss: 0.26475813697744854\n",
      "epoch 15195: train loss: 0.10679352100141569, test loss: 0.26475872921794985\n",
      "epoch 15196: train loss: 0.10679177448130198, test loss: 0.2647593214937057\n",
      "epoch 15197: train loss: 0.10679002811399346, test loss: 0.2647599138047075\n",
      "epoch 15198: train loss: 0.10678828189946567, test loss: 0.264760506150947\n",
      "epoch 15199: train loss: 0.10678653583769403, test loss: 0.26476109853241575\n",
      "epoch 15200: train loss: 0.10678478992865399, test loss: 0.2647616909491054\n",
      "epoch 15201: train loss: 0.10678304417232103, test loss: 0.2647622834010076\n",
      "epoch 15202: train loss: 0.1067812985686706, test loss: 0.264762875888114\n",
      "epoch 15203: train loss: 0.10677955311767821, test loss: 0.26476346841041604\n",
      "epoch 15204: train loss: 0.10677780781931931, test loss: 0.2647640609679056\n",
      "epoch 15205: train loss: 0.10677606267356941, test loss: 0.26476465356057416\n",
      "epoch 15206: train loss: 0.10677431768040399, test loss: 0.2647652461884134\n",
      "epoch 15207: train loss: 0.10677257283979857, test loss: 0.2647658388514149\n",
      "epoch 15208: train loss: 0.10677082815172863, test loss: 0.26476643154957036\n",
      "epoch 15209: train loss: 0.10676908361616969, test loss: 0.2647670242828715\n",
      "epoch 15210: train loss: 0.10676733923309727, test loss: 0.26476761705130974\n",
      "epoch 15211: train loss: 0.10676559500248686, test loss: 0.2647682098548769\n",
      "epoch 15212: train loss: 0.10676385092431402, test loss: 0.2647688026935645\n",
      "epoch 15213: train loss: 0.10676210699855428, test loss: 0.26476939556736434\n",
      "epoch 15214: train loss: 0.10676036322518317, test loss: 0.264769988476268\n",
      "epoch 15215: train loss: 0.10675861960417624, test loss: 0.264770581420267\n",
      "epoch 15216: train loss: 0.10675687613550905, test loss: 0.26477117439935316\n",
      "epoch 15217: train loss: 0.10675513281915709, test loss: 0.2647717674135181\n",
      "epoch 15218: train loss: 0.10675338965509602, test loss: 0.26477236046275343\n",
      "epoch 15219: train loss: 0.10675164664330133, test loss: 0.2647729535470508\n",
      "epoch 15220: train loss: 0.10674990378374861, test loss: 0.264773546666402\n",
      "epoch 15221: train loss: 0.10674816107641344, test loss: 0.2647741398207985\n",
      "epoch 15222: train loss: 0.10674641852127145, test loss: 0.2647747330102321\n",
      "epoch 15223: train loss: 0.10674467611829812, test loss: 0.2647753262346945\n",
      "epoch 15224: train loss: 0.10674293386746914, test loss: 0.2647759194941772\n",
      "epoch 15225: train loss: 0.10674119176876008, test loss: 0.2647765127886721\n",
      "epoch 15226: train loss: 0.10673944982214653, test loss: 0.2647771061181707\n",
      "epoch 15227: train loss: 0.1067377080276041, test loss: 0.2647776994826646\n",
      "epoch 15228: train loss: 0.10673596638510845, test loss: 0.26477829288214577\n",
      "epoch 15229: train loss: 0.10673422489463512, test loss: 0.26477888631660557\n",
      "epoch 15230: train loss: 0.1067324835561598, test loss: 0.2647794797860359\n",
      "epoch 15231: train loss: 0.10673074236965814, test loss: 0.2647800732904284\n",
      "epoch 15232: train loss: 0.1067290013351057, test loss: 0.26478066682977464\n",
      "epoch 15233: train loss: 0.10672726045247818, test loss: 0.2647812604040664\n",
      "epoch 15234: train loss: 0.10672551972175125, test loss: 0.2647818540132954\n",
      "epoch 15235: train loss: 0.10672377914290049, test loss: 0.2647824476574533\n",
      "epoch 15236: train loss: 0.10672203871590163, test loss: 0.2647830413365318\n",
      "epoch 15237: train loss: 0.1067202984407303, test loss: 0.2647836350505225\n",
      "epoch 15238: train loss: 0.10671855831736217, test loss: 0.26478422879941726\n",
      "epoch 15239: train loss: 0.10671681834577294, test loss: 0.2647848225832076\n",
      "epoch 15240: train loss: 0.10671507852593827, test loss: 0.26478541640188547\n",
      "epoch 15241: train loss: 0.10671333885783386, test loss: 0.2647860102554423\n",
      "epoch 15242: train loss: 0.10671159934143538, test loss: 0.26478660414387\n",
      "epoch 15243: train loss: 0.10670985997671856, test loss: 0.26478719806716017\n",
      "epoch 15244: train loss: 0.10670812076365911, test loss: 0.26478779202530445\n",
      "epoch 15245: train loss: 0.1067063817022327, test loss: 0.2647883860182947\n",
      "epoch 15246: train loss: 0.1067046427924151, test loss: 0.2647889800461226\n",
      "epoch 15247: train loss: 0.10670290403418196, test loss: 0.26478957410877985\n",
      "epoch 15248: train loss: 0.10670116542750906, test loss: 0.2647901682062582\n",
      "epoch 15249: train loss: 0.10669942697237211, test loss: 0.26479076233854926\n",
      "epoch 15250: train loss: 0.10669768866874688, test loss: 0.26479135650564484\n",
      "epoch 15251: train loss: 0.10669595051660907, test loss: 0.2647919507075367\n",
      "epoch 15252: train loss: 0.10669421251593444, test loss: 0.26479254494421633\n",
      "epoch 15253: train loss: 0.10669247466669876, test loss: 0.2647931392156758\n",
      "epoch 15254: train loss: 0.10669073696887779, test loss: 0.2647937335219066\n",
      "epoch 15255: train loss: 0.10668899942244728, test loss: 0.26479432786290064\n",
      "epoch 15256: train loss: 0.10668726202738299, test loss: 0.2647949222386495\n",
      "epoch 15257: train loss: 0.10668552478366074, test loss: 0.2647955166491449\n",
      "epoch 15258: train loss: 0.10668378769125625, test loss: 0.26479611109437867\n",
      "epoch 15259: train loss: 0.10668205075014538, test loss: 0.26479670557434254\n",
      "epoch 15260: train loss: 0.10668031396030385, test loss: 0.26479730008902824\n",
      "epoch 15261: train loss: 0.1066785773217075, test loss: 0.26479789463842746\n",
      "epoch 15262: train loss: 0.10667684083433214, test loss: 0.264798489222532\n",
      "epoch 15263: train loss: 0.10667510449815355, test loss: 0.26479908384133366\n",
      "epoch 15264: train loss: 0.10667336831314757, test loss: 0.2647996784948241\n",
      "epoch 15265: train loss: 0.10667163227929001, test loss: 0.2648002731829951\n",
      "epoch 15266: train loss: 0.10666989639655668, test loss: 0.26480086790583846\n",
      "epoch 15267: train loss: 0.10666816066492345, test loss: 0.2648014626633458\n",
      "epoch 15268: train loss: 0.10666642508436613, test loss: 0.26480205745550905\n",
      "epoch 15269: train loss: 0.10666468965486053, test loss: 0.2648026522823198\n",
      "epoch 15270: train loss: 0.10666295437638258, test loss: 0.2648032471437699\n",
      "epoch 15271: train loss: 0.10666121924890806, test loss: 0.2648038420398512\n",
      "epoch 15272: train loss: 0.10665948427241286, test loss: 0.2648044369705553\n",
      "epoch 15273: train loss: 0.10665774944687284, test loss: 0.264805031935874\n",
      "epoch 15274: train loss: 0.10665601477226387, test loss: 0.2648056269357992\n",
      "epoch 15275: train loss: 0.10665428024856181, test loss: 0.26480622197032255\n",
      "epoch 15276: train loss: 0.10665254587574256, test loss: 0.2648068170394358\n",
      "epoch 15277: train loss: 0.10665081165378201, test loss: 0.26480741214313086\n",
      "epoch 15278: train loss: 0.10664907758265604, test loss: 0.2648080072813994\n",
      "epoch 15279: train loss: 0.10664734366234054, test loss: 0.2648086024542332\n",
      "epoch 15280: train loss: 0.10664560989281142, test loss: 0.26480919766162403\n",
      "epoch 15281: train loss: 0.1066438762740446, test loss: 0.26480979290356377\n",
      "epoch 15282: train loss: 0.10664214280601599, test loss: 0.2648103881800441\n",
      "epoch 15283: train loss: 0.10664040948870147, test loss: 0.2648109834910569\n",
      "epoch 15284: train loss: 0.106638676322077, test loss: 0.26481157883659384\n",
      "epoch 15285: train loss: 0.10663694330611855, test loss: 0.26481217421664693\n",
      "epoch 15286: train loss: 0.10663521044080194, test loss: 0.26481276963120776\n",
      "epoch 15287: train loss: 0.10663347772610322, test loss: 0.26481336508026815\n",
      "epoch 15288: train loss: 0.10663174516199829, test loss: 0.2648139605638199\n",
      "epoch 15289: train loss: 0.10663001274846312, test loss: 0.26481455608185483\n",
      "epoch 15290: train loss: 0.10662828048547364, test loss: 0.26481515163436475\n",
      "epoch 15291: train loss: 0.10662654837300582, test loss: 0.26481574722134155\n",
      "epoch 15292: train loss: 0.10662481641103565, test loss: 0.26481634284277683\n",
      "epoch 15293: train loss: 0.10662308459953909, test loss: 0.26481693849866256\n",
      "epoch 15294: train loss: 0.1066213529384921, test loss: 0.2648175341889905\n",
      "epoch 15295: train loss: 0.1066196214278707, test loss: 0.2648181299137525\n",
      "epoch 15296: train loss: 0.10661789006765086, test loss: 0.2648187256729402\n",
      "epoch 15297: train loss: 0.10661615885780858, test loss: 0.26481932146654563\n",
      "epoch 15298: train loss: 0.10661442779831985, test loss: 0.2648199172945605\n",
      "epoch 15299: train loss: 0.10661269688916068, test loss: 0.2648205131569767\n",
      "epoch 15300: train loss: 0.10661096613030707, test loss: 0.26482110905378603\n",
      "epoch 15301: train loss: 0.10660923552173507, test loss: 0.2648217049849803\n",
      "epoch 15302: train loss: 0.10660750506342069, test loss: 0.2648223009505512\n",
      "epoch 15303: train loss: 0.10660577475533994, test loss: 0.26482289695049077\n",
      "epoch 15304: train loss: 0.10660404459746887, test loss: 0.26482349298479074\n",
      "epoch 15305: train loss: 0.1066023145897835, test loss: 0.26482408905344285\n",
      "epoch 15306: train loss: 0.10660058473225993, test loss: 0.26482468515643914\n",
      "epoch 15307: train loss: 0.10659885502487412, test loss: 0.2648252812937713\n",
      "epoch 15308: train loss: 0.10659712546760222, test loss: 0.26482587746543124\n",
      "epoch 15309: train loss: 0.1065953960604202, test loss: 0.2648264736714107\n",
      "epoch 15310: train loss: 0.10659366680330418, test loss: 0.2648270699117016\n",
      "epoch 15311: train loss: 0.10659193769623024, test loss: 0.26482766618629566\n",
      "epoch 15312: train loss: 0.10659020873917441, test loss: 0.26482826249518504\n",
      "epoch 15313: train loss: 0.10658847993211282, test loss: 0.26482885883836127\n",
      "epoch 15314: train loss: 0.1065867512750215, test loss: 0.26482945521581625\n",
      "epoch 15315: train loss: 0.10658502276787661, test loss: 0.2648300516275419\n",
      "epoch 15316: train loss: 0.10658329441065421, test loss: 0.26483064807353\n",
      "epoch 15317: train loss: 0.10658156620333042, test loss: 0.26483124455377266\n",
      "epoch 15318: train loss: 0.1065798381458813, test loss: 0.26483184106826135\n",
      "epoch 15319: train loss: 0.10657811023828305, test loss: 0.2648324376169882\n",
      "epoch 15320: train loss: 0.10657638248051172, test loss: 0.2648330341999449\n",
      "epoch 15321: train loss: 0.10657465487254346, test loss: 0.26483363081712347\n",
      "epoch 15322: train loss: 0.10657292741435441, test loss: 0.26483422746851565\n",
      "epoch 15323: train loss: 0.10657120010592071, test loss: 0.26483482415411336\n",
      "epoch 15324: train loss: 0.10656947294721844, test loss: 0.26483542087390843\n",
      "epoch 15325: train loss: 0.10656774593822384, test loss: 0.26483601762789294\n",
      "epoch 15326: train loss: 0.106566019078913, test loss: 0.2648366144160584\n",
      "epoch 15327: train loss: 0.10656429236926207, test loss: 0.26483721123839693\n",
      "epoch 15328: train loss: 0.10656256580924729, test loss: 0.2648378080949004\n",
      "epoch 15329: train loss: 0.10656083939884475, test loss: 0.2648384049855605\n",
      "epoch 15330: train loss: 0.10655911313803065, test loss: 0.2648390019103693\n",
      "epoch 15331: train loss: 0.10655738702678118, test loss: 0.26483959886931857\n",
      "epoch 15332: train loss: 0.10655566106507251, test loss: 0.2648401958624003\n",
      "epoch 15333: train loss: 0.10655393525288083, test loss: 0.26484079288960627\n",
      "epoch 15334: train loss: 0.10655220959018234, test loss: 0.26484138995092854\n",
      "epoch 15335: train loss: 0.10655048407695324, test loss: 0.2648419870463588\n",
      "epoch 15336: train loss: 0.10654875871316975, test loss: 0.264842584175889\n",
      "epoch 15337: train loss: 0.10654703349880805, test loss: 0.2648431813395111\n",
      "epoch 15338: train loss: 0.1065453084338444, test loss: 0.26484377853721697\n",
      "epoch 15339: train loss: 0.10654358351825495, test loss: 0.2648443757689984\n",
      "epoch 15340: train loss: 0.10654185875201602, test loss: 0.26484497303484733\n",
      "epoch 15341: train loss: 0.10654013413510377, test loss: 0.26484557033475586\n",
      "epoch 15342: train loss: 0.10653840966749449, test loss: 0.26484616766871566\n",
      "epoch 15343: train loss: 0.10653668534916441, test loss: 0.2648467650367188\n",
      "epoch 15344: train loss: 0.10653496118008973, test loss: 0.264847362438757\n",
      "epoch 15345: train loss: 0.10653323716024678, test loss: 0.26484795987482224\n",
      "epoch 15346: train loss: 0.1065315132896118, test loss: 0.2648485573449066\n",
      "epoch 15347: train loss: 0.10652978956816099, test loss: 0.2648491548490018\n",
      "epoch 15348: train loss: 0.1065280659958707, test loss: 0.2648497523870997\n",
      "epoch 15349: train loss: 0.10652634257271718, test loss: 0.2648503499591924\n",
      "epoch 15350: train loss: 0.1065246192986767, test loss: 0.2648509475652718\n",
      "epoch 15351: train loss: 0.10652289617372557, test loss: 0.26485154520532966\n",
      "epoch 15352: train loss: 0.10652117319784005, test loss: 0.2648521428793581\n",
      "epoch 15353: train loss: 0.10651945037099647, test loss: 0.26485274058734887\n",
      "epoch 15354: train loss: 0.10651772769317108, test loss: 0.2648533383292941\n",
      "epoch 15355: train loss: 0.10651600516434029, test loss: 0.2648539361051855\n",
      "epoch 15356: train loss: 0.1065142827844803, test loss: 0.2648545339150151\n",
      "epoch 15357: train loss: 0.10651256055356749, test loss: 0.26485513175877484\n",
      "epoch 15358: train loss: 0.10651083847157818, test loss: 0.2648557296364567\n",
      "epoch 15359: train loss: 0.10650911653848869, test loss: 0.2648563275480525\n",
      "epoch 15360: train loss: 0.10650739475427536, test loss: 0.26485692549355433\n",
      "epoch 15361: train loss: 0.1065056731189145, test loss: 0.2648575234729539\n",
      "epoch 15362: train loss: 0.10650395163238251, test loss: 0.2648581214862434\n",
      "epoch 15363: train loss: 0.10650223029465572, test loss: 0.2648587195334146\n",
      "epoch 15364: train loss: 0.10650050910571046, test loss: 0.26485931761445947\n",
      "epoch 15365: train loss: 0.10649878806552313, test loss: 0.2648599157293701\n",
      "epoch 15366: train loss: 0.10649706717407006, test loss: 0.26486051387813836\n",
      "epoch 15367: train loss: 0.10649534643132766, test loss: 0.2648611120607561\n",
      "epoch 15368: train loss: 0.10649362583727229, test loss: 0.26486171027721533\n",
      "epoch 15369: train loss: 0.10649190539188033, test loss: 0.26486230852750814\n",
      "epoch 15370: train loss: 0.10649018509512814, test loss: 0.2648629068116264\n",
      "epoch 15371: train loss: 0.1064884649469922, test loss: 0.26486350512956186\n",
      "epoch 15372: train loss: 0.1064867449474488, test loss: 0.26486410348130685\n",
      "epoch 15373: train loss: 0.10648502509647444, test loss: 0.26486470186685296\n",
      "epoch 15374: train loss: 0.10648330539404544, test loss: 0.2648653002861925\n",
      "epoch 15375: train loss: 0.1064815858401383, test loss: 0.26486589873931726\n",
      "epoch 15376: train loss: 0.10647986643472937, test loss: 0.26486649722621924\n",
      "epoch 15377: train loss: 0.10647814717779515, test loss: 0.2648670957468904\n",
      "epoch 15378: train loss: 0.10647642806931198, test loss: 0.2648676943013227\n",
      "epoch 15379: train loss: 0.10647470910925637, test loss: 0.26486829288950814\n",
      "epoch 15380: train loss: 0.10647299029760475, test loss: 0.2648688915114387\n",
      "epoch 15381: train loss: 0.10647127163433354, test loss: 0.2648694901671063\n",
      "epoch 15382: train loss: 0.10646955311941919, test loss: 0.26487008885650304\n",
      "epoch 15383: train loss: 0.10646783475283819, test loss: 0.2648706875796208\n",
      "epoch 15384: train loss: 0.10646611653456699, test loss: 0.2648712863364516\n",
      "epoch 15385: train loss: 0.10646439846458203, test loss: 0.26487188512698745\n",
      "epoch 15386: train loss: 0.10646268054285983, test loss: 0.2648724839512202\n",
      "epoch 15387: train loss: 0.10646096276937683, test loss: 0.2648730828091421\n",
      "epoch 15388: train loss: 0.10645924514410954, test loss: 0.264873681700745\n",
      "epoch 15389: train loss: 0.10645752766703442, test loss: 0.2648742806260208\n",
      "epoch 15390: train loss: 0.106455810338128, test loss: 0.2648748795849616\n",
      "epoch 15391: train loss: 0.10645409315736674, test loss: 0.26487547857755944\n",
      "epoch 15392: train loss: 0.10645237612472719, test loss: 0.2648760776038062\n",
      "epoch 15393: train loss: 0.10645065924018583, test loss: 0.264876676663694\n",
      "epoch 15394: train loss: 0.1064489425037192, test loss: 0.2648772757572147\n",
      "epoch 15395: train loss: 0.1064472259153038, test loss: 0.2648778748843605\n",
      "epoch 15396: train loss: 0.10644550947491617, test loss: 0.2648784740451232\n",
      "epoch 15397: train loss: 0.10644379318253279, test loss: 0.26487907323949506\n",
      "epoch 15398: train loss: 0.10644207703813027, test loss: 0.26487967246746785\n",
      "epoch 15399: train loss: 0.10644036104168514, test loss: 0.26488027172903367\n",
      "epoch 15400: train loss: 0.10643864519317389, test loss: 0.26488087102418456\n",
      "epoch 15401: train loss: 0.10643692949257315, test loss: 0.26488147035291254\n",
      "epoch 15402: train loss: 0.10643521393985943, test loss: 0.2648820697152096\n",
      "epoch 15403: train loss: 0.10643349853500929, test loss: 0.2648826691110678\n",
      "epoch 15404: train loss: 0.1064317832779993, test loss: 0.26488326854047906\n",
      "epoch 15405: train loss: 0.10643006816880607, test loss: 0.2648838680034355\n",
      "epoch 15406: train loss: 0.10642835320740617, test loss: 0.26488446749992917\n",
      "epoch 15407: train loss: 0.10642663839377614, test loss: 0.264885067029952\n",
      "epoch 15408: train loss: 0.1064249237278926, test loss: 0.26488566659349616\n",
      "epoch 15409: train loss: 0.10642320920973215, test loss: 0.26488626619055355\n",
      "epoch 15410: train loss: 0.10642149483927138, test loss: 0.26488686582111626\n",
      "epoch 15411: train loss: 0.10641978061648694, test loss: 0.26488746548517633\n",
      "epoch 15412: train loss: 0.10641806654135536, test loss: 0.26488806518272584\n",
      "epoch 15413: train loss: 0.10641635261385328, test loss: 0.2648886649137566\n",
      "epoch 15414: train loss: 0.10641463883395737, test loss: 0.264889264678261\n",
      "epoch 15415: train loss: 0.10641292520164422, test loss: 0.26488986447623086\n",
      "epoch 15416: train loss: 0.10641121171689047, test loss: 0.26489046430765834\n",
      "epoch 15417: train loss: 0.10640949837967273, test loss: 0.2648910641725354\n",
      "epoch 15418: train loss: 0.10640778518996769, test loss: 0.26489166407085407\n",
      "epoch 15419: train loss: 0.10640607214775198, test loss: 0.26489226400260646\n",
      "epoch 15420: train loss: 0.10640435925300222, test loss: 0.26489286396778466\n",
      "epoch 15421: train loss: 0.10640264650569513, test loss: 0.26489346396638064\n",
      "epoch 15422: train loss: 0.1064009339058073, test loss: 0.26489406399838655\n",
      "epoch 15423: train loss: 0.10639922145331543, test loss: 0.26489466406379436\n",
      "epoch 15424: train loss: 0.10639750914819623, test loss: 0.26489526416259623\n",
      "epoch 15425: train loss: 0.10639579699042633, test loss: 0.2648958642947841\n",
      "epoch 15426: train loss: 0.10639408497998243, test loss: 0.2648964644603501\n",
      "epoch 15427: train loss: 0.10639237311684126, test loss: 0.26489706465928636\n",
      "epoch 15428: train loss: 0.10639066140097946, test loss: 0.2648976648915848\n",
      "epoch 15429: train loss: 0.10638894983237371, test loss: 0.2648982651572377\n",
      "epoch 15430: train loss: 0.10638723841100077, test loss: 0.2648988654562369\n",
      "epoch 15431: train loss: 0.10638552713683735, test loss: 0.26489946578857454\n",
      "epoch 15432: train loss: 0.10638381600986013, test loss: 0.26490006615424283\n",
      "epoch 15433: train loss: 0.10638210503004586, test loss: 0.2649006665532337\n",
      "epoch 15434: train loss: 0.10638039419737125, test loss: 0.26490126698553945\n",
      "epoch 15435: train loss: 0.10637868351181304, test loss: 0.2649018674511518\n",
      "epoch 15436: train loss: 0.10637697297334796, test loss: 0.2649024679500631\n",
      "epoch 15437: train loss: 0.10637526258195278, test loss: 0.26490306848226547\n",
      "epoch 15438: train loss: 0.1063735523376042, test loss: 0.2649036690477507\n",
      "epoch 15439: train loss: 0.10637184224027901, test loss: 0.26490426964651126\n",
      "epoch 15440: train loss: 0.10637013228995393, test loss: 0.26490487027853904\n",
      "epoch 15441: train loss: 0.10636842248660575, test loss: 0.2649054709438261\n",
      "epoch 15442: train loss: 0.10636671283021125, test loss: 0.2649060716423646\n",
      "epoch 15443: train loss: 0.10636500332074718, test loss: 0.26490667237414667\n",
      "epoch 15444: train loss: 0.10636329395819033, test loss: 0.2649072731391643\n",
      "epoch 15445: train loss: 0.10636158474251746, test loss: 0.2649078739374097\n",
      "epoch 15446: train loss: 0.10635987567370538, test loss: 0.26490847476887497\n",
      "epoch 15447: train loss: 0.10635816675173089, test loss: 0.2649090756335522\n",
      "epoch 15448: train loss: 0.10635645797657077, test loss: 0.2649096765314334\n",
      "epoch 15449: train loss: 0.10635474934820184, test loss: 0.2649102774625107\n",
      "epoch 15450: train loss: 0.10635304086660088, test loss: 0.26491087842677635\n",
      "epoch 15451: train loss: 0.10635133253174477, test loss: 0.26491147942422244\n",
      "epoch 15452: train loss: 0.10634962434361028, test loss: 0.26491208045484094\n",
      "epoch 15453: train loss: 0.10634791630217424, test loss: 0.26491268151862407\n",
      "epoch 15454: train loss: 0.1063462084074135, test loss: 0.26491328261556396\n",
      "epoch 15455: train loss: 0.10634450065930487, test loss: 0.2649138837456526\n",
      "epoch 15456: train loss: 0.10634279305782518, test loss: 0.26491448490888225\n",
      "epoch 15457: train loss: 0.10634108560295134, test loss: 0.26491508610524506\n",
      "epoch 15458: train loss: 0.10633937829466014, test loss: 0.26491568733473303\n",
      "epoch 15459: train loss: 0.10633767113292844, test loss: 0.2649162885973384\n",
      "epoch 15460: train loss: 0.1063359641177331, test loss: 0.2649168898930532\n",
      "epoch 15461: train loss: 0.10633425724905106, test loss: 0.26491749122186964\n",
      "epoch 15462: train loss: 0.10633255052685911, test loss: 0.26491809258377985\n",
      "epoch 15463: train loss: 0.10633084395113414, test loss: 0.2649186939787759\n",
      "epoch 15464: train loss: 0.10632913752185306, test loss: 0.26491929540684994\n",
      "epoch 15465: train loss: 0.10632743123899274, test loss: 0.26491989686799416\n",
      "epoch 15466: train loss: 0.10632572510253008, test loss: 0.2649204983622006\n",
      "epoch 15467: train loss: 0.10632401911244198, test loss: 0.26492109988946155\n",
      "epoch 15468: train loss: 0.10632231326870531, test loss: 0.2649217014497692\n",
      "epoch 15469: train loss: 0.10632060757129703, test loss: 0.2649223030431154\n",
      "epoch 15470: train loss: 0.10631890202019402, test loss: 0.26492290466949253\n",
      "epoch 15471: train loss: 0.10631719661537321, test loss: 0.2649235063288928\n",
      "epoch 15472: train loss: 0.10631549135681154, test loss: 0.2649241080213081\n",
      "epoch 15473: train loss: 0.1063137862444859, test loss: 0.2649247097467309\n",
      "epoch 15474: train loss: 0.10631208127837322, test loss: 0.26492531150515297\n",
      "epoch 15475: train loss: 0.10631037645845051, test loss: 0.26492591329656684\n",
      "epoch 15476: train loss: 0.10630867178469464, test loss: 0.2649265151209645\n",
      "epoch 15477: train loss: 0.10630696725708262, test loss: 0.2649271169783382\n",
      "epoch 15478: train loss: 0.10630526287559133, test loss: 0.26492771886867994\n",
      "epoch 15479: train loss: 0.1063035586401978, test loss: 0.26492832079198203\n",
      "epoch 15480: train loss: 0.10630185455087897, test loss: 0.26492892274823654\n",
      "epoch 15481: train loss: 0.10630015060761178, test loss: 0.2649295247374357\n",
      "epoch 15482: train loss: 0.10629844681037326, test loss: 0.2649301267595717\n",
      "epoch 15483: train loss: 0.10629674315914038, test loss: 0.26493072881463675\n",
      "epoch 15484: train loss: 0.10629503965389007, test loss: 0.26493133090262283\n",
      "epoch 15485: train loss: 0.10629333629459939, test loss: 0.26493193302352236\n",
      "epoch 15486: train loss: 0.1062916330812453, test loss: 0.26493253517732734\n",
      "epoch 15487: train loss: 0.10628993001380481, test loss: 0.26493313736403\n",
      "epoch 15488: train loss: 0.10628822709225492, test loss: 0.26493373958362265\n",
      "epoch 15489: train loss: 0.10628652431657265, test loss: 0.2649343418360972\n",
      "epoch 15490: train loss: 0.106284821686735, test loss: 0.2649349441214461\n",
      "epoch 15491: train loss: 0.10628311920271902, test loss: 0.2649355464396614\n",
      "epoch 15492: train loss: 0.10628141686450174, test loss: 0.2649361487907353\n",
      "epoch 15493: train loss: 0.10627971467206017, test loss: 0.2649367511746601\n",
      "epoch 15494: train loss: 0.10627801262537132, test loss: 0.26493735359142784\n",
      "epoch 15495: train loss: 0.10627631072441228, test loss: 0.2649379560410308\n",
      "epoch 15496: train loss: 0.10627460896916008, test loss: 0.26493855852346115\n",
      "epoch 15497: train loss: 0.10627290735959179, test loss: 0.2649391610387111\n",
      "epoch 15498: train loss: 0.10627120589568444, test loss: 0.26493976358677296\n",
      "epoch 15499: train loss: 0.10626950457741512, test loss: 0.2649403661676388\n",
      "epoch 15500: train loss: 0.10626780340476084, test loss: 0.26494096878130075\n",
      "epoch 15501: train loss: 0.10626610237769875, test loss: 0.26494157142775115\n",
      "epoch 15502: train loss: 0.10626440149620589, test loss: 0.2649421741069823\n",
      "epoch 15503: train loss: 0.10626270076025936, test loss: 0.2649427768189861\n",
      "epoch 15504: train loss: 0.10626100016983621, test loss: 0.264943379563755\n",
      "epoch 15505: train loss: 0.1062592997249136, test loss: 0.2649439823412813\n",
      "epoch 15506: train loss: 0.10625759942546857, test loss: 0.264944585151557\n",
      "epoch 15507: train loss: 0.10625589927147823, test loss: 0.2649451879945744\n",
      "epoch 15508: train loss: 0.10625419926291972, test loss: 0.26494579087032566\n",
      "epoch 15509: train loss: 0.1062524993997701, test loss: 0.26494639377880314\n",
      "epoch 15510: train loss: 0.10625079968200658, test loss: 0.26494699671999894\n",
      "epoch 15511: train loss: 0.10624910010960621, test loss: 0.26494759969390536\n",
      "epoch 15512: train loss: 0.10624740068254614, test loss: 0.2649482027005146\n",
      "epoch 15513: train loss: 0.10624570140080351, test loss: 0.2649488057398189\n",
      "epoch 15514: train loss: 0.10624400226435543, test loss: 0.2649494088118104\n",
      "epoch 15515: train loss: 0.10624230327317909, test loss: 0.2649500119164815\n",
      "epoch 15516: train loss: 0.10624060442725164, test loss: 0.26495061505382433\n",
      "epoch 15517: train loss: 0.10623890572655018, test loss: 0.2649512182238311\n",
      "epoch 15518: train loss: 0.10623720717105194, test loss: 0.2649518214264941\n",
      "epoch 15519: train loss: 0.10623550876073402, test loss: 0.26495242466180563\n",
      "epoch 15520: train loss: 0.10623381049557362, test loss: 0.2649530279297578\n",
      "epoch 15521: train loss: 0.10623211237554792, test loss: 0.26495363123034293\n",
      "epoch 15522: train loss: 0.1062304144006341, test loss: 0.26495423456355327\n",
      "epoch 15523: train loss: 0.10622871657080936, test loss: 0.264954837929381\n",
      "epoch 15524: train loss: 0.10622701888605085, test loss: 0.2649554413278185\n",
      "epoch 15525: train loss: 0.1062253213463358, test loss: 0.26495604475885787\n",
      "epoch 15526: train loss: 0.10622362395164141, test loss: 0.2649566482224915\n",
      "epoch 15527: train loss: 0.10622192670194487, test loss: 0.2649572517187115\n",
      "epoch 15528: train loss: 0.10622022959722338, test loss: 0.26495785524751025\n",
      "epoch 15529: train loss: 0.10621853263745422, test loss: 0.26495845880888\n",
      "epoch 15530: train loss: 0.10621683582261453, test loss: 0.2649590624028129\n",
      "epoch 15531: train loss: 0.10621513915268156, test loss: 0.2649596660293014\n",
      "epoch 15532: train loss: 0.10621344262763259, test loss: 0.2649602696883376\n",
      "epoch 15533: train loss: 0.10621174624744478, test loss: 0.2649608733799138\n",
      "epoch 15534: train loss: 0.10621005001209545, test loss: 0.2649614771040223\n",
      "epoch 15535: train loss: 0.10620835392156179, test loss: 0.26496208086065537\n",
      "epoch 15536: train loss: 0.10620665797582106, test loss: 0.2649626846498052\n",
      "epoch 15537: train loss: 0.10620496217485055, test loss: 0.2649632884714642\n",
      "epoch 15538: train loss: 0.10620326651862749, test loss: 0.2649638923256246\n",
      "epoch 15539: train loss: 0.10620157100712917, test loss: 0.2649644962122787\n",
      "epoch 15540: train loss: 0.10619987564033283, test loss: 0.26496510013141866\n",
      "epoch 15541: train loss: 0.10619818041821577, test loss: 0.2649657040830369\n",
      "epoch 15542: train loss: 0.10619648534075526, test loss: 0.2649663080671256\n",
      "epoch 15543: train loss: 0.1061947904079286, test loss: 0.264966912083677\n",
      "epoch 15544: train loss: 0.1061930956197131, test loss: 0.26496751613268366\n",
      "epoch 15545: train loss: 0.10619140097608604, test loss: 0.26496812021413757\n",
      "epoch 15546: train loss: 0.10618970647702469, test loss: 0.26496872432803104\n",
      "epoch 15547: train loss: 0.10618801212250639, test loss: 0.2649693284743566\n",
      "epoch 15548: train loss: 0.10618631791250847, test loss: 0.26496993265310637\n",
      "epoch 15549: train loss: 0.10618462384700819, test loss: 0.26497053686427263\n",
      "epoch 15550: train loss: 0.10618292992598294, test loss: 0.2649711411078478\n",
      "epoch 15551: train loss: 0.10618123614941002, test loss: 0.26497174538382395\n",
      "epoch 15552: train loss: 0.10617954251726675, test loss: 0.26497234969219363\n",
      "epoch 15553: train loss: 0.10617784902953048, test loss: 0.2649729540329492\n",
      "epoch 15554: train loss: 0.10617615568617854, test loss: 0.26497355840608255\n",
      "epoch 15555: train loss: 0.10617446248718831, test loss: 0.26497416281158637\n",
      "epoch 15556: train loss: 0.1061727694325371, test loss: 0.2649747672494528\n",
      "epoch 15557: train loss: 0.10617107652220231, test loss: 0.2649753717196743\n",
      "epoch 15558: train loss: 0.10616938375616129, test loss: 0.264975976222243\n",
      "epoch 15559: train loss: 0.10616769113439138, test loss: 0.2649765807571513\n",
      "epoch 15560: train loss: 0.10616599865686999, test loss: 0.2649771853243915\n",
      "epoch 15561: train loss: 0.10616430632357449, test loss: 0.26497778992395593\n",
      "epoch 15562: train loss: 0.10616261413448225, test loss: 0.26497839455583694\n",
      "epoch 15563: train loss: 0.10616092208957068, test loss: 0.2649789992200268\n",
      "epoch 15564: train loss: 0.10615923018881712, test loss: 0.26497960391651787\n",
      "epoch 15565: train loss: 0.10615753843219902, test loss: 0.2649802086453024\n",
      "epoch 15566: train loss: 0.10615584681969378, test loss: 0.2649808134063728\n",
      "epoch 15567: train loss: 0.10615415535127884, test loss: 0.26498141819972143\n",
      "epoch 15568: train loss: 0.10615246402693153, test loss: 0.2649820230253405\n",
      "epoch 15569: train loss: 0.10615077284662933, test loss: 0.26498262788322247\n",
      "epoch 15570: train loss: 0.10614908181034963, test loss: 0.26498323277335956\n",
      "epoch 15571: train loss: 0.10614739091806989, test loss: 0.2649838376957442\n",
      "epoch 15572: train loss: 0.10614570016976752, test loss: 0.2649844426503686\n",
      "epoch 15573: train loss: 0.10614400956541997, test loss: 0.2649850476372253\n",
      "epoch 15574: train loss: 0.10614231910500468, test loss: 0.26498565265630647\n",
      "epoch 15575: train loss: 0.10614062878849911, test loss: 0.2649862577076045\n",
      "epoch 15576: train loss: 0.1061389386158807, test loss: 0.2649868627911118\n",
      "epoch 15577: train loss: 0.10613724858712689, test loss: 0.26498746790682054\n",
      "epoch 15578: train loss: 0.10613555870221521, test loss: 0.2649880730547233\n",
      "epoch 15579: train loss: 0.10613386896112306, test loss: 0.26498867823481237\n",
      "epoch 15580: train loss: 0.10613217936382795, test loss: 0.26498928344708\n",
      "epoch 15581: train loss: 0.10613048991030734, test loss: 0.2649898886915186\n",
      "epoch 15582: train loss: 0.10612880060053871, test loss: 0.2649904939681206\n",
      "epoch 15583: train loss: 0.10612711143449959, test loss: 0.26499109927687814\n",
      "epoch 15584: train loss: 0.10612542241216744, test loss: 0.2649917046177838\n",
      "epoch 15585: train loss: 0.10612373353351977, test loss: 0.2649923099908299\n",
      "epoch 15586: train loss: 0.10612204479853406, test loss: 0.2649929153960088\n",
      "epoch 15587: train loss: 0.10612035620718785, test loss: 0.26499352083331273\n",
      "epoch 15588: train loss: 0.10611866775945863, test loss: 0.2649941263027342\n",
      "epoch 15589: train loss: 0.10611697945532395, test loss: 0.26499473180426564\n",
      "epoch 15590: train loss: 0.1061152912947613, test loss: 0.2649953373378992\n",
      "epoch 15591: train loss: 0.10611360327774821, test loss: 0.2649959429036275\n",
      "epoch 15592: train loss: 0.10611191540426226, test loss: 0.2649965485014427\n",
      "epoch 15593: train loss: 0.10611022767428095, test loss: 0.26499715413133723\n",
      "epoch 15594: train loss: 0.10610854008778181, test loss: 0.2649977597933036\n",
      "epoch 15595: train loss: 0.10610685264474241, test loss: 0.26499836548733413\n",
      "epoch 15596: train loss: 0.10610516534514033, test loss: 0.2649989712134211\n",
      "epoch 15597: train loss: 0.1061034781889531, test loss: 0.26499957697155696\n",
      "epoch 15598: train loss: 0.10610179117615827, test loss: 0.26500018276173415\n",
      "epoch 15599: train loss: 0.10610010430673344, test loss: 0.26500078858394494\n",
      "epoch 15600: train loss: 0.10609841758065616, test loss: 0.2650013944381818\n",
      "epoch 15601: train loss: 0.10609673099790402, test loss: 0.2650020003244372\n",
      "epoch 15602: train loss: 0.1060950445584546, test loss: 0.26500260624270333\n",
      "epoch 15603: train loss: 0.10609335826228553, test loss: 0.2650032121929728\n",
      "epoch 15604: train loss: 0.10609167210937433, test loss: 0.2650038181752379\n",
      "epoch 15605: train loss: 0.10608998609969861, test loss: 0.2650044241894909\n",
      "epoch 15606: train loss: 0.10608830023323602, test loss: 0.26500503023572436\n",
      "epoch 15607: train loss: 0.10608661450996416, test loss: 0.26500563631393076\n",
      "epoch 15608: train loss: 0.10608492892986061, test loss: 0.26500624242410226\n",
      "epoch 15609: train loss: 0.10608324349290303, test loss: 0.2650068485662315\n",
      "epoch 15610: train loss: 0.10608155819906902, test loss: 0.2650074547403108\n",
      "epoch 15611: train loss: 0.10607987304833622, test loss: 0.26500806094633256\n",
      "epoch 15612: train loss: 0.10607818804068224, test loss: 0.2650086671842891\n",
      "epoch 15613: train loss: 0.1060765031760847, test loss: 0.26500927345417297\n",
      "epoch 15614: train loss: 0.10607481845452132, test loss: 0.26500987975597656\n",
      "epoch 15615: train loss: 0.10607313387596971, test loss: 0.26501048608969224\n",
      "epoch 15616: train loss: 0.10607144944040751, test loss: 0.26501109245531246\n",
      "epoch 15617: train loss: 0.1060697651478124, test loss: 0.2650116988528296\n",
      "epoch 15618: train loss: 0.10606808099816202, test loss: 0.2650123052822362\n",
      "epoch 15619: train loss: 0.10606639699143404, test loss: 0.26501291174352454\n",
      "epoch 15620: train loss: 0.10606471312760617, test loss: 0.26501351823668706\n",
      "epoch 15621: train loss: 0.10606302940665603, test loss: 0.26501412476171626\n",
      "epoch 15622: train loss: 0.10606134582856136, test loss: 0.2650147313186046\n",
      "epoch 15623: train loss: 0.10605966239329982, test loss: 0.26501533790734444\n",
      "epoch 15624: train loss: 0.10605797910084909, test loss: 0.26501594452792815\n",
      "epoch 15625: train loss: 0.1060562959511869, test loss: 0.26501655118034834\n",
      "epoch 15626: train loss: 0.10605461294429094, test loss: 0.2650171578645972\n",
      "epoch 15627: train loss: 0.1060529300801389, test loss: 0.26501776458066734\n",
      "epoch 15628: train loss: 0.10605124735870852, test loss: 0.26501837132855127\n",
      "epoch 15629: train loss: 0.10604956477997753, test loss: 0.26501897810824127\n",
      "epoch 15630: train loss: 0.1060478823439236, test loss: 0.2650195849197299\n",
      "epoch 15631: train loss: 0.1060462000505245, test loss: 0.26502019176300945\n",
      "epoch 15632: train loss: 0.10604451789975793, test loss: 0.2650207986380726\n",
      "epoch 15633: train loss: 0.10604283589160168, test loss: 0.2650214055449116\n",
      "epoch 15634: train loss: 0.10604115402603345, test loss: 0.265022012483519\n",
      "epoch 15635: train loss: 0.10603947230303098, test loss: 0.26502261945388716\n",
      "epoch 15636: train loss: 0.10603779072257204, test loss: 0.2650232264560086\n",
      "epoch 15637: train loss: 0.10603610928463442, test loss: 0.2650238334898758\n",
      "epoch 15638: train loss: 0.10603442798919582, test loss: 0.2650244405554812\n",
      "epoch 15639: train loss: 0.10603274683623404, test loss: 0.26502504765281726\n",
      "epoch 15640: train loss: 0.10603106582572686, test loss: 0.26502565478187645\n",
      "epoch 15641: train loss: 0.10602938495765205, test loss: 0.2650262619426511\n",
      "epoch 15642: train loss: 0.10602770423198742, test loss: 0.26502686913513396\n",
      "epoch 15643: train loss: 0.1060260236487107, test loss: 0.2650274763593172\n",
      "epoch 15644: train loss: 0.10602434320779969, test loss: 0.2650280836151935\n",
      "epoch 15645: train loss: 0.10602266290923222, test loss: 0.26502869090275516\n",
      "epoch 15646: train loss: 0.10602098275298605, test loss: 0.2650292982219948\n",
      "epoch 15647: train loss: 0.10601930273903905, test loss: 0.2650299055729048\n",
      "epoch 15648: train loss: 0.106017622867369, test loss: 0.2650305129554777\n",
      "epoch 15649: train loss: 0.1060159431379537, test loss: 0.265031120369706\n",
      "epoch 15650: train loss: 0.10601426355077098, test loss: 0.26503172781558204\n",
      "epoch 15651: train loss: 0.1060125841057987, test loss: 0.2650323352930984\n",
      "epoch 15652: train loss: 0.10601090480301462, test loss: 0.26503294280224754\n",
      "epoch 15653: train loss: 0.10600922564239663, test loss: 0.2650335503430219\n",
      "epoch 15654: train loss: 0.10600754662392257, test loss: 0.2650341579154142\n",
      "epoch 15655: train loss: 0.10600586774757026, test loss: 0.26503476551941657\n",
      "epoch 15656: train loss: 0.10600418901331757, test loss: 0.2650353731550218\n",
      "epoch 15657: train loss: 0.10600251042114234, test loss: 0.26503598082222224\n",
      "epoch 15658: train loss: 0.10600083197102247, test loss: 0.26503658852101036\n",
      "epoch 15659: train loss: 0.10599915366293577, test loss: 0.2650371962513788\n",
      "epoch 15660: train loss: 0.10599747549686014, test loss: 0.26503780401331983\n",
      "epoch 15661: train loss: 0.10599579747277349, test loss: 0.2650384118068261\n",
      "epoch 15662: train loss: 0.10599411959065363, test loss: 0.26503901963189014\n",
      "epoch 15663: train loss: 0.10599244185047851, test loss: 0.26503962748850435\n",
      "epoch 15664: train loss: 0.10599076425222596, test loss: 0.2650402353766614\n",
      "epoch 15665: train loss: 0.10598908679587392, test loss: 0.26504084329635363\n",
      "epoch 15666: train loss: 0.10598740948140026, test loss: 0.2650414512475735\n",
      "epoch 15667: train loss: 0.1059857323087829, test loss: 0.26504205923031376\n",
      "epoch 15668: train loss: 0.10598405527799977, test loss: 0.26504266724456677\n",
      "epoch 15669: train loss: 0.10598237838902876, test loss: 0.26504327529032495\n",
      "epoch 15670: train loss: 0.1059807016418478, test loss: 0.265043883367581\n",
      "epoch 15671: train loss: 0.10597902503643479, test loss: 0.26504449147632736\n",
      "epoch 15672: train loss: 0.10597734857276768, test loss: 0.2650450996165566\n",
      "epoch 15673: train loss: 0.1059756722508244, test loss: 0.2650457077882611\n",
      "epoch 15674: train loss: 0.10597399607058289, test loss: 0.2650463159914335\n",
      "epoch 15675: train loss: 0.1059723200320211, test loss: 0.2650469242260663\n",
      "epoch 15676: train loss: 0.10597064413511698, test loss: 0.26504753249215196\n",
      "epoch 15677: train loss: 0.10596896837984845, test loss: 0.2650481407896832\n",
      "epoch 15678: train loss: 0.10596729276619353, test loss: 0.26504874911865234\n",
      "epoch 15679: train loss: 0.10596561729413013, test loss: 0.26504935747905195\n",
      "epoch 15680: train loss: 0.10596394196363622, test loss: 0.2650499658708746\n",
      "epoch 15681: train loss: 0.10596226677468984, test loss: 0.26505057429411294\n",
      "epoch 15682: train loss: 0.10596059172726889, test loss: 0.2650511827487594\n",
      "epoch 15683: train loss: 0.10595891682135138, test loss: 0.26505179123480643\n",
      "epoch 15684: train loss: 0.10595724205691529, test loss: 0.2650523997522467\n",
      "epoch 15685: train loss: 0.10595556743393862, test loss: 0.26505300830107253\n",
      "epoch 15686: train loss: 0.10595389295239936, test loss: 0.2650536168812769\n",
      "epoch 15687: train loss: 0.10595221861227555, test loss: 0.2650542254928519\n",
      "epoch 15688: train loss: 0.10595054441354516, test loss: 0.26505483413579045\n",
      "epoch 15689: train loss: 0.10594887035618622, test loss: 0.2650554428100848\n",
      "epoch 15690: train loss: 0.10594719644017672, test loss: 0.26505605151572764\n",
      "epoch 15691: train loss: 0.1059455226654947, test loss: 0.2650566602527115\n",
      "epoch 15692: train loss: 0.1059438490321182, test loss: 0.265057269021029\n",
      "epoch 15693: train loss: 0.10594217554002522, test loss: 0.2650578778206725\n",
      "epoch 15694: train loss: 0.10594050218919385, test loss: 0.26505848665163484\n",
      "epoch 15695: train loss: 0.10593882897960206, test loss: 0.2650590955139084\n",
      "epoch 15696: train loss: 0.10593715591122795, test loss: 0.2650597044074858\n",
      "epoch 15697: train loss: 0.10593548298404955, test loss: 0.26506031333235947\n",
      "epoch 15698: train loss: 0.10593381019804489, test loss: 0.2650609222885221\n",
      "epoch 15699: train loss: 0.10593213755319213, test loss: 0.26506153127596627\n",
      "epoch 15700: train loss: 0.1059304650494692, test loss: 0.2650621402946846\n",
      "epoch 15701: train loss: 0.10592879268685423, test loss: 0.2650627493446695\n",
      "epoch 15702: train loss: 0.10592712046532533, test loss: 0.26506335842591355\n",
      "epoch 15703: train loss: 0.10592544838486057, test loss: 0.26506396753840955\n",
      "epoch 15704: train loss: 0.10592377644543795, test loss: 0.2650645766821498\n",
      "epoch 15705: train loss: 0.10592210464703566, test loss: 0.265065185857127\n",
      "epoch 15706: train loss: 0.10592043298963176, test loss: 0.26506579506333383\n",
      "epoch 15707: train loss: 0.10591876147320438, test loss: 0.26506640430076267\n",
      "epoch 15708: train loss: 0.10591709009773155, test loss: 0.26506701356940615\n",
      "epoch 15709: train loss: 0.10591541886319143, test loss: 0.26506762286925706\n",
      "epoch 15710: train loss: 0.10591374776956214, test loss: 0.26506823220030773\n",
      "epoch 15711: train loss: 0.10591207681682178, test loss: 0.2650688415625509\n",
      "epoch 15712: train loss: 0.10591040600494846, test loss: 0.2650694509559791\n",
      "epoch 15713: train loss: 0.10590873533392035, test loss: 0.26507006038058484\n",
      "epoch 15714: train loss: 0.10590706480371555, test loss: 0.26507066983636096\n",
      "epoch 15715: train loss: 0.1059053944143122, test loss: 0.26507127932329977\n",
      "epoch 15716: train loss: 0.10590372416568845, test loss: 0.26507188884139404\n",
      "epoch 15717: train loss: 0.10590205405782246, test loss: 0.26507249839063635\n",
      "epoch 15718: train loss: 0.10590038409069237, test loss: 0.26507310797101924\n",
      "epoch 15719: train loss: 0.10589871426427636, test loss: 0.2650737175825353\n",
      "epoch 15720: train loss: 0.10589704457855252, test loss: 0.26507432722517726\n",
      "epoch 15721: train loss: 0.1058953750334991, test loss: 0.26507493689893763\n",
      "epoch 15722: train loss: 0.10589370562909423, test loss: 0.26507554660380905\n",
      "epoch 15723: train loss: 0.1058920363653161, test loss: 0.2650761563397841\n",
      "epoch 15724: train loss: 0.10589036724214289, test loss: 0.2650767661068554\n",
      "epoch 15725: train loss: 0.10588869825955277, test loss: 0.26507737590501557\n",
      "epoch 15726: train loss: 0.10588702941752393, test loss: 0.2650779857342571\n",
      "epoch 15727: train loss: 0.1058853607160346, test loss: 0.26507859559457286\n",
      "epoch 15728: train loss: 0.10588369215506298, test loss: 0.2650792054859553\n",
      "epoch 15729: train loss: 0.10588202373458724, test loss: 0.2650798154083971\n",
      "epoch 15730: train loss: 0.1058803554545856, test loss: 0.26508042536189086\n",
      "epoch 15731: train loss: 0.1058786873150363, test loss: 0.2650810353464292\n",
      "epoch 15732: train loss: 0.10587701931591752, test loss: 0.2650816453620047\n",
      "epoch 15733: train loss: 0.1058753514572075, test loss: 0.26508225540860997\n",
      "epoch 15734: train loss: 0.10587368373888449, test loss: 0.2650828654862378\n",
      "epoch 15735: train loss: 0.1058720161609267, test loss: 0.2650834755948807\n",
      "epoch 15736: train loss: 0.10587034872331237, test loss: 0.26508408573453135\n",
      "epoch 15737: train loss: 0.1058686814260198, test loss: 0.2650846959051823\n",
      "epoch 15738: train loss: 0.10586701426902714, test loss: 0.2650853061068263\n",
      "epoch 15739: train loss: 0.10586534725231271, test loss: 0.26508591633945583\n",
      "epoch 15740: train loss: 0.10586368037585477, test loss: 0.26508652660306375\n",
      "epoch 15741: train loss: 0.10586201363963156, test loss: 0.2650871368976425\n",
      "epoch 15742: train loss: 0.10586034704362132, test loss: 0.2650877472231848\n",
      "epoch 15743: train loss: 0.10585868058780239, test loss: 0.2650883575796833\n",
      "epoch 15744: train loss: 0.10585701427215302, test loss: 0.26508896796713066\n",
      "epoch 15745: train loss: 0.10585534809665145, test loss: 0.26508957838551944\n",
      "epoch 15746: train loss: 0.10585368206127603, test loss: 0.26509018883484237\n",
      "epoch 15747: train loss: 0.10585201616600502, test loss: 0.2650907993150922\n",
      "epoch 15748: train loss: 0.10585035041081671, test loss: 0.26509140982626134\n",
      "epoch 15749: train loss: 0.10584868479568943, test loss: 0.2650920203683427\n",
      "epoch 15750: train loss: 0.10584701932060145, test loss: 0.2650926309413287\n",
      "epoch 15751: train loss: 0.10584535398553112, test loss: 0.2650932415452121\n",
      "epoch 15752: train loss: 0.10584368879045673, test loss: 0.26509385217998566\n",
      "epoch 15753: train loss: 0.10584202373535657, test loss: 0.2650944628456419\n",
      "epoch 15754: train loss: 0.10584035882020902, test loss: 0.2650950735421735\n",
      "epoch 15755: train loss: 0.10583869404499242, test loss: 0.26509568426957325\n",
      "epoch 15756: train loss: 0.10583702940968505, test loss: 0.2650962950278337\n",
      "epoch 15757: train loss: 0.10583536491426529, test loss: 0.2650969058169474\n",
      "epoch 15758: train loss: 0.10583370055871147, test loss: 0.26509751663690734\n",
      "epoch 15759: train loss: 0.10583203634300191, test loss: 0.26509812748770595\n",
      "epoch 15760: train loss: 0.105830372267115, test loss: 0.265098738369336\n",
      "epoch 15761: train loss: 0.10582870833102909, test loss: 0.26509934928179013\n",
      "epoch 15762: train loss: 0.10582704453472257, test loss: 0.26509996022506094\n",
      "epoch 15763: train loss: 0.10582538087817374, test loss: 0.2651005711991413\n",
      "epoch 15764: train loss: 0.10582371736136104, test loss: 0.26510118220402373\n",
      "epoch 15765: train loss: 0.10582205398426282, test loss: 0.265101793239701\n",
      "epoch 15766: train loss: 0.10582039074685744, test loss: 0.2651024043061657\n",
      "epoch 15767: train loss: 0.10581872764912333, test loss: 0.2651030154034107\n",
      "epoch 15768: train loss: 0.10581706469103888, test loss: 0.2651036265314285\n",
      "epoch 15769: train loss: 0.10581540187258245, test loss: 0.2651042376902118\n",
      "epoch 15770: train loss: 0.10581373919373246, test loss: 0.2651048488797534\n",
      "epoch 15771: train loss: 0.10581207665446733, test loss: 0.26510546010004593\n",
      "epoch 15772: train loss: 0.10581041425476545, test loss: 0.26510607135108205\n",
      "epoch 15773: train loss: 0.10580875199460525, test loss: 0.26510668263285453\n",
      "epoch 15774: train loss: 0.10580708987396513, test loss: 0.26510729394535604\n",
      "epoch 15775: train loss: 0.10580542789282354, test loss: 0.26510790528857925\n",
      "epoch 15776: train loss: 0.10580376605115888, test loss: 0.26510851666251695\n",
      "epoch 15777: train loss: 0.10580210434894961, test loss: 0.2651091280671618\n",
      "epoch 15778: train loss: 0.10580044278617418, test loss: 0.2651097395025064\n",
      "epoch 15779: train loss: 0.10579878136281103, test loss: 0.2651103509685435\n",
      "epoch 15780: train loss: 0.10579712007883856, test loss: 0.265110962465266\n",
      "epoch 15781: train loss: 0.10579545893423528, test loss: 0.2651115739926664\n",
      "epoch 15782: train loss: 0.1057937979289796, test loss: 0.26511218555073746\n",
      "epoch 15783: train loss: 0.10579213706305005, test loss: 0.2651127971394719\n",
      "epoch 15784: train loss: 0.10579047633642504, test loss: 0.2651134087588625\n",
      "epoch 15785: train loss: 0.10578881574908305, test loss: 0.2651140204089019\n",
      "epoch 15786: train loss: 0.10578715530100256, test loss: 0.2651146320895828\n",
      "epoch 15787: train loss: 0.10578549499216208, test loss: 0.265115243800898\n",
      "epoch 15788: train loss: 0.10578383482254008, test loss: 0.2651158555428401\n",
      "epoch 15789: train loss: 0.10578217479211505, test loss: 0.26511646731540206\n",
      "epoch 15790: train loss: 0.10578051490086544, test loss: 0.2651170791185763\n",
      "epoch 15791: train loss: 0.10577885514876986, test loss: 0.26511769095235577\n",
      "epoch 15792: train loss: 0.10577719553580668, test loss: 0.2651183028167331\n",
      "epoch 15793: train loss: 0.10577553606195454, test loss: 0.26511891471170107\n",
      "epoch 15794: train loss: 0.10577387672719187, test loss: 0.26511952663725247\n",
      "epoch 15795: train loss: 0.10577221753149722, test loss: 0.2651201385933798\n",
      "epoch 15796: train loss: 0.10577055847484909, test loss: 0.26512075058007606\n",
      "epoch 15797: train loss: 0.10576889955722604, test loss: 0.2651213625973339\n",
      "epoch 15798: train loss: 0.10576724077860661, test loss: 0.265121974645146\n",
      "epoch 15799: train loss: 0.1057655821389693, test loss: 0.26512258672350514\n",
      "epoch 15800: train loss: 0.10576392363829268, test loss: 0.2651231988324041\n",
      "epoch 15801: train loss: 0.1057622652765553, test loss: 0.26512381097183557\n",
      "epoch 15802: train loss: 0.10576060705373568, test loss: 0.26512442314179224\n",
      "epoch 15803: train loss: 0.10575894896981243, test loss: 0.2651250353422671\n",
      "epoch 15804: train loss: 0.10575729102476407, test loss: 0.2651256475732526\n",
      "epoch 15805: train loss: 0.10575563321856919, test loss: 0.2651262598347417\n",
      "epoch 15806: train loss: 0.10575397555120634, test loss: 0.265126872126727\n",
      "epoch 15807: train loss: 0.10575231802265411, test loss: 0.26512748444920137\n",
      "epoch 15808: train loss: 0.10575066063289107, test loss: 0.26512809680215754\n",
      "epoch 15809: train loss: 0.10574900338189583, test loss: 0.26512870918558823\n",
      "epoch 15810: train loss: 0.10574734626964698, test loss: 0.2651293215994863\n",
      "epoch 15811: train loss: 0.10574568929612309, test loss: 0.2651299340438444\n",
      "epoch 15812: train loss: 0.10574403246130276, test loss: 0.26513054651865536\n",
      "epoch 15813: train loss: 0.1057423757651646, test loss: 0.2651311590239119\n",
      "epoch 15814: train loss: 0.10574071920768724, test loss: 0.2651317715596069\n",
      "epoch 15815: train loss: 0.10573906278884924, test loss: 0.2651323841257328\n",
      "epoch 15816: train loss: 0.10573740650862931, test loss: 0.26513299672228285\n",
      "epoch 15817: train loss: 0.10573575036700598, test loss: 0.26513360934924946\n",
      "epoch 15818: train loss: 0.10573409436395792, test loss: 0.2651342220066255\n",
      "epoch 15819: train loss: 0.10573243849946377, test loss: 0.26513483469440385\n",
      "epoch 15820: train loss: 0.10573078277350216, test loss: 0.2651354474125772\n",
      "epoch 15821: train loss: 0.10572912718605171, test loss: 0.2651360601611384\n",
      "epoch 15822: train loss: 0.10572747173709111, test loss: 0.26513667294008003\n",
      "epoch 15823: train loss: 0.10572581642659898, test loss: 0.26513728574939505\n",
      "epoch 15824: train loss: 0.10572416125455397, test loss: 0.26513789858907627\n",
      "epoch 15825: train loss: 0.10572250622093474, test loss: 0.2651385114591163\n",
      "epoch 15826: train loss: 0.10572085132572, test loss: 0.26513912435950815\n",
      "epoch 15827: train loss: 0.10571919656888838, test loss: 0.2651397372902444\n",
      "epoch 15828: train loss: 0.10571754195041853, test loss: 0.265140350251318\n",
      "epoch 15829: train loss: 0.10571588747028919, test loss: 0.2651409632427217\n",
      "epoch 15830: train loss: 0.10571423312847901, test loss: 0.26514157626444823\n",
      "epoch 15831: train loss: 0.10571257892496667, test loss: 0.2651421893164904\n",
      "epoch 15832: train loss: 0.10571092485973091, test loss: 0.26514280239884114\n",
      "epoch 15833: train loss: 0.10570927093275038, test loss: 0.26514341551149306\n",
      "epoch 15834: train loss: 0.1057076171440038, test loss: 0.26514402865443903\n",
      "epoch 15835: train loss: 0.10570596349346985, test loss: 0.26514464182767195\n",
      "epoch 15836: train loss: 0.1057043099811273, test loss: 0.26514525503118447\n",
      "epoch 15837: train loss: 0.10570265660695481, test loss: 0.2651458682649696\n",
      "epoch 15838: train loss: 0.10570100337093115, test loss: 0.26514648152901993\n",
      "epoch 15839: train loss: 0.10569935027303501, test loss: 0.2651470948233284\n",
      "epoch 15840: train loss: 0.10569769731324512, test loss: 0.2651477081478877\n",
      "epoch 15841: train loss: 0.10569604449154024, test loss: 0.26514832150269085\n",
      "epoch 15842: train loss: 0.10569439180789908, test loss: 0.2651489348877305\n",
      "epoch 15843: train loss: 0.10569273926230043, test loss: 0.2651495483029994\n",
      "epoch 15844: train loss: 0.10569108685472299, test loss: 0.2651501617484906\n",
      "epoch 15845: train loss: 0.10568943458514553, test loss: 0.2651507752241967\n",
      "epoch 15846: train loss: 0.10568778245354682, test loss: 0.26515138873011085\n",
      "epoch 15847: train loss: 0.10568613045990563, test loss: 0.2651520022662254\n",
      "epoch 15848: train loss: 0.1056844786042007, test loss: 0.26515261583253347\n",
      "epoch 15849: train loss: 0.1056828268864108, test loss: 0.26515322942902786\n",
      "epoch 15850: train loss: 0.10568117530651477, test loss: 0.2651538430557014\n",
      "epoch 15851: train loss: 0.10567952386449128, test loss: 0.2651544567125469\n",
      "epoch 15852: train loss: 0.10567787256031924, test loss: 0.2651550703995571\n",
      "epoch 15853: train loss: 0.10567622139397737, test loss: 0.2651556841167249\n",
      "epoch 15854: train loss: 0.10567457036544445, test loss: 0.2651562978640433\n",
      "epoch 15855: train loss: 0.10567291947469933, test loss: 0.2651569116415049\n",
      "epoch 15856: train loss: 0.10567126872172078, test loss: 0.2651575254491027\n",
      "epoch 15857: train loss: 0.10566961810648764, test loss: 0.26515813928682935\n",
      "epoch 15858: train loss: 0.10566796762897872, test loss: 0.2651587531546779\n",
      "epoch 15859: train loss: 0.10566631728917279, test loss: 0.2651593670526411\n",
      "epoch 15860: train loss: 0.10566466708704873, test loss: 0.26515998098071175\n",
      "epoch 15861: train loss: 0.10566301702258535, test loss: 0.2651605949388828\n",
      "epoch 15862: train loss: 0.10566136709576149, test loss: 0.265161208927147\n",
      "epoch 15863: train loss: 0.10565971730655596, test loss: 0.2651618229454973\n",
      "epoch 15864: train loss: 0.10565806765494763, test loss: 0.26516243699392644\n",
      "epoch 15865: train loss: 0.10565641814091531, test loss: 0.26516305107242744\n",
      "epoch 15866: train loss: 0.10565476876443793, test loss: 0.26516366518099294\n",
      "epoch 15867: train loss: 0.10565311952549425, test loss: 0.26516427931961595\n",
      "epoch 15868: train loss: 0.1056514704240632, test loss: 0.26516489348828925\n",
      "epoch 15869: train loss: 0.10564982146012358, test loss: 0.2651655076870058\n",
      "epoch 15870: train loss: 0.10564817263365434, test loss: 0.2651661219157584\n",
      "epoch 15871: train loss: 0.10564652394463428, test loss: 0.2651667361745399\n",
      "epoch 15872: train loss: 0.10564487539304233, test loss: 0.26516735046334317\n",
      "epoch 15873: train loss: 0.10564322697885736, test loss: 0.265167964782161\n",
      "epoch 15874: train loss: 0.10564157870205823, test loss: 0.2651685791309864\n",
      "epoch 15875: train loss: 0.1056399305626239, test loss: 0.2651691935098123\n",
      "epoch 15876: train loss: 0.1056382825605332, test loss: 0.2651698079186313\n",
      "epoch 15877: train loss: 0.10563663469576506, test loss: 0.26517042235743654\n",
      "epoch 15878: train loss: 0.10563498696829837, test loss: 0.26517103682622073\n",
      "epoch 15879: train loss: 0.10563333937811209, test loss: 0.2651716513249768\n",
      "epoch 15880: train loss: 0.10563169192518504, test loss: 0.2651722658536977\n",
      "epoch 15881: train loss: 0.10563004460949625, test loss: 0.2651728804123762\n",
      "epoch 15882: train loss: 0.10562839743102458, test loss: 0.26517349500100523\n",
      "epoch 15883: train loss: 0.10562675038974895, test loss: 0.26517410961957766\n",
      "epoch 15884: train loss: 0.10562510348564835, test loss: 0.26517472426808647\n",
      "epoch 15885: train loss: 0.10562345671870169, test loss: 0.2651753389465244\n",
      "epoch 15886: train loss: 0.10562181008888791, test loss: 0.26517595365488444\n",
      "epoch 15887: train loss: 0.10562016359618594, test loss: 0.2651765683931594\n",
      "epoch 15888: train loss: 0.10561851724057476, test loss: 0.2651771831613423\n",
      "epoch 15889: train loss: 0.10561687102203333, test loss: 0.26517779795942586\n",
      "epoch 15890: train loss: 0.1056152249405406, test loss: 0.2651784127874031\n",
      "epoch 15891: train loss: 0.10561357899607554, test loss: 0.2651790276452669\n",
      "epoch 15892: train loss: 0.10561193318861711, test loss: 0.26517964253301013\n",
      "epoch 15893: train loss: 0.1056102875181443, test loss: 0.2651802574506258\n",
      "epoch 15894: train loss: 0.10560864198463608, test loss: 0.26518087239810656\n",
      "epoch 15895: train loss: 0.10560699658807142, test loss: 0.26518148737544556\n",
      "epoch 15896: train loss: 0.10560535132842938, test loss: 0.26518210238263573\n",
      "epoch 15897: train loss: 0.10560370620568886, test loss: 0.26518271741966976\n",
      "epoch 15898: train loss: 0.1056020612198289, test loss: 0.2651833324865407\n",
      "epoch 15899: train loss: 0.10560041637082852, test loss: 0.2651839475832414\n",
      "epoch 15900: train loss: 0.1055987716586667, test loss: 0.2651845627097648\n",
      "epoch 15901: train loss: 0.10559712708332249, test loss: 0.2651851778661038\n",
      "epoch 15902: train loss: 0.10559548264477488, test loss: 0.2651857930522514\n",
      "epoch 15903: train loss: 0.10559383834300286, test loss: 0.2651864082682004\n",
      "epoch 15904: train loss: 0.10559219417798552, test loss: 0.2651870235139438\n",
      "epoch 15905: train loss: 0.10559055014970184, test loss: 0.26518763878947454\n",
      "epoch 15906: train loss: 0.10558890625813089, test loss: 0.26518825409478536\n",
      "epoch 15907: train loss: 0.10558726250325166, test loss: 0.26518886942986947\n",
      "epoch 15908: train loss: 0.10558561888504325, test loss: 0.2651894847947196\n",
      "epoch 15909: train loss: 0.10558397540348469, test loss: 0.26519010018932865\n",
      "epoch 15910: train loss: 0.10558233205855502, test loss: 0.26519071561368973\n",
      "epoch 15911: train loss: 0.10558068885023332, test loss: 0.26519133106779563\n",
      "epoch 15912: train loss: 0.10557904577849865, test loss: 0.2651919465516393\n",
      "epoch 15913: train loss: 0.10557740284333005, test loss: 0.26519256206521374\n",
      "epoch 15914: train loss: 0.10557576004470662, test loss: 0.26519317760851174\n",
      "epoch 15915: train loss: 0.10557411738260743, test loss: 0.26519379318152647\n",
      "epoch 15916: train loss: 0.10557247485701154, test loss: 0.26519440878425066\n",
      "epoch 15917: train loss: 0.10557083246789806, test loss: 0.26519502441667725\n",
      "epoch 15918: train loss: 0.10556919021524606, test loss: 0.26519564007879937\n",
      "epoch 15919: train loss: 0.10556754809903467, test loss: 0.2651962557706099\n",
      "epoch 15920: train loss: 0.10556590611924294, test loss: 0.26519687149210164\n",
      "epoch 15921: train loss: 0.10556426427585003, test loss: 0.2651974872432677\n",
      "epoch 15922: train loss: 0.105562622568835, test loss: 0.2651981030241009\n",
      "epoch 15923: train loss: 0.10556098099817696, test loss: 0.2651987188345944\n",
      "epoch 15924: train loss: 0.10555933956385505, test loss: 0.2651993346747408\n",
      "epoch 15925: train loss: 0.10555769826584843, test loss: 0.2651999505445335\n",
      "epoch 15926: train loss: 0.10555605710413615, test loss: 0.26520056644396506\n",
      "epoch 15927: train loss: 0.10555441607869737, test loss: 0.2652011823730287\n",
      "epoch 15928: train loss: 0.10555277518951126, test loss: 0.2652017983317172\n",
      "epoch 15929: train loss: 0.10555113443655692, test loss: 0.26520241432002367\n",
      "epoch 15930: train loss: 0.1055494938198135, test loss: 0.265203030337941\n",
      "epoch 15931: train loss: 0.10554785333926016, test loss: 0.2652036463854622\n",
      "epoch 15932: train loss: 0.10554621299487604, test loss: 0.26520426246258005\n",
      "epoch 15933: train loss: 0.10554457278664031, test loss: 0.26520487856928776\n",
      "epoch 15934: train loss: 0.10554293271453213, test loss: 0.26520549470557825\n",
      "epoch 15935: train loss: 0.10554129277853068, test loss: 0.26520611087144436\n",
      "epoch 15936: train loss: 0.10553965297861509, test loss: 0.2652067270668792\n",
      "epoch 15937: train loss: 0.10553801331476459, test loss: 0.2652073432918757\n",
      "epoch 15938: train loss: 0.10553637378695832, test loss: 0.2652079595464268\n",
      "epoch 15939: train loss: 0.10553473439517547, test loss: 0.2652085758305255\n",
      "epoch 15940: train loss: 0.10553309513939528, test loss: 0.26520919214416483\n",
      "epoch 15941: train loss: 0.10553145601959686, test loss: 0.26520980848733766\n",
      "epoch 15942: train loss: 0.10552981703575946, test loss: 0.2652104248600371\n",
      "epoch 15943: train loss: 0.1055281781878623, test loss: 0.26521104126225603\n",
      "epoch 15944: train loss: 0.10552653947588456, test loss: 0.2652116576939876\n",
      "epoch 15945: train loss: 0.10552490089980543, test loss: 0.2652122741552245\n",
      "epoch 15946: train loss: 0.1055232624596042, test loss: 0.26521289064596\n",
      "epoch 15947: train loss: 0.10552162415526001, test loss: 0.2652135071661871\n",
      "epoch 15948: train loss: 0.10551998598675212, test loss: 0.2652141237158985\n",
      "epoch 15949: train loss: 0.10551834795405977, test loss: 0.26521474029508746\n",
      "epoch 15950: train loss: 0.10551671005716218, test loss: 0.2652153569037468\n",
      "epoch 15951: train loss: 0.1055150722960386, test loss: 0.26521597354186965\n",
      "epoch 15952: train loss: 0.10551343467066829, test loss: 0.26521659020944904\n",
      "epoch 15953: train loss: 0.10551179718103047, test loss: 0.26521720690647793\n",
      "epoch 15954: train loss: 0.10551015982710439, test loss: 0.26521782363294916\n",
      "epoch 15955: train loss: 0.10550852260886931, test loss: 0.26521844038885595\n",
      "epoch 15956: train loss: 0.10550688552630452, test loss: 0.26521905717419114\n",
      "epoch 15957: train loss: 0.10550524857938927, test loss: 0.26521967398894797\n",
      "epoch 15958: train loss: 0.1055036117681028, test loss: 0.2652202908331192\n",
      "epoch 15959: train loss: 0.10550197509242447, test loss: 0.265220907706698\n",
      "epoch 15960: train loss: 0.10550033855233348, test loss: 0.2652215246096773\n",
      "epoch 15961: train loss: 0.10549870214780911, test loss: 0.2652221415420501\n",
      "epoch 15962: train loss: 0.10549706587883073, test loss: 0.26522275850380944\n",
      "epoch 15963: train loss: 0.10549542974537754, test loss: 0.26522337549494845\n",
      "epoch 15964: train loss: 0.10549379374742891, test loss: 0.26522399251545997\n",
      "epoch 15965: train loss: 0.10549215788496412, test loss: 0.2652246095653371\n",
      "epoch 15966: train loss: 0.10549052215796245, test loss: 0.26522522664457293\n",
      "epoch 15967: train loss: 0.10548888656640322, test loss: 0.2652258437531604\n",
      "epoch 15968: train loss: 0.10548725111026577, test loss: 0.2652264608910924\n",
      "epoch 15969: train loss: 0.10548561578952942, test loss: 0.2652270780583623\n",
      "epoch 15970: train loss: 0.10548398060417348, test loss: 0.2652276952549629\n",
      "epoch 15971: train loss: 0.10548234555417729, test loss: 0.2652283124808873\n",
      "epoch 15972: train loss: 0.10548071063952015, test loss: 0.2652289297361284\n",
      "epoch 15973: train loss: 0.10547907586018145, test loss: 0.26522954702067947\n",
      "epoch 15974: train loss: 0.10547744121614051, test loss: 0.2652301643345333\n",
      "epoch 15975: train loss: 0.10547580670737666, test loss: 0.26523078167768305\n",
      "epoch 15976: train loss: 0.1054741723338693, test loss: 0.2652313990501219\n",
      "epoch 15977: train loss: 0.10547253809559773, test loss: 0.2652320164518426\n",
      "epoch 15978: train loss: 0.10547090399254135, test loss: 0.2652326338828383\n",
      "epoch 15979: train loss: 0.1054692700246795, test loss: 0.2652332513431021\n",
      "epoch 15980: train loss: 0.10546763619199157, test loss: 0.2652338688326271\n",
      "epoch 15981: train loss: 0.10546600249445692, test loss: 0.2652344863514062\n",
      "epoch 15982: train loss: 0.10546436893205494, test loss: 0.2652351038994325\n",
      "epoch 15983: train loss: 0.10546273550476501, test loss: 0.2652357214766991\n",
      "epoch 15984: train loss: 0.10546110221256652, test loss: 0.26523633908319905\n",
      "epoch 15985: train loss: 0.10545946905543886, test loss: 0.26523695671892533\n",
      "epoch 15986: train loss: 0.1054578360333614, test loss: 0.2652375743838711\n",
      "epoch 15987: train loss: 0.10545620314631357, test loss: 0.2652381920780293\n",
      "epoch 15988: train loss: 0.10545457039427479, test loss: 0.26523880980139297\n",
      "epoch 15989: train loss: 0.10545293777722446, test loss: 0.2652394275539553\n",
      "epoch 15990: train loss: 0.10545130529514196, test loss: 0.26524004533570933\n",
      "epoch 15991: train loss: 0.10544967294800676, test loss: 0.2652406631466481\n",
      "epoch 15992: train loss: 0.10544804073579826, test loss: 0.2652412809867646\n",
      "epoch 15993: train loss: 0.10544640865849587, test loss: 0.26524189885605204\n",
      "epoch 15994: train loss: 0.10544477671607907, test loss: 0.2652425167545034\n",
      "epoch 15995: train loss: 0.10544314490852724, test loss: 0.2652431346821117\n",
      "epoch 15996: train loss: 0.10544151323581985, test loss: 0.26524375263887007\n",
      "epoch 15997: train loss: 0.10543988169793636, test loss: 0.26524437062477163\n",
      "epoch 15998: train loss: 0.1054382502948562, test loss: 0.2652449886398094\n",
      "epoch 15999: train loss: 0.1054366190265588, test loss: 0.26524560668397645\n",
      "epoch 16000: train loss: 0.10543498789302369, test loss: 0.2652462247572659\n",
      "epoch 16001: train loss: 0.10543335689423028, test loss: 0.2652468428596707\n",
      "epoch 16002: train loss: 0.10543172603015805, test loss: 0.2652474609911842\n",
      "epoch 16003: train loss: 0.10543009530078645, test loss: 0.26524807915179915\n",
      "epoch 16004: train loss: 0.10542846470609504, test loss: 0.26524869734150897\n",
      "epoch 16005: train loss: 0.1054268342460632, test loss: 0.26524931556030645\n",
      "epoch 16006: train loss: 0.10542520392067045, test loss: 0.26524993380818485\n",
      "epoch 16007: train loss: 0.10542357372989629, test loss: 0.26525055208513726\n",
      "epoch 16008: train loss: 0.10542194367372022, test loss: 0.26525117039115664\n",
      "epoch 16009: train loss: 0.10542031375212174, test loss: 0.2652517887262362\n",
      "epoch 16010: train loss: 0.10541868396508035, test loss: 0.265252407090369\n",
      "epoch 16011: train loss: 0.10541705431257554, test loss: 0.2652530254835482\n",
      "epoch 16012: train loss: 0.10541542479458683, test loss: 0.2652536439057668\n",
      "epoch 16013: train loss: 0.10541379541109377, test loss: 0.2652542623570179\n",
      "epoch 16014: train loss: 0.10541216616207585, test loss: 0.26525488083729465\n",
      "epoch 16015: train loss: 0.1054105370475126, test loss: 0.26525549934659015\n",
      "epoch 16016: train loss: 0.10540890806738354, test loss: 0.2652561178848975\n",
      "epoch 16017: train loss: 0.10540727922166822, test loss: 0.2652567364522098\n",
      "epoch 16018: train loss: 0.10540565051034616, test loss: 0.26525735504852016\n",
      "epoch 16019: train loss: 0.10540402193339694, test loss: 0.2652579736738217\n",
      "epoch 16020: train loss: 0.10540239349080009, test loss: 0.2652585923281075\n",
      "epoch 16021: train loss: 0.10540076518253513, test loss: 0.2652592110113707\n",
      "epoch 16022: train loss: 0.10539913700858168, test loss: 0.2652598297236044\n",
      "epoch 16023: train loss: 0.10539750896891925, test loss: 0.2652604484648017\n",
      "epoch 16024: train loss: 0.10539588106352746, test loss: 0.26526106723495574\n",
      "epoch 16025: train loss: 0.10539425329238579, test loss: 0.2652616860340597\n",
      "epoch 16026: train loss: 0.10539262565547389, test loss: 0.26526230486210656\n",
      "epoch 16027: train loss: 0.10539099815277131, test loss: 0.26526292371908955\n",
      "epoch 16028: train loss: 0.10538937078425765, test loss: 0.2652635426050017\n",
      "epoch 16029: train loss: 0.10538774354991251, test loss: 0.2652641615198363\n",
      "epoch 16030: train loss: 0.10538611644971542, test loss: 0.2652647804635862\n",
      "epoch 16031: train loss: 0.10538448948364604, test loss: 0.2652653994362449\n",
      "epoch 16032: train loss: 0.10538286265168394, test loss: 0.2652660184378053\n",
      "epoch 16033: train loss: 0.10538123595380874, test loss: 0.26526663746826046\n",
      "epoch 16034: train loss: 0.10537960939000005, test loss: 0.2652672565276037\n",
      "epoch 16035: train loss: 0.10537798296023744, test loss: 0.2652678756158281\n",
      "epoch 16036: train loss: 0.10537635666450061, test loss: 0.26526849473292663\n",
      "epoch 16037: train loss: 0.10537473050276913, test loss: 0.2652691138788926\n",
      "epoch 16038: train loss: 0.10537310447502264, test loss: 0.2652697330537192\n",
      "epoch 16039: train loss: 0.10537147858124075, test loss: 0.2652703522573995\n",
      "epoch 16040: train loss: 0.10536985282140313, test loss: 0.2652709714899266\n",
      "epoch 16041: train loss: 0.10536822719548943, test loss: 0.2652715907512937\n",
      "epoch 16042: train loss: 0.10536660170347924, test loss: 0.26527221004149376\n",
      "epoch 16043: train loss: 0.10536497634535223, test loss: 0.2652728293605202\n",
      "epoch 16044: train loss: 0.10536335112108809, test loss: 0.26527344870836617\n",
      "epoch 16045: train loss: 0.10536172603066646, test loss: 0.2652740680850246\n",
      "epoch 16046: train loss: 0.10536010107406697, test loss: 0.26527468749048866\n",
      "epoch 16047: train loss: 0.10535847625126933, test loss: 0.26527530692475176\n",
      "epoch 16048: train loss: 0.1053568515622532, test loss: 0.2652759263878067\n",
      "epoch 16049: train loss: 0.10535522700699826, test loss: 0.26527654587964705\n",
      "epoch 16050: train loss: 0.10535360258548415, test loss: 0.2652771654002657\n",
      "epoch 16051: train loss: 0.10535197829769061, test loss: 0.26527778494965576\n",
      "epoch 16052: train loss: 0.1053503541435973, test loss: 0.26527840452781054\n",
      "epoch 16053: train loss: 0.10534873012318391, test loss: 0.2652790241347232\n",
      "epoch 16054: train loss: 0.10534710623643015, test loss: 0.26527964377038676\n",
      "epoch 16055: train loss: 0.10534548248331571, test loss: 0.26528026343479455\n",
      "epoch 16056: train loss: 0.10534385886382033, test loss: 0.26528088312793974\n",
      "epoch 16057: train loss: 0.10534223537792368, test loss: 0.26528150284981533\n",
      "epoch 16058: train loss: 0.1053406120256055, test loss: 0.26528212260041467\n",
      "epoch 16059: train loss: 0.1053389888068455, test loss: 0.2652827423797308\n",
      "epoch 16060: train loss: 0.1053373657216234, test loss: 0.26528336218775705\n",
      "epoch 16061: train loss: 0.10533574276991894, test loss: 0.26528398202448644\n",
      "epoch 16062: train loss: 0.10533411995171182, test loss: 0.26528460188991226\n",
      "epoch 16063: train loss: 0.10533249726698185, test loss: 0.2652852217840275\n",
      "epoch 16064: train loss: 0.10533087471570869, test loss: 0.2652858417068256\n",
      "epoch 16065: train loss: 0.10532925229787216, test loss: 0.26528646165829967\n",
      "epoch 16066: train loss: 0.10532763001345195, test loss: 0.26528708163844283\n",
      "epoch 16067: train loss: 0.10532600786242785, test loss: 0.2652877016472483\n",
      "epoch 16068: train loss: 0.10532438584477959, test loss: 0.2652883216847092\n",
      "epoch 16069: train loss: 0.10532276396048694, test loss: 0.2652889417508188\n",
      "epoch 16070: train loss: 0.10532114220952969, test loss: 0.26528956184557023\n",
      "epoch 16071: train loss: 0.10531952059188761, test loss: 0.26529018196895676\n",
      "epoch 16072: train loss: 0.10531789910754047, test loss: 0.2652908021209716\n",
      "epoch 16073: train loss: 0.10531627775646804, test loss: 0.26529142230160785\n",
      "epoch 16074: train loss: 0.1053146565386501, test loss: 0.2652920425108587\n",
      "epoch 16075: train loss: 0.10531303545406648, test loss: 0.26529266274871743\n",
      "epoch 16076: train loss: 0.10531141450269693, test loss: 0.26529328301517724\n",
      "epoch 16077: train loss: 0.10530979368452126, test loss: 0.2652939033102313\n",
      "epoch 16078: train loss: 0.10530817299951928, test loss: 0.26529452363387274\n",
      "epoch 16079: train loss: 0.10530655244767083, test loss: 0.2652951439860949\n",
      "epoch 16080: train loss: 0.10530493202895563, test loss: 0.2652957643668909\n",
      "epoch 16081: train loss: 0.10530331174335358, test loss: 0.26529638477625395\n",
      "epoch 16082: train loss: 0.10530169159084447, test loss: 0.2652970052141774\n",
      "epoch 16083: train loss: 0.10530007157140815, test loss: 0.26529762568065424\n",
      "epoch 16084: train loss: 0.1052984516850244, test loss: 0.2652982461756778\n",
      "epoch 16085: train loss: 0.10529683193167308, test loss: 0.2652988666992414\n",
      "epoch 16086: train loss: 0.10529521231133404, test loss: 0.26529948725133806\n",
      "epoch 16087: train loss: 0.10529359282398712, test loss: 0.2653001078319611\n",
      "epoch 16088: train loss: 0.10529197346961215, test loss: 0.2653007284411037\n",
      "epoch 16089: train loss: 0.10529035424818897, test loss: 0.26530134907875913\n",
      "epoch 16090: train loss: 0.10528873515969747, test loss: 0.2653019697449206\n",
      "epoch 16091: train loss: 0.10528711620411749, test loss: 0.2653025904395812\n",
      "epoch 16092: train loss: 0.10528549738142887, test loss: 0.2653032111627345\n",
      "epoch 16093: train loss: 0.10528387869161153, test loss: 0.26530383191437334\n",
      "epoch 16094: train loss: 0.1052822601346453, test loss: 0.26530445269449116\n",
      "epoch 16095: train loss: 0.10528064171051006, test loss: 0.26530507350308113\n",
      "epoch 16096: train loss: 0.10527902341918573, test loss: 0.26530569434013646\n",
      "epoch 16097: train loss: 0.10527740526065216, test loss: 0.2653063152056505\n",
      "epoch 16098: train loss: 0.10527578723488924, test loss: 0.2653069360996164\n",
      "epoch 16099: train loss: 0.10527416934187686, test loss: 0.2653075570220274\n",
      "epoch 16100: train loss: 0.10527255158159494, test loss: 0.2653081779728767\n",
      "epoch 16101: train loss: 0.10527093395402337, test loss: 0.2653087989521577\n",
      "epoch 16102: train loss: 0.10526931645914209, test loss: 0.2653094199598634\n",
      "epoch 16103: train loss: 0.10526769909693094, test loss: 0.26531004099598726\n",
      "epoch 16104: train loss: 0.1052660818673699, test loss: 0.2653106620605224\n",
      "epoch 16105: train loss: 0.10526446477043887, test loss: 0.26531128315346214\n",
      "epoch 16106: train loss: 0.10526284780611775, test loss: 0.2653119042747997\n",
      "epoch 16107: train loss: 0.1052612309743865, test loss: 0.2653125254245283\n",
      "epoch 16108: train loss: 0.10525961427522504, test loss: 0.26531314660264116\n",
      "epoch 16109: train loss: 0.10525799770861331, test loss: 0.26531376780913163\n",
      "epoch 16110: train loss: 0.10525638127453128, test loss: 0.26531438904399296\n",
      "epoch 16111: train loss: 0.10525476497295883, test loss: 0.26531501030721827\n",
      "epoch 16112: train loss: 0.10525314880387594, test loss: 0.265315631598801\n",
      "epoch 16113: train loss: 0.10525153276726261, test loss: 0.26531625291873434\n",
      "epoch 16114: train loss: 0.10524991686309873, test loss: 0.26531687426701156\n",
      "epoch 16115: train loss: 0.1052483010913643, test loss: 0.26531749564362583\n",
      "epoch 16116: train loss: 0.1052466854520393, test loss: 0.2653181170485705\n",
      "epoch 16117: train loss: 0.10524506994510366, test loss: 0.26531873848183885\n",
      "epoch 16118: train loss: 0.1052434545705374, test loss: 0.26531935994342404\n",
      "epoch 16119: train loss: 0.10524183932832046, test loss: 0.26531998143331953\n",
      "epoch 16120: train loss: 0.10524022421843286, test loss: 0.26532060295151844\n",
      "epoch 16121: train loss: 0.10523860924085454, test loss: 0.26532122449801404\n",
      "epoch 16122: train loss: 0.1052369943955656, test loss: 0.2653218460727997\n",
      "epoch 16123: train loss: 0.1052353796825459, test loss: 0.26532246767586853\n",
      "epoch 16124: train loss: 0.10523376510177553, test loss: 0.26532308930721404\n",
      "epoch 16125: train loss: 0.10523215065323446, test loss: 0.2653237109668294\n",
      "epoch 16126: train loss: 0.10523053633690271, test loss: 0.2653243326547078\n",
      "epoch 16127: train loss: 0.1052289221527603, test loss: 0.26532495437084264\n",
      "epoch 16128: train loss: 0.10522730810078727, test loss: 0.26532557611522717\n",
      "epoch 16129: train loss: 0.1052256941809636, test loss: 0.2653261978878546\n",
      "epoch 16130: train loss: 0.10522408039326937, test loss: 0.26532681968871835\n",
      "epoch 16131: train loss: 0.10522246673768455, test loss: 0.26532744151781157\n",
      "epoch 16132: train loss: 0.10522085321418921, test loss: 0.26532806337512765\n",
      "epoch 16133: train loss: 0.10521923982276339, test loss: 0.2653286852606599\n",
      "epoch 16134: train loss: 0.10521762656338712, test loss: 0.2653293071744015\n",
      "epoch 16135: train loss: 0.10521601343604049, test loss: 0.26532992911634584\n",
      "epoch 16136: train loss: 0.10521440044070351, test loss: 0.2653305510864862\n",
      "epoch 16137: train loss: 0.10521278757735625, test loss: 0.2653311730848158\n",
      "epoch 16138: train loss: 0.10521117484597876, test loss: 0.2653317951113281\n",
      "epoch 16139: train loss: 0.10520956224655116, test loss: 0.26533241716601613\n",
      "epoch 16140: train loss: 0.10520794977905347, test loss: 0.2653330392488735\n",
      "epoch 16141: train loss: 0.10520633744346576, test loss: 0.2653336613598934\n",
      "epoch 16142: train loss: 0.10520472523976811, test loss: 0.26533428349906907\n",
      "epoch 16143: train loss: 0.10520311316794065, test loss: 0.26533490566639384\n",
      "epoch 16144: train loss: 0.10520150122796341, test loss: 0.2653355278618611\n",
      "epoch 16145: train loss: 0.10519988941981655, test loss: 0.2653361500854641\n",
      "epoch 16146: train loss: 0.10519827774348009, test loss: 0.265336772337196\n",
      "epoch 16147: train loss: 0.10519666619893417, test loss: 0.2653373946170504\n",
      "epoch 16148: train loss: 0.10519505478615891, test loss: 0.26533801692502046\n",
      "epoch 16149: train loss: 0.10519344350513439, test loss: 0.26533863926109946\n",
      "epoch 16150: train loss: 0.10519183235584073, test loss: 0.26533926162528076\n",
      "epoch 16151: train loss: 0.10519022133825803, test loss: 0.26533988401755776\n",
      "epoch 16152: train loss: 0.10518861045236645, test loss: 0.2653405064379237\n",
      "epoch 16153: train loss: 0.10518699969814613, test loss: 0.26534112888637185\n",
      "epoch 16154: train loss: 0.10518538907557712, test loss: 0.2653417513628957\n",
      "epoch 16155: train loss: 0.10518377858463963, test loss: 0.26534237386748843\n",
      "epoch 16156: train loss: 0.10518216822531375, test loss: 0.2653429964001433\n",
      "epoch 16157: train loss: 0.10518055799757967, test loss: 0.26534361896085384\n",
      "epoch 16158: train loss: 0.10517894790141752, test loss: 0.26534424154961345\n",
      "epoch 16159: train loss: 0.1051773379368074, test loss: 0.2653448641664151\n",
      "epoch 16160: train loss: 0.10517572810372958, test loss: 0.26534548681125236\n",
      "epoch 16161: train loss: 0.10517411840216412, test loss: 0.2653461094841186\n",
      "epoch 16162: train loss: 0.10517250883209121, test loss: 0.26534673218500704\n",
      "epoch 16163: train loss: 0.10517089939349102, test loss: 0.265347354913911\n",
      "epoch 16164: train loss: 0.10516929008634372, test loss: 0.26534797767082396\n",
      "epoch 16165: train loss: 0.1051676809106295, test loss: 0.26534860045573916\n",
      "epoch 16166: train loss: 0.10516607186632858, test loss: 0.26534922326864996\n",
      "epoch 16167: train loss: 0.10516446295342105, test loss: 0.2653498461095497\n",
      "epoch 16168: train loss: 0.10516285417188719, test loss: 0.2653504689784319\n",
      "epoch 16169: train loss: 0.10516124552170712, test loss: 0.26535109187528955\n",
      "epoch 16170: train loss: 0.10515963700286111, test loss: 0.2653517148001162\n",
      "epoch 16171: train loss: 0.10515802861532932, test loss: 0.26535233775290534\n",
      "epoch 16172: train loss: 0.10515642035909195, test loss: 0.2653529607336501\n",
      "epoch 16173: train loss: 0.10515481223412923, test loss: 0.26535358374234397\n",
      "epoch 16174: train loss: 0.10515320424042139, test loss: 0.2653542067789802\n",
      "epoch 16175: train loss: 0.10515159637794863, test loss: 0.2653548298435521\n",
      "epoch 16176: train loss: 0.10514998864669117, test loss: 0.2653554529360532\n",
      "epoch 16177: train loss: 0.10514838104662924, test loss: 0.2653560760564768\n",
      "epoch 16178: train loss: 0.10514677357774307, test loss: 0.2653566992048163\n",
      "epoch 16179: train loss: 0.10514516624001292, test loss: 0.26535732238106496\n",
      "epoch 16180: train loss: 0.10514355903341899, test loss: 0.26535794558521614\n",
      "epoch 16181: train loss: 0.10514195195794156, test loss: 0.26535856881726333\n",
      "epoch 16182: train loss: 0.1051403450135609, test loss: 0.26535919207719977\n",
      "epoch 16183: train loss: 0.10513873820025721, test loss: 0.26535981536501896\n",
      "epoch 16184: train loss: 0.10513713151801075, test loss: 0.2653604386807142\n",
      "epoch 16185: train loss: 0.10513552496680184, test loss: 0.2653610620242788\n",
      "epoch 16186: train loss: 0.10513391854661068, test loss: 0.26536168539570615\n",
      "epoch 16187: train loss: 0.10513231225741759, test loss: 0.2653623087949898\n",
      "epoch 16188: train loss: 0.10513070609920282, test loss: 0.2653629322221228\n",
      "epoch 16189: train loss: 0.10512910007194666, test loss: 0.26536355567709885\n",
      "epoch 16190: train loss: 0.10512749417562936, test loss: 0.2653641791599113\n",
      "epoch 16191: train loss: 0.10512588841023125, test loss: 0.2653648026705533\n",
      "epoch 16192: train loss: 0.10512428277573262, test loss: 0.2653654262090183\n",
      "epoch 16193: train loss: 0.10512267727211373, test loss: 0.2653660497752999\n",
      "epoch 16194: train loss: 0.10512107189935492, test loss: 0.26536667336939135\n",
      "epoch 16195: train loss: 0.10511946665743647, test loss: 0.265367296991286\n",
      "epoch 16196: train loss: 0.10511786154633868, test loss: 0.2653679206409772\n",
      "epoch 16197: train loss: 0.1051162565660419, test loss: 0.2653685443184584\n",
      "epoch 16198: train loss: 0.10511465171652641, test loss: 0.26536916802372307\n",
      "epoch 16199: train loss: 0.10511304699777256, test loss: 0.2653697917567645\n",
      "epoch 16200: train loss: 0.10511144240976067, test loss: 0.2653704155175761\n",
      "epoch 16201: train loss: 0.10510983795247103, test loss: 0.26537103930615136\n",
      "epoch 16202: train loss: 0.10510823362588403, test loss: 0.2653716631224836\n",
      "epoch 16203: train loss: 0.10510662942998, test loss: 0.2653722869665662\n",
      "epoch 16204: train loss: 0.10510502536473922, test loss: 0.26537291083839265\n",
      "epoch 16205: train loss: 0.1051034214301421, test loss: 0.2653735347379562\n",
      "epoch 16206: train loss: 0.10510181762616896, test loss: 0.2653741586652504\n",
      "epoch 16207: train loss: 0.1051002139528002, test loss: 0.26537478262026853\n",
      "epoch 16208: train loss: 0.10509861041001614, test loss: 0.2653754066030041\n",
      "epoch 16209: train loss: 0.10509700699779712, test loss: 0.26537603061345055\n",
      "epoch 16210: train loss: 0.10509540371612357, test loss: 0.26537665465160115\n",
      "epoch 16211: train loss: 0.10509380056497583, test loss: 0.2653772787174495\n",
      "epoch 16212: train loss: 0.10509219754433426, test loss: 0.26537790281098883\n",
      "epoch 16213: train loss: 0.10509059465417928, test loss: 0.2653785269322127\n",
      "epoch 16214: train loss: 0.10508899189449121, test loss: 0.26537915108111443\n",
      "epoch 16215: train loss: 0.10508738926525048, test loss: 0.26537977525768747\n",
      "epoch 16216: train loss: 0.10508578676643751, test loss: 0.2653803994619252\n",
      "epoch 16217: train loss: 0.10508418439803265, test loss: 0.26538102369382116\n",
      "epoch 16218: train loss: 0.10508258216001634, test loss: 0.2653816479533686\n",
      "epoch 16219: train loss: 0.10508098005236892, test loss: 0.26538227224056116\n",
      "epoch 16220: train loss: 0.1050793780750709, test loss: 0.265382896555392\n",
      "epoch 16221: train loss: 0.10507777622810259, test loss: 0.2653835208978548\n",
      "epoch 16222: train loss: 0.10507617451144448, test loss: 0.26538414526794285\n",
      "epoch 16223: train loss: 0.10507457292507696, test loss: 0.26538476966564956\n",
      "epoch 16224: train loss: 0.10507297146898047, test loss: 0.26538539409096856\n",
      "epoch 16225: train loss: 0.10507137014313545, test loss: 0.26538601854389304\n",
      "epoch 16226: train loss: 0.10506976894752228, test loss: 0.2653866430244166\n",
      "epoch 16227: train loss: 0.10506816788212145, test loss: 0.26538726753253267\n",
      "epoch 16228: train loss: 0.10506656694691338, test loss: 0.2653878920682345\n",
      "epoch 16229: train loss: 0.10506496614187856, test loss: 0.26538851663151564\n",
      "epoch 16230: train loss: 0.10506336546699739, test loss: 0.2653891412223696\n",
      "epoch 16231: train loss: 0.10506176492225032, test loss: 0.2653897658407898\n",
      "epoch 16232: train loss: 0.10506016450761786, test loss: 0.26539039048676977\n",
      "epoch 16233: train loss: 0.10505856422308046, test loss: 0.2653910151603027\n",
      "epoch 16234: train loss: 0.10505696406861854, test loss: 0.2653916398613822\n",
      "epoch 16235: train loss: 0.10505536404421263, test loss: 0.2653922645900018\n",
      "epoch 16236: train loss: 0.10505376414984319, test loss: 0.26539288934615485\n",
      "epoch 16237: train loss: 0.10505216438549068, test loss: 0.2653935141298347\n",
      "epoch 16238: train loss: 0.10505056475113561, test loss: 0.26539413894103503\n",
      "epoch 16239: train loss: 0.10504896524675843, test loss: 0.26539476377974913\n",
      "epoch 16240: train loss: 0.10504736587233969, test loss: 0.2653953886459705\n",
      "epoch 16241: train loss: 0.10504576662785985, test loss: 0.2653960135396926\n",
      "epoch 16242: train loss: 0.10504416751329941, test loss: 0.265396638460909\n",
      "epoch 16243: train loss: 0.1050425685286389, test loss: 0.26539726340961295\n",
      "epoch 16244: train loss: 0.10504096967385883, test loss: 0.26539788838579803\n",
      "epoch 16245: train loss: 0.10503937094893966, test loss: 0.26539851338945775\n",
      "epoch 16246: train loss: 0.10503777235386196, test loss: 0.26539913842058555\n",
      "epoch 16247: train loss: 0.10503617388860623, test loss: 0.2653997634791748\n",
      "epoch 16248: train loss: 0.105034575553153, test loss: 0.265400388565219\n",
      "epoch 16249: train loss: 0.10503297734748283, test loss: 0.2654010136787117\n",
      "epoch 16250: train loss: 0.10503137927157623, test loss: 0.26540163881964646\n",
      "epoch 16251: train loss: 0.10502978132541373, test loss: 0.2654022639880166\n",
      "epoch 16252: train loss: 0.10502818350897587, test loss: 0.2654028891838155\n",
      "epoch 16253: train loss: 0.10502658582224322, test loss: 0.2654035144070367\n",
      "epoch 16254: train loss: 0.10502498826519632, test loss: 0.2654041396576739\n",
      "epoch 16255: train loss: 0.10502339083781571, test loss: 0.2654047649357204\n",
      "epoch 16256: train loss: 0.10502179354008198, test loss: 0.2654053902411696\n",
      "epoch 16257: train loss: 0.10502019637197567, test loss: 0.26540601557401516\n",
      "epoch 16258: train loss: 0.10501859933347736, test loss: 0.2654066409342505\n",
      "epoch 16259: train loss: 0.10501700242456763, test loss: 0.2654072663218691\n",
      "epoch 16260: train loss: 0.10501540564522704, test loss: 0.26540789173686435\n",
      "epoch 16261: train loss: 0.10501380899543616, test loss: 0.26540851717923\n",
      "epoch 16262: train loss: 0.1050122124751756, test loss: 0.2654091426489592\n",
      "epoch 16263: train loss: 0.10501061608442593, test loss: 0.26540976814604567\n",
      "epoch 16264: train loss: 0.10500901982316777, test loss: 0.26541039367048286\n",
      "epoch 16265: train loss: 0.10500742369138166, test loss: 0.2654110192222643\n",
      "epoch 16266: train loss: 0.10500582768904826, test loss: 0.26541164480138335\n",
      "epoch 16267: train loss: 0.10500423181614814, test loss: 0.26541227040783366\n",
      "epoch 16268: train loss: 0.10500263607266193, test loss: 0.2654128960416086\n",
      "epoch 16269: train loss: 0.10500104045857026, test loss: 0.2654135217027018\n",
      "epoch 16270: train loss: 0.10499944497385368, test loss: 0.2654141473911067\n",
      "epoch 16271: train loss: 0.1049978496184929, test loss: 0.26541477310681677\n",
      "epoch 16272: train loss: 0.10499625439246846, test loss: 0.2654153988498256\n",
      "epoch 16273: train loss: 0.10499465929576104, test loss: 0.26541602462012664\n",
      "epoch 16274: train loss: 0.10499306432835126, test loss: 0.26541665041771334\n",
      "epoch 16275: train loss: 0.10499146949021977, test loss: 0.2654172762425794\n",
      "epoch 16276: train loss: 0.1049898747813472, test loss: 0.26541790209471805\n",
      "epoch 16277: train loss: 0.10498828020171419, test loss: 0.26541852797412313\n",
      "epoch 16278: train loss: 0.10498668575130143, test loss: 0.26541915388078785\n",
      "epoch 16279: train loss: 0.10498509143008952, test loss: 0.26541977981470594\n",
      "epoch 16280: train loss: 0.10498349723805918, test loss: 0.2654204057758708\n",
      "epoch 16281: train loss: 0.10498190317519102, test loss: 0.26542103176427606\n",
      "epoch 16282: train loss: 0.10498030924146569, test loss: 0.26542165777991505\n",
      "epoch 16283: train loss: 0.10497871543686393, test loss: 0.26542228382278155\n",
      "epoch 16284: train loss: 0.10497712176136638, test loss: 0.2654229098928689\n",
      "epoch 16285: train loss: 0.10497552821495372, test loss: 0.2654235359901707\n",
      "epoch 16286: train loss: 0.1049739347976066, test loss: 0.26542416211468045\n",
      "epoch 16287: train loss: 0.10497234150930579, test loss: 0.2654247882663917\n",
      "epoch 16288: train loss: 0.1049707483500319, test loss: 0.2654254144452979\n",
      "epoch 16289: train loss: 0.10496915531976567, test loss: 0.2654260406513926\n",
      "epoch 16290: train loss: 0.10496756241848779, test loss: 0.2654266668846694\n",
      "epoch 16291: train loss: 0.10496596964617896, test loss: 0.26542729314512187\n",
      "epoch 16292: train loss: 0.10496437700281991, test loss: 0.26542791943274346\n",
      "epoch 16293: train loss: 0.1049627844883913, test loss: 0.2654285457475277\n",
      "epoch 16294: train loss: 0.1049611921028739, test loss: 0.2654291720894682\n",
      "epoch 16295: train loss: 0.10495959984624839, test loss: 0.26542979845855846\n",
      "epoch 16296: train loss: 0.1049580077184955, test loss: 0.26543042485479207\n",
      "epoch 16297: train loss: 0.104956415719596, test loss: 0.26543105127816247\n",
      "epoch 16298: train loss: 0.1049548238495306, test loss: 0.2654316777286633\n",
      "epoch 16299: train loss: 0.10495323210828003, test loss: 0.26543230420628805\n",
      "epoch 16300: train loss: 0.10495164049582502, test loss: 0.26543293071103025\n",
      "epoch 16301: train loss: 0.10495004901214632, test loss: 0.2654335572428836\n",
      "epoch 16302: train loss: 0.10494845765722471, test loss: 0.2654341838018414\n",
      "epoch 16303: train loss: 0.10494686643104091, test loss: 0.2654348103878975\n",
      "epoch 16304: train loss: 0.1049452753335757, test loss: 0.26543543700104516\n",
      "epoch 16305: train loss: 0.1049436843648098, test loss: 0.26543606364127814\n",
      "epoch 16306: train loss: 0.10494209352472403, test loss: 0.2654366903085899\n",
      "epoch 16307: train loss: 0.10494050281329911, test loss: 0.26543731700297407\n",
      "epoch 16308: train loss: 0.10493891223051585, test loss: 0.26543794372442414\n",
      "epoch 16309: train loss: 0.10493732177635502, test loss: 0.2654385704729336\n",
      "epoch 16310: train loss: 0.1049357314507974, test loss: 0.2654391972484963\n",
      "epoch 16311: train loss: 0.10493414125382376, test loss: 0.2654398240511055\n",
      "epoch 16312: train loss: 0.10493255118541493, test loss: 0.2654404508807549\n",
      "epoch 16313: train loss: 0.10493096124555165, test loss: 0.265441077737438\n",
      "epoch 16314: train loss: 0.10492937143421475, test loss: 0.2654417046211486\n",
      "epoch 16315: train loss: 0.10492778175138502, test loss: 0.26544233153187996\n",
      "epoch 16316: train loss: 0.1049261921970433, test loss: 0.2654429584696258\n",
      "epoch 16317: train loss: 0.10492460277117037, test loss: 0.2654435854343797\n",
      "epoch 16318: train loss: 0.10492301347374702, test loss: 0.2654442124261351\n",
      "epoch 16319: train loss: 0.10492142430475411, test loss: 0.2654448394448857\n",
      "epoch 16320: train loss: 0.10491983526417246, test loss: 0.2654454664906252\n",
      "epoch 16321: train loss: 0.10491824635198288, test loss: 0.265446093563347\n",
      "epoch 16322: train loss: 0.10491665756816619, test loss: 0.26544672066304476\n",
      "epoch 16323: train loss: 0.10491506891270327, test loss: 0.2654473477897119\n",
      "epoch 16324: train loss: 0.10491348038557494, test loss: 0.26544797494334227\n",
      "epoch 16325: train loss: 0.10491189198676201, test loss: 0.2654486021239293\n",
      "epoch 16326: train loss: 0.10491030371624536, test loss: 0.26544922933146653\n",
      "epoch 16327: train loss: 0.10490871557400583, test loss: 0.2654498565659476\n",
      "epoch 16328: train loss: 0.1049071275600243, test loss: 0.26545048382736625\n",
      "epoch 16329: train loss: 0.10490553967428157, test loss: 0.26545111111571584\n",
      "epoch 16330: train loss: 0.10490395191675855, test loss: 0.26545173843099007\n",
      "epoch 16331: train loss: 0.1049023642874361, test loss: 0.2654523657731825\n",
      "epoch 16332: train loss: 0.1049007767862951, test loss: 0.26545299314228676\n",
      "epoch 16333: train loss: 0.10489918941331641, test loss: 0.2654536205382965\n",
      "epoch 16334: train loss: 0.1048976021684809, test loss: 0.2654542479612052\n",
      "epoch 16335: train loss: 0.1048960150517695, test loss: 0.26545487541100665\n",
      "epoch 16336: train loss: 0.10489442806316303, test loss: 0.2654555028876942\n",
      "epoch 16337: train loss: 0.10489284120264245, test loss: 0.26545613039126154\n",
      "epoch 16338: train loss: 0.1048912544701886, test loss: 0.26545675792170237\n",
      "epoch 16339: train loss: 0.10488966786578242, test loss: 0.26545738547901016\n",
      "epoch 16340: train loss: 0.1048880813894048, test loss: 0.2654580130631787\n",
      "epoch 16341: train loss: 0.10488649504103663, test loss: 0.26545864067420144\n",
      "epoch 16342: train loss: 0.10488490882065882, test loss: 0.26545926831207206\n",
      "epoch 16343: train loss: 0.10488332272825236, test loss: 0.26545989597678415\n",
      "epoch 16344: train loss: 0.10488173676379806, test loss: 0.2654605236683313\n",
      "epoch 16345: train loss: 0.10488015092727693, test loss: 0.26546115138670723\n",
      "epoch 16346: train loss: 0.10487856521866984, test loss: 0.2654617791319054\n",
      "epoch 16347: train loss: 0.10487697963795777, test loss: 0.26546240690391953\n",
      "epoch 16348: train loss: 0.10487539418512162, test loss: 0.26546303470274324\n",
      "epoch 16349: train loss: 0.10487380886014235, test loss: 0.2654636625283701\n",
      "epoch 16350: train loss: 0.1048722236630009, test loss: 0.26546429038079383\n",
      "epoch 16351: train loss: 0.10487063859367822, test loss: 0.26546491826000795\n",
      "epoch 16352: train loss: 0.10486905365215525, test loss: 0.26546554616600615\n",
      "epoch 16353: train loss: 0.10486746883841298, test loss: 0.265466174098782\n",
      "epoch 16354: train loss: 0.10486588415243232, test loss: 0.2654668020583291\n",
      "epoch 16355: train loss: 0.10486429959419429, test loss: 0.26546743004464135\n",
      "epoch 16356: train loss: 0.1048627151636798, test loss: 0.265468058057712\n",
      "epoch 16357: train loss: 0.10486113086086987, test loss: 0.2654686860975349\n",
      "epoch 16358: train loss: 0.10485954668574546, test loss: 0.2654693141641036\n",
      "epoch 16359: train loss: 0.10485796263828755, test loss: 0.2654699422574118\n",
      "epoch 16360: train loss: 0.10485637871847711, test loss: 0.2654705703774532\n",
      "epoch 16361: train loss: 0.10485479492629515, test loss: 0.2654711985242213\n",
      "epoch 16362: train loss: 0.1048532112617227, test loss: 0.26547182669770975\n",
      "epoch 16363: train loss: 0.10485162772474067, test loss: 0.26547245489791227\n",
      "epoch 16364: train loss: 0.10485004431533014, test loss: 0.26547308312482243\n",
      "epoch 16365: train loss: 0.10484846103347206, test loss: 0.26547371137843406\n",
      "epoch 16366: train loss: 0.10484687787914744, test loss: 0.2654743396587405\n",
      "epoch 16367: train loss: 0.10484529485233733, test loss: 0.26547496796573566\n",
      "epoch 16368: train loss: 0.10484371195302272, test loss: 0.26547559629941303\n",
      "epoch 16369: train loss: 0.10484212918118463, test loss: 0.26547622465976634\n",
      "epoch 16370: train loss: 0.10484054653680414, test loss: 0.2654768530467893\n",
      "epoch 16371: train loss: 0.10483896401986219, test loss: 0.2654774814604754\n",
      "epoch 16372: train loss: 0.10483738163033987, test loss: 0.2654781099008183\n",
      "epoch 16373: train loss: 0.10483579936821819, test loss: 0.2654787383678119\n",
      "epoch 16374: train loss: 0.10483421723347824, test loss: 0.26547936686144963\n",
      "epoch 16375: train loss: 0.10483263522610102, test loss: 0.2654799953817253\n",
      "epoch 16376: train loss: 0.10483105334606757, test loss: 0.26548062392863236\n",
      "epoch 16377: train loss: 0.10482947159335898, test loss: 0.26548125250216464\n",
      "epoch 16378: train loss: 0.1048278899679563, test loss: 0.26548188110231585\n",
      "epoch 16379: train loss: 0.10482630846984055, test loss: 0.2654825097290795\n",
      "epoch 16380: train loss: 0.10482472709899283, test loss: 0.26548313838244936\n",
      "epoch 16381: train loss: 0.10482314585539423, test loss: 0.26548376706241905\n",
      "epoch 16382: train loss: 0.10482156473902578, test loss: 0.26548439576898225\n",
      "epoch 16383: train loss: 0.10481998374986856, test loss: 0.2654850245021326\n",
      "epoch 16384: train loss: 0.10481840288790371, test loss: 0.2654856532618639\n",
      "epoch 16385: train loss: 0.10481682215311224, test loss: 0.2654862820481697\n",
      "epoch 16386: train loss: 0.1048152415454753, test loss: 0.26548691086104376\n",
      "epoch 16387: train loss: 0.10481366106497393, test loss: 0.2654875397004796\n",
      "epoch 16388: train loss: 0.10481208071158925, test loss: 0.26548816856647117\n",
      "epoch 16389: train loss: 0.1048105004853024, test loss: 0.2654887974590118\n",
      "epoch 16390: train loss: 0.1048089203860944, test loss: 0.2654894263780955\n",
      "epoch 16391: train loss: 0.10480734041394642, test loss: 0.26549005532371583\n",
      "epoch 16392: train loss: 0.10480576056883958, test loss: 0.26549068429586636\n",
      "epoch 16393: train loss: 0.10480418085075495, test loss: 0.265491313294541\n",
      "epoch 16394: train loss: 0.1048026012596737, test loss: 0.2654919423197332\n",
      "epoch 16395: train loss: 0.10480102179557693, test loss: 0.2654925713714369\n",
      "epoch 16396: train loss: 0.10479944245844579, test loss: 0.26549320044964553\n",
      "epoch 16397: train loss: 0.10479786324826139, test loss: 0.2654938295543529\n",
      "epoch 16398: train loss: 0.10479628416500486, test loss: 0.2654944586855528\n",
      "epoch 16399: train loss: 0.10479470520865736, test loss: 0.26549508784323883\n",
      "epoch 16400: train loss: 0.10479312637920003, test loss: 0.26549571702740465\n",
      "epoch 16401: train loss: 0.10479154767661403, test loss: 0.26549634623804397\n",
      "epoch 16402: train loss: 0.10478996910088048, test loss: 0.2654969754751506\n",
      "epoch 16403: train loss: 0.10478839065198056, test loss: 0.26549760473871814\n",
      "epoch 16404: train loss: 0.10478681232989544, test loss: 0.2654982340287403\n",
      "epoch 16405: train loss: 0.10478523413460629, test loss: 0.2654988633452108\n",
      "epoch 16406: train loss: 0.10478365606609426, test loss: 0.26549949268812334\n",
      "epoch 16407: train loss: 0.1047820781243405, test loss: 0.26550012205747164\n",
      "epoch 16408: train loss: 0.10478050030932624, test loss: 0.26550075145324936\n",
      "epoch 16409: train loss: 0.10477892262103264, test loss: 0.2655013808754504\n",
      "epoch 16410: train loss: 0.10477734505944088, test loss: 0.2655020103240681\n",
      "epoch 16411: train loss: 0.10477576762453214, test loss: 0.2655026397990965\n",
      "epoch 16412: train loss: 0.10477419031628761, test loss: 0.26550326930052925\n",
      "epoch 16413: train loss: 0.10477261313468852, test loss: 0.26550389882836006\n",
      "epoch 16414: train loss: 0.10477103607971604, test loss: 0.2655045283825824\n",
      "epoch 16415: train loss: 0.1047694591513514, test loss: 0.2655051579631904\n",
      "epoch 16416: train loss: 0.10476788234957578, test loss: 0.2655057875701775\n",
      "epoch 16417: train loss: 0.10476630567437041, test loss: 0.2655064172035375\n",
      "epoch 16418: train loss: 0.10476472912571651, test loss: 0.2655070468632641\n",
      "epoch 16419: train loss: 0.10476315270359532, test loss: 0.2655076765493512\n",
      "epoch 16420: train loss: 0.104761576407988, test loss: 0.2655083062617922\n",
      "epoch 16421: train loss: 0.10476000023887581, test loss: 0.265508936000581\n",
      "epoch 16422: train loss: 0.10475842419624001, test loss: 0.2655095657657114\n",
      "epoch 16423: train loss: 0.1047568482800618, test loss: 0.2655101955571771\n",
      "epoch 16424: train loss: 0.10475527249032245, test loss: 0.2655108253749717\n",
      "epoch 16425: train loss: 0.10475369682700322, test loss: 0.26551145521908914\n",
      "epoch 16426: train loss: 0.10475212129008529, test loss: 0.26551208508952295\n",
      "epoch 16427: train loss: 0.10475054587954996, test loss: 0.26551271498626694\n",
      "epoch 16428: train loss: 0.1047489705953785, test loss: 0.2655133449093149\n",
      "epoch 16429: train loss: 0.10474739543755213, test loss: 0.26551397485866046\n",
      "epoch 16430: train loss: 0.10474582040605213, test loss: 0.26551460483429756\n",
      "epoch 16431: train loss: 0.10474424550085977, test loss: 0.26551523483621975\n",
      "epoch 16432: train loss: 0.10474267072195637, test loss: 0.26551586486442086\n",
      "epoch 16433: train loss: 0.10474109606932311, test loss: 0.26551649491889456\n",
      "epoch 16434: train loss: 0.10473952154294133, test loss: 0.26551712499963465\n",
      "epoch 16435: train loss: 0.10473794714279233, test loss: 0.26551775510663495\n",
      "epoch 16436: train loss: 0.10473637286885737, test loss: 0.265518385239889\n",
      "epoch 16437: train loss: 0.10473479872111771, test loss: 0.2655190153993908\n",
      "epoch 16438: train loss: 0.10473322469955469, test loss: 0.265519645585134\n",
      "epoch 16439: train loss: 0.10473165080414962, test loss: 0.2655202757971123\n",
      "epoch 16440: train loss: 0.10473007703488375, test loss: 0.2655209060353195\n",
      "epoch 16441: train loss: 0.10472850339173846, test loss: 0.26552153629974934\n",
      "epoch 16442: train loss: 0.10472692987469501, test loss: 0.2655221665903957\n",
      "epoch 16443: train loss: 0.10472535648373472, test loss: 0.26552279690725217\n",
      "epoch 16444: train loss: 0.10472378321883893, test loss: 0.26552342725031247\n",
      "epoch 16445: train loss: 0.10472221007998896, test loss: 0.2655240576195705\n",
      "epoch 16446: train loss: 0.10472063706716611, test loss: 0.26552468801502\n",
      "epoch 16447: train loss: 0.10471906418035172, test loss: 0.2655253184366548\n",
      "epoch 16448: train loss: 0.10471749141952717, test loss: 0.26552594888446845\n",
      "epoch 16449: train loss: 0.10471591878467375, test loss: 0.2655265793584549\n",
      "epoch 16450: train loss: 0.10471434627577281, test loss: 0.26552720985860795\n",
      "epoch 16451: train loss: 0.10471277389280571, test loss: 0.2655278403849212\n",
      "epoch 16452: train loss: 0.10471120163575381, test loss: 0.26552847093738857\n",
      "epoch 16453: train loss: 0.10470962950459843, test loss: 0.2655291015160038\n",
      "epoch 16454: train loss: 0.10470805749932097, test loss: 0.2655297321207607\n",
      "epoch 16455: train loss: 0.10470648561990277, test loss: 0.26553036275165287\n",
      "epoch 16456: train loss: 0.10470491386632519, test loss: 0.2655309934086743\n",
      "epoch 16457: train loss: 0.10470334223856963, test loss: 0.2655316240918186\n",
      "epoch 16458: train loss: 0.1047017707366174, test loss: 0.26553225480107967\n",
      "epoch 16459: train loss: 0.10470019936044998, test loss: 0.2655328855364513\n",
      "epoch 16460: train loss: 0.10469862811004865, test loss: 0.2655335162979272\n",
      "epoch 16461: train loss: 0.10469705698539489, test loss: 0.26553414708550116\n",
      "epoch 16462: train loss: 0.10469548598647002, test loss: 0.2655347778991671\n",
      "epoch 16463: train loss: 0.10469391511325543, test loss: 0.26553540873891857\n",
      "epoch 16464: train loss: 0.10469234436573259, test loss: 0.26553603960474953\n",
      "epoch 16465: train loss: 0.10469077374388283, test loss: 0.2655366704966537\n",
      "epoch 16466: train loss: 0.1046892032476876, test loss: 0.26553730141462495\n",
      "epoch 16467: train loss: 0.10468763287712829, test loss: 0.26553793235865697\n",
      "epoch 16468: train loss: 0.10468606263218629, test loss: 0.2655385633287437\n",
      "epoch 16469: train loss: 0.10468449251284309, test loss: 0.26553919432487877\n",
      "epoch 16470: train loss: 0.10468292251908004, test loss: 0.26553982534705606\n",
      "epoch 16471: train loss: 0.10468135265087858, test loss: 0.2655404563952694\n",
      "epoch 16472: train loss: 0.10467978290822018, test loss: 0.26554108746951255\n",
      "epoch 16473: train loss: 0.10467821329108622, test loss: 0.2655417185697793\n",
      "epoch 16474: train loss: 0.10467664379945817, test loss: 0.26554234969606355\n",
      "epoch 16475: train loss: 0.10467507443331746, test loss: 0.265542980848359\n",
      "epoch 16476: train loss: 0.10467350519264557, test loss: 0.26554361202665944\n",
      "epoch 16477: train loss: 0.10467193607742387, test loss: 0.2655442432309587\n",
      "epoch 16478: train loss: 0.10467036708763387, test loss: 0.2655448744612507\n",
      "epoch 16479: train loss: 0.10466879822325705, test loss: 0.2655455057175291\n",
      "epoch 16480: train loss: 0.10466722948427482, test loss: 0.2655461369997878\n",
      "epoch 16481: train loss: 0.10466566087066864, test loss: 0.2655467683080206\n",
      "epoch 16482: train loss: 0.10466409238242003, test loss: 0.26554739964222135\n",
      "epoch 16483: train loss: 0.10466252401951043, test loss: 0.2655480310023837\n",
      "epoch 16484: train loss: 0.10466095578192128, test loss: 0.2655486623885017\n",
      "epoch 16485: train loss: 0.10465938766963415, test loss: 0.26554929380056913\n",
      "epoch 16486: train loss: 0.10465781968263045, test loss: 0.26554992523857956\n",
      "epoch 16487: train loss: 0.1046562518208917, test loss: 0.26555055670252714\n",
      "epoch 16488: train loss: 0.10465468408439937, test loss: 0.26555118819240553\n",
      "epoch 16489: train loss: 0.10465311647313501, test loss: 0.2655518197082085\n",
      "epoch 16490: train loss: 0.10465154898708003, test loss: 0.26555245124993\n",
      "epoch 16491: train loss: 0.104649981626216, test loss: 0.26555308281756385\n",
      "epoch 16492: train loss: 0.1046484143905244, test loss: 0.2655537144111038\n",
      "epoch 16493: train loss: 0.10464684727998677, test loss: 0.2655543460305437\n",
      "epoch 16494: train loss: 0.1046452802945846, test loss: 0.26555497767587743\n",
      "epoch 16495: train loss: 0.1046437134342994, test loss: 0.2655556093470988\n",
      "epoch 16496: train loss: 0.10464214669911275, test loss: 0.2655562410442017\n",
      "epoch 16497: train loss: 0.10464058008900609, test loss: 0.2655568727671799\n",
      "epoch 16498: train loss: 0.10463901360396101, test loss: 0.26555750451602717\n",
      "epoch 16499: train loss: 0.10463744724395906, test loss: 0.2655581362907375\n",
      "epoch 16500: train loss: 0.10463588100898172, test loss: 0.2655587680913046\n",
      "epoch 16501: train loss: 0.10463431489901055, test loss: 0.2655593999177224\n",
      "epoch 16502: train loss: 0.10463274891402713, test loss: 0.26556003176998483\n",
      "epoch 16503: train loss: 0.10463118305401299, test loss: 0.26556066364808545\n",
      "epoch 16504: train loss: 0.10462961731894968, test loss: 0.2655612955520184\n",
      "epoch 16505: train loss: 0.10462805170881874, test loss: 0.2655619274817773\n",
      "epoch 16506: train loss: 0.10462648622360175, test loss: 0.26556255943735624\n",
      "epoch 16507: train loss: 0.10462492086328032, test loss: 0.26556319141874885\n",
      "epoch 16508: train loss: 0.10462335562783594, test loss: 0.26556382342594914\n",
      "epoch 16509: train loss: 0.10462179051725023, test loss: 0.26556445545895085\n",
      "epoch 16510: train loss: 0.10462022553150475, test loss: 0.2655650875177479\n",
      "epoch 16511: train loss: 0.1046186606705811, test loss: 0.26556571960233405\n",
      "epoch 16512: train loss: 0.10461709593446088, test loss: 0.2655663517127034\n",
      "epoch 16513: train loss: 0.10461553132312564, test loss: 0.2655669838488495\n",
      "epoch 16514: train loss: 0.10461396683655697, test loss: 0.26556761601076645\n",
      "epoch 16515: train loss: 0.10461240247473645, test loss: 0.265568248198448\n",
      "epoch 16516: train loss: 0.10461083823764575, test loss: 0.26556888041188803\n",
      "epoch 16517: train loss: 0.10460927412526644, test loss: 0.2655695126510804\n",
      "epoch 16518: train loss: 0.10460771013758013, test loss: 0.26557014491601894\n",
      "epoch 16519: train loss: 0.10460614627456842, test loss: 0.2655707772066977\n",
      "epoch 16520: train loss: 0.10460458253621291, test loss: 0.2655714095231103\n",
      "epoch 16521: train loss: 0.10460301892249525, test loss: 0.26557204186525074\n",
      "epoch 16522: train loss: 0.10460145543339708, test loss: 0.2655726742331129\n",
      "epoch 16523: train loss: 0.10459989206889998, test loss: 0.26557330662669065\n",
      "epoch 16524: train loss: 0.10459832882898563, test loss: 0.26557393904597787\n",
      "epoch 16525: train loss: 0.10459676571363563, test loss: 0.2655745714909684\n",
      "epoch 16526: train loss: 0.10459520272283163, test loss: 0.2655752039616561\n",
      "epoch 16527: train loss: 0.10459363985655527, test loss: 0.2655758364580349\n",
      "epoch 16528: train loss: 0.10459207711478821, test loss: 0.2655764689800987\n",
      "epoch 16529: train loss: 0.10459051449751207, test loss: 0.2655771015278413\n",
      "epoch 16530: train loss: 0.10458895200470852, test loss: 0.2655777341012567\n",
      "epoch 16531: train loss: 0.10458738963635923, test loss: 0.26557836670033863\n",
      "epoch 16532: train loss: 0.10458582739244587, test loss: 0.2655789993250811\n",
      "epoch 16533: train loss: 0.1045842652729501, test loss: 0.265579631975478\n",
      "epoch 16534: train loss: 0.10458270327785357, test loss: 0.26558026465152307\n",
      "epoch 16535: train loss: 0.10458114140713795, test loss: 0.26558089735321044\n",
      "epoch 16536: train loss: 0.10457957966078492, test loss: 0.26558153008053387\n",
      "epoch 16537: train loss: 0.10457801803877619, test loss: 0.2655821628334872\n",
      "epoch 16538: train loss: 0.10457645654109343, test loss: 0.26558279561206427\n",
      "epoch 16539: train loss: 0.10457489516771835, test loss: 0.26558342841625926\n",
      "epoch 16540: train loss: 0.10457333391863259, test loss: 0.26558406124606593\n",
      "epoch 16541: train loss: 0.10457177279381788, test loss: 0.265584694101478\n",
      "epoch 16542: train loss: 0.10457021179325593, test loss: 0.26558532698248966\n",
      "epoch 16543: train loss: 0.10456865091692844, test loss: 0.2655859598890945\n",
      "epoch 16544: train loss: 0.10456709016481708, test loss: 0.2655865928212868\n",
      "epoch 16545: train loss: 0.10456552953690361, test loss: 0.26558722577906013\n",
      "epoch 16546: train loss: 0.10456396903316971, test loss: 0.2655878587624085\n",
      "epoch 16547: train loss: 0.10456240865359716, test loss: 0.265588491771326\n",
      "epoch 16548: train loss: 0.10456084839816761, test loss: 0.2655891248058062\n",
      "epoch 16549: train loss: 0.10455928826686282, test loss: 0.26558975786584327\n",
      "epoch 16550: train loss: 0.10455772825966456, test loss: 0.26559039095143105\n",
      "epoch 16551: train loss: 0.10455616837655449, test loss: 0.2655910240625634\n",
      "epoch 16552: train loss: 0.10455460861751437, test loss: 0.26559165719923433\n",
      "epoch 16553: train loss: 0.10455304898252596, test loss: 0.2655922903614378\n",
      "epoch 16554: train loss: 0.10455148947157103, test loss: 0.26559292354916747\n",
      "epoch 16555: train loss: 0.10454993008463129, test loss: 0.2655935567624176\n",
      "epoch 16556: train loss: 0.10454837082168851, test loss: 0.26559419000118184\n",
      "epoch 16557: train loss: 0.10454681168272444, test loss: 0.2655948232654542\n",
      "epoch 16558: train loss: 0.10454525266772087, test loss: 0.26559545655522865\n",
      "epoch 16559: train loss: 0.10454369377665952, test loss: 0.26559608987049904\n",
      "epoch 16560: train loss: 0.10454213500952221, test loss: 0.2655967232112594\n",
      "epoch 16561: train loss: 0.10454057636629065, test loss: 0.26559735657750355\n",
      "epoch 16562: train loss: 0.10453901784694666, test loss: 0.26559798996922546\n",
      "epoch 16563: train loss: 0.10453745945147204, test loss: 0.26559862338641904\n",
      "epoch 16564: train loss: 0.10453590117984853, test loss: 0.26559925682907837\n",
      "epoch 16565: train loss: 0.10453434303205793, test loss: 0.2655998902971971\n",
      "epoch 16566: train loss: 0.10453278500808207, test loss: 0.2656005237907694\n",
      "epoch 16567: train loss: 0.10453122710790269, test loss: 0.26560115730978917\n",
      "epoch 16568: train loss: 0.10452966933150161, test loss: 0.2656017908542502\n",
      "epoch 16569: train loss: 0.10452811167886067, test loss: 0.2656024244241466\n",
      "epoch 16570: train loss: 0.10452655414996162, test loss: 0.2656030580194722\n",
      "epoch 16571: train loss: 0.10452499674478632, test loss: 0.265603691640221\n",
      "epoch 16572: train loss: 0.10452343946331656, test loss: 0.265604325286387\n",
      "epoch 16573: train loss: 0.10452188230553418, test loss: 0.265604958957964\n",
      "epoch 16574: train loss: 0.10452032527142094, test loss: 0.26560559265494604\n",
      "epoch 16575: train loss: 0.10451876836095876, test loss: 0.26560622637732706\n",
      "epoch 16576: train loss: 0.10451721157412937, test loss: 0.265606860125101\n",
      "epoch 16577: train loss: 0.10451565491091472, test loss: 0.2656074938982617\n",
      "epoch 16578: train loss: 0.10451409837129655, test loss: 0.2656081276968033\n",
      "epoch 16579: train loss: 0.10451254195525675, test loss: 0.2656087615207197\n",
      "epoch 16580: train loss: 0.10451098566277713, test loss: 0.2656093953700048\n",
      "epoch 16581: train loss: 0.10450942949383958, test loss: 0.26561002924465266\n",
      "epoch 16582: train loss: 0.10450787344842592, test loss: 0.26561066314465703\n",
      "epoch 16583: train loss: 0.10450631752651804, test loss: 0.265611297070012\n",
      "epoch 16584: train loss: 0.10450476172809779, test loss: 0.26561193102071157\n",
      "epoch 16585: train loss: 0.10450320605314702, test loss: 0.2656125649967497\n",
      "epoch 16586: train loss: 0.10450165050164757, test loss: 0.26561319899812025\n",
      "epoch 16587: train loss: 0.10450009507358139, test loss: 0.2656138330248172\n",
      "epoch 16588: train loss: 0.1044985397689303, test loss: 0.26561446707683467\n",
      "epoch 16589: train loss: 0.10449698458767621, test loss: 0.2656151011541665\n",
      "epoch 16590: train loss: 0.10449542952980098, test loss: 0.2656157352568066\n",
      "epoch 16591: train loss: 0.10449387459528652, test loss: 0.26561636938474903\n",
      "epoch 16592: train loss: 0.10449231978411466, test loss: 0.2656170035379878\n",
      "epoch 16593: train loss: 0.10449076509626738, test loss: 0.26561763771651675\n",
      "epoch 16594: train loss: 0.10448921053172652, test loss: 0.26561827192033\n",
      "epoch 16595: train loss: 0.10448765609047403, test loss: 0.2656189061494214\n",
      "epoch 16596: train loss: 0.10448610177249176, test loss: 0.265619540403785\n",
      "epoch 16597: train loss: 0.10448454757776166, test loss: 0.2656201746834148\n",
      "epoch 16598: train loss: 0.10448299350626562, test loss: 0.26562080898830465\n",
      "epoch 16599: train loss: 0.1044814395579856, test loss: 0.2656214433184487\n",
      "epoch 16600: train loss: 0.10447988573290345, test loss: 0.26562207767384083\n",
      "epoch 16601: train loss: 0.10447833203100117, test loss: 0.2656227120544751\n",
      "epoch 16602: train loss: 0.10447677845226065, test loss: 0.26562334646034536\n",
      "epoch 16603: train loss: 0.10447522499666383, test loss: 0.2656239808914457\n",
      "epoch 16604: train loss: 0.10447367166419262, test loss: 0.26562461534777\n",
      "epoch 16605: train loss: 0.10447211845482902, test loss: 0.26562524982931246\n",
      "epoch 16606: train loss: 0.10447056536855491, test loss: 0.2656258843360669\n",
      "epoch 16607: train loss: 0.1044690124053523, test loss: 0.26562651886802735\n",
      "epoch 16608: train loss: 0.10446745956520306, test loss: 0.2656271534251878\n",
      "epoch 16609: train loss: 0.10446590684808921, test loss: 0.26562778800754233\n",
      "epoch 16610: train loss: 0.10446435425399273, test loss: 0.26562842261508474\n",
      "epoch 16611: train loss: 0.10446280178289552, test loss: 0.2656290572478092\n",
      "epoch 16612: train loss: 0.10446124943477955, test loss: 0.26562969190570973\n",
      "epoch 16613: train loss: 0.10445969720962683, test loss: 0.26563032658878016\n",
      "epoch 16614: train loss: 0.10445814510741933, test loss: 0.2656309612970146\n",
      "epoch 16615: train loss: 0.10445659312813901, test loss: 0.2656315960304071\n",
      "epoch 16616: train loss: 0.10445504127176783, test loss: 0.26563223078895154\n",
      "epoch 16617: train loss: 0.10445348953828784, test loss: 0.265632865572642\n",
      "epoch 16618: train loss: 0.10445193792768098, test loss: 0.26563350038147254\n",
      "epoch 16619: train loss: 0.10445038643992924, test loss: 0.2656341352154371\n",
      "epoch 16620: train loss: 0.10444883507501462, test loss: 0.2656347700745295\n",
      "epoch 16621: train loss: 0.10444728383291917, test loss: 0.2656354049587442\n",
      "epoch 16622: train loss: 0.10444573271362483, test loss: 0.26563603986807477\n",
      "epoch 16623: train loss: 0.10444418171711362, test loss: 0.26563667480251557\n",
      "epoch 16624: train loss: 0.10444263084336759, test loss: 0.2656373097620604\n",
      "epoch 16625: train loss: 0.10444108009236873, test loss: 0.2656379447467033\n",
      "epoch 16626: train loss: 0.10443952946409904, test loss: 0.26563857975643834\n",
      "epoch 16627: train loss: 0.10443797895854058, test loss: 0.2656392147912596\n",
      "epoch 16628: train loss: 0.10443642857567535, test loss: 0.26563984985116096\n",
      "epoch 16629: train loss: 0.1044348783154854, test loss: 0.2656404849361365\n",
      "epoch 16630: train loss: 0.10443332817795273, test loss: 0.2656411200461803\n",
      "epoch 16631: train loss: 0.10443177816305944, test loss: 0.26564175518128624\n",
      "epoch 16632: train loss: 0.10443022827078749, test loss: 0.2656423903414485\n",
      "epoch 16633: train loss: 0.10442867850111902, test loss: 0.2656430255266611\n",
      "epoch 16634: train loss: 0.10442712885403597, test loss: 0.2656436607369179\n",
      "epoch 16635: train loss: 0.10442557932952051, test loss: 0.2656442959722131\n",
      "epoch 16636: train loss: 0.10442402992755462, test loss: 0.2656449312325407\n",
      "epoch 16637: train loss: 0.10442248064812036, test loss: 0.2656455665178947\n",
      "epoch 16638: train loss: 0.1044209314911998, test loss: 0.2656462018282692\n",
      "epoch 16639: train loss: 0.10441938245677505, test loss: 0.2656468371636581\n",
      "epoch 16640: train loss: 0.10441783354482816, test loss: 0.26564747252405563\n",
      "epoch 16641: train loss: 0.10441628475534116, test loss: 0.2656481079094556\n",
      "epoch 16642: train loss: 0.1044147360882962, test loss: 0.2656487433198523\n",
      "epoch 16643: train loss: 0.10441318754367533, test loss: 0.2656493787552396\n",
      "epoch 16644: train loss: 0.10441163912146063, test loss: 0.2656500142156115\n",
      "epoch 16645: train loss: 0.10441009082163419, test loss: 0.26565064970096214\n",
      "epoch 16646: train loss: 0.1044085426441781, test loss: 0.26565128521128567\n",
      "epoch 16647: train loss: 0.10440699458907449, test loss: 0.26565192074657595\n",
      "epoch 16648: train loss: 0.10440544665630541, test loss: 0.26565255630682716\n",
      "epoch 16649: train loss: 0.10440389884585305, test loss: 0.26565319189203324\n",
      "epoch 16650: train loss: 0.10440235115769941, test loss: 0.2656538275021883\n",
      "epoch 16651: train loss: 0.10440080359182667, test loss: 0.2656544631372864\n",
      "epoch 16652: train loss: 0.10439925614821696, test loss: 0.2656550987973215\n",
      "epoch 16653: train loss: 0.10439770882685234, test loss: 0.26565573448228785\n",
      "epoch 16654: train loss: 0.10439616162771498, test loss: 0.26565637019217936\n",
      "epoch 16655: train loss: 0.104394614550787, test loss: 0.2656570059269901\n",
      "epoch 16656: train loss: 0.1043930675960505, test loss: 0.26565764168671413\n",
      "epoch 16657: train loss: 0.10439152076348769, test loss: 0.26565827747134557\n",
      "epoch 16658: train loss: 0.10438997405308062, test loss: 0.26565891328087843\n",
      "epoch 16659: train loss: 0.1043884274648115, test loss: 0.26565954911530676\n",
      "epoch 16660: train loss: 0.10438688099866242, test loss: 0.2656601849746247\n",
      "epoch 16661: train loss: 0.10438533465461557, test loss: 0.26566082085882625\n",
      "epoch 16662: train loss: 0.10438378843265309, test loss: 0.2656614567679055\n",
      "epoch 16663: train loss: 0.10438224233275716, test loss: 0.26566209270185637\n",
      "epoch 16664: train loss: 0.1043806963549099, test loss: 0.2656627286606733\n",
      "epoch 16665: train loss: 0.10437915049909349, test loss: 0.2656633646443499\n",
      "epoch 16666: train loss: 0.10437760476529011, test loss: 0.26566400065288065\n",
      "epoch 16667: train loss: 0.10437605915348193, test loss: 0.2656646366862595\n",
      "epoch 16668: train loss: 0.10437451366365112, test loss: 0.2656652727444804\n",
      "epoch 16669: train loss: 0.10437296829577986, test loss: 0.2656659088275375\n",
      "epoch 16670: train loss: 0.10437142304985034, test loss: 0.26566654493542485\n",
      "epoch 16671: train loss: 0.10436987792584473, test loss: 0.2656671810681365\n",
      "epoch 16672: train loss: 0.10436833292374523, test loss: 0.26566781722566674\n",
      "epoch 16673: train loss: 0.10436678804353405, test loss: 0.26566845340800943\n",
      "epoch 16674: train loss: 0.10436524328519338, test loss: 0.2656690896151588\n",
      "epoch 16675: train loss: 0.1043636986487054, test loss: 0.2656697258471088\n",
      "epoch 16676: train loss: 0.10436215413405232, test loss: 0.26567036210385364\n",
      "epoch 16677: train loss: 0.1043606097412164, test loss: 0.2656709983853873\n",
      "epoch 16678: train loss: 0.10435906547017978, test loss: 0.26567163469170396\n",
      "epoch 16679: train loss: 0.10435752132092474, test loss: 0.26567227102279767\n",
      "epoch 16680: train loss: 0.10435597729343346, test loss: 0.26567290737866256\n",
      "epoch 16681: train loss: 0.10435443338768817, test loss: 0.26567354375929264\n",
      "epoch 16682: train loss: 0.1043528896036711, test loss: 0.265674180164682\n",
      "epoch 16683: train loss: 0.1043513459413645, test loss: 0.26567481659482484\n",
      "epoch 16684: train loss: 0.10434980240075056, test loss: 0.26567545304971524\n",
      "epoch 16685: train loss: 0.10434825898181158, test loss: 0.2656760895293472\n",
      "epoch 16686: train loss: 0.10434671568452974, test loss: 0.2656767260337149\n",
      "epoch 16687: train loss: 0.10434517250888735, test loss: 0.26567736256281255\n",
      "epoch 16688: train loss: 0.10434362945486661, test loss: 0.265677999116634\n",
      "epoch 16689: train loss: 0.10434208652244979, test loss: 0.26567863569517347\n",
      "epoch 16690: train loss: 0.10434054371161917, test loss: 0.2656792722984252\n",
      "epoch 16691: train loss: 0.10433900102235694, test loss: 0.26567990892638305\n",
      "epoch 16692: train loss: 0.10433745845464544, test loss: 0.26568054557904136\n",
      "epoch 16693: train loss: 0.10433591600846691, test loss: 0.2656811822563941\n",
      "epoch 16694: train loss: 0.10433437368380365, test loss: 0.26568181895843535\n",
      "epoch 16695: train loss: 0.10433283148063786, test loss: 0.2656824556851594\n",
      "epoch 16696: train loss: 0.1043312893989519, test loss: 0.2656830924365602\n",
      "epoch 16697: train loss: 0.10432974743872801, test loss: 0.26568372921263195\n",
      "epoch 16698: train loss: 0.1043282055999485, test loss: 0.26568436601336876\n",
      "epoch 16699: train loss: 0.10432666388259562, test loss: 0.2656850028387646\n",
      "epoch 16700: train loss: 0.10432512228665171, test loss: 0.26568563968881376\n",
      "epoch 16701: train loss: 0.10432358081209907, test loss: 0.2656862765635104\n",
      "epoch 16702: train loss: 0.10432203945891998, test loss: 0.26568691346284856\n",
      "epoch 16703: train loss: 0.10432049822709671, test loss: 0.26568755038682224\n",
      "epoch 16704: train loss: 0.10431895711661164, test loss: 0.26568818733542576\n",
      "epoch 16705: train loss: 0.10431741612744704, test loss: 0.2656888243086532\n",
      "epoch 16706: train loss: 0.10431587525958522, test loss: 0.26568946130649856\n",
      "epoch 16707: train loss: 0.10431433451300853, test loss: 0.2656900983289561\n",
      "epoch 16708: train loss: 0.10431279388769926, test loss: 0.26569073537602\n",
      "epoch 16709: train loss: 0.10431125338363977, test loss: 0.26569137244768426\n",
      "epoch 16710: train loss: 0.10430971300081236, test loss: 0.26569200954394306\n",
      "epoch 16711: train loss: 0.10430817273919939, test loss: 0.2656926466647905\n",
      "epoch 16712: train loss: 0.10430663259878319, test loss: 0.26569328381022084\n",
      "epoch 16713: train loss: 0.1043050925795461, test loss: 0.265693920980228\n",
      "epoch 16714: train loss: 0.10430355268147044, test loss: 0.26569455817480636\n",
      "epoch 16715: train loss: 0.10430201290453862, test loss: 0.26569519539394987\n",
      "epoch 16716: train loss: 0.10430047324873294, test loss: 0.26569583263765284\n",
      "epoch 16717: train loss: 0.10429893371403576, test loss: 0.26569646990590917\n",
      "epoch 16718: train loss: 0.10429739430042946, test loss: 0.26569710719871326\n",
      "epoch 16719: train loss: 0.10429585500789643, test loss: 0.26569774451605915\n",
      "epoch 16720: train loss: 0.10429431583641896, test loss: 0.265698381857941\n",
      "epoch 16721: train loss: 0.10429277678597951, test loss: 0.26569901922435285\n",
      "epoch 16722: train loss: 0.10429123785656037, test loss: 0.26569965661528905\n",
      "epoch 16723: train loss: 0.10428969904814397, test loss: 0.26570029403074363\n",
      "epoch 16724: train loss: 0.10428816036071269, test loss: 0.2657009314707107\n",
      "epoch 16725: train loss: 0.10428662179424888, test loss: 0.2657015689351845\n",
      "epoch 16726: train loss: 0.10428508334873499, test loss: 0.2657022064241591\n",
      "epoch 16727: train loss: 0.10428354502415337, test loss: 0.2657028439376288\n",
      "epoch 16728: train loss: 0.1042820068204864, test loss: 0.26570348147558753\n",
      "epoch 16729: train loss: 0.10428046873771653, test loss: 0.26570411903802965\n",
      "epoch 16730: train loss: 0.10427893077582613, test loss: 0.2657047566249493\n",
      "epoch 16731: train loss: 0.10427739293479762, test loss: 0.26570539423634054\n",
      "epoch 16732: train loss: 0.10427585521461342, test loss: 0.26570603187219766\n",
      "epoch 16733: train loss: 0.10427431761525593, test loss: 0.26570666953251465\n",
      "epoch 16734: train loss: 0.10427278013670756, test loss: 0.26570730721728575\n",
      "epoch 16735: train loss: 0.10427124277895075, test loss: 0.2657079449265052\n",
      "epoch 16736: train loss: 0.1042697055419679, test loss: 0.2657085826601671\n",
      "epoch 16737: train loss: 0.10426816842574148, test loss: 0.2657092204182657\n",
      "epoch 16738: train loss: 0.1042666314302539, test loss: 0.2657098582007951\n",
      "epoch 16739: train loss: 0.1042650945554876, test loss: 0.2657104960077496\n",
      "epoch 16740: train loss: 0.104263557801425, test loss: 0.26571113383912304\n",
      "epoch 16741: train loss: 0.10426202116804859, test loss: 0.2657117716949099\n",
      "epoch 16742: train loss: 0.10426048465534077, test loss: 0.2657124095751043\n",
      "epoch 16743: train loss: 0.10425894826328402, test loss: 0.26571304747970026\n",
      "epoch 16744: train loss: 0.1042574119918608, test loss: 0.26571368540869217\n",
      "epoch 16745: train loss: 0.10425587584105353, test loss: 0.2657143233620741\n",
      "epoch 16746: train loss: 0.10425433981084473, test loss: 0.2657149613398403\n",
      "epoch 16747: train loss: 0.10425280390121681, test loss: 0.26571559934198485\n",
      "epoch 16748: train loss: 0.10425126811215225, test loss: 0.265716237368502\n",
      "epoch 16749: train loss: 0.10424973244363356, test loss: 0.265716875419386\n",
      "epoch 16750: train loss: 0.1042481968956432, test loss: 0.26571751349463085\n",
      "epoch 16751: train loss: 0.1042466614681636, test loss: 0.2657181515942309\n",
      "epoch 16752: train loss: 0.10424512616117732, test loss: 0.26571878971818025\n",
      "epoch 16753: train loss: 0.1042435909746668, test loss: 0.2657194278664732\n",
      "epoch 16754: train loss: 0.10424205590861459, test loss: 0.26572006603910375\n",
      "epoch 16755: train loss: 0.10424052096300308, test loss: 0.2657207042360663\n",
      "epoch 16756: train loss: 0.10423898613781488, test loss: 0.26572134245735496\n",
      "epoch 16757: train loss: 0.1042374514330324, test loss: 0.2657219807029639\n",
      "epoch 16758: train loss: 0.10423591684863819, test loss: 0.26572261897288735\n",
      "epoch 16759: train loss: 0.10423438238461478, test loss: 0.2657232572671195\n",
      "epoch 16760: train loss: 0.10423284804094464, test loss: 0.26572389558565457\n",
      "epoch 16761: train loss: 0.10423131381761032, test loss: 0.2657245339284868\n",
      "epoch 16762: train loss: 0.10422977971459431, test loss: 0.2657251722956102\n",
      "epoch 16763: train loss: 0.10422824573187917, test loss: 0.26572581068701917\n",
      "epoch 16764: train loss: 0.10422671186944737, test loss: 0.2657264491027078\n",
      "epoch 16765: train loss: 0.10422517812728152, test loss: 0.26572708754267044\n",
      "epoch 16766: train loss: 0.10422364450536409, test loss: 0.26572772600690114\n",
      "epoch 16767: train loss: 0.10422211100367765, test loss: 0.2657283644953942\n",
      "epoch 16768: train loss: 0.10422057762220473, test loss: 0.26572900300814384\n",
      "epoch 16769: train loss: 0.10421904436092787, test loss: 0.26572964154514417\n",
      "epoch 16770: train loss: 0.10421751121982963, test loss: 0.26573028010638955\n",
      "epoch 16771: train loss: 0.10421597819889256, test loss: 0.2657309186918741\n",
      "epoch 16772: train loss: 0.10421444529809921, test loss: 0.265731557301592\n",
      "epoch 16773: train loss: 0.10421291251743213, test loss: 0.2657321959355376\n",
      "epoch 16774: train loss: 0.10421137985687395, test loss: 0.26573283459370495\n",
      "epoch 16775: train loss: 0.10420984731640713, test loss: 0.2657334732760884\n",
      "epoch 16776: train loss: 0.10420831489601433, test loss: 0.26573411198268215\n",
      "epoch 16777: train loss: 0.10420678259567812, test loss: 0.2657347507134804\n",
      "epoch 16778: train loss: 0.10420525041538099, test loss: 0.2657353894684774\n",
      "epoch 16779: train loss: 0.1042037183551056, test loss: 0.26573602824766723\n",
      "epoch 16780: train loss: 0.10420218641483452, test loss: 0.2657366670510444\n",
      "epoch 16781: train loss: 0.10420065459455033, test loss: 0.2657373058786029\n",
      "epoch 16782: train loss: 0.10419912289423565, test loss: 0.2657379447303371\n",
      "epoch 16783: train loss: 0.10419759131387302, test loss: 0.2657385836062411\n",
      "epoch 16784: train loss: 0.10419605985344509, test loss: 0.26573922250630916\n",
      "epoch 16785: train loss: 0.10419452851293447, test loss: 0.2657398614305357\n",
      "epoch 16786: train loss: 0.10419299729232367, test loss: 0.2657405003789146\n",
      "epoch 16787: train loss: 0.10419146619159544, test loss: 0.2657411393514404\n",
      "epoch 16788: train loss: 0.10418993521073229, test loss: 0.26574177834810725\n",
      "epoch 16789: train loss: 0.10418840434971688, test loss: 0.2657424173689094\n",
      "epoch 16790: train loss: 0.10418687360853184, test loss: 0.26574305641384105\n",
      "epoch 16791: train loss: 0.10418534298715976, test loss: 0.26574369548289645\n",
      "epoch 16792: train loss: 0.1041838124855833, test loss: 0.2657443345760699\n",
      "epoch 16793: train loss: 0.10418228210378508, test loss: 0.2657449736933555\n",
      "epoch 16794: train loss: 0.10418075184174774, test loss: 0.2657456128347476\n",
      "epoch 16795: train loss: 0.10417922169945391, test loss: 0.2657462520002405\n",
      "epoch 16796: train loss: 0.10417769167688623, test loss: 0.2657468911898284\n",
      "epoch 16797: train loss: 0.10417616177402735, test loss: 0.2657475304035055\n",
      "epoch 16798: train loss: 0.10417463199085991, test loss: 0.265748169641266\n",
      "epoch 16799: train loss: 0.10417310232736661, test loss: 0.2657488089031044\n",
      "epoch 16800: train loss: 0.10417157278353004, test loss: 0.2657494481890148\n",
      "epoch 16801: train loss: 0.10417004335933293, test loss: 0.26575008749899126\n",
      "epoch 16802: train loss: 0.10416851405475785, test loss: 0.2657507268330283\n",
      "epoch 16803: train loss: 0.10416698486978757, test loss: 0.2657513661911201\n",
      "epoch 16804: train loss: 0.1041654558044047, test loss: 0.2657520055732609\n",
      "epoch 16805: train loss: 0.10416392685859194, test loss: 0.26575264497944495\n",
      "epoch 16806: train loss: 0.10416239803233195, test loss: 0.26575328440966667\n",
      "epoch 16807: train loss: 0.10416086932560743, test loss: 0.2657539238639201\n",
      "epoch 16808: train loss: 0.10415934073840105, test loss: 0.2657545633421996\n",
      "epoch 16809: train loss: 0.10415781227069551, test loss: 0.26575520284449927\n",
      "epoch 16810: train loss: 0.10415628392247347, test loss: 0.26575584237081373\n",
      "epoch 16811: train loss: 0.10415475569371768, test loss: 0.265756481921137\n",
      "epoch 16812: train loss: 0.10415322758441083, test loss: 0.26575712149546343\n",
      "epoch 16813: train loss: 0.10415169959453557, test loss: 0.2657577610937871\n",
      "epoch 16814: train loss: 0.10415017172407466, test loss: 0.2657584007161027\n",
      "epoch 16815: train loss: 0.10414864397301081, test loss: 0.265759040362404\n",
      "epoch 16816: train loss: 0.1041471163413267, test loss: 0.26575968003268563\n",
      "epoch 16817: train loss: 0.10414558882900508, test loss: 0.26576031972694175\n",
      "epoch 16818: train loss: 0.10414406143602864, test loss: 0.26576095944516664\n",
      "epoch 16819: train loss: 0.10414253416238015, test loss: 0.26576159918735454\n",
      "epoch 16820: train loss: 0.10414100700804228, test loss: 0.26576223895349976\n",
      "epoch 16821: train loss: 0.10413947997299783, test loss: 0.26576287874359666\n",
      "epoch 16822: train loss: 0.10413795305722946, test loss: 0.2657635185576394\n",
      "epoch 16823: train loss: 0.10413642626071998, test loss: 0.2657641583956224\n",
      "epoch 16824: train loss: 0.10413489958345207, test loss: 0.2657647982575397\n",
      "epoch 16825: train loss: 0.10413337302540851, test loss: 0.26576543814338577\n",
      "epoch 16826: train loss: 0.10413184658657204, test loss: 0.2657660780531549\n",
      "epoch 16827: train loss: 0.10413032026692544, test loss: 0.26576671798684137\n",
      "epoch 16828: train loss: 0.10412879406645144, test loss: 0.26576735794443945\n",
      "epoch 16829: train loss: 0.10412726798513279, test loss: 0.2657679979259434\n",
      "epoch 16830: train loss: 0.10412574202295233, test loss: 0.26576863793134753\n",
      "epoch 16831: train loss: 0.1041242161798927, test loss: 0.26576927796064626\n",
      "epoch 16832: train loss: 0.10412269045593675, test loss: 0.2657699180138337\n",
      "epoch 16833: train loss: 0.10412116485106726, test loss: 0.26577055809090416\n",
      "epoch 16834: train loss: 0.10411963936526697, test loss: 0.265771198191852\n",
      "epoch 16835: train loss: 0.10411811399851871, test loss: 0.26577183831667156\n",
      "epoch 16836: train loss: 0.1041165887508052, test loss: 0.26577247846535706\n",
      "epoch 16837: train loss: 0.1041150636221093, test loss: 0.2657731186379029\n",
      "epoch 16838: train loss: 0.10411353861241374, test loss: 0.26577375883430326\n",
      "epoch 16839: train loss: 0.10411201372170137, test loss: 0.2657743990545525\n",
      "epoch 16840: train loss: 0.10411048894995495, test loss: 0.26577503929864504\n",
      "epoch 16841: train loss: 0.10410896429715731, test loss: 0.2657756795665749\n",
      "epoch 16842: train loss: 0.10410743976329123, test loss: 0.2657763198583367\n",
      "epoch 16843: train loss: 0.10410591534833953, test loss: 0.26577696017392455\n",
      "epoch 16844: train loss: 0.10410439105228506, test loss: 0.2657776005133329\n",
      "epoch 16845: train loss: 0.10410286687511056, test loss: 0.2657782408765559\n",
      "epoch 16846: train loss: 0.1041013428167989, test loss: 0.26577888126358795\n",
      "epoch 16847: train loss: 0.10409981887733291, test loss: 0.2657795216744233\n",
      "epoch 16848: train loss: 0.10409829505669538, test loss: 0.2657801621090565\n",
      "epoch 16849: train loss: 0.10409677135486924, test loss: 0.2657808025674816\n",
      "epoch 16850: train loss: 0.10409524777183718, test loss: 0.26578144304969303\n",
      "epoch 16851: train loss: 0.10409372430758214, test loss: 0.26578208355568506\n",
      "epoch 16852: train loss: 0.10409220096208692, test loss: 0.26578272408545206\n",
      "epoch 16853: train loss: 0.10409067773533437, test loss: 0.26578336463898844\n",
      "epoch 16854: train loss: 0.10408915462730736, test loss: 0.26578400521628825\n",
      "epoch 16855: train loss: 0.10408763163798873, test loss: 0.2657846458173461\n",
      "epoch 16856: train loss: 0.10408610876736134, test loss: 0.26578528644215615\n",
      "epoch 16857: train loss: 0.10408458601540804, test loss: 0.2657859270907128\n",
      "epoch 16858: train loss: 0.10408306338211169, test loss: 0.2657865677630104\n",
      "epoch 16859: train loss: 0.10408154086745518, test loss: 0.26578720845904324\n",
      "epoch 16860: train loss: 0.10408001847142132, test loss: 0.2657878491788056\n",
      "epoch 16861: train loss: 0.10407849619399306, test loss: 0.2657884899222919\n",
      "epoch 16862: train loss: 0.10407697403515324, test loss: 0.26578913068949644\n",
      "epoch 16863: train loss: 0.10407545199488474, test loss: 0.26578977148041355\n",
      "epoch 16864: train loss: 0.10407393007317045, test loss: 0.2657904122950376\n",
      "epoch 16865: train loss: 0.10407240826999326, test loss: 0.2657910531333629\n",
      "epoch 16866: train loss: 0.10407088658533606, test loss: 0.2657916939953837\n",
      "epoch 16867: train loss: 0.10406936501918172, test loss: 0.2657923348810945\n",
      "epoch 16868: train loss: 0.10406784357151318, test loss: 0.2657929757904896\n",
      "epoch 16869: train loss: 0.10406632224231334, test loss: 0.2657936167235633\n",
      "epoch 16870: train loss: 0.10406480103156505, test loss: 0.26579425768030984\n",
      "epoch 16871: train loss: 0.10406327993925128, test loss: 0.2657948986607238\n",
      "epoch 16872: train loss: 0.10406175896535491, test loss: 0.26579553966479946\n",
      "epoch 16873: train loss: 0.10406023810985886, test loss: 0.265796180692531\n",
      "epoch 16874: train loss: 0.10405871737274605, test loss: 0.265796821743913\n",
      "epoch 16875: train loss: 0.10405719675399942, test loss: 0.2657974628189396\n",
      "epoch 16876: train loss: 0.10405567625360188, test loss: 0.26579810391760533\n",
      "epoch 16877: train loss: 0.10405415587153634, test loss: 0.26579874503990447\n",
      "epoch 16878: train loss: 0.10405263560778577, test loss: 0.2657993861858314\n",
      "epoch 16879: train loss: 0.10405111546233309, test loss: 0.2658000273553804\n",
      "epoch 16880: train loss: 0.10404959543516125, test loss: 0.26580066854854584\n",
      "epoch 16881: train loss: 0.10404807552625318, test loss: 0.2658013097653221\n",
      "epoch 16882: train loss: 0.10404655573559182, test loss: 0.26580195100570364\n",
      "epoch 16883: train loss: 0.10404503606316017, test loss: 0.2658025922696848\n",
      "epoch 16884: train loss: 0.10404351650894111, test loss: 0.26580323355725965\n",
      "epoch 16885: train loss: 0.10404199707291767, test loss: 0.265803874868423\n",
      "epoch 16886: train loss: 0.10404047775507275, test loss: 0.2658045162031689\n",
      "epoch 16887: train loss: 0.10403895855538935, test loss: 0.26580515756149187\n",
      "epoch 16888: train loss: 0.10403743947385041, test loss: 0.26580579894338613\n",
      "epoch 16889: train loss: 0.10403592051043895, test loss: 0.26580644034884626\n",
      "epoch 16890: train loss: 0.10403440166513793, test loss: 0.2658070817778665\n",
      "epoch 16891: train loss: 0.10403288293793028, test loss: 0.2658077232304411\n",
      "epoch 16892: train loss: 0.10403136432879904, test loss: 0.2658083647065647\n",
      "epoch 16893: train loss: 0.10402984583772718, test loss: 0.26580900620623155\n",
      "epoch 16894: train loss: 0.10402832746469767, test loss: 0.26580964772943594\n",
      "epoch 16895: train loss: 0.10402680920969352, test loss: 0.26581028927617234\n",
      "epoch 16896: train loss: 0.10402529107269774, test loss: 0.26581093084643515\n",
      "epoch 16897: train loss: 0.10402377305369331, test loss: 0.2658115724402187\n",
      "epoch 16898: train loss: 0.10402225515266324, test loss: 0.26581221405751737\n",
      "epoch 16899: train loss: 0.1040207373695905, test loss: 0.2658128556983256\n",
      "epoch 16900: train loss: 0.10401921970445814, test loss: 0.26581349736263765\n",
      "epoch 16901: train loss: 0.10401770215724918, test loss: 0.265814139050448\n",
      "epoch 16902: train loss: 0.10401618472794663, test loss: 0.265814780761751\n",
      "epoch 16903: train loss: 0.10401466741653351, test loss: 0.2658154224965411\n",
      "epoch 16904: train loss: 0.1040131502229928, test loss: 0.2658160642548127\n",
      "epoch 16905: train loss: 0.10401163314730759, test loss: 0.26581670603656005\n",
      "epoch 16906: train loss: 0.10401011618946089, test loss: 0.26581734784177763\n",
      "epoch 16907: train loss: 0.10400859934943572, test loss: 0.26581798967045983\n",
      "epoch 16908: train loss: 0.10400708262721516, test loss: 0.2658186315226011\n",
      "epoch 16909: train loss: 0.10400556602278219, test loss: 0.2658192733981958\n",
      "epoch 16910: train loss: 0.1040040495361199, test loss: 0.2658199152972381\n",
      "epoch 16911: train loss: 0.10400253316721132, test loss: 0.2658205572197228\n",
      "epoch 16912: train loss: 0.10400101691603951, test loss: 0.26582119916564406\n",
      "epoch 16913: train loss: 0.10399950078258752, test loss: 0.26582184113499624\n",
      "epoch 16914: train loss: 0.10399798476683841, test loss: 0.2658224831277739\n",
      "epoch 16915: train loss: 0.10399646886877525, test loss: 0.26582312514397133\n",
      "epoch 16916: train loss: 0.10399495308838108, test loss: 0.2658237671835829\n",
      "epoch 16917: train loss: 0.10399343742563903, test loss: 0.2658244092466032\n",
      "epoch 16918: train loss: 0.10399192188053212, test loss: 0.2658250513330265\n",
      "epoch 16919: train loss: 0.10399040645304343, test loss: 0.2658256934428472\n",
      "epoch 16920: train loss: 0.10398889114315603, test loss: 0.26582633557605967\n",
      "epoch 16921: train loss: 0.10398737595085304, test loss: 0.2658269777326584\n",
      "epoch 16922: train loss: 0.10398586087611751, test loss: 0.26582761991263787\n",
      "epoch 16923: train loss: 0.10398434591893256, test loss: 0.26582826211599236\n",
      "epoch 16924: train loss: 0.10398283107928127, test loss: 0.26582890434271633\n",
      "epoch 16925: train loss: 0.10398131635714676, test loss: 0.26582954659280417\n",
      "epoch 16926: train loss: 0.10397980175251209, test loss: 0.26583018886625037\n",
      "epoch 16927: train loss: 0.10397828726536039, test loss: 0.26583083116304934\n",
      "epoch 16928: train loss: 0.10397677289567475, test loss: 0.26583147348319536\n",
      "epoch 16929: train loss: 0.1039752586434383, test loss: 0.26583211582668304\n",
      "epoch 16930: train loss: 0.10397374450863416, test loss: 0.26583275819350666\n",
      "epoch 16931: train loss: 0.10397223049124542, test loss: 0.2658334005836607\n",
      "epoch 16932: train loss: 0.10397071659125519, test loss: 0.26583404299713953\n",
      "epoch 16933: train loss: 0.10396920280864666, test loss: 0.26583468543393773\n",
      "epoch 16934: train loss: 0.1039676891434029, test loss: 0.2658353278940496\n",
      "epoch 16935: train loss: 0.10396617559550705, test loss: 0.26583597037746953\n",
      "epoch 16936: train loss: 0.1039646621649423, test loss: 0.2658366128841921\n",
      "epoch 16937: train loss: 0.10396314885169172, test loss: 0.2658372554142116\n",
      "epoch 16938: train loss: 0.10396163565573843, test loss: 0.2658378979675225\n",
      "epoch 16939: train loss: 0.10396012257706568, test loss: 0.2658385405441193\n",
      "epoch 16940: train loss: 0.10395860961565655, test loss: 0.2658391831439963\n",
      "epoch 16941: train loss: 0.10395709677149419, test loss: 0.26583982576714804\n",
      "epoch 16942: train loss: 0.10395558404456176, test loss: 0.26584046841356895\n",
      "epoch 16943: train loss: 0.10395407143484245, test loss: 0.26584111108325353\n",
      "epoch 16944: train loss: 0.10395255894231938, test loss: 0.26584175377619595\n",
      "epoch 16945: train loss: 0.10395104656697576, test loss: 0.265842396492391\n",
      "epoch 16946: train loss: 0.10394953430879472, test loss: 0.2658430392318329\n",
      "epoch 16947: train loss: 0.10394802216775945, test loss: 0.26584368199451625\n",
      "epoch 16948: train loss: 0.10394651014385314, test loss: 0.2658443247804353\n",
      "epoch 16949: train loss: 0.10394499823705892, test loss: 0.2658449675895846\n",
      "epoch 16950: train loss: 0.10394348644736005, test loss: 0.26584561042195864\n",
      "epoch 16951: train loss: 0.10394197477473967, test loss: 0.2658462532775518\n",
      "epoch 16952: train loss: 0.10394046321918095, test loss: 0.2658468961563586\n",
      "epoch 16953: train loss: 0.10393895178066714, test loss: 0.2658475390583734\n",
      "epoch 16954: train loss: 0.10393744045918138, test loss: 0.2658481819835906\n",
      "epoch 16955: train loss: 0.10393592925470692, test loss: 0.26584882493200496\n",
      "epoch 16956: train loss: 0.10393441816722693, test loss: 0.2658494679036106\n",
      "epoch 16957: train loss: 0.10393290719672464, test loss: 0.26585011089840216\n",
      "epoch 16958: train loss: 0.10393139634318325, test loss: 0.2658507539163739\n",
      "epoch 16959: train loss: 0.10392988560658598, test loss: 0.2658513969575205\n",
      "epoch 16960: train loss: 0.10392837498691603, test loss: 0.2658520400218364\n",
      "epoch 16961: train loss: 0.10392686448415663, test loss: 0.265852683109316\n",
      "epoch 16962: train loss: 0.10392535409829101, test loss: 0.26585332621995367\n",
      "epoch 16963: train loss: 0.10392384382930238, test loss: 0.265853969353744\n",
      "epoch 16964: train loss: 0.10392233367717402, test loss: 0.26585461251068143\n",
      "epoch 16965: train loss: 0.10392082364188912, test loss: 0.26585525569076035\n",
      "epoch 16966: train loss: 0.10391931372343094, test loss: 0.2658558988939754\n",
      "epoch 16967: train loss: 0.10391780392178272, test loss: 0.26585654212032095\n",
      "epoch 16968: train loss: 0.10391629423692766, test loss: 0.26585718536979136\n",
      "epoch 16969: train loss: 0.10391478466884907, test loss: 0.26585782864238117\n",
      "epoch 16970: train loss: 0.1039132752175302, test loss: 0.265858471938085\n",
      "epoch 16971: train loss: 0.10391176588295424, test loss: 0.26585911525689715\n",
      "epoch 16972: train loss: 0.1039102566651045, test loss: 0.26585975859881217\n",
      "epoch 16973: train loss: 0.10390874756396429, test loss: 0.26586040196382454\n",
      "epoch 16974: train loss: 0.10390723857951675, test loss: 0.26586104535192867\n",
      "epoch 16975: train loss: 0.10390572971174526, test loss: 0.26586168876311916\n",
      "epoch 16976: train loss: 0.10390422096063306, test loss: 0.2658623321973903\n",
      "epoch 16977: train loss: 0.10390271232616338, test loss: 0.26586297565473677\n",
      "epoch 16978: train loss: 0.10390120380831958, test loss: 0.265863619135153\n",
      "epoch 16979: train loss: 0.10389969540708488, test loss: 0.2658642626386334\n",
      "epoch 16980: train loss: 0.10389818712244259, test loss: 0.26586490616517244\n",
      "epoch 16981: train loss: 0.10389667895437599, test loss: 0.26586554971476484\n",
      "epoch 16982: train loss: 0.10389517090286839, test loss: 0.26586619328740474\n",
      "epoch 16983: train loss: 0.10389366296790307, test loss: 0.2658668368830868\n",
      "epoch 16984: train loss: 0.10389215514946333, test loss: 0.2658674805018056\n",
      "epoch 16985: train loss: 0.10389064744753249, test loss: 0.26586812414355554\n",
      "epoch 16986: train loss: 0.10388913986209385, test loss: 0.2658687678083311\n",
      "epoch 16987: train loss: 0.10388763239313072, test loss: 0.26586941149612675\n",
      "epoch 16988: train loss: 0.10388612504062639, test loss: 0.2658700552069371\n",
      "epoch 16989: train loss: 0.1038846178045642, test loss: 0.26587069894075643\n",
      "epoch 16990: train loss: 0.10388311068492748, test loss: 0.26587134269757945\n",
      "epoch 16991: train loss: 0.10388160368169952, test loss: 0.2658719864774006\n",
      "epoch 16992: train loss: 0.1038800967948637, test loss: 0.26587263028021446\n",
      "epoch 16993: train loss: 0.10387859002440326, test loss: 0.2658732741060154\n",
      "epoch 16994: train loss: 0.10387708337030166, test loss: 0.26587391795479787\n",
      "epoch 16995: train loss: 0.10387557683254212, test loss: 0.26587456182655655\n",
      "epoch 16996: train loss: 0.10387407041110805, test loss: 0.2658752057212858\n",
      "epoch 16997: train loss: 0.10387256410598278, test loss: 0.2658758496389802\n",
      "epoch 16998: train loss: 0.10387105791714965, test loss: 0.26587649357963433\n",
      "epoch 16999: train loss: 0.103869551844592, test loss: 0.2658771375432425\n",
      "epoch 17000: train loss: 0.10386804588829321, test loss: 0.2658777815297994\n",
      "epoch 17001: train loss: 0.10386654004823662, test loss: 0.26587842553929947\n",
      "epoch 17002: train loss: 0.10386503432440561, test loss: 0.26587906957173724\n",
      "epoch 17003: train loss: 0.10386352871678352, test loss: 0.26587971362710716\n",
      "epoch 17004: train loss: 0.10386202322535372, test loss: 0.26588035770540386\n",
      "epoch 17005: train loss: 0.10386051785009957, test loss: 0.26588100180662183\n",
      "epoch 17006: train loss: 0.10385901259100452, test loss: 0.2658816459307554\n",
      "epoch 17007: train loss: 0.10385750744805186, test loss: 0.2658822900777994\n",
      "epoch 17008: train loss: 0.10385600242122499, test loss: 0.26588293424774817\n",
      "epoch 17009: train loss: 0.10385449751050732, test loss: 0.2658835784405962\n",
      "epoch 17010: train loss: 0.10385299271588223, test loss: 0.265884222656338\n",
      "epoch 17011: train loss: 0.1038514880373331, test loss: 0.2658848668949682\n",
      "epoch 17012: train loss: 0.10384998347484334, test loss: 0.26588551115648135\n",
      "epoch 17013: train loss: 0.10384847902839635, test loss: 0.26588615544087185\n",
      "epoch 17014: train loss: 0.1038469746979755, test loss: 0.2658867997481343\n",
      "epoch 17015: train loss: 0.10384547048356424, test loss: 0.26588744407826326\n",
      "epoch 17016: train loss: 0.10384396638514597, test loss: 0.2658880884312533\n",
      "epoch 17017: train loss: 0.10384246240270407, test loss: 0.2658887328070987\n",
      "epoch 17018: train loss: 0.10384095853622197, test loss: 0.2658893772057942\n",
      "epoch 17019: train loss: 0.10383945478568309, test loss: 0.26589002162733427\n",
      "epoch 17020: train loss: 0.10383795115107088, test loss: 0.26589066607171363\n",
      "epoch 17021: train loss: 0.10383644763236873, test loss: 0.2658913105389265\n",
      "epoch 17022: train loss: 0.10383494422956005, test loss: 0.2658919550289676\n",
      "epoch 17023: train loss: 0.10383344094262835, test loss: 0.26589259954183153\n",
      "epoch 17024: train loss: 0.10383193777155697, test loss: 0.26589324407751275\n",
      "epoch 17025: train loss: 0.10383043471632943, test loss: 0.26589388863600566\n",
      "epoch 17026: train loss: 0.10382893177692915, test loss: 0.2658945332173051\n",
      "epoch 17027: train loss: 0.10382742895333953, test loss: 0.2658951778214053\n",
      "epoch 17028: train loss: 0.10382592624554406, test loss: 0.26589582244830107\n",
      "epoch 17029: train loss: 0.1038244236535262, test loss: 0.26589646709798687\n",
      "epoch 17030: train loss: 0.10382292117726938, test loss: 0.26589711177045716\n",
      "epoch 17031: train loss: 0.10382141881675705, test loss: 0.2658977564657065\n",
      "epoch 17032: train loss: 0.10381991657197272, test loss: 0.2658984011837296\n",
      "epoch 17033: train loss: 0.10381841444289983, test loss: 0.26589904592452085\n",
      "epoch 17034: train loss: 0.10381691242952183, test loss: 0.2658996906880748\n",
      "epoch 17035: train loss: 0.10381541053182224, test loss: 0.2659003354743862\n",
      "epoch 17036: train loss: 0.10381390874978448, test loss: 0.2659009802834493\n",
      "epoch 17037: train loss: 0.10381240708339207, test loss: 0.26590162511525894\n",
      "epoch 17038: train loss: 0.10381090553262848, test loss: 0.2659022699698095\n",
      "epoch 17039: train loss: 0.10380940409747717, test loss: 0.26590291484709566\n",
      "epoch 17040: train loss: 0.10380790277792169, test loss: 0.2659035597471119\n",
      "epoch 17041: train loss: 0.10380640157394547, test loss: 0.26590420466985276\n",
      "epoch 17042: train loss: 0.10380490048553202, test loss: 0.2659048496153128\n",
      "epoch 17043: train loss: 0.10380339951266489, test loss: 0.2659054945834868\n",
      "epoch 17044: train loss: 0.10380189865532755, test loss: 0.265906139574369\n",
      "epoch 17045: train loss: 0.10380039791350346, test loss: 0.26590678458795414\n",
      "epoch 17046: train loss: 0.10379889728717621, test loss: 0.2659074296242368\n",
      "epoch 17047: train loss: 0.10379739677632926, test loss: 0.26590807468321154\n",
      "epoch 17048: train loss: 0.10379589638094613, test loss: 0.2659087197648728\n",
      "epoch 17049: train loss: 0.10379439610101036, test loss: 0.2659093648692153\n",
      "epoch 17050: train loss: 0.10379289593650548, test loss: 0.2659100099962336\n",
      "epoch 17051: train loss: 0.10379139588741497, test loss: 0.2659106551459222\n",
      "epoch 17052: train loss: 0.1037898959537224, test loss: 0.2659113003182757\n",
      "epoch 17053: train loss: 0.10378839613541128, test loss: 0.2659119455132888\n",
      "epoch 17054: train loss: 0.10378689643246516, test loss: 0.2659125907309559\n",
      "epoch 17055: train loss: 0.10378539684486762, test loss: 0.2659132359712716\n",
      "epoch 17056: train loss: 0.10378389737260214, test loss: 0.2659138812342306\n",
      "epoch 17057: train loss: 0.1037823980156523, test loss: 0.2659145265198274\n",
      "epoch 17058: train loss: 0.10378089877400161, test loss: 0.2659151718280565\n",
      "epoch 17059: train loss: 0.10377939964763364, test loss: 0.2659158171589127\n",
      "epoch 17060: train loss: 0.10377790063653197, test loss: 0.2659164625123904\n",
      "epoch 17061: train loss: 0.10377640174068016, test loss: 0.2659171078884842\n",
      "epoch 17062: train loss: 0.10377490296006175, test loss: 0.2659177532871888\n",
      "epoch 17063: train loss: 0.10377340429466035, test loss: 0.26591839870849876\n",
      "epoch 17064: train loss: 0.10377190574445944, test loss: 0.2659190441524086\n",
      "epoch 17065: train loss: 0.1037704073094427, test loss: 0.2659196896189129\n",
      "epoch 17066: train loss: 0.10376890898959364, test loss: 0.26592033510800633\n",
      "epoch 17067: train loss: 0.10376741078489583, test loss: 0.2659209806196834\n",
      "epoch 17068: train loss: 0.10376591269533292, test loss: 0.2659216261539387\n",
      "epoch 17069: train loss: 0.10376441472088843, test loss: 0.265922271710767\n",
      "epoch 17070: train loss: 0.10376291686154597, test loss: 0.26592291729016276\n",
      "epoch 17071: train loss: 0.10376141911728914, test loss: 0.2659235628921206\n",
      "epoch 17072: train loss: 0.10375992148810155, test loss: 0.26592420851663506\n",
      "epoch 17073: train loss: 0.10375842397396678, test loss: 0.2659248541637008\n",
      "epoch 17074: train loss: 0.10375692657486846, test loss: 0.2659254998333125\n",
      "epoch 17075: train loss: 0.10375542929079015, test loss: 0.26592614552546456\n",
      "epoch 17076: train loss: 0.10375393212171549, test loss: 0.2659267912401518\n",
      "epoch 17077: train loss: 0.1037524350676281, test loss: 0.2659274369773687\n",
      "epoch 17078: train loss: 0.10375093812851156, test loss: 0.2659280827371099\n",
      "epoch 17079: train loss: 0.10374944130434954, test loss: 0.26592872851937\n",
      "epoch 17080: train loss: 0.10374794459512562, test loss: 0.26592937432414365\n",
      "epoch 17081: train loss: 0.10374644800082346, test loss: 0.26593002015142536\n",
      "epoch 17082: train loss: 0.10374495152142664, test loss: 0.2659306660012098\n",
      "epoch 17083: train loss: 0.10374345515691885, test loss: 0.26593131187349167\n",
      "epoch 17084: train loss: 0.10374195890728372, test loss: 0.26593195776826556\n",
      "epoch 17085: train loss: 0.10374046277250486, test loss: 0.2659326036855258\n",
      "epoch 17086: train loss: 0.10373896675256589, test loss: 0.26593324962526743\n",
      "epoch 17087: train loss: 0.10373747084745054, test loss: 0.26593389558748476\n",
      "epoch 17088: train loss: 0.10373597505714241, test loss: 0.2659345415721726\n",
      "epoch 17089: train loss: 0.10373447938162514, test loss: 0.26593518757932555\n",
      "epoch 17090: train loss: 0.1037329838208824, test loss: 0.265935833608938\n",
      "epoch 17091: train loss: 0.10373148837489786, test loss: 0.2659364796610049\n",
      "epoch 17092: train loss: 0.10372999304365514, test loss: 0.26593712573552064\n",
      "epoch 17093: train loss: 0.10372849782713799, test loss: 0.26593777183248\n",
      "epoch 17094: train loss: 0.10372700272533, test loss: 0.26593841795187756\n",
      "epoch 17095: train loss: 0.10372550773821489, test loss: 0.2659390640937079\n",
      "epoch 17096: train loss: 0.10372401286577632, test loss: 0.26593971025796553\n",
      "epoch 17097: train loss: 0.10372251810799796, test loss: 0.2659403564446453\n",
      "epoch 17098: train loss: 0.1037210234648635, test loss: 0.2659410026537419\n",
      "epoch 17099: train loss: 0.10371952893635662, test loss: 0.26594164888524974\n",
      "epoch 17100: train loss: 0.10371803452246102, test loss: 0.2659422951391635\n",
      "epoch 17101: train loss: 0.1037165402231604, test loss: 0.26594294141547786\n",
      "epoch 17102: train loss: 0.10371504603843845, test loss: 0.26594358771418747\n",
      "epoch 17103: train loss: 0.10371355196827886, test loss: 0.2659442340352869\n",
      "epoch 17104: train loss: 0.10371205801266535, test loss: 0.265944880378771\n",
      "epoch 17105: train loss: 0.10371056417158162, test loss: 0.26594552674463406\n",
      "epoch 17106: train loss: 0.10370907044501135, test loss: 0.2659461731328711\n",
      "epoch 17107: train loss: 0.10370757683293828, test loss: 0.26594681954347643\n",
      "epoch 17108: train loss: 0.10370608333534614, test loss: 0.26594746597644486\n",
      "epoch 17109: train loss: 0.1037045899522186, test loss: 0.2659481124317711\n",
      "epoch 17110: train loss: 0.10370309668353944, test loss: 0.2659487589094496\n",
      "epoch 17111: train loss: 0.10370160352929236, test loss: 0.2659494054094751\n",
      "epoch 17112: train loss: 0.10370011048946108, test loss: 0.2659500519318423\n",
      "epoch 17113: train loss: 0.10369861756402933, test loss: 0.26595069847654584\n",
      "epoch 17114: train loss: 0.10369712475298087, test loss: 0.2659513450435803\n",
      "epoch 17115: train loss: 0.10369563205629945, test loss: 0.2659519916329405\n",
      "epoch 17116: train loss: 0.10369413947396876, test loss: 0.2659526382446208\n",
      "epoch 17117: train loss: 0.10369264700597255, test loss: 0.26595328487861614\n",
      "epoch 17118: train loss: 0.10369115465229464, test loss: 0.26595393153492103\n",
      "epoch 17119: train loss: 0.10368966241291871, test loss: 0.26595457821353014\n",
      "epoch 17120: train loss: 0.10368817028782855, test loss: 0.2659552249144381\n",
      "epoch 17121: train loss: 0.1036866782770079, test loss: 0.26595587163763973\n",
      "epoch 17122: train loss: 0.10368518638044055, test loss: 0.26595651838312956\n",
      "epoch 17123: train loss: 0.10368369459811025, test loss: 0.2659571651509021\n",
      "epoch 17124: train loss: 0.10368220293000074, test loss: 0.26595781194095236\n",
      "epoch 17125: train loss: 0.10368071137609583, test loss: 0.26595845875327484\n",
      "epoch 17126: train loss: 0.10367921993637928, test loss: 0.26595910558786406\n",
      "epoch 17127: train loss: 0.10367772861083488, test loss: 0.26595975244471487\n",
      "epoch 17128: train loss: 0.10367623739944638, test loss: 0.2659603993238219\n",
      "epoch 17129: train loss: 0.1036747463021976, test loss: 0.26596104622517985\n",
      "epoch 17130: train loss: 0.10367325531907232, test loss: 0.26596169314878326\n",
      "epoch 17131: train loss: 0.10367176445005433, test loss: 0.26596234009462694\n",
      "epoch 17132: train loss: 0.10367027369512741, test loss: 0.26596298706270555\n",
      "epoch 17133: train loss: 0.10366878305427538, test loss: 0.2659636340530136\n",
      "epoch 17134: train loss: 0.10366729252748204, test loss: 0.26596428106554604\n",
      "epoch 17135: train loss: 0.10366580211473117, test loss: 0.2659649281002972\n",
      "epoch 17136: train loss: 0.1036643118160066, test loss: 0.26596557515726216\n",
      "epoch 17137: train loss: 0.10366282163129213, test loss: 0.26596622223643523\n",
      "epoch 17138: train loss: 0.10366133156057161, test loss: 0.2659668693378114\n",
      "epoch 17139: train loss: 0.1036598416038288, test loss: 0.2659675164613851\n",
      "epoch 17140: train loss: 0.10365835176104757, test loss: 0.2659681636071511\n",
      "epoch 17141: train loss: 0.1036568620322117, test loss: 0.26596881077510415\n",
      "epoch 17142: train loss: 0.10365537241730503, test loss: 0.2659694579652389\n",
      "epoch 17143: train loss: 0.10365388291631145, test loss: 0.26597010517755004\n",
      "epoch 17144: train loss: 0.10365239352921471, test loss: 0.2659707524120322\n",
      "epoch 17145: train loss: 0.10365090425599871, test loss: 0.26597139966868005\n",
      "epoch 17146: train loss: 0.10364941509664724, test loss: 0.2659720469474884\n",
      "epoch 17147: train loss: 0.1036479260511442, test loss: 0.2659726942484519\n",
      "epoch 17148: train loss: 0.10364643711947337, test loss: 0.26597334157156527\n",
      "epoch 17149: train loss: 0.10364494830161868, test loss: 0.265973988916823\n",
      "epoch 17150: train loss: 0.10364345959756392, test loss: 0.2659746362842201\n",
      "epoch 17151: train loss: 0.10364197100729294, test loss: 0.2659752836737509\n",
      "epoch 17152: train loss: 0.10364048253078967, test loss: 0.26597593108541046\n",
      "epoch 17153: train loss: 0.1036389941680379, test loss: 0.2659765785191933\n",
      "epoch 17154: train loss: 0.10363750591902153, test loss: 0.26597722597509404\n",
      "epoch 17155: train loss: 0.10363601778372444, test loss: 0.2659778734531076\n",
      "epoch 17156: train loss: 0.1036345297621305, test loss: 0.2659785209532285\n",
      "epoch 17157: train loss: 0.10363304185422358, test loss: 0.26597916847545156\n",
      "epoch 17158: train loss: 0.10363155405998756, test loss: 0.2659798160197714\n",
      "epoch 17159: train loss: 0.10363006637940629, test loss: 0.26598046358618277\n",
      "epoch 17160: train loss: 0.10362857881246372, test loss: 0.2659811111746803\n",
      "epoch 17161: train loss: 0.10362709135914372, test loss: 0.2659817587852588\n",
      "epoch 17162: train loss: 0.10362560401943015, test loss: 0.2659824064179129\n",
      "epoch 17163: train loss: 0.10362411679330691, test loss: 0.2659830540726374\n",
      "epoch 17164: train loss: 0.10362262968075793, test loss: 0.2659837017494269\n",
      "epoch 17165: train loss: 0.10362114268176711, test loss: 0.2659843494482763\n",
      "epoch 17166: train loss: 0.10361965579631834, test loss: 0.26598499716918\n",
      "epoch 17167: train loss: 0.10361816902439552, test loss: 0.26598564491213295\n",
      "epoch 17168: train loss: 0.1036166823659826, test loss: 0.2659862926771299\n",
      "epoch 17169: train loss: 0.10361519582106346, test loss: 0.26598694046416543\n",
      "epoch 17170: train loss: 0.10361370938962203, test loss: 0.2659875882732344\n",
      "epoch 17171: train loss: 0.10361222307164222, test loss: 0.26598823610433137\n",
      "epoch 17172: train loss: 0.10361073686710799, test loss: 0.2659888839574511\n",
      "epoch 17173: train loss: 0.10360925077600325, test loss: 0.26598953183258844\n",
      "epoch 17174: train loss: 0.10360776479831191, test loss: 0.26599017972973793\n",
      "epoch 17175: train loss: 0.10360627893401794, test loss: 0.2659908276488944\n",
      "epoch 17176: train loss: 0.10360479318310524, test loss: 0.26599147559005254\n",
      "epoch 17177: train loss: 0.10360330754555783, test loss: 0.26599212355320717\n",
      "epoch 17178: train loss: 0.10360182202135952, test loss: 0.26599277153835293\n",
      "epoch 17179: train loss: 0.10360033661049439, test loss: 0.2659934195454845\n",
      "epoch 17180: train loss: 0.10359885131294634, test loss: 0.2659940675745968\n",
      "epoch 17181: train loss: 0.10359736612869931, test loss: 0.2659947156256843\n",
      "epoch 17182: train loss: 0.10359588105773727, test loss: 0.26599536369874194\n",
      "epoch 17183: train loss: 0.1035943961000442, test loss: 0.26599601179376436\n",
      "epoch 17184: train loss: 0.10359291125560403, test loss: 0.26599665991074634\n",
      "epoch 17185: train loss: 0.10359142652440073, test loss: 0.26599730804968263\n",
      "epoch 17186: train loss: 0.10358994190641832, test loss: 0.2659979562105679\n",
      "epoch 17187: train loss: 0.10358845740164073, test loss: 0.26599860439339684\n",
      "epoch 17188: train loss: 0.10358697301005193, test loss: 0.26599925259816437\n",
      "epoch 17189: train loss: 0.10358548873163594, test loss: 0.26599990082486513\n",
      "epoch 17190: train loss: 0.10358400456637672, test loss: 0.2660005490734939\n",
      "epoch 17191: train loss: 0.10358252051425824, test loss: 0.2660011973440453\n",
      "epoch 17192: train loss: 0.1035810365752645, test loss: 0.26600184563651424\n",
      "epoch 17193: train loss: 0.10357955274937952, test loss: 0.26600249395089526\n",
      "epoch 17194: train loss: 0.10357806903658727, test loss: 0.2660031422871834\n",
      "epoch 17195: train loss: 0.1035765854368718, test loss: 0.2660037906453732\n",
      "epoch 17196: train loss: 0.10357510195021699, test loss: 0.2660044390254594\n",
      "epoch 17197: train loss: 0.10357361857660699, test loss: 0.2660050874274369\n",
      "epoch 17198: train loss: 0.10357213531602573, test loss: 0.26600573585130033\n",
      "epoch 17199: train loss: 0.10357065216845723, test loss: 0.2660063842970445\n",
      "epoch 17200: train loss: 0.10356916913388553, test loss: 0.26600703276466414\n",
      "epoch 17201: train loss: 0.10356768621229463, test loss: 0.2660076812541539\n",
      "epoch 17202: train loss: 0.10356620340366857, test loss: 0.2660083297655087\n",
      "epoch 17203: train loss: 0.10356472070799133, test loss: 0.2660089782987233\n",
      "epoch 17204: train loss: 0.103563238125247, test loss: 0.2660096268537923\n",
      "epoch 17205: train loss: 0.10356175565541957, test loss: 0.26601027543071065\n",
      "epoch 17206: train loss: 0.1035602732984931, test loss: 0.26601092402947296\n",
      "epoch 17207: train loss: 0.10355879105445162, test loss: 0.2660115726500741\n",
      "epoch 17208: train loss: 0.10355730892327916, test loss: 0.26601222129250873\n",
      "epoch 17209: train loss: 0.10355582690495976, test loss: 0.2660128699567717\n",
      "epoch 17210: train loss: 0.10355434499947752, test loss: 0.26601351864285777\n",
      "epoch 17211: train loss: 0.10355286320681642, test loss: 0.2660141673507617\n",
      "epoch 17212: train loss: 0.10355138152696057, test loss: 0.2660148160804782\n",
      "epoch 17213: train loss: 0.103549899959894, test loss: 0.2660154648320021\n",
      "epoch 17214: train loss: 0.10354841850560077, test loss: 0.2660161136053281\n",
      "epoch 17215: train loss: 0.10354693716406498, test loss: 0.26601676240045113\n",
      "epoch 17216: train loss: 0.10354545593527063, test loss: 0.2660174112173659\n",
      "epoch 17217: train loss: 0.10354397481920188, test loss: 0.26601806005606704\n",
      "epoch 17218: train loss: 0.1035424938158427, test loss: 0.2660187089165494\n",
      "epoch 17219: train loss: 0.10354101292517726, test loss: 0.2660193577988078\n",
      "epoch 17220: train loss: 0.10353953214718958, test loss: 0.26602000670283715\n",
      "epoch 17221: train loss: 0.10353805148186378, test loss: 0.26602065562863203\n",
      "epoch 17222: train loss: 0.10353657092918389, test loss: 0.2660213045761872\n",
      "epoch 17223: train loss: 0.10353509048913409, test loss: 0.26602195354549757\n",
      "epoch 17224: train loss: 0.10353361016169842, test loss: 0.26602260253655785\n",
      "epoch 17225: train loss: 0.10353212994686097, test loss: 0.2660232515493629\n",
      "epoch 17226: train loss: 0.10353064984460585, test loss: 0.26602390058390746\n",
      "epoch 17227: train loss: 0.10352916985491716, test loss: 0.26602454964018624\n",
      "epoch 17228: train loss: 0.10352768997777902, test loss: 0.26602519871819413\n",
      "epoch 17229: train loss: 0.10352621021317553, test loss: 0.2660258478179259\n",
      "epoch 17230: train loss: 0.10352473056109078, test loss: 0.2660264969393763\n",
      "epoch 17231: train loss: 0.10352325102150893, test loss: 0.26602714608254013\n",
      "epoch 17232: train loss: 0.10352177159441407, test loss: 0.2660277952474122\n",
      "epoch 17233: train loss: 0.10352029227979029, test loss: 0.2660284444339874\n",
      "epoch 17234: train loss: 0.10351881307762179, test loss: 0.26602909364226035\n",
      "epoch 17235: train loss: 0.10351733398789266, test loss: 0.2660297428722259\n",
      "epoch 17236: train loss: 0.10351585501058702, test loss: 0.2660303921238789\n",
      "epoch 17237: train loss: 0.10351437614568901, test loss: 0.2660310413972141\n",
      "epoch 17238: train loss: 0.10351289739318278, test loss: 0.26603169069222643\n",
      "epoch 17239: train loss: 0.10351141875305249, test loss: 0.26603234000891046\n",
      "epoch 17240: train loss: 0.10350994022528223, test loss: 0.2660329893472611\n",
      "epoch 17241: train loss: 0.10350846180985618, test loss: 0.26603363870727326\n",
      "epoch 17242: train loss: 0.1035069835067585, test loss: 0.26603428808894164\n",
      "epoch 17243: train loss: 0.10350550531597331, test loss: 0.26603493749226104\n",
      "epoch 17244: train loss: 0.10350402723748481, test loss: 0.26603558691722623\n",
      "epoch 17245: train loss: 0.10350254927127711, test loss: 0.2660362363638321\n",
      "epoch 17246: train loss: 0.10350107141733442, test loss: 0.2660368858320734\n",
      "epoch 17247: train loss: 0.10349959367564089, test loss: 0.266037535321945\n",
      "epoch 17248: train loss: 0.1034981160461807, test loss: 0.26603818483344166\n",
      "epoch 17249: train loss: 0.10349663852893802, test loss: 0.26603883436655823\n",
      "epoch 17250: train loss: 0.10349516112389699, test loss: 0.2660394839212895\n",
      "epoch 17251: train loss: 0.10349368383104181, test loss: 0.2660401334976303\n",
      "epoch 17252: train loss: 0.10349220665035669, test loss: 0.26604078309557544\n",
      "epoch 17253: train loss: 0.10349072958182579, test loss: 0.26604143271511965\n",
      "epoch 17254: train loss: 0.1034892526254333, test loss: 0.2660420823562579\n",
      "epoch 17255: train loss: 0.10348777578116339, test loss: 0.266042732018985\n",
      "epoch 17256: train loss: 0.10348629904900034, test loss: 0.26604338170329567\n",
      "epoch 17257: train loss: 0.10348482242892824, test loss: 0.26604403140918476\n",
      "epoch 17258: train loss: 0.10348334592093135, test loss: 0.2660446811366471\n",
      "epoch 17259: train loss: 0.10348186952499389, test loss: 0.2660453308856775\n",
      "epoch 17260: train loss: 0.1034803932411, test loss: 0.26604598065627083\n",
      "epoch 17261: train loss: 0.10347891706923397, test loss: 0.2660466304484219\n",
      "epoch 17262: train loss: 0.10347744100937996, test loss: 0.26604728026212554\n",
      "epoch 17263: train loss: 0.10347596506152222, test loss: 0.26604793009737654\n",
      "epoch 17264: train loss: 0.10347448922564492, test loss: 0.26604857995416986\n",
      "epoch 17265: train loss: 0.10347301350173237, test loss: 0.26604922983250007\n",
      "epoch 17266: train loss: 0.10347153788976872, test loss: 0.26604987973236227\n",
      "epoch 17267: train loss: 0.10347006238973823, test loss: 0.2660505296537512\n",
      "epoch 17268: train loss: 0.10346858700162512, test loss: 0.2660511795966616\n",
      "epoch 17269: train loss: 0.10346711172541365, test loss: 0.2660518295610884\n",
      "epoch 17270: train loss: 0.10346563656108804, test loss: 0.2660524795470265\n",
      "epoch 17271: train loss: 0.10346416150863255, test loss: 0.2660531295544706\n",
      "epoch 17272: train loss: 0.10346268656803143, test loss: 0.2660537795834156\n",
      "epoch 17273: train loss: 0.1034612117392689, test loss: 0.26605442963385634\n",
      "epoch 17274: train loss: 0.10345973702232919, test loss: 0.2660550797057877\n",
      "epoch 17275: train loss: 0.10345826241719663, test loss: 0.26605572979920444\n",
      "epoch 17276: train loss: 0.10345678792385542, test loss: 0.2660563799141016\n",
      "epoch 17277: train loss: 0.10345531354228985, test loss: 0.26605703005047365\n",
      "epoch 17278: train loss: 0.10345383927248418, test loss: 0.2660576802083158\n",
      "epoch 17279: train loss: 0.10345236511442268, test loss: 0.26605833038762283\n",
      "epoch 17280: train loss: 0.10345089106808962, test loss: 0.26605898058838934\n",
      "epoch 17281: train loss: 0.10344941713346924, test loss: 0.26605963081061046\n",
      "epoch 17282: train loss: 0.10344794331054587, test loss: 0.2660602810542809\n",
      "epoch 17283: train loss: 0.10344646959930376, test loss: 0.26606093131939557\n",
      "epoch 17284: train loss: 0.10344499599972719, test loss: 0.26606158160594934\n",
      "epoch 17285: train loss: 0.1034435225118005, test loss: 0.2660622319139371\n",
      "epoch 17286: train loss: 0.1034420491355079, test loss: 0.26606288224335345\n",
      "epoch 17287: train loss: 0.10344057587083373, test loss: 0.2660635325941935\n",
      "epoch 17288: train loss: 0.1034391027177623, test loss: 0.26606418296645207\n",
      "epoch 17289: train loss: 0.10343762967627786, test loss: 0.26606483336012404\n",
      "epoch 17290: train loss: 0.10343615674636475, test loss: 0.2660654837752041\n",
      "epoch 17291: train loss: 0.10343468392800727, test loss: 0.2660661342116874\n",
      "epoch 17292: train loss: 0.10343321122118974, test loss: 0.26606678466956857\n",
      "epoch 17293: train loss: 0.10343173862589644, test loss: 0.26606743514884246\n",
      "epoch 17294: train loss: 0.1034302661421117, test loss: 0.26606808564950407\n",
      "epoch 17295: train loss: 0.10342879376981987, test loss: 0.26606873617154825\n",
      "epoch 17296: train loss: 0.1034273215090052, test loss: 0.2660693867149698\n",
      "epoch 17297: train loss: 0.10342584935965209, test loss: 0.2660700372797636\n",
      "epoch 17298: train loss: 0.10342437732174484, test loss: 0.26607068786592447\n",
      "epoch 17299: train loss: 0.10342290539526776, test loss: 0.2660713384734475\n",
      "epoch 17300: train loss: 0.10342143358020518, test loss: 0.2660719891023273\n",
      "epoch 17301: train loss: 0.1034199618765415, test loss: 0.26607263975255885\n",
      "epoch 17302: train loss: 0.103418490284261, test loss: 0.2660732904241371\n",
      "epoch 17303: train loss: 0.10341701880334804, test loss: 0.26607394111705684\n",
      "epoch 17304: train loss: 0.10341554743378698, test loss: 0.2660745918313129\n",
      "epoch 17305: train loss: 0.10341407617556214, test loss: 0.2660752425669003\n",
      "epoch 17306: train loss: 0.1034126050286579, test loss: 0.2660758933238137\n",
      "epoch 17307: train loss: 0.10341113399305861, test loss: 0.26607654410204823\n",
      "epoch 17308: train loss: 0.10340966306874864, test loss: 0.26607719490159865\n",
      "epoch 17309: train loss: 0.10340819225571232, test loss: 0.26607784572245974\n",
      "epoch 17310: train loss: 0.10340672155393404, test loss: 0.2660784965646267\n",
      "epoch 17311: train loss: 0.10340525096339817, test loss: 0.26607914742809397\n",
      "epoch 17312: train loss: 0.10340378048408906, test loss: 0.2660797983128569\n",
      "epoch 17313: train loss: 0.10340231011599112, test loss: 0.26608044921891005\n",
      "epoch 17314: train loss: 0.1034008398590887, test loss: 0.26608110014624836\n",
      "epoch 17315: train loss: 0.1033993697133662, test loss: 0.26608175109486687\n",
      "epoch 17316: train loss: 0.10339789967880797, test loss: 0.2660824020647603\n",
      "epoch 17317: train loss: 0.10339642975539846, test loss: 0.2660830530559236\n",
      "epoch 17318: train loss: 0.10339495994312198, test loss: 0.2660837040683518\n",
      "epoch 17319: train loss: 0.10339349024196298, test loss: 0.2660843551020395\n",
      "epoch 17320: train loss: 0.10339202065190585, test loss: 0.26608500615698183\n",
      "epoch 17321: train loss: 0.10339055117293498, test loss: 0.2660856572331736\n",
      "epoch 17322: train loss: 0.10338908180503477, test loss: 0.2660863083306098\n",
      "epoch 17323: train loss: 0.10338761254818962, test loss: 0.26608695944928523\n",
      "epoch 17324: train loss: 0.10338614340238397, test loss: 0.2660876105891948\n",
      "epoch 17325: train loss: 0.10338467436760222, test loss: 0.2660882617503334\n",
      "epoch 17326: train loss: 0.10338320544382874, test loss: 0.266088912932696\n",
      "epoch 17327: train loss: 0.10338173663104802, test loss: 0.26608956413627743\n",
      "epoch 17328: train loss: 0.10338026792924446, test loss: 0.26609021536107264\n",
      "epoch 17329: train loss: 0.10337879933840245, test loss: 0.2660908666070765\n",
      "epoch 17330: train loss: 0.10337733085850644, test loss: 0.266091517874284\n",
      "epoch 17331: train loss: 0.10337586248954089, test loss: 0.2660921691626899\n",
      "epoch 17332: train loss: 0.10337439423149018, test loss: 0.26609282047228927\n",
      "epoch 17333: train loss: 0.10337292608433878, test loss: 0.26609347180307696\n",
      "epoch 17334: train loss: 0.10337145804807113, test loss: 0.2660941231550478\n",
      "epoch 17335: train loss: 0.10336999012267167, test loss: 0.2660947745281968\n",
      "epoch 17336: train loss: 0.10336852230812482, test loss: 0.2660954259225189\n",
      "epoch 17337: train loss: 0.10336705460441509, test loss: 0.26609607733800883\n",
      "epoch 17338: train loss: 0.10336558701152689, test loss: 0.26609672877466184\n",
      "epoch 17339: train loss: 0.10336411952944466, test loss: 0.26609738023247237\n",
      "epoch 17340: train loss: 0.10336265215815292, test loss: 0.26609803171143576\n",
      "epoch 17341: train loss: 0.10336118489763607, test loss: 0.2660986832115468\n",
      "epoch 17342: train loss: 0.10335971774787861, test loss: 0.2660993347328004\n",
      "epoch 17343: train loss: 0.10335825070886498, test loss: 0.2660999862751914\n",
      "epoch 17344: train loss: 0.1033567837805797, test loss: 0.2661006378387148\n",
      "epoch 17345: train loss: 0.10335531696300722, test loss: 0.26610128942336553\n",
      "epoch 17346: train loss: 0.10335385025613197, test loss: 0.26610194102913853\n",
      "epoch 17347: train loss: 0.10335238365993851, test loss: 0.26610259265602865\n",
      "epoch 17348: train loss: 0.10335091717441128, test loss: 0.2661032443040309\n",
      "epoch 17349: train loss: 0.10334945079953477, test loss: 0.26610389597314016\n",
      "epoch 17350: train loss: 0.10334798453529345, test loss: 0.2661045476633514\n",
      "epoch 17351: train loss: 0.1033465183816719, test loss: 0.26610519937465954\n",
      "epoch 17352: train loss: 0.1033450523386545, test loss: 0.2661058511070594\n",
      "epoch 17353: train loss: 0.10334358640622582, test loss: 0.26610650286054616\n",
      "epoch 17354: train loss: 0.10334212058437035, test loss: 0.2661071546351144\n",
      "epoch 17355: train loss: 0.1033406548730726, test loss: 0.2661078064307595\n",
      "epoch 17356: train loss: 0.10333918927231707, test loss: 0.2661084582474761\n",
      "epoch 17357: train loss: 0.10333772378208828, test loss: 0.2661091100852591\n",
      "epoch 17358: train loss: 0.10333625840237072, test loss: 0.2661097619441035\n",
      "epoch 17359: train loss: 0.10333479313314893, test loss: 0.26611041382400447\n",
      "epoch 17360: train loss: 0.10333332797440743, test loss: 0.2661110657249566\n",
      "epoch 17361: train loss: 0.10333186292613072, test loss: 0.2661117176469549\n",
      "epoch 17362: train loss: 0.10333039798830339, test loss: 0.26611236958999457\n",
      "epoch 17363: train loss: 0.10332893316090991, test loss: 0.2661130215540704\n",
      "epoch 17364: train loss: 0.10332746844393484, test loss: 0.2661136735391772\n",
      "epoch 17365: train loss: 0.10332600383736266, test loss: 0.26611432554531006\n",
      "epoch 17366: train loss: 0.10332453934117802, test loss: 0.266114977572464\n",
      "epoch 17367: train loss: 0.10332307495536536, test loss: 0.26611562962063373\n",
      "epoch 17368: train loss: 0.10332161067990932, test loss: 0.2661162816898145\n",
      "epoch 17369: train loss: 0.10332014651479433, test loss: 0.26611693378000095\n",
      "epoch 17370: train loss: 0.10331868246000506, test loss: 0.2661175858911883\n",
      "epoch 17371: train loss: 0.10331721851552599, test loss: 0.26611823802337137\n",
      "epoch 17372: train loss: 0.10331575468134169, test loss: 0.2661188901765451\n",
      "epoch 17373: train loss: 0.10331429095743676, test loss: 0.26611954235070456\n",
      "epoch 17374: train loss: 0.10331282734379572, test loss: 0.26612019454584457\n",
      "epoch 17375: train loss: 0.10331136384040317, test loss: 0.2661208467619602\n",
      "epoch 17376: train loss: 0.10330990044724364, test loss: 0.26612149899904625\n",
      "epoch 17377: train loss: 0.10330843716430176, test loss: 0.26612215125709787\n",
      "epoch 17378: train loss: 0.10330697399156204, test loss: 0.26612280353610995\n",
      "epoch 17379: train loss: 0.10330551092900912, test loss: 0.26612345583607744\n",
      "epoch 17380: train loss: 0.10330404797662757, test loss: 0.2661241081569952\n",
      "epoch 17381: train loss: 0.10330258513440192, test loss: 0.26612476049885847\n",
      "epoch 17382: train loss: 0.10330112240231681, test loss: 0.2661254128616619\n",
      "epoch 17383: train loss: 0.10329965978035688, test loss: 0.2661260652454007\n",
      "epoch 17384: train loss: 0.10329819726850661, test loss: 0.26612671765006973\n",
      "epoch 17385: train loss: 0.1032967348667507, test loss: 0.26612737007566395\n",
      "epoch 17386: train loss: 0.10329527257507368, test loss: 0.2661280225221784\n",
      "epoch 17387: train loss: 0.10329381039346018, test loss: 0.26612867498960796\n",
      "epoch 17388: train loss: 0.10329234832189481, test loss: 0.26612932747794765\n",
      "epoch 17389: train loss: 0.10329088636036221, test loss: 0.2661299799871924\n",
      "epoch 17390: train loss: 0.10328942450884694, test loss: 0.26613063251733726\n",
      "epoch 17391: train loss: 0.10328796276733365, test loss: 0.2661312850683773\n",
      "epoch 17392: train loss: 0.10328650113580697, test loss: 0.26613193764030724\n",
      "epoch 17393: train loss: 0.10328503961425146, test loss: 0.2661325902331222\n",
      "epoch 17394: train loss: 0.10328357820265183, test loss: 0.26613324284681716\n",
      "epoch 17395: train loss: 0.10328211690099265, test loss: 0.2661338954813871\n",
      "epoch 17396: train loss: 0.10328065570925858, test loss: 0.266134548136827\n",
      "epoch 17397: train loss: 0.10327919462743423, test loss: 0.2661352008131319\n",
      "epoch 17398: train loss: 0.10327773365550426, test loss: 0.26613585351029667\n",
      "epoch 17399: train loss: 0.10327627279345332, test loss: 0.26613650622831636\n",
      "epoch 17400: train loss: 0.10327481204126603, test loss: 0.2661371589671858\n",
      "epoch 17401: train loss: 0.10327335139892703, test loss: 0.2661378117269003\n",
      "epoch 17402: train loss: 0.10327189086642101, test loss: 0.26613846450745465\n",
      "epoch 17403: train loss: 0.10327043044373259, test loss: 0.2661391173088438\n",
      "epoch 17404: train loss: 0.10326897013084643, test loss: 0.2661397701310628\n",
      "epoch 17405: train loss: 0.10326750992774719, test loss: 0.26614042297410667\n",
      "epoch 17406: train loss: 0.10326604983441957, test loss: 0.26614107583797036\n",
      "epoch 17407: train loss: 0.10326458985084816, test loss: 0.2661417287226489\n",
      "epoch 17408: train loss: 0.1032631299770177, test loss: 0.26614238162813725\n",
      "epoch 17409: train loss: 0.10326167021291283, test loss: 0.26614303455443045\n",
      "epoch 17410: train loss: 0.10326021055851822, test loss: 0.2661436875015235\n",
      "epoch 17411: train loss: 0.10325875101381858, test loss: 0.2661443404694113\n",
      "epoch 17412: train loss: 0.10325729157879854, test loss: 0.266144993458089\n",
      "epoch 17413: train loss: 0.10325583225344281, test loss: 0.2661456464675514\n",
      "epoch 17414: train loss: 0.10325437303773612, test loss: 0.2661462994977937\n",
      "epoch 17415: train loss: 0.10325291393166307, test loss: 0.2661469525488108\n",
      "epoch 17416: train loss: 0.10325145493520843, test loss: 0.26614760562059786\n",
      "epoch 17417: train loss: 0.10324999604835686, test loss: 0.2661482587131496\n",
      "epoch 17418: train loss: 0.10324853727109305, test loss: 0.2661489118264613\n",
      "epoch 17419: train loss: 0.10324707860340174, test loss: 0.26614956496052783\n",
      "epoch 17420: train loss: 0.10324562004526762, test loss: 0.2661502181153442\n",
      "epoch 17421: train loss: 0.10324416159667535, test loss: 0.26615087129090537\n",
      "epoch 17422: train loss: 0.1032427032576097, test loss: 0.2661515244872065\n",
      "epoch 17423: train loss: 0.1032412450280554, test loss: 0.26615217770424254\n",
      "epoch 17424: train loss: 0.10323978690799712, test loss: 0.2661528309420085\n",
      "epoch 17425: train loss: 0.1032383288974196, test loss: 0.26615348420049934\n",
      "epoch 17426: train loss: 0.10323687099630754, test loss: 0.2661541374797102\n",
      "epoch 17427: train loss: 0.10323541320464572, test loss: 0.26615479077963594\n",
      "epoch 17428: train loss: 0.1032339555224188, test loss: 0.26615544410027164\n",
      "epoch 17429: train loss: 0.10323249794961158, test loss: 0.2661560974416124\n",
      "epoch 17430: train loss: 0.10323104048620874, test loss: 0.2661567508036531\n",
      "epoch 17431: train loss: 0.10322958313219503, test loss: 0.26615740418638895\n",
      "epoch 17432: train loss: 0.10322812588755521, test loss: 0.26615805758981476\n",
      "epoch 17433: train loss: 0.10322666875227401, test loss: 0.2661587110139257\n",
      "epoch 17434: train loss: 0.10322521172633622, test loss: 0.2661593644587167\n",
      "epoch 17435: train loss: 0.10322375480972655, test loss: 0.2661600179241829\n",
      "epoch 17436: train loss: 0.10322229800242974, test loss: 0.2661606714103193\n",
      "epoch 17437: train loss: 0.1032208413044306, test loss: 0.2661613249171208\n",
      "epoch 17438: train loss: 0.10321938471571385, test loss: 0.2661619784445826\n",
      "epoch 17439: train loss: 0.10321792823626424, test loss: 0.26616263199269957\n",
      "epoch 17440: train loss: 0.10321647186606658, test loss: 0.26616328556146684\n",
      "epoch 17441: train loss: 0.10321501560510558, test loss: 0.2661639391508795\n",
      "epoch 17442: train loss: 0.10321355945336608, test loss: 0.2661645927609324\n",
      "epoch 17443: train loss: 0.1032121034108328, test loss: 0.2661652463916208\n",
      "epoch 17444: train loss: 0.10321064747749055, test loss: 0.2661659000429395\n",
      "epoch 17445: train loss: 0.10320919165332412, test loss: 0.2661665537148838\n",
      "epoch 17446: train loss: 0.10320773593831828, test loss: 0.2661672074074485\n",
      "epoch 17447: train loss: 0.10320628033245778, test loss: 0.2661678611206287\n",
      "epoch 17448: train loss: 0.10320482483572746, test loss: 0.26616851485441956\n",
      "epoch 17449: train loss: 0.10320336944811212, test loss: 0.26616916860881595\n",
      "epoch 17450: train loss: 0.10320191416959651, test loss: 0.26616982238381315\n",
      "epoch 17451: train loss: 0.10320045900016545, test loss: 0.2661704761794059\n",
      "epoch 17452: train loss: 0.10319900393980375, test loss: 0.2661711299955895\n",
      "epoch 17453: train loss: 0.1031975489884962, test loss: 0.26617178383235884\n",
      "epoch 17454: train loss: 0.1031960941462276, test loss: 0.2661724376897091\n",
      "epoch 17455: train loss: 0.10319463941298285, test loss: 0.2661730915676352\n",
      "epoch 17456: train loss: 0.10319318478874663, test loss: 0.26617374546613226\n",
      "epoch 17457: train loss: 0.10319173027350387, test loss: 0.26617439938519527\n",
      "epoch 17458: train loss: 0.1031902758672393, test loss: 0.2661750533248193\n",
      "epoch 17459: train loss: 0.1031888215699378, test loss: 0.2661757072849995\n",
      "epoch 17460: train loss: 0.10318736738158417, test loss: 0.2661763612657308\n",
      "epoch 17461: train loss: 0.10318591330216326, test loss: 0.2661770152670084\n",
      "epoch 17462: train loss: 0.1031844593316599, test loss: 0.2661776692888271\n",
      "epoch 17463: train loss: 0.1031830054700589, test loss: 0.26617832333118213\n",
      "epoch 17464: train loss: 0.10318155171734514, test loss: 0.2661789773940687\n",
      "epoch 17465: train loss: 0.10318009807350342, test loss: 0.26617963147748164\n",
      "epoch 17466: train loss: 0.10317864453851863, test loss: 0.26618028558141604\n",
      "epoch 17467: train loss: 0.10317719111237554, test loss: 0.26618093970586704\n",
      "epoch 17468: train loss: 0.1031757377950591, test loss: 0.2661815938508297\n",
      "epoch 17469: train loss: 0.1031742845865541, test loss: 0.2661822480162989\n",
      "epoch 17470: train loss: 0.10317283148684542, test loss: 0.26618290220227\n",
      "epoch 17471: train loss: 0.10317137849591788, test loss: 0.2661835564087378\n",
      "epoch 17472: train loss: 0.10316992561375639, test loss: 0.2661842106356976\n",
      "epoch 17473: train loss: 0.10316847284034582, test loss: 0.2661848648831443\n",
      "epoch 17474: train loss: 0.10316702017567099, test loss: 0.2661855191510729\n",
      "epoch 17475: train loss: 0.10316556761971682, test loss: 0.2661861734394788\n",
      "epoch 17476: train loss: 0.10316411517246815, test loss: 0.2661868277483568\n",
      "epoch 17477: train loss: 0.10316266283390987, test loss: 0.26618748207770204\n",
      "epoch 17478: train loss: 0.10316121060402686, test loss: 0.2661881364275096\n",
      "epoch 17479: train loss: 0.10315975848280401, test loss: 0.2661887907977744\n",
      "epoch 17480: train loss: 0.10315830647022621, test loss: 0.2661894451884918\n",
      "epoch 17481: train loss: 0.10315685456627832, test loss: 0.2661900995996567\n",
      "epoch 17482: train loss: 0.10315540277094527, test loss: 0.2661907540312643\n",
      "epoch 17483: train loss: 0.10315395108421194, test loss: 0.2661914084833096\n",
      "epoch 17484: train loss: 0.10315249950606323, test loss: 0.2661920629557876\n",
      "epoch 17485: train loss: 0.10315104803648403, test loss: 0.2661927174486935\n",
      "epoch 17486: train loss: 0.10314959667545924, test loss: 0.2661933719620223\n",
      "epoch 17487: train loss: 0.1031481454229738, test loss: 0.2661940264957692\n",
      "epoch 17488: train loss: 0.10314669427901257, test loss: 0.2661946810499291\n",
      "epoch 17489: train loss: 0.10314524324356056, test loss: 0.26619533562449726\n",
      "epoch 17490: train loss: 0.10314379231660258, test loss: 0.26619599021946877\n",
      "epoch 17491: train loss: 0.10314234149812357, test loss: 0.26619664483483857\n",
      "epoch 17492: train loss: 0.10314089078810852, test loss: 0.26619729947060183\n",
      "epoch 17493: train loss: 0.10313944018654228, test loss: 0.2661979541267538\n",
      "epoch 17494: train loss: 0.10313798969340983, test loss: 0.2661986088032892\n",
      "epoch 17495: train loss: 0.10313653930869604, test loss: 0.26619926350020345\n",
      "epoch 17496: train loss: 0.1031350890323859, test loss: 0.2661999182174915\n",
      "epoch 17497: train loss: 0.10313363886446435, test loss: 0.2662005729551486\n",
      "epoch 17498: train loss: 0.10313218880491631, test loss: 0.2662012277131696\n",
      "epoch 17499: train loss: 0.1031307388537267, test loss: 0.2662018824915497\n",
      "epoch 17500: train loss: 0.10312928901088048, test loss: 0.26620253729028415\n",
      "epoch 17501: train loss: 0.10312783927636264, test loss: 0.26620319210936777\n",
      "epoch 17502: train loss: 0.10312638965015809, test loss: 0.26620384694879584\n",
      "epoch 17503: train loss: 0.10312494013225182, test loss: 0.26620450180856353\n",
      "epoch 17504: train loss: 0.10312349072262875, test loss: 0.2662051566886658\n",
      "epoch 17505: train loss: 0.10312204142127385, test loss: 0.2662058115890978\n",
      "epoch 17506: train loss: 0.10312059222817208, test loss: 0.2662064665098547\n",
      "epoch 17507: train loss: 0.10311914314330842, test loss: 0.26620712145093145\n",
      "epoch 17508: train loss: 0.10311769416666787, test loss: 0.2662077764123232\n",
      "epoch 17509: train loss: 0.10311624529823536, test loss: 0.2662084313940251\n",
      "epoch 17510: train loss: 0.10311479653799588, test loss: 0.2662090863960323\n",
      "epoch 17511: train loss: 0.10311334788593438, test loss: 0.26620974141834\n",
      "epoch 17512: train loss: 0.10311189934203589, test loss: 0.2662103964609431\n",
      "epoch 17513: train loss: 0.10311045090628537, test loss: 0.2662110515238368\n",
      "epoch 17514: train loss: 0.10310900257866783, test loss: 0.2662117066070162\n",
      "epoch 17515: train loss: 0.10310755435916821, test loss: 0.26621236171047635\n",
      "epoch 17516: train loss: 0.10310610624777157, test loss: 0.26621301683421256\n",
      "epoch 17517: train loss: 0.10310465824446284, test loss: 0.26621367197821977\n",
      "epoch 17518: train loss: 0.1031032103492271, test loss: 0.26621432714249316\n",
      "epoch 17519: train loss: 0.10310176256204928, test loss: 0.2662149823270279\n",
      "epoch 17520: train loss: 0.10310031488291439, test loss: 0.26621563753181904\n",
      "epoch 17521: train loss: 0.10309886731180751, test loss: 0.2662162927568617\n",
      "epoch 17522: train loss: 0.10309741984871358, test loss: 0.26621694800215107\n",
      "epoch 17523: train loss: 0.10309597249361763, test loss: 0.2662176032676821\n",
      "epoch 17524: train loss: 0.1030945252465047, test loss: 0.26621825855345016\n",
      "epoch 17525: train loss: 0.1030930781073598, test loss: 0.26621891385945023\n",
      "epoch 17526: train loss: 0.10309163107616792, test loss: 0.26621956918567746\n",
      "epoch 17527: train loss: 0.10309018415291414, test loss: 0.266220224532127\n",
      "epoch 17528: train loss: 0.10308873733758346, test loss: 0.26622087989879395\n",
      "epoch 17529: train loss: 0.10308729063016095, test loss: 0.26622153528567344\n",
      "epoch 17530: train loss: 0.10308584403063158, test loss: 0.26622219069276054\n",
      "epoch 17531: train loss: 0.10308439753898041, test loss: 0.26622284612005054\n",
      "epoch 17532: train loss: 0.10308295115519252, test loss: 0.26622350156753855\n",
      "epoch 17533: train loss: 0.10308150487925292, test loss: 0.26622415703521946\n",
      "epoch 17534: train loss: 0.10308005871114669, test loss: 0.2662248125230887\n",
      "epoch 17535: train loss: 0.10307861265085883, test loss: 0.26622546803114133\n",
      "epoch 17536: train loss: 0.10307716669837444, test loss: 0.26622612355937236\n",
      "epoch 17537: train loss: 0.10307572085367851, test loss: 0.2662267791077771\n",
      "epoch 17538: train loss: 0.1030742751167562, test loss: 0.26622743467635057\n",
      "epoch 17539: train loss: 0.10307282948759248, test loss: 0.266228090265088\n",
      "epoch 17540: train loss: 0.10307138396617248, test loss: 0.2662287458739845\n",
      "epoch 17541: train loss: 0.10306993855248123, test loss: 0.2662294015030351\n",
      "epoch 17542: train loss: 0.10306849324650379, test loss: 0.2662300571522351\n",
      "epoch 17543: train loss: 0.10306704804822528, test loss: 0.26623071282157956\n",
      "epoch 17544: train loss: 0.10306560295763077, test loss: 0.2662313685110637\n",
      "epoch 17545: train loss: 0.10306415797470528, test loss: 0.2662320242206826\n",
      "epoch 17546: train loss: 0.10306271309943395, test loss: 0.26623267995043143\n",
      "epoch 17547: train loss: 0.10306126833180188, test loss: 0.2662333357003054\n",
      "epoch 17548: train loss: 0.10305982367179407, test loss: 0.2662339914702995\n",
      "epoch 17549: train loss: 0.10305837911939571, test loss: 0.266234647260409\n",
      "epoch 17550: train loss: 0.10305693467459186, test loss: 0.2662353030706291\n",
      "epoch 17551: train loss: 0.1030554903373676, test loss: 0.26623595890095486\n",
      "epoch 17552: train loss: 0.10305404610770803, test loss: 0.26623661475138144\n",
      "epoch 17553: train loss: 0.10305260198559829, test loss: 0.2662372706219041\n",
      "epoch 17554: train loss: 0.10305115797102343, test loss: 0.2662379265125178\n",
      "epoch 17555: train loss: 0.10304971406396862, test loss: 0.26623858242321796\n",
      "epoch 17556: train loss: 0.10304827026441896, test loss: 0.2662392383539995\n",
      "epoch 17557: train loss: 0.10304682657235953, test loss: 0.2662398943048577\n",
      "epoch 17558: train loss: 0.10304538298777546, test loss: 0.26624055027578775\n",
      "epoch 17559: train loss: 0.10304393951065188, test loss: 0.26624120626678477\n",
      "epoch 17560: train loss: 0.10304249614097392, test loss: 0.2662418622778439\n",
      "epoch 17561: train loss: 0.10304105287872671, test loss: 0.2662425183089602\n",
      "epoch 17562: train loss: 0.10303960972389536, test loss: 0.26624317436012906\n",
      "epoch 17563: train loss: 0.103038166676465, test loss: 0.26624383043134553\n",
      "epoch 17564: train loss: 0.10303672373642078, test loss: 0.2662444865226049\n",
      "epoch 17565: train loss: 0.10303528090374785, test loss: 0.2662451426339021\n",
      "epoch 17566: train loss: 0.10303383817843134, test loss: 0.2662457987652326\n",
      "epoch 17567: train loss: 0.10303239556045636, test loss: 0.26624645491659116\n",
      "epoch 17568: train loss: 0.10303095304980815, test loss: 0.26624711108797333\n",
      "epoch 17569: train loss: 0.10302951064647176, test loss: 0.26624776727937416\n",
      "epoch 17570: train loss: 0.10302806835043238, test loss: 0.26624842349078887\n",
      "epoch 17571: train loss: 0.10302662616167517, test loss: 0.26624907972221246\n",
      "epoch 17572: train loss: 0.1030251840801853, test loss: 0.26624973597364027\n",
      "epoch 17573: train loss: 0.10302374210594792, test loss: 0.2662503922450675\n",
      "epoch 17574: train loss: 0.1030223002389482, test loss: 0.26625104853648923\n",
      "epoch 17575: train loss: 0.1030208584791713, test loss: 0.26625170484790067\n",
      "epoch 17576: train loss: 0.10301941682660239, test loss: 0.266252361179297\n",
      "epoch 17577: train loss: 0.10301797528122666, test loss: 0.2662530175306735\n",
      "epoch 17578: train loss: 0.10301653384302928, test loss: 0.2662536739020252\n",
      "epoch 17579: train loss: 0.10301509251199541, test loss: 0.2662543302933474\n",
      "epoch 17580: train loss: 0.10301365128811024, test loss: 0.26625498670463515\n",
      "epoch 17581: train loss: 0.10301221017135898, test loss: 0.2662556431358838\n",
      "epoch 17582: train loss: 0.10301076916172681, test loss: 0.26625629958708835\n",
      "epoch 17583: train loss: 0.1030093282591989, test loss: 0.2662569560582442\n",
      "epoch 17584: train loss: 0.10300788746376044, test loss: 0.26625761254934643\n",
      "epoch 17585: train loss: 0.10300644677539669, test loss: 0.26625826906039024\n",
      "epoch 17586: train loss: 0.10300500619409277, test loss: 0.2662589255913709\n",
      "epoch 17587: train loss: 0.10300356571983393, test loss: 0.26625958214228335\n",
      "epoch 17588: train loss: 0.10300212535260535, test loss: 0.2662602387131231\n",
      "epoch 17589: train loss: 0.10300068509239228, test loss: 0.26626089530388514\n",
      "epoch 17590: train loss: 0.1029992449391799, test loss: 0.26626155191456474\n",
      "epoch 17591: train loss: 0.1029978048929534, test loss: 0.26626220854515714\n",
      "epoch 17592: train loss: 0.10299636495369803, test loss: 0.2662628651956574\n",
      "epoch 17593: train loss: 0.10299492512139903, test loss: 0.2662635218660609\n",
      "epoch 17594: train loss: 0.10299348539604161, test loss: 0.2662641785563627\n",
      "epoch 17595: train loss: 0.10299204577761097, test loss: 0.2662648352665581\n",
      "epoch 17596: train loss: 0.10299060626609234, test loss: 0.26626549199664223\n",
      "epoch 17597: train loss: 0.10298916686147098, test loss: 0.2662661487466103\n",
      "epoch 17598: train loss: 0.1029877275637321, test loss: 0.26626680551645754\n",
      "epoch 17599: train loss: 0.10298628837286099, test loss: 0.26626746230617915\n",
      "epoch 17600: train loss: 0.10298484928884283, test loss: 0.2662681191157703\n",
      "epoch 17601: train loss: 0.10298341031166287, test loss: 0.2662687759452263\n",
      "epoch 17602: train loss: 0.1029819714413064, test loss: 0.26626943279454224\n",
      "epoch 17603: train loss: 0.10298053267775865, test loss: 0.26627008966371346\n",
      "epoch 17604: train loss: 0.10297909402100483, test loss: 0.2662707465527351\n",
      "epoch 17605: train loss: 0.10297765547103026, test loss: 0.2662714034616023\n",
      "epoch 17606: train loss: 0.10297621702782016, test loss: 0.26627206039031037\n",
      "epoch 17607: train loss: 0.10297477869135982, test loss: 0.2662727173388545\n",
      "epoch 17608: train loss: 0.10297334046163444, test loss: 0.26627337430722986\n",
      "epoch 17609: train loss: 0.10297190233862936, test loss: 0.2662740312954318\n",
      "epoch 17610: train loss: 0.1029704643223298, test loss: 0.2662746883034554\n",
      "epoch 17611: train loss: 0.10296902641272108, test loss: 0.2662753453312959\n",
      "epoch 17612: train loss: 0.10296758860978844, test loss: 0.26627600237894855\n",
      "epoch 17613: train loss: 0.10296615091351717, test loss: 0.26627665944640866\n",
      "epoch 17614: train loss: 0.10296471332389254, test loss: 0.26627731653367126\n",
      "epoch 17615: train loss: 0.10296327584089987, test loss: 0.26627797364073175\n",
      "epoch 17616: train loss: 0.10296183846452438, test loss: 0.2662786307675852\n",
      "epoch 17617: train loss: 0.10296040119475144, test loss: 0.266279287914227\n",
      "epoch 17618: train loss: 0.10295896403156629, test loss: 0.2662799450806523\n",
      "epoch 17619: train loss: 0.10295752697495424, test loss: 0.2662806022668562\n",
      "epoch 17620: train loss: 0.10295609002490057, test loss: 0.2662812594728341\n",
      "epoch 17621: train loss: 0.10295465318139063, test loss: 0.2662819166985812\n",
      "epoch 17622: train loss: 0.10295321644440969, test loss: 0.26628257394409277\n",
      "epoch 17623: train loss: 0.10295177981394307, test loss: 0.26628323120936387\n",
      "epoch 17624: train loss: 0.10295034328997608, test loss: 0.26628388849438983\n",
      "epoch 17625: train loss: 0.10294890687249401, test loss: 0.266284545799166\n",
      "epoch 17626: train loss: 0.10294747056148221, test loss: 0.2662852031236875\n",
      "epoch 17627: train loss: 0.10294603435692597, test loss: 0.2662858604679495\n",
      "epoch 17628: train loss: 0.10294459825881062, test loss: 0.2662865178319474\n",
      "epoch 17629: train loss: 0.1029431622671215, test loss: 0.2662871752156763\n",
      "epoch 17630: train loss: 0.10294172638184391, test loss: 0.2662878326191316\n",
      "epoch 17631: train loss: 0.10294029060296322, test loss: 0.26628849004230826\n",
      "epoch 17632: train loss: 0.10293885493046474, test loss: 0.2662891474852018\n",
      "epoch 17633: train loss: 0.10293741936433382, test loss: 0.26628980494780735\n",
      "epoch 17634: train loss: 0.10293598390455579, test loss: 0.2662904624301202\n",
      "epoch 17635: train loss: 0.10293454855111595, test loss: 0.2662911199321354\n",
      "epoch 17636: train loss: 0.10293311330399973, test loss: 0.26629177745384847\n",
      "epoch 17637: train loss: 0.1029316781631924, test loss: 0.2662924349952547\n",
      "epoch 17638: train loss: 0.10293024312867935, test loss: 0.2662930925563489\n",
      "epoch 17639: train loss: 0.10292880820044595, test loss: 0.2662937501371267\n",
      "epoch 17640: train loss: 0.10292737337847754, test loss: 0.26629440773758334\n",
      "epoch 17641: train loss: 0.10292593866275944, test loss: 0.26629506535771386\n",
      "epoch 17642: train loss: 0.10292450405327706, test loss: 0.26629572299751364\n",
      "epoch 17643: train loss: 0.10292306955001576, test loss: 0.26629638065697797\n",
      "epoch 17644: train loss: 0.10292163515296089, test loss: 0.266297038336102\n",
      "epoch 17645: train loss: 0.10292020086209783, test loss: 0.2662976960348811\n",
      "epoch 17646: train loss: 0.10291876667741198, test loss: 0.26629835375331046\n",
      "epoch 17647: train loss: 0.10291733259888865, test loss: 0.26629901149138535\n",
      "epoch 17648: train loss: 0.10291589862651328, test loss: 0.266299669249101\n",
      "epoch 17649: train loss: 0.10291446476027126, test loss: 0.2663003270264527\n",
      "epoch 17650: train loss: 0.10291303100014793, test loss: 0.2663009848234358\n",
      "epoch 17651: train loss: 0.10291159734612869, test loss: 0.2663016426400453\n",
      "epoch 17652: train loss: 0.10291016379819894, test loss: 0.2663023004762766\n",
      "epoch 17653: train loss: 0.10290873035634407, test loss: 0.2663029583321252\n",
      "epoch 17654: train loss: 0.1029072970205495, test loss: 0.2663036162075861\n",
      "epoch 17655: train loss: 0.10290586379080058, test loss: 0.26630427410265456\n",
      "epoch 17656: train loss: 0.10290443066708276, test loss: 0.2663049320173259\n",
      "epoch 17657: train loss: 0.10290299764938143, test loss: 0.2663055899515955\n",
      "epoch 17658: train loss: 0.102901564737682, test loss: 0.2663062479054585\n",
      "epoch 17659: train loss: 0.10290013193196985, test loss: 0.26630690587891015\n",
      "epoch 17660: train loss: 0.10289869923223044, test loss: 0.2663075638719458\n",
      "epoch 17661: train loss: 0.10289726663844917, test loss: 0.2663082218845607\n",
      "epoch 17662: train loss: 0.10289583415061145, test loss: 0.2663088799167502\n",
      "epoch 17663: train loss: 0.10289440176870272, test loss: 0.2663095379685093\n",
      "epoch 17664: train loss: 0.10289296949270837, test loss: 0.2663101960398336\n",
      "epoch 17665: train loss: 0.10289153732261389, test loss: 0.2663108541307182\n",
      "epoch 17666: train loss: 0.10289010525840465, test loss: 0.2663115122411584\n",
      "epoch 17667: train loss: 0.10288867330006612, test loss: 0.2663121703711495\n",
      "epoch 17668: train loss: 0.10288724144758372, test loss: 0.2663128285206869\n",
      "epoch 17669: train loss: 0.10288580970094291, test loss: 0.26631348668976557\n",
      "epoch 17670: train loss: 0.10288437806012912, test loss: 0.26631414487838123\n",
      "epoch 17671: train loss: 0.10288294652512779, test loss: 0.26631480308652866\n",
      "epoch 17672: train loss: 0.10288151509592434, test loss: 0.2663154613142035\n",
      "epoch 17673: train loss: 0.10288008377250428, test loss: 0.266316119561401\n",
      "epoch 17674: train loss: 0.10287865255485305, test loss: 0.2663167778281163\n",
      "epoch 17675: train loss: 0.1028772214429561, test loss: 0.2663174361143448\n",
      "epoch 17676: train loss: 0.10287579043679886, test loss: 0.26631809442008175\n",
      "epoch 17677: train loss: 0.10287435953636685, test loss: 0.26631875274532246\n",
      "epoch 17678: train loss: 0.10287292874164546, test loss: 0.2663194110900622\n",
      "epoch 17679: train loss: 0.10287149805262022, test loss: 0.26632006945429626\n",
      "epoch 17680: train loss: 0.10287006746927659, test loss: 0.26632072783801997\n",
      "epoch 17681: train loss: 0.10286863699160004, test loss: 0.2663213862412286\n",
      "epoch 17682: train loss: 0.10286720661957602, test loss: 0.2663220446639174\n",
      "epoch 17683: train loss: 0.10286577635319005, test loss: 0.26632270310608164\n",
      "epoch 17684: train loss: 0.1028643461924276, test loss: 0.2663233615677168\n",
      "epoch 17685: train loss: 0.10286291613727414, test loss: 0.26632402004881794\n",
      "epoch 17686: train loss: 0.10286148618771516, test loss: 0.2663246785493806\n",
      "epoch 17687: train loss: 0.1028600563437362, test loss: 0.2663253370693999\n",
      "epoch 17688: train loss: 0.10285862660532268, test loss: 0.2663259956088712\n",
      "epoch 17689: train loss: 0.10285719697246015, test loss: 0.26632665416778983\n",
      "epoch 17690: train loss: 0.10285576744513407, test loss: 0.26632731274615096\n",
      "epoch 17691: train loss: 0.10285433802333, test loss: 0.26632797134395014\n",
      "epoch 17692: train loss: 0.10285290870703338, test loss: 0.2663286299611824\n",
      "epoch 17693: train loss: 0.10285147949622975, test loss: 0.2663292885978432\n",
      "epoch 17694: train loss: 0.10285005039090461, test loss: 0.266329947253928\n",
      "epoch 17695: train loss: 0.10284862139104352, test loss: 0.26633060592943175\n",
      "epoch 17696: train loss: 0.10284719249663192, test loss: 0.26633126462435003\n",
      "epoch 17697: train loss: 0.10284576370765539, test loss: 0.26633192333867794\n",
      "epoch 17698: train loss: 0.10284433502409941, test loss: 0.266332582072411\n",
      "epoch 17699: train loss: 0.10284290644594954, test loss: 0.2663332408255444\n",
      "epoch 17700: train loss: 0.10284147797319129, test loss: 0.2663338995980736\n",
      "epoch 17701: train loss: 0.10284004960581021, test loss: 0.26633455838999376\n",
      "epoch 17702: train loss: 0.1028386213437918, test loss: 0.2663352172013001\n",
      "epoch 17703: train loss: 0.10283719318712162, test loss: 0.2663358760319883\n",
      "epoch 17704: train loss: 0.1028357651357852, test loss: 0.2663365348820532\n",
      "epoch 17705: train loss: 0.10283433718976812, test loss: 0.2663371937514906\n",
      "epoch 17706: train loss: 0.10283290934905585, test loss: 0.26633785264029547\n",
      "epoch 17707: train loss: 0.102831481613634, test loss: 0.2663385115484633\n",
      "epoch 17708: train loss: 0.10283005398348809, test loss: 0.2663391704759894\n",
      "epoch 17709: train loss: 0.10282862645860369, test loss: 0.26633982942286893\n",
      "epoch 17710: train loss: 0.10282719903896637, test loss: 0.26634048838909746\n",
      "epoch 17711: train loss: 0.10282577172456164, test loss: 0.2663411473746702\n",
      "epoch 17712: train loss: 0.1028243445153751, test loss: 0.2663418063795824\n",
      "epoch 17713: train loss: 0.10282291741139232, test loss: 0.26634246540382944\n",
      "epoch 17714: train loss: 0.10282149041259885, test loss: 0.2663431244474068\n",
      "epoch 17715: train loss: 0.10282006351898026, test loss: 0.2663437835103097\n",
      "epoch 17716: train loss: 0.10281863673052212, test loss: 0.26634444259253326\n",
      "epoch 17717: train loss: 0.10281721004721002, test loss: 0.26634510169407316\n",
      "epoch 17718: train loss: 0.10281578346902955, test loss: 0.26634576081492445\n",
      "epoch 17719: train loss: 0.10281435699596625, test loss: 0.26634641995508257\n",
      "epoch 17720: train loss: 0.10281293062800571, test loss: 0.266347079114543\n",
      "epoch 17721: train loss: 0.10281150436513359, test loss: 0.2663477382933009\n",
      "epoch 17722: train loss: 0.10281007820733538, test loss: 0.2663483974913516\n",
      "epoch 17723: train loss: 0.10280865215459677, test loss: 0.2663490567086905\n",
      "epoch 17724: train loss: 0.10280722620690326, test loss: 0.266349715945313\n",
      "epoch 17725: train loss: 0.10280580036424053, test loss: 0.26635037520121435\n",
      "epoch 17726: train loss: 0.10280437462659414, test loss: 0.2663510344763898\n",
      "epoch 17727: train loss: 0.10280294899394969, test loss: 0.26635169377083495\n",
      "epoch 17728: train loss: 0.10280152346629279, test loss: 0.26635235308454497\n",
      "epoch 17729: train loss: 0.10280009804360908, test loss: 0.26635301241751513\n",
      "epoch 17730: train loss: 0.10279867272588414, test loss: 0.26635367176974095\n",
      "epoch 17731: train loss: 0.1027972475131036, test loss: 0.26635433114121776\n",
      "epoch 17732: train loss: 0.10279582240525308, test loss: 0.2663549905319406\n",
      "epoch 17733: train loss: 0.10279439740231817, test loss: 0.26635564994190525\n",
      "epoch 17734: train loss: 0.10279297250428457, test loss: 0.2663563093711069\n",
      "epoch 17735: train loss: 0.10279154771113781, test loss: 0.2663569688195407\n",
      "epoch 17736: train loss: 0.10279012302286357, test loss: 0.26635762828720233\n",
      "epoch 17737: train loss: 0.1027886984394475, test loss: 0.266358287774087\n",
      "epoch 17738: train loss: 0.10278727396087518, test loss: 0.26635894728018994\n",
      "epoch 17739: train loss: 0.10278584958713234, test loss: 0.26635960680550663\n",
      "epoch 17740: train loss: 0.10278442531820453, test loss: 0.26636026635003246\n",
      "epoch 17741: train loss: 0.10278300115407742, test loss: 0.2663609259137627\n",
      "epoch 17742: train loss: 0.10278157709473668, test loss: 0.2663615854966927\n",
      "epoch 17743: train loss: 0.10278015314016793, test loss: 0.26636224509881795\n",
      "epoch 17744: train loss: 0.10277872929035685, test loss: 0.2663629047201337\n",
      "epoch 17745: train loss: 0.10277730554528908, test loss: 0.2663635643606353\n",
      "epoch 17746: train loss: 0.10277588190495028, test loss: 0.26636422402031806\n",
      "epoch 17747: train loss: 0.10277445836932607, test loss: 0.26636488369917755\n",
      "epoch 17748: train loss: 0.10277303493840219, test loss: 0.266365543397209\n",
      "epoch 17749: train loss: 0.10277161161216426, test loss: 0.26636620311440773\n",
      "epoch 17750: train loss: 0.10277018839059796, test loss: 0.2663668628507692\n",
      "epoch 17751: train loss: 0.10276876527368893, test loss: 0.26636752260628865\n",
      "epoch 17752: train loss: 0.10276734226142292, test loss: 0.2663681823809616\n",
      "epoch 17753: train loss: 0.10276591935378553, test loss: 0.26636884217478324\n",
      "epoch 17754: train loss: 0.10276449655076247, test loss: 0.26636950198774917\n",
      "epoch 17755: train loss: 0.10276307385233943, test loss: 0.2663701618198546\n",
      "epoch 17756: train loss: 0.1027616512585021, test loss: 0.266370821671095\n",
      "epoch 17757: train loss: 0.10276022876923614, test loss: 0.26637148154146567\n",
      "epoch 17758: train loss: 0.10275880638452728, test loss: 0.266372141430962\n",
      "epoch 17759: train loss: 0.10275738410436118, test loss: 0.2663728013395793\n",
      "epoch 17760: train loss: 0.10275596192872355, test loss: 0.26637346126731304\n",
      "epoch 17761: train loss: 0.1027545398576001, test loss: 0.26637412121415854\n",
      "epoch 17762: train loss: 0.10275311789097649, test loss: 0.26637478118011126\n",
      "epoch 17763: train loss: 0.10275169602883849, test loss: 0.2663754411651665\n",
      "epoch 17764: train loss: 0.10275027427117177, test loss: 0.26637610116931965\n",
      "epoch 17765: train loss: 0.10274885261796203, test loss: 0.2663767611925661\n",
      "epoch 17766: train loss: 0.10274743106919501, test loss: 0.26637742123490127\n",
      "epoch 17767: train loss: 0.10274600962485643, test loss: 0.2663780812963205\n",
      "epoch 17768: train loss: 0.102744588284932, test loss: 0.26637874137681916\n",
      "epoch 17769: train loss: 0.1027431670494074, test loss: 0.26637940147639266\n",
      "epoch 17770: train loss: 0.10274174591826842, test loss: 0.2663800615950364\n",
      "epoch 17771: train loss: 0.10274032489150076, test loss: 0.2663807217327458\n",
      "epoch 17772: train loss: 0.10273890396909016, test loss: 0.2663813818895161\n",
      "epoch 17773: train loss: 0.10273748315102232, test loss: 0.26638204206534283\n",
      "epoch 17774: train loss: 0.102736062437283, test loss: 0.2663827022602214\n",
      "epoch 17775: train loss: 0.10273464182785798, test loss: 0.2663833624741471\n",
      "epoch 17776: train loss: 0.1027332213227329, test loss: 0.26638402270711536\n",
      "epoch 17777: train loss: 0.1027318009218936, test loss: 0.2663846829591216\n",
      "epoch 17778: train loss: 0.10273038062532577, test loss: 0.26638534323016116\n",
      "epoch 17779: train loss: 0.1027289604330152, test loss: 0.26638600352022945\n",
      "epoch 17780: train loss: 0.10272754034494763, test loss: 0.26638666382932197\n",
      "epoch 17781: train loss: 0.10272612036110879, test loss: 0.26638732415743394\n",
      "epoch 17782: train loss: 0.10272470048148445, test loss: 0.26638798450456097\n",
      "epoch 17783: train loss: 0.1027232807060604, test loss: 0.2663886448706983\n",
      "epoch 17784: train loss: 0.10272186103482238, test loss: 0.26638930525584126\n",
      "epoch 17785: train loss: 0.10272044146775615, test loss: 0.26638996565998546\n",
      "epoch 17786: train loss: 0.10271902200484748, test loss: 0.2663906260831262\n",
      "epoch 17787: train loss: 0.10271760264608215, test loss: 0.26639128652525895\n",
      "epoch 17788: train loss: 0.10271618339144595, test loss: 0.2663919469863789\n",
      "epoch 17789: train loss: 0.10271476424092464, test loss: 0.2663926074664817\n",
      "epoch 17790: train loss: 0.10271334519450398, test loss: 0.2663932679655627\n",
      "epoch 17791: train loss: 0.10271192625216981, test loss: 0.2663939284836172\n",
      "epoch 17792: train loss: 0.10271050741390787, test loss: 0.2663945890206407\n",
      "epoch 17793: train loss: 0.10270908867970396, test loss: 0.2663952495766287\n",
      "epoch 17794: train loss: 0.10270767004954387, test loss: 0.26639591015157643\n",
      "epoch 17795: train loss: 0.10270625152341338, test loss: 0.2663965707454793\n",
      "epoch 17796: train loss: 0.10270483310129834, test loss: 0.2663972313583329\n",
      "epoch 17797: train loss: 0.10270341478318447, test loss: 0.26639789199013253\n",
      "epoch 17798: train loss: 0.10270199656905764, test loss: 0.26639855264087364\n",
      "epoch 17799: train loss: 0.10270057845890364, test loss: 0.2663992133105515\n",
      "epoch 17800: train loss: 0.10269916045270824, test loss: 0.2663998739991619\n",
      "epoch 17801: train loss: 0.10269774255045729, test loss: 0.2664005347066998\n",
      "epoch 17802: train loss: 0.1026963247521366, test loss: 0.2664011954331609\n",
      "epoch 17803: train loss: 0.10269490705773197, test loss: 0.2664018561785405\n",
      "epoch 17804: train loss: 0.10269348946722923, test loss: 0.2664025169428341\n",
      "epoch 17805: train loss: 0.1026920719806142, test loss: 0.2664031777260371\n",
      "epoch 17806: train loss: 0.1026906545978727, test loss: 0.2664038385281449\n",
      "epoch 17807: train loss: 0.10268923731899057, test loss: 0.26640449934915295\n",
      "epoch 17808: train loss: 0.10268782014395363, test loss: 0.26640516018905663\n",
      "epoch 17809: train loss: 0.10268640307274772, test loss: 0.2664058210478515\n",
      "epoch 17810: train loss: 0.10268498610535869, test loss: 0.2664064819255328\n",
      "epoch 17811: train loss: 0.10268356924177235, test loss: 0.2664071428220961\n",
      "epoch 17812: train loss: 0.10268215248197451, test loss: 0.26640780373753675\n",
      "epoch 17813: train loss: 0.1026807358259511, test loss: 0.2664084646718502\n",
      "epoch 17814: train loss: 0.10267931927368792, test loss: 0.26640912562503194\n",
      "epoch 17815: train loss: 0.1026779028251708, test loss: 0.26640978659707737\n",
      "epoch 17816: train loss: 0.1026764864803856, test loss: 0.2664104475879818\n",
      "epoch 17817: train loss: 0.10267507023931823, test loss: 0.26641110859774086\n",
      "epoch 17818: train loss: 0.10267365410195448, test loss: 0.2664117696263499\n",
      "epoch 17819: train loss: 0.10267223806828024, test loss: 0.26641243067380427\n",
      "epoch 17820: train loss: 0.10267082213828135, test loss: 0.2664130917400995\n",
      "epoch 17821: train loss: 0.10266940631194373, test loss: 0.26641375282523105\n",
      "epoch 17822: train loss: 0.1026679905892532, test loss: 0.2664144139291944\n",
      "epoch 17823: train loss: 0.10266657497019564, test loss: 0.2664150750519848\n",
      "epoch 17824: train loss: 0.10266515945475695, test loss: 0.2664157361935978\n",
      "epoch 17825: train loss: 0.10266374404292294, test loss: 0.2664163973540289\n",
      "epoch 17826: train loss: 0.10266232873467958, test loss: 0.26641705853327347\n",
      "epoch 17827: train loss: 0.10266091353001268, test loss: 0.26641771973132705\n",
      "epoch 17828: train loss: 0.10265949842890819, test loss: 0.266418380948185\n",
      "epoch 17829: train loss: 0.10265808343135195, test loss: 0.2664190421838427\n",
      "epoch 17830: train loss: 0.10265666853732981, test loss: 0.2664197034382958\n",
      "epoch 17831: train loss: 0.1026552537468278, test loss: 0.26642036471153946\n",
      "epoch 17832: train loss: 0.10265383905983168, test loss: 0.2664210260035695\n",
      "epoch 17833: train loss: 0.10265242447632743, test loss: 0.2664216873143811\n",
      "epoch 17834: train loss: 0.10265100999630089, test loss: 0.26642234864396974\n",
      "epoch 17835: train loss: 0.10264959561973802, test loss: 0.2664230099923309\n",
      "epoch 17836: train loss: 0.10264818134662469, test loss: 0.26642367135946005\n",
      "epoch 17837: train loss: 0.10264676717694683, test loss: 0.26642433274535265\n",
      "epoch 17838: train loss: 0.10264535311069035, test loss: 0.2664249941500042\n",
      "epoch 17839: train loss: 0.10264393914784115, test loss: 0.2664256555734101\n",
      "epoch 17840: train loss: 0.10264252528838519, test loss: 0.26642631701556574\n",
      "epoch 17841: train loss: 0.10264111153230834, test loss: 0.26642697847646674\n",
      "epoch 17842: train loss: 0.10263969787959654, test loss: 0.2664276399561084\n",
      "epoch 17843: train loss: 0.10263828433023572, test loss: 0.2664283014544862\n",
      "epoch 17844: train loss: 0.10263687088421182, test loss: 0.26642896297159585\n",
      "epoch 17845: train loss: 0.10263545754151078, test loss: 0.26642962450743246\n",
      "epoch 17846: train loss: 0.10263404430211849, test loss: 0.26643028606199165\n",
      "epoch 17847: train loss: 0.10263263116602094, test loss: 0.26643094763526887\n",
      "epoch 17848: train loss: 0.102631218133204, test loss: 0.26643160922725967\n",
      "epoch 17849: train loss: 0.1026298052036537, test loss: 0.2664322708379594\n",
      "epoch 17850: train loss: 0.10262839237735592, test loss: 0.26643293246736366\n",
      "epoch 17851: train loss: 0.10262697965429662, test loss: 0.26643359411546774\n",
      "epoch 17852: train loss: 0.10262556703446178, test loss: 0.26643425578226726\n",
      "epoch 17853: train loss: 0.10262415451783732, test loss: 0.26643491746775766\n",
      "epoch 17854: train loss: 0.1026227421044092, test loss: 0.26643557917193433\n",
      "epoch 17855: train loss: 0.10262132979416343, test loss: 0.26643624089479284\n",
      "epoch 17856: train loss: 0.10261991758708593, test loss: 0.2664369026363286\n",
      "epoch 17857: train loss: 0.10261850548316265, test loss: 0.2664375643965371\n",
      "epoch 17858: train loss: 0.10261709348237956, test loss: 0.2664382261754138\n",
      "epoch 17859: train loss: 0.10261568158472265, test loss: 0.26643888797295423\n",
      "epoch 17860: train loss: 0.10261426979017788, test loss: 0.26643954978915396\n",
      "epoch 17861: train loss: 0.10261285809873126, test loss: 0.26644021162400816\n",
      "epoch 17862: train loss: 0.1026114465103687, test loss: 0.26644087347751266\n",
      "epoch 17863: train loss: 0.10261003502507625, test loss: 0.26644153534966264\n",
      "epoch 17864: train loss: 0.10260862364283986, test loss: 0.26644219724045387\n",
      "epoch 17865: train loss: 0.10260721236364553, test loss: 0.2664428591498816\n",
      "epoch 17866: train loss: 0.10260580118747922, test loss: 0.2664435210779414\n",
      "epoch 17867: train loss: 0.10260439011432694, test loss: 0.2664441830246287\n",
      "epoch 17868: train loss: 0.10260297914417471, test loss: 0.2664448449899391\n",
      "epoch 17869: train loss: 0.10260156827700846, test loss: 0.26644550697386804\n",
      "epoch 17870: train loss: 0.10260015751281429, test loss: 0.266446168976411\n",
      "epoch 17871: train loss: 0.10259874685157812, test loss: 0.2664468309975635\n",
      "epoch 17872: train loss: 0.10259733629328598, test loss: 0.26644749303732096\n",
      "epoch 17873: train loss: 0.10259592583792386, test loss: 0.2664481550956788\n",
      "epoch 17874: train loss: 0.10259451548547782, test loss: 0.26644881717263275\n",
      "epoch 17875: train loss: 0.10259310523593382, test loss: 0.2664494792681782\n",
      "epoch 17876: train loss: 0.1025916950892779, test loss: 0.2664501413823105\n",
      "epoch 17877: train loss: 0.1025902850454961, test loss: 0.2664508035150254\n",
      "epoch 17878: train loss: 0.1025888751045744, test loss: 0.26645146566631817\n",
      "epoch 17879: train loss: 0.10258746526649885, test loss: 0.2664521278361844\n",
      "epoch 17880: train loss: 0.10258605553125548, test loss: 0.2664527900246195\n",
      "epoch 17881: train loss: 0.10258464589883029, test loss: 0.26645345223161926\n",
      "epoch 17882: train loss: 0.10258323636920937, test loss: 0.2664541144571788\n",
      "epoch 17883: train loss: 0.10258182694237866, test loss: 0.2664547767012938\n",
      "epoch 17884: train loss: 0.10258041761832434, test loss: 0.26645543896395985\n",
      "epoch 17885: train loss: 0.10257900839703231, test loss: 0.26645610124517227\n",
      "epoch 17886: train loss: 0.10257759927848868, test loss: 0.26645676354492664\n",
      "epoch 17887: train loss: 0.1025761902626795, test loss: 0.2664574258632186\n",
      "epoch 17888: train loss: 0.10257478134959078, test loss: 0.2664580882000434\n",
      "epoch 17889: train loss: 0.1025733725392086, test loss: 0.2664587505553968\n",
      "epoch 17890: train loss: 0.10257196383151904, test loss: 0.2664594129292741\n",
      "epoch 17891: train loss: 0.10257055522650811, test loss: 0.26646007532167093\n",
      "epoch 17892: train loss: 0.10256914672416187, test loss: 0.2664607377325828\n",
      "epoch 17893: train loss: 0.10256773832446642, test loss: 0.26646140016200515\n",
      "epoch 17894: train loss: 0.10256633002740781, test loss: 0.26646206260993355\n",
      "epoch 17895: train loss: 0.10256492183297208, test loss: 0.2664627250763635\n",
      "epoch 17896: train loss: 0.10256351374114533, test loss: 0.26646338756129045\n",
      "epoch 17897: train loss: 0.10256210575191364, test loss: 0.26646405006470997\n",
      "epoch 17898: train loss: 0.10256069786526306, test loss: 0.26646471258661775\n",
      "epoch 17899: train loss: 0.10255929008117967, test loss: 0.26646537512700896\n",
      "epoch 17900: train loss: 0.10255788239964957, test loss: 0.2664660376858794\n",
      "epoch 17901: train loss: 0.10255647482065886, test loss: 0.2664667002632244\n",
      "epoch 17902: train loss: 0.10255506734419358, test loss: 0.2664673628590396\n",
      "epoch 17903: train loss: 0.10255365997023981, test loss: 0.2664680254733205\n",
      "epoch 17904: train loss: 0.1025522526987837, test loss: 0.26646868810606256\n",
      "epoch 17905: train loss: 0.10255084552981131, test loss: 0.26646935075726136\n",
      "epoch 17906: train loss: 0.10254943846330875, test loss: 0.26647001342691246\n",
      "epoch 17907: train loss: 0.10254803149926212, test loss: 0.2664706761150113\n",
      "epoch 17908: train loss: 0.10254662463765751, test loss: 0.2664713388215535\n",
      "epoch 17909: train loss: 0.10254521787848102, test loss: 0.2664720015465345\n",
      "epoch 17910: train loss: 0.10254381122171878, test loss: 0.26647266428994987\n",
      "epoch 17911: train loss: 0.1025424046673569, test loss: 0.2664733270517951\n",
      "epoch 17912: train loss: 0.10254099821538148, test loss: 0.26647398983206577\n",
      "epoch 17913: train loss: 0.10253959186577863, test loss: 0.2664746526307574\n",
      "epoch 17914: train loss: 0.10253818561853446, test loss: 0.2664753154478656\n",
      "epoch 17915: train loss: 0.10253677947363514, test loss: 0.26647597828338576\n",
      "epoch 17916: train loss: 0.10253537343106676, test loss: 0.2664766411373134\n",
      "epoch 17917: train loss: 0.10253396749081546, test loss: 0.26647730400964414\n",
      "epoch 17918: train loss: 0.10253256165286734, test loss: 0.2664779669003736\n",
      "epoch 17919: train loss: 0.10253115591720857, test loss: 0.2664786298094971\n",
      "epoch 17920: train loss: 0.10252975028382526, test loss: 0.2664792927370104\n",
      "epoch 17921: train loss: 0.10252834475270356, test loss: 0.2664799556829089\n",
      "epoch 17922: train loss: 0.10252693932382961, test loss: 0.2664806186471882\n",
      "epoch 17923: train loss: 0.10252553399718951, test loss: 0.26648128162984375\n",
      "epoch 17924: train loss: 0.1025241287727695, test loss: 0.2664819446308712\n",
      "epoch 17925: train loss: 0.10252272365055565, test loss: 0.266482607650266\n",
      "epoch 17926: train loss: 0.10252131863053413, test loss: 0.2664832706880238\n",
      "epoch 17927: train loss: 0.1025199137126911, test loss: 0.26648393374414003\n",
      "epoch 17928: train loss: 0.1025185088970127, test loss: 0.2664845968186105\n",
      "epoch 17929: train loss: 0.10251710418348513, test loss: 0.2664852599114303\n",
      "epoch 17930: train loss: 0.10251569957209451, test loss: 0.2664859230225955\n",
      "epoch 17931: train loss: 0.10251429506282703, test loss: 0.2664865861521012\n",
      "epoch 17932: train loss: 0.10251289065566883, test loss: 0.26648724929994316\n",
      "epoch 17933: train loss: 0.10251148635060611, test loss: 0.2664879124661168\n",
      "epoch 17934: train loss: 0.10251008214762504, test loss: 0.2664885756506179\n",
      "epoch 17935: train loss: 0.10250867804671174, test loss: 0.26648923885344183\n",
      "epoch 17936: train loss: 0.10250727404785247, test loss: 0.26648990207458434\n",
      "epoch 17937: train loss: 0.10250587015103335, test loss: 0.2664905653140407\n",
      "epoch 17938: train loss: 0.10250446635624058, test loss: 0.2664912285718066\n",
      "epoch 17939: train loss: 0.10250306266346038, test loss: 0.2664918918478777\n",
      "epoch 17940: train loss: 0.10250165907267889, test loss: 0.2664925551422494\n",
      "epoch 17941: train loss: 0.10250025558388233, test loss: 0.26649321845491736\n",
      "epoch 17942: train loss: 0.10249885219705686, test loss: 0.26649388178587713\n",
      "epoch 17943: train loss: 0.1024974489121887, test loss: 0.2664945451351242\n",
      "epoch 17944: train loss: 0.10249604572926409, test loss: 0.26649520850265423\n",
      "epoch 17945: train loss: 0.10249464264826916, test loss: 0.2664958718884627\n",
      "epoch 17946: train loss: 0.10249323966919015, test loss: 0.26649653529254524\n",
      "epoch 17947: train loss: 0.10249183679201326, test loss: 0.2664971987148973\n",
      "epoch 17948: train loss: 0.1024904340167247, test loss: 0.26649786215551463\n",
      "epoch 17949: train loss: 0.1024890313433107, test loss: 0.26649852561439263\n",
      "epoch 17950: train loss: 0.10248762877175743, test loss: 0.26649918909152687\n",
      "epoch 17951: train loss: 0.10248622630205118, test loss: 0.266499852586913\n",
      "epoch 17952: train loss: 0.1024848239341781, test loss: 0.2665005161005467\n",
      "epoch 17953: train loss: 0.10248342166812442, test loss: 0.2665011796324233\n",
      "epoch 17954: train loss: 0.10248201950387642, test loss: 0.2665018431825385\n",
      "epoch 17955: train loss: 0.1024806174414203, test loss: 0.2665025067508879\n",
      "epoch 17956: train loss: 0.10247921548074228, test loss: 0.2665031703374669\n",
      "epoch 17957: train loss: 0.10247781362182858, test loss: 0.26650383394227134\n",
      "epoch 17958: train loss: 0.10247641186466548, test loss: 0.26650449756529665\n",
      "epoch 17959: train loss: 0.1024750102092392, test loss: 0.26650516120653833\n",
      "epoch 17960: train loss: 0.10247360865553594, test loss: 0.26650582486599206\n",
      "epoch 17961: train loss: 0.10247220720354201, test loss: 0.26650648854365344\n",
      "epoch 17962: train loss: 0.10247080585324361, test loss: 0.266507152239518\n",
      "epoch 17963: train loss: 0.10246940460462704, test loss: 0.2665078159535813\n",
      "epoch 17964: train loss: 0.10246800345767851, test loss: 0.266508479685839\n",
      "epoch 17965: train loss: 0.10246660241238427, test loss: 0.2665091434362866\n",
      "epoch 17966: train loss: 0.1024652014687306, test loss: 0.26650980720491974\n",
      "epoch 17967: train loss: 0.10246380062670378, test loss: 0.26651047099173397\n",
      "epoch 17968: train loss: 0.10246239988629, test loss: 0.26651113479672484\n",
      "epoch 17969: train loss: 0.10246099924747558, test loss: 0.266511798619888\n",
      "epoch 17970: train loss: 0.1024595987102468, test loss: 0.2665124624612191\n",
      "epoch 17971: train loss: 0.1024581982745899, test loss: 0.2665131263207136\n",
      "epoch 17972: train loss: 0.10245679794049119, test loss: 0.26651379019836713\n",
      "epoch 17973: train loss: 0.10245539770793688, test loss: 0.26651445409417535\n",
      "epoch 17974: train loss: 0.10245399757691333, test loss: 0.26651511800813377\n",
      "epoch 17975: train loss: 0.10245259754740677, test loss: 0.2665157819402379\n",
      "epoch 17976: train loss: 0.10245119761940344, test loss: 0.2665164458904835\n",
      "epoch 17977: train loss: 0.10244979779288978, test loss: 0.2665171098588662\n",
      "epoch 17978: train loss: 0.1024483980678519, test loss: 0.26651777384538133\n",
      "epoch 17979: train loss: 0.10244699844427622, test loss: 0.26651843785002477\n",
      "epoch 17980: train loss: 0.10244559892214898, test loss: 0.26651910187279193\n",
      "epoch 17981: train loss: 0.10244419950145645, test loss: 0.2665197659136785\n",
      "epoch 17982: train loss: 0.102442800182185, test loss: 0.2665204299726801\n",
      "epoch 17983: train loss: 0.1024414009643209, test loss: 0.26652109404979224\n",
      "epoch 17984: train loss: 0.10244000184785045, test loss: 0.26652175814501056\n",
      "epoch 17985: train loss: 0.10243860283275992, test loss: 0.2665224222583307\n",
      "epoch 17986: train loss: 0.10243720391903573, test loss: 0.2665230863897482\n",
      "epoch 17987: train loss: 0.10243580510666407, test loss: 0.26652375053925875\n",
      "epoch 17988: train loss: 0.10243440639563132, test loss: 0.2665244147068579\n",
      "epoch 17989: train loss: 0.10243300778592382, test loss: 0.26652507889254123\n",
      "epoch 17990: train loss: 0.10243160927752784, test loss: 0.2665257430963044\n",
      "epoch 17991: train loss: 0.10243021087042972, test loss: 0.2665264073181429\n",
      "epoch 17992: train loss: 0.10242881256461582, test loss: 0.26652707155805255\n",
      "epoch 17993: train loss: 0.1024274143600724, test loss: 0.26652773581602873\n",
      "epoch 17994: train loss: 0.10242601625678585, test loss: 0.2665284000920673\n",
      "epoch 17995: train loss: 0.10242461825474247, test loss: 0.26652906438616364\n",
      "epoch 17996: train loss: 0.10242322035392863, test loss: 0.2665297286983135\n",
      "epoch 17997: train loss: 0.10242182255433066, test loss: 0.2665303930285124\n",
      "epoch 17998: train loss: 0.10242042485593487, test loss: 0.2665310573767561\n",
      "epoch 17999: train loss: 0.10241902725872765, test loss: 0.26653172174303996\n",
      "epoch 18000: train loss: 0.1024176297626953, test loss: 0.26653238612735997\n",
      "epoch 18001: train loss: 0.10241623236782424, test loss: 0.2665330505297113\n",
      "epoch 18002: train loss: 0.10241483507410074, test loss: 0.26653371495009\n",
      "epoch 18003: train loss: 0.10241343788151122, test loss: 0.2665343793884914\n",
      "epoch 18004: train loss: 0.10241204079004199, test loss: 0.2665350438449112\n",
      "epoch 18005: train loss: 0.10241064379967942, test loss: 0.26653570831934514\n",
      "epoch 18006: train loss: 0.10240924691040991, test loss: 0.2665363728117887\n",
      "epoch 18007: train loss: 0.10240785012221981, test loss: 0.2665370373222375\n",
      "epoch 18008: train loss: 0.10240645343509548, test loss: 0.2665377018506872\n",
      "epoch 18009: train loss: 0.10240505684902328, test loss: 0.26653836639713346\n",
      "epoch 18010: train loss: 0.10240366036398958, test loss: 0.2665390309615719\n",
      "epoch 18011: train loss: 0.10240226397998078, test loss: 0.2665396955439981\n",
      "epoch 18012: train loss: 0.10240086769698328, test loss: 0.2665403601444077\n",
      "epoch 18013: train loss: 0.10239947151498342, test loss: 0.26654102476279634\n",
      "epoch 18014: train loss: 0.10239807543396758, test loss: 0.26654168939915973\n",
      "epoch 18015: train loss: 0.10239667945392217, test loss: 0.2665423540534934\n",
      "epoch 18016: train loss: 0.10239528357483355, test loss: 0.26654301872579295\n",
      "epoch 18017: train loss: 0.10239388779668816, test loss: 0.2665436834160541\n",
      "epoch 18018: train loss: 0.10239249211947236, test loss: 0.2665443481242724\n",
      "epoch 18019: train loss: 0.10239109654317258, test loss: 0.26654501285044374\n",
      "epoch 18020: train loss: 0.10238970106777517, test loss: 0.2665456775945633\n",
      "epoch 18021: train loss: 0.10238830569326657, test loss: 0.2665463423566271\n",
      "epoch 18022: train loss: 0.10238691041963316, test loss: 0.26654700713663076\n",
      "epoch 18023: train loss: 0.10238551524686137, test loss: 0.26654767193456963\n",
      "epoch 18024: train loss: 0.10238412017493762, test loss: 0.2665483367504396\n",
      "epoch 18025: train loss: 0.1023827252038483, test loss: 0.2665490015842362\n",
      "epoch 18026: train loss: 0.10238133033357981, test loss: 0.2665496664359551\n",
      "epoch 18027: train loss: 0.1023799355641186, test loss: 0.26655033130559214\n",
      "epoch 18028: train loss: 0.10237854089545106, test loss: 0.2665509961931426\n",
      "epoch 18029: train loss: 0.10237714632756367, test loss: 0.2665516610986024\n",
      "epoch 18030: train loss: 0.1023757518604428, test loss: 0.26655232602196705\n",
      "epoch 18031: train loss: 0.10237435749407492, test loss: 0.26655299096323226\n",
      "epoch 18032: train loss: 0.10237296322844641, test loss: 0.2665536559223936\n",
      "epoch 18033: train loss: 0.10237156906354375, test loss: 0.2665543208994468\n",
      "epoch 18034: train loss: 0.10237017499935334, test loss: 0.2665549858943875\n",
      "epoch 18035: train loss: 0.10236878103586164, test loss: 0.2665556509072114\n",
      "epoch 18036: train loss: 0.1023673871730551, test loss: 0.26655631593791407\n",
      "epoch 18037: train loss: 0.10236599341092018, test loss: 0.26655698098649105\n",
      "epoch 18038: train loss: 0.10236459974944324, test loss: 0.2665576460529382\n",
      "epoch 18039: train loss: 0.10236320618861085, test loss: 0.26655831113725104\n",
      "epoch 18040: train loss: 0.10236181272840937, test loss: 0.26655897623942537\n",
      "epoch 18041: train loss: 0.10236041936882531, test loss: 0.26655964135945676\n",
      "epoch 18042: train loss: 0.1023590261098451, test loss: 0.26656030649734086\n",
      "epoch 18043: train loss: 0.10235763295145517, test loss: 0.2665609716530733\n",
      "epoch 18044: train loss: 0.10235623989364206, test loss: 0.26656163682664974\n",
      "epoch 18045: train loss: 0.10235484693639219, test loss: 0.266562302018066\n",
      "epoch 18046: train loss: 0.10235345407969203, test loss: 0.2665629672273175\n",
      "epoch 18047: train loss: 0.10235206132352805, test loss: 0.26656363245440007\n",
      "epoch 18048: train loss: 0.10235066866788671, test loss: 0.2665642976993093\n",
      "epoch 18049: train loss: 0.10234927611275449, test loss: 0.2665649629620409\n",
      "epoch 18050: train loss: 0.1023478836581179, test loss: 0.2665656282425905\n",
      "epoch 18051: train loss: 0.10234649130396338, test loss: 0.26656629354095374\n",
      "epoch 18052: train loss: 0.10234509905027746, test loss: 0.2665669588571264\n",
      "epoch 18053: train loss: 0.10234370689704658, test loss: 0.266567624191104\n",
      "epoch 18054: train loss: 0.10234231484425724, test loss: 0.2665682895428823\n",
      "epoch 18055: train loss: 0.10234092289189593, test loss: 0.26656895491245697\n",
      "epoch 18056: train loss: 0.10233953103994917, test loss: 0.2665696202998237\n",
      "epoch 18057: train loss: 0.10233813928840345, test loss: 0.266570285704978\n",
      "epoch 18058: train loss: 0.10233674763724522, test loss: 0.2665709511279158\n",
      "epoch 18059: train loss: 0.10233535608646104, test loss: 0.2665716165686326\n",
      "epoch 18060: train loss: 0.10233396463603739, test loss: 0.266572282027124\n",
      "epoch 18061: train loss: 0.10233257328596077, test loss: 0.26657294750338595\n",
      "epoch 18062: train loss: 0.10233118203621772, test loss: 0.26657361299741394\n",
      "epoch 18063: train loss: 0.1023297908867947, test loss: 0.26657427850920357\n",
      "epoch 18064: train loss: 0.10232839983767825, test loss: 0.2665749440387507\n",
      "epoch 18065: train loss: 0.1023270088888549, test loss: 0.26657560958605103\n",
      "epoch 18066: train loss: 0.10232561804031119, test loss: 0.2665762751511\n",
      "epoch 18067: train loss: 0.10232422729203357, test loss: 0.2665769407338935\n",
      "epoch 18068: train loss: 0.10232283664400864, test loss: 0.266577606334427\n",
      "epoch 18069: train loss: 0.10232144609622289, test loss: 0.26657827195269657\n",
      "epoch 18070: train loss: 0.10232005564866282, test loss: 0.2665789375886975\n",
      "epoch 18071: train loss: 0.10231866530131505, test loss: 0.2665796032424257\n",
      "epoch 18072: train loss: 0.10231727505416605, test loss: 0.2665802689138767\n",
      "epoch 18073: train loss: 0.10231588490720235, test loss: 0.2665809346030463\n",
      "epoch 18074: train loss: 0.1023144948604105, test loss: 0.2665816003099302\n",
      "epoch 18075: train loss: 0.10231310491377706, test loss: 0.26658226603452406\n",
      "epoch 18076: train loss: 0.10231171506728859, test loss: 0.2665829317768235\n",
      "epoch 18077: train loss: 0.1023103253209316, test loss: 0.26658359753682426\n",
      "epoch 18078: train loss: 0.10230893567469268, test loss: 0.2665842633145221\n",
      "epoch 18079: train loss: 0.10230754612855834, test loss: 0.26658492910991266\n",
      "epoch 18080: train loss: 0.10230615668251515, test loss: 0.2665855949229916\n",
      "epoch 18081: train loss: 0.10230476733654967, test loss: 0.2665862607537547\n",
      "epoch 18082: train loss: 0.10230337809064849, test loss: 0.26658692660219757\n",
      "epoch 18083: train loss: 0.10230198894479817, test loss: 0.2665875924683159\n",
      "epoch 18084: train loss: 0.10230059989898521, test loss: 0.2665882583521055\n",
      "epoch 18085: train loss: 0.10229921095319622, test loss: 0.26658892425356184\n",
      "epoch 18086: train loss: 0.10229782210741782, test loss: 0.2665895901726809\n",
      "epoch 18087: train loss: 0.1022964333616365, test loss: 0.26659025610945825\n",
      "epoch 18088: train loss: 0.10229504471583889, test loss: 0.26659092206388957\n",
      "epoch 18089: train loss: 0.10229365617001157, test loss: 0.2665915880359706\n",
      "epoch 18090: train loss: 0.10229226772414109, test loss: 0.26659225402569703\n",
      "epoch 18091: train loss: 0.10229087937821403, test loss: 0.2665929200330646\n",
      "epoch 18092: train loss: 0.10228949113221703, test loss: 0.2665935860580689\n",
      "epoch 18093: train loss: 0.10228810298613662, test loss: 0.26659425210070575\n",
      "epoch 18094: train loss: 0.10228671493995943, test loss: 0.2665949181609708\n",
      "epoch 18095: train loss: 0.10228532699367206, test loss: 0.2665955842388599\n",
      "epoch 18096: train loss: 0.10228393914726107, test loss: 0.2665962503343685\n",
      "epoch 18097: train loss: 0.10228255140071309, test loss: 0.26659691644749245\n",
      "epoch 18098: train loss: 0.10228116375401468, test loss: 0.2665975825782276\n",
      "epoch 18099: train loss: 0.1022797762071525, test loss: 0.2665982487265694\n",
      "epoch 18100: train loss: 0.10227838876011314, test loss: 0.2665989148925137\n",
      "epoch 18101: train loss: 0.10227700141288318, test loss: 0.26659958107605625\n",
      "epoch 18102: train loss: 0.10227561416544928, test loss: 0.26660024727719267\n",
      "epoch 18103: train loss: 0.102274227017798, test loss: 0.2666009134959188\n",
      "epoch 18104: train loss: 0.10227283996991603, test loss: 0.26660157973223014\n",
      "epoch 18105: train loss: 0.1022714530217899, test loss: 0.26660224598612264\n",
      "epoch 18106: train loss: 0.10227006617340631, test loss: 0.2666029122575919\n",
      "epoch 18107: train loss: 0.10226867942475185, test loss: 0.2666035785466337\n",
      "epoch 18108: train loss: 0.10226729277581313, test loss: 0.2666042448532437\n",
      "epoch 18109: train loss: 0.10226590622657682, test loss: 0.2666049111774177\n",
      "epoch 18110: train loss: 0.10226451977702954, test loss: 0.26660557751915126\n",
      "epoch 18111: train loss: 0.10226313342715791, test loss: 0.2666062438784403\n",
      "epoch 18112: train loss: 0.10226174717694861, test loss: 0.2666069102552803\n",
      "epoch 18113: train loss: 0.10226036102638819, test loss: 0.2666075766496674\n",
      "epoch 18114: train loss: 0.10225897497546342, test loss: 0.2666082430615968\n",
      "epoch 18115: train loss: 0.10225758902416082, test loss: 0.26660890949106475\n",
      "epoch 18116: train loss: 0.10225620317246714, test loss: 0.26660957593806656\n",
      "epoch 18117: train loss: 0.10225481742036897, test loss: 0.2666102424025982\n",
      "epoch 18118: train loss: 0.102253431767853, test loss: 0.2666109088846552\n",
      "epoch 18119: train loss: 0.10225204621490583, test loss: 0.2666115753842335\n",
      "epoch 18120: train loss: 0.10225066076151418, test loss: 0.26661224190132865\n",
      "epoch 18121: train loss: 0.10224927540766467, test loss: 0.2666129084359366\n",
      "epoch 18122: train loss: 0.10224789015334398, test loss: 0.26661357498805294\n",
      "epoch 18123: train loss: 0.1022465049985388, test loss: 0.2666142415576734\n",
      "epoch 18124: train loss: 0.10224511994323576, test loss: 0.26661490814479366\n",
      "epoch 18125: train loss: 0.10224373498742155, test loss: 0.26661557474940967\n",
      "epoch 18126: train loss: 0.10224235013108281, test loss: 0.26661624137151696\n",
      "epoch 18127: train loss: 0.10224096537420628, test loss: 0.2666169080111114\n",
      "epoch 18128: train loss: 0.10223958071677859, test loss: 0.26661757466818864\n",
      "epoch 18129: train loss: 0.10223819615878645, test loss: 0.2666182413427445\n",
      "epoch 18130: train loss: 0.10223681170021653, test loss: 0.26661890803477456\n",
      "epoch 18131: train loss: 0.10223542734105548, test loss: 0.2666195747442747\n",
      "epoch 18132: train loss: 0.10223404308129005, test loss: 0.2666202414712407\n",
      "epoch 18133: train loss: 0.10223265892090691, test loss: 0.2666209082156682\n",
      "epoch 18134: train loss: 0.10223127485989275, test loss: 0.26662157497755307\n",
      "epoch 18135: train loss: 0.10222989089823425, test loss: 0.2666222417568908\n",
      "epoch 18136: train loss: 0.10222850703591815, test loss: 0.2666229085536775\n",
      "epoch 18137: train loss: 0.1022271232729311, test loss: 0.26662357536790865\n",
      "epoch 18138: train loss: 0.10222573960925989, test loss: 0.26662424219958014\n",
      "epoch 18139: train loss: 0.10222435604489111, test loss: 0.2666249090486875\n",
      "epoch 18140: train loss: 0.10222297257981154, test loss: 0.2666255759152268\n",
      "epoch 18141: train loss: 0.10222158921400787, test loss: 0.26662624279919356\n",
      "epoch 18142: train loss: 0.10222020594746688, test loss: 0.2666269097005836\n",
      "epoch 18143: train loss: 0.10221882278017518, test loss: 0.2666275766193927\n",
      "epoch 18144: train loss: 0.10221743971211955, test loss: 0.2666282435556166\n",
      "epoch 18145: train loss: 0.10221605674328671, test loss: 0.26662891050925097\n",
      "epoch 18146: train loss: 0.10221467387366338, test loss: 0.2666295774802918\n",
      "epoch 18147: train loss: 0.10221329110323628, test loss: 0.26663024446873457\n",
      "epoch 18148: train loss: 0.10221190843199214, test loss: 0.26663091147457507\n",
      "epoch 18149: train loss: 0.1022105258599177, test loss: 0.26663157849780933\n",
      "epoch 18150: train loss: 0.1022091433869997, test loss: 0.26663224553843284\n",
      "epoch 18151: train loss: 0.10220776101322487, test loss: 0.26663291259644145\n",
      "epoch 18152: train loss: 0.10220637873857996, test loss: 0.26663357967183093\n",
      "epoch 18153: train loss: 0.10220499656305168, test loss: 0.26663424676459696\n",
      "epoch 18154: train loss: 0.10220361448662679, test loss: 0.26663491387473554\n",
      "epoch 18155: train loss: 0.10220223250929206, test loss: 0.26663558100224216\n",
      "epoch 18156: train loss: 0.1022008506310342, test loss: 0.2666362481471127\n",
      "epoch 18157: train loss: 0.10219946885184, test loss: 0.2666369153093429\n",
      "epoch 18158: train loss: 0.10219808717169622, test loss: 0.2666375824889286\n",
      "epoch 18159: train loss: 0.10219670559058958, test loss: 0.2666382496858656\n",
      "epoch 18160: train loss: 0.10219532410850686, test loss: 0.2666389169001494\n",
      "epoch 18161: train loss: 0.1021939427254348, test loss: 0.26663958413177613\n",
      "epoch 18162: train loss: 0.10219256144136021, test loss: 0.2666402513807413\n",
      "epoch 18163: train loss: 0.10219118025626983, test loss: 0.2666409186470409\n",
      "epoch 18164: train loss: 0.10218979917015045, test loss: 0.2666415859306704\n",
      "epoch 18165: train loss: 0.10218841818298881, test loss: 0.26664225323162577\n",
      "epoch 18166: train loss: 0.1021870372947717, test loss: 0.2666429205499029\n",
      "epoch 18167: train loss: 0.10218565650548589, test loss: 0.26664358788549736\n",
      "epoch 18168: train loss: 0.10218427581511819, test loss: 0.26664425523840496\n",
      "epoch 18169: train loss: 0.10218289522365534, test loss: 0.2666449226086216\n",
      "epoch 18170: train loss: 0.10218151473108417, test loss: 0.26664558999614285\n",
      "epoch 18171: train loss: 0.10218013433739141, test loss: 0.2666462574009647\n",
      "epoch 18172: train loss: 0.10217875404256394, test loss: 0.26664692482308283\n",
      "epoch 18173: train loss: 0.10217737384658847, test loss: 0.266647592262493\n",
      "epoch 18174: train loss: 0.1021759937494518, test loss: 0.26664825971919115\n",
      "epoch 18175: train loss: 0.10217461375114079, test loss: 0.26664892719317285\n",
      "epoch 18176: train loss: 0.10217323385164218, test loss: 0.26664959468443394\n",
      "epoch 18177: train loss: 0.10217185405094281, test loss: 0.2666502621929704\n",
      "epoch 18178: train loss: 0.10217047434902944, test loss: 0.2666509297187777\n",
      "epoch 18179: train loss: 0.10216909474588895, test loss: 0.2666515972618518\n",
      "epoch 18180: train loss: 0.10216771524150808, test loss: 0.2666522648221886\n",
      "epoch 18181: train loss: 0.1021663358358737, test loss: 0.2666529323997837\n",
      "epoch 18182: train loss: 0.10216495652897258, test loss: 0.2666535999946329\n",
      "epoch 18183: train loss: 0.10216357732079155, test loss: 0.26665426760673205\n",
      "epoch 18184: train loss: 0.10216219821131745, test loss: 0.26665493523607703\n",
      "epoch 18185: train loss: 0.10216081920053707, test loss: 0.26665560288266343\n",
      "epoch 18186: train loss: 0.10215944028843726, test loss: 0.26665627054648716\n",
      "epoch 18187: train loss: 0.10215806147500485, test loss: 0.2666569382275441\n",
      "epoch 18188: train loss: 0.10215668276022666, test loss: 0.26665760592582993\n",
      "epoch 18189: train loss: 0.10215530414408952, test loss: 0.2666582736413405\n",
      "epoch 18190: train loss: 0.1021539256265803, test loss: 0.2666589413740714\n",
      "epoch 18191: train loss: 0.10215254720768577, test loss: 0.26665960912401876\n",
      "epoch 18192: train loss: 0.10215116888739285, test loss: 0.26666027689117827\n",
      "epoch 18193: train loss: 0.1021497906656883, test loss: 0.2666609446755455\n",
      "epoch 18194: train loss: 0.10214841254255905, test loss: 0.2666616124771166\n",
      "epoch 18195: train loss: 0.10214703451799188, test loss: 0.2666622802958872\n",
      "epoch 18196: train loss: 0.1021456565919737, test loss: 0.26666294813185304\n",
      "epoch 18197: train loss: 0.10214427876449132, test loss: 0.26666361598501\n",
      "epoch 18198: train loss: 0.10214290103553159, test loss: 0.26666428385535396\n",
      "epoch 18199: train loss: 0.1021415234050814, test loss: 0.2666649517428806\n",
      "epoch 18200: train loss: 0.10214014587312759, test loss: 0.2666656196475857\n",
      "epoch 18201: train loss: 0.10213876843965704, test loss: 0.26666628756946525\n",
      "epoch 18202: train loss: 0.10213739110465658, test loss: 0.2666669555085149\n",
      "epoch 18203: train loss: 0.10213601386811312, test loss: 0.2666676234647304\n",
      "epoch 18204: train loss: 0.10213463673001354, test loss: 0.2666682914381078\n",
      "epoch 18205: train loss: 0.10213325969034465, test loss: 0.2666689594286428\n",
      "epoch 18206: train loss: 0.10213188274909336, test loss: 0.26666962743633116\n",
      "epoch 18207: train loss: 0.10213050590624657, test loss: 0.26667029546116866\n",
      "epoch 18208: train loss: 0.10212912916179112, test loss: 0.2666709635031513\n",
      "epoch 18209: train loss: 0.10212775251571393, test loss: 0.2666716315622747\n",
      "epoch 18210: train loss: 0.10212637596800189, test loss: 0.2666722996385347\n",
      "epoch 18211: train loss: 0.10212499951864183, test loss: 0.2666729677319272\n",
      "epoch 18212: train loss: 0.10212362316762072, test loss: 0.26667363584244813\n",
      "epoch 18213: train loss: 0.10212224691492537, test loss: 0.266674303970093\n",
      "epoch 18214: train loss: 0.10212087076054274, test loss: 0.26667497211485786\n",
      "epoch 18215: train loss: 0.1021194947044597, test loss: 0.26667564027673846\n",
      "epoch 18216: train loss: 0.10211811874666317, test loss: 0.2666763084557306\n",
      "epoch 18217: train loss: 0.10211674288714004, test loss: 0.26667697665183016\n",
      "epoch 18218: train loss: 0.10211536712587721, test loss: 0.2666776448650329\n",
      "epoch 18219: train loss: 0.10211399146286158, test loss: 0.2666783130953347\n",
      "epoch 18220: train loss: 0.10211261589808009, test loss: 0.26667898134273144\n",
      "epoch 18221: train loss: 0.10211124043151963, test loss: 0.2666796496072187\n",
      "epoch 18222: train loss: 0.10210986506316713, test loss: 0.2666803178887926\n",
      "epoch 18223: train loss: 0.10210848979300949, test loss: 0.2666809861874488\n",
      "epoch 18224: train loss: 0.10210711462103365, test loss: 0.26668165450318315\n",
      "epoch 18225: train loss: 0.10210573954722654, test loss: 0.26668232283599147\n",
      "epoch 18226: train loss: 0.10210436457157507, test loss: 0.26668299118586974\n",
      "epoch 18227: train loss: 0.10210298969406612, test loss: 0.26668365955281353\n",
      "epoch 18228: train loss: 0.1021016149146867, test loss: 0.2666843279368189\n",
      "epoch 18229: train loss: 0.1021002402334237, test loss: 0.26668499633788156\n",
      "epoch 18230: train loss: 0.1020988656502641, test loss: 0.2666856647559975\n",
      "epoch 18231: train loss: 0.10209749116519479, test loss: 0.26668633319116236\n",
      "epoch 18232: train loss: 0.10209611677820271, test loss: 0.266687001643372\n",
      "epoch 18233: train loss: 0.10209474248927485, test loss: 0.26668767011262245\n",
      "epoch 18234: train loss: 0.1020933682983981, test loss: 0.2666883385989092\n",
      "epoch 18235: train loss: 0.10209199420555944, test loss: 0.2666890071022286\n",
      "epoch 18236: train loss: 0.1020906202107458, test loss: 0.26668967562257584\n",
      "epoch 18237: train loss: 0.10208924631394417, test loss: 0.2666903441599473\n",
      "epoch 18238: train loss: 0.10208787251514145, test loss: 0.26669101271433865\n",
      "epoch 18239: train loss: 0.10208649881432466, test loss: 0.2666916812857457\n",
      "epoch 18240: train loss: 0.1020851252114807, test loss: 0.2666923498741643\n",
      "epoch 18241: train loss: 0.10208375170659657, test loss: 0.26669301847959026\n",
      "epoch 18242: train loss: 0.10208237829965924, test loss: 0.26669368710201957\n",
      "epoch 18243: train loss: 0.10208100499065566, test loss: 0.2666943557414479\n",
      "epoch 18244: train loss: 0.10207963177957283, test loss: 0.26669502439787124\n",
      "epoch 18245: train loss: 0.10207825866639766, test loss: 0.2666956930712852\n",
      "epoch 18246: train loss: 0.10207688565111717, test loss: 0.26669636176168604\n",
      "epoch 18247: train loss: 0.10207551273371833, test loss: 0.2666970304690692\n",
      "epoch 18248: train loss: 0.10207413991418812, test loss: 0.2666976991934307\n",
      "epoch 18249: train loss: 0.10207276719251354, test loss: 0.2666983679347665\n",
      "epoch 18250: train loss: 0.10207139456868154, test loss: 0.26669903669307227\n",
      "epoch 18251: train loss: 0.10207002204267916, test loss: 0.26669970546834393\n",
      "epoch 18252: train loss: 0.1020686496144933, test loss: 0.2667003742605773\n",
      "epoch 18253: train loss: 0.10206727728411105, test loss: 0.26670104306976844\n",
      "epoch 18254: train loss: 0.10206590505151934, test loss: 0.26670171189591285\n",
      "epoch 18255: train loss: 0.1020645329167052, test loss: 0.26670238073900676\n",
      "epoch 18256: train loss: 0.10206316087965565, test loss: 0.2667030495990457\n",
      "epoch 18257: train loss: 0.10206178894035763, test loss: 0.26670371847602575\n",
      "epoch 18258: train loss: 0.10206041709879816, test loss: 0.2667043873699428\n",
      "epoch 18259: train loss: 0.10205904535496431, test loss: 0.26670505628079244\n",
      "epoch 18260: train loss: 0.10205767370884303, test loss: 0.26670572520857083\n",
      "epoch 18261: train loss: 0.10205630216042134, test loss: 0.26670639415327363\n",
      "epoch 18262: train loss: 0.10205493070968626, test loss: 0.26670706311489684\n",
      "epoch 18263: train loss: 0.10205355935662482, test loss: 0.26670773209343623\n",
      "epoch 18264: train loss: 0.10205218810122402, test loss: 0.26670840108888766\n",
      "epoch 18265: train loss: 0.1020508169434709, test loss: 0.26670907010124706\n",
      "epoch 18266: train loss: 0.10204944588335245, test loss: 0.26670973913051027\n",
      "epoch 18267: train loss: 0.10204807492085574, test loss: 0.26671040817667324\n",
      "epoch 18268: train loss: 0.10204670405596779, test loss: 0.26671107723973175\n",
      "epoch 18269: train loss: 0.10204533328867563, test loss: 0.2667117463196816\n",
      "epoch 18270: train loss: 0.10204396261896626, test loss: 0.26671241541651874\n",
      "epoch 18271: train loss: 0.10204259204682678, test loss: 0.2667130845302391\n",
      "epoch 18272: train loss: 0.10204122157224418, test loss: 0.2667137536608385\n",
      "epoch 18273: train loss: 0.1020398511952055, test loss: 0.2667144228083128\n",
      "epoch 18274: train loss: 0.10203848091569782, test loss: 0.26671509197265786\n",
      "epoch 18275: train loss: 0.10203711073370818, test loss: 0.2667157611538696\n",
      "epoch 18276: train loss: 0.1020357406492236, test loss: 0.26671643035194387\n",
      "epoch 18277: train loss: 0.10203437066223113, test loss: 0.2667170995668766\n",
      "epoch 18278: train loss: 0.10203300077271787, test loss: 0.26671776879866355\n",
      "epoch 18279: train loss: 0.10203163098067083, test loss: 0.26671843804730067\n",
      "epoch 18280: train loss: 0.1020302612860771, test loss: 0.26671910731278387\n",
      "epoch 18281: train loss: 0.10202889168892373, test loss: 0.2667197765951091\n",
      "epoch 18282: train loss: 0.10202752218919776, test loss: 0.2667204458942721\n",
      "epoch 18283: train loss: 0.10202615278688633, test loss: 0.26672111521026876\n",
      "epoch 18284: train loss: 0.10202478348197641, test loss: 0.266721784543095\n",
      "epoch 18285: train loss: 0.10202341427445515, test loss: 0.26672245389274674\n",
      "epoch 18286: train loss: 0.10202204516430957, test loss: 0.2667231232592197\n",
      "epoch 18287: train loss: 0.10202067615152678, test loss: 0.26672379264251\n",
      "epoch 18288: train loss: 0.10201930723609384, test loss: 0.26672446204261346\n",
      "epoch 18289: train loss: 0.10201793841799786, test loss: 0.2667251314595259\n",
      "epoch 18290: train loss: 0.10201656969722589, test loss: 0.2667258008932433\n",
      "epoch 18291: train loss: 0.10201520107376502, test loss: 0.2667264703437614\n",
      "epoch 18292: train loss: 0.10201383254760238, test loss: 0.2667271398110762\n",
      "epoch 18293: train loss: 0.102012464118725, test loss: 0.2667278092951836\n",
      "epoch 18294: train loss: 0.10201109578712, test loss: 0.2667284787960796\n",
      "epoch 18295: train loss: 0.10200972755277447, test loss: 0.26672914831375977\n",
      "epoch 18296: train loss: 0.10200835941567554, test loss: 0.26672981784822025\n",
      "epoch 18297: train loss: 0.10200699137581028, test loss: 0.26673048739945693\n",
      "epoch 18298: train loss: 0.10200562343316578, test loss: 0.26673115696746563\n",
      "epoch 18299: train loss: 0.10200425558772919, test loss: 0.2667318265522423\n",
      "epoch 18300: train loss: 0.10200288783948758, test loss: 0.2667324961537828\n",
      "epoch 18301: train loss: 0.10200152018842808, test loss: 0.26673316577208306\n",
      "epoch 18302: train loss: 0.10200015263453778, test loss: 0.2667338354071389\n",
      "epoch 18303: train loss: 0.10199878517780382, test loss: 0.26673450505894636\n",
      "epoch 18304: train loss: 0.10199741781821331, test loss: 0.26673517472750125\n",
      "epoch 18305: train loss: 0.10199605055575336, test loss: 0.2667358444127995\n",
      "epoch 18306: train loss: 0.10199468339041111, test loss: 0.26673651411483695\n",
      "epoch 18307: train loss: 0.10199331632217368, test loss: 0.2667371838336096\n",
      "epoch 18308: train loss: 0.1019919493510282, test loss: 0.26673785356911334\n",
      "epoch 18309: train loss: 0.10199058247696177, test loss: 0.266738523321344\n",
      "epoch 18310: train loss: 0.10198921569996157, test loss: 0.26673919309029764\n",
      "epoch 18311: train loss: 0.10198784902001469, test loss: 0.2667398628759699\n",
      "epoch 18312: train loss: 0.10198648243710831, test loss: 0.26674053267835707\n",
      "epoch 18313: train loss: 0.10198511595122955, test loss: 0.2667412024974547\n",
      "epoch 18314: train loss: 0.10198374956236553, test loss: 0.26674187233325886\n",
      "epoch 18315: train loss: 0.10198238327050342, test loss: 0.2667425421857655\n",
      "epoch 18316: train loss: 0.10198101707563037, test loss: 0.26674321205497037\n",
      "epoch 18317: train loss: 0.10197965097773352, test loss: 0.26674388194086956\n",
      "epoch 18318: train loss: 0.10197828497680002, test loss: 0.26674455184345897\n",
      "epoch 18319: train loss: 0.10197691907281702, test loss: 0.2667452217627344\n",
      "epoch 18320: train loss: 0.1019755532657717, test loss: 0.26674589169869184\n",
      "epoch 18321: train loss: 0.10197418755565119, test loss: 0.26674656165132726\n",
      "epoch 18322: train loss: 0.10197282194244268, test loss: 0.26674723162063646\n",
      "epoch 18323: train loss: 0.10197145642613328, test loss: 0.26674790160661543\n",
      "epoch 18324: train loss: 0.10197009100671023, test loss: 0.26674857160926\n",
      "epoch 18325: train loss: 0.10196872568416067, test loss: 0.2667492416285662\n",
      "epoch 18326: train loss: 0.10196736045847177, test loss: 0.26674991166453\n",
      "epoch 18327: train loss: 0.10196599532963066, test loss: 0.2667505817171472\n",
      "epoch 18328: train loss: 0.10196463029762459, test loss: 0.2667512517864138\n",
      "epoch 18329: train loss: 0.10196326536244069, test loss: 0.2667519218723256\n",
      "epoch 18330: train loss: 0.10196190052406616, test loss: 0.2667525919748787\n",
      "epoch 18331: train loss: 0.10196053578248819, test loss: 0.26675326209406885\n",
      "epoch 18332: train loss: 0.10195917113769394, test loss: 0.2667539322298922\n",
      "epoch 18333: train loss: 0.1019578065896706, test loss: 0.26675460238234444\n",
      "epoch 18334: train loss: 0.10195644213840538, test loss: 0.2667552725514216\n",
      "epoch 18335: train loss: 0.10195507778388546, test loss: 0.26675594273711967\n",
      "epoch 18336: train loss: 0.10195371352609808, test loss: 0.2667566129394344\n",
      "epoch 18337: train loss: 0.10195234936503036, test loss: 0.266757283158362\n",
      "epoch 18338: train loss: 0.10195098530066955, test loss: 0.26675795339389813\n",
      "epoch 18339: train loss: 0.10194962133300284, test loss: 0.2667586236460389\n",
      "epoch 18340: train loss: 0.10194825746201745, test loss: 0.2667592939147802\n",
      "epoch 18341: train loss: 0.10194689368770056, test loss: 0.266759964200118\n",
      "epoch 18342: train loss: 0.1019455300100394, test loss: 0.2667606345020481\n",
      "epoch 18343: train loss: 0.1019441664290212, test loss: 0.26676130482056665\n",
      "epoch 18344: train loss: 0.10194280294463312, test loss: 0.26676197515566924\n",
      "epoch 18345: train loss: 0.10194143955686244, test loss: 0.2667626455073522\n",
      "epoch 18346: train loss: 0.10194007626569633, test loss: 0.26676331587561125\n",
      "epoch 18347: train loss: 0.10193871307112204, test loss: 0.2667639862604424\n",
      "epoch 18348: train loss: 0.10193734997312678, test loss: 0.2667646566618415\n",
      "epoch 18349: train loss: 0.1019359869716978, test loss: 0.26676532707980466\n",
      "epoch 18350: train loss: 0.1019346240668223, test loss: 0.26676599751432767\n",
      "epoch 18351: train loss: 0.10193326125848752, test loss: 0.26676666796540655\n",
      "epoch 18352: train loss: 0.1019318985466807, test loss: 0.26676733843303724\n",
      "epoch 18353: train loss: 0.1019305359313891, test loss: 0.26676800891721564\n",
      "epoch 18354: train loss: 0.10192917341259988, test loss: 0.26676867941793775\n",
      "epoch 18355: train loss: 0.10192781099030039, test loss: 0.2667693499351995\n",
      "epoch 18356: train loss: 0.1019264486644778, test loss: 0.2667700204689968\n",
      "epoch 18357: train loss: 0.10192508643511937, test loss: 0.2667706910193257\n",
      "epoch 18358: train loss: 0.10192372430221237, test loss: 0.2667713615861821\n",
      "epoch 18359: train loss: 0.10192236226574403, test loss: 0.2667720321695619\n",
      "epoch 18360: train loss: 0.1019210003257016, test loss: 0.2667727027694611\n",
      "epoch 18361: train loss: 0.10191963848207236, test loss: 0.26677337338587553\n",
      "epoch 18362: train loss: 0.10191827673484355, test loss: 0.2667740440188014\n",
      "epoch 18363: train loss: 0.10191691508400243, test loss: 0.26677471466823444\n",
      "epoch 18364: train loss: 0.10191555352953624, test loss: 0.26677538533417083\n",
      "epoch 18365: train loss: 0.10191419207143233, test loss: 0.26677605601660626\n",
      "epoch 18366: train loss: 0.10191283070967788, test loss: 0.2667767267155367\n",
      "epoch 18367: train loss: 0.1019114694442602, test loss: 0.2667773974309584\n",
      "epoch 18368: train loss: 0.10191010827516653, test loss: 0.26677806816286703\n",
      "epoch 18369: train loss: 0.10190874720238419, test loss: 0.2667787389112588\n",
      "epoch 18370: train loss: 0.10190738622590043, test loss: 0.26677940967612934\n",
      "epoch 18371: train loss: 0.10190602534570256, test loss: 0.266780080457475\n",
      "epoch 18372: train loss: 0.10190466456177781, test loss: 0.2667807512552914\n",
      "epoch 18373: train loss: 0.10190330387411349, test loss: 0.2667814220695747\n",
      "epoch 18374: train loss: 0.1019019432826969, test loss: 0.26678209290032073\n",
      "epoch 18375: train loss: 0.10190058278751532, test loss: 0.26678276374752563\n",
      "epoch 18376: train loss: 0.10189922238855605, test loss: 0.2667834346111853\n",
      "epoch 18377: train loss: 0.10189786208580637, test loss: 0.2667841054912956\n",
      "epoch 18378: train loss: 0.10189650187925357, test loss: 0.2667847763878527\n",
      "epoch 18379: train loss: 0.10189514176888499, test loss: 0.2667854473008523\n",
      "epoch 18380: train loss: 0.1018937817546879, test loss: 0.2667861182302905\n",
      "epoch 18381: train loss: 0.1018924218366496, test loss: 0.26678678917616344\n",
      "epoch 18382: train loss: 0.10189106201475738, test loss: 0.26678746013846694\n",
      "epoch 18383: train loss: 0.1018897022889986, test loss: 0.26678813111719685\n",
      "epoch 18384: train loss: 0.10188834265936053, test loss: 0.2667888021123493\n",
      "epoch 18385: train loss: 0.10188698312583051, test loss: 0.26678947312392026\n",
      "epoch 18386: train loss: 0.10188562368839583, test loss: 0.26679014415190566\n",
      "epoch 18387: train loss: 0.10188426434704383, test loss: 0.26679081519630155\n",
      "epoch 18388: train loss: 0.1018829051017618, test loss: 0.2667914862571038\n",
      "epoch 18389: train loss: 0.10188154595253712, test loss: 0.2667921573343084\n",
      "epoch 18390: train loss: 0.10188018689935706, test loss: 0.2667928284279114\n",
      "epoch 18391: train loss: 0.10187882794220898, test loss: 0.26679349953790876\n",
      "epoch 18392: train loss: 0.1018774690810802, test loss: 0.26679417066429645\n",
      "epoch 18393: train loss: 0.10187611031595804, test loss: 0.2667948418070704\n",
      "epoch 18394: train loss: 0.10187475164682985, test loss: 0.2667955129662268\n",
      "epoch 18395: train loss: 0.10187339307368298, test loss: 0.2667961841417613\n",
      "epoch 18396: train loss: 0.1018720345965047, test loss: 0.2667968553336701\n",
      "epoch 18397: train loss: 0.10187067621528247, test loss: 0.26679752654194916\n",
      "epoch 18398: train loss: 0.10186931793000353, test loss: 0.2667981977665944\n",
      "epoch 18399: train loss: 0.10186795974065528, test loss: 0.26679886900760186\n",
      "epoch 18400: train loss: 0.10186660164722504, test loss: 0.2667995402649676\n",
      "epoch 18401: train loss: 0.10186524364970019, test loss: 0.2668002115386875\n",
      "epoch 18402: train loss: 0.10186388574806808, test loss: 0.26680088282875764\n",
      "epoch 18403: train loss: 0.10186252794231604, test loss: 0.26680155413517387\n",
      "epoch 18404: train loss: 0.10186117023243145, test loss: 0.2668022254579323\n",
      "epoch 18405: train loss: 0.10185981261840168, test loss: 0.26680289679702884\n",
      "epoch 18406: train loss: 0.1018584551002141, test loss: 0.26680356815245954\n",
      "epoch 18407: train loss: 0.10185709767785601, test loss: 0.26680423952422033\n",
      "epoch 18408: train loss: 0.10185574035131488, test loss: 0.2668049109123073\n",
      "epoch 18409: train loss: 0.10185438312057798, test loss: 0.26680558231671647\n",
      "epoch 18410: train loss: 0.10185302598563276, test loss: 0.2668062537374437\n",
      "epoch 18411: train loss: 0.10185166894646654, test loss: 0.266806925174485\n",
      "epoch 18412: train loss: 0.10185031200306675, test loss: 0.2668075966278366\n",
      "epoch 18413: train loss: 0.10184895515542072, test loss: 0.2668082680974942\n",
      "epoch 18414: train loss: 0.10184759840351588, test loss: 0.2668089395834539\n",
      "epoch 18415: train loss: 0.10184624174733958, test loss: 0.2668096110857118\n",
      "epoch 18416: train loss: 0.10184488518687922, test loss: 0.26681028260426376\n",
      "epoch 18417: train loss: 0.1018435287221222, test loss: 0.2668109541391059\n",
      "epoch 18418: train loss: 0.10184217235305588, test loss: 0.26681162569023403\n",
      "epoch 18419: train loss: 0.10184081607966768, test loss: 0.26681229725764444\n",
      "epoch 18420: train loss: 0.10183945990194498, test loss: 0.26681296884133293\n",
      "epoch 18421: train loss: 0.1018381038198752, test loss: 0.26681364044129563\n",
      "epoch 18422: train loss: 0.10183674783344575, test loss: 0.2668143120575284\n",
      "epoch 18423: train loss: 0.10183539194264402, test loss: 0.2668149836900274\n",
      "epoch 18424: train loss: 0.1018340361474574, test loss: 0.26681565533878854\n",
      "epoch 18425: train loss: 0.10183268044787329, test loss: 0.2668163270038078\n",
      "epoch 18426: train loss: 0.10183132484387915, test loss: 0.2668169986850814\n",
      "epoch 18427: train loss: 0.10182996933546237, test loss: 0.2668176703826051\n",
      "epoch 18428: train loss: 0.10182861392261035, test loss: 0.266818342096375\n",
      "epoch 18429: train loss: 0.10182725860531054, test loss: 0.2668190138263871\n",
      "epoch 18430: train loss: 0.1018259033835503, test loss: 0.2668196855726376\n",
      "epoch 18431: train loss: 0.10182454825731713, test loss: 0.2668203573351222\n",
      "epoch 18432: train loss: 0.1018231932265984, test loss: 0.2668210291138371\n",
      "epoch 18433: train loss: 0.10182183829138157, test loss: 0.26682170090877827\n",
      "epoch 18434: train loss: 0.10182048345165404, test loss: 0.2668223727199418\n",
      "epoch 18435: train loss: 0.10181912870740327, test loss: 0.2668230445473237\n",
      "epoch 18436: train loss: 0.1018177740586167, test loss: 0.26682371639091984\n",
      "epoch 18437: train loss: 0.10181641950528172, test loss: 0.26682438825072646\n",
      "epoch 18438: train loss: 0.10181506504738583, test loss: 0.2668250601267393\n",
      "epoch 18439: train loss: 0.10181371068491642, test loss: 0.2668257320189546\n",
      "epoch 18440: train loss: 0.10181235641786099, test loss: 0.26682640392736834\n",
      "epoch 18441: train loss: 0.10181100224620693, test loss: 0.2668270758519766\n",
      "epoch 18442: train loss: 0.1018096481699417, test loss: 0.2668277477927753\n",
      "epoch 18443: train loss: 0.10180829418905278, test loss: 0.2668284197497605\n",
      "epoch 18444: train loss: 0.10180694030352759, test loss: 0.2668290917229282\n",
      "epoch 18445: train loss: 0.1018055865133536, test loss: 0.26682976371227446\n",
      "epoch 18446: train loss: 0.1018042328185183, test loss: 0.2668304357177953\n",
      "epoch 18447: train loss: 0.10180287921900913, test loss: 0.2668311077394868\n",
      "epoch 18448: train loss: 0.1018015257148135, test loss: 0.26683177977734496\n",
      "epoch 18449: train loss: 0.10180017230591894, test loss: 0.2668324518313657\n",
      "epoch 18450: train loss: 0.1017988189923129, test loss: 0.2668331239015452\n",
      "epoch 18451: train loss: 0.10179746577398287, test loss: 0.26683379598787943\n",
      "epoch 18452: train loss: 0.1017961126509163, test loss: 0.2668344680903644\n",
      "epoch 18453: train loss: 0.10179475962310064, test loss: 0.2668351402089962\n",
      "epoch 18454: train loss: 0.1017934066905234, test loss: 0.2668358123437708\n",
      "epoch 18455: train loss: 0.10179205385317205, test loss: 0.26683648449468433\n",
      "epoch 18456: train loss: 0.10179070111103408, test loss: 0.2668371566617327\n",
      "epoch 18457: train loss: 0.10178934846409698, test loss: 0.26683782884491214\n",
      "epoch 18458: train loss: 0.10178799591234823, test loss: 0.2668385010442184\n",
      "epoch 18459: train loss: 0.10178664345577529, test loss: 0.26683917325964784\n",
      "epoch 18460: train loss: 0.10178529109436568, test loss: 0.26683984549119627\n",
      "epoch 18461: train loss: 0.10178393882810688, test loss: 0.2668405177388598\n",
      "epoch 18462: train loss: 0.10178258665698642, test loss: 0.26684119000263457\n",
      "epoch 18463: train loss: 0.10178123458099175, test loss: 0.2668418622825165\n",
      "epoch 18464: train loss: 0.10177988260011044, test loss: 0.2668425345785016\n",
      "epoch 18465: train loss: 0.10177853071432992, test loss: 0.266843206890586\n",
      "epoch 18466: train loss: 0.1017771789236377, test loss: 0.26684387921876573\n",
      "epoch 18467: train loss: 0.10177582722802134, test loss: 0.2668445515630368\n",
      "epoch 18468: train loss: 0.10177447562746829, test loss: 0.26684522392339544\n",
      "epoch 18469: train loss: 0.10177312412196614, test loss: 0.26684589629983735\n",
      "epoch 18470: train loss: 0.10177177271150231, test loss: 0.2668465686923589\n",
      "epoch 18471: train loss: 0.10177042139606439, test loss: 0.26684724110095603\n",
      "epoch 18472: train loss: 0.1017690701756399, test loss: 0.26684791352562465\n",
      "epoch 18473: train loss: 0.1017677190502163, test loss: 0.266848585966361\n",
      "epoch 18474: train loss: 0.10176636801978116, test loss: 0.2668492584231611\n",
      "epoch 18475: train loss: 0.10176501708432201, test loss: 0.26684993089602094\n",
      "epoch 18476: train loss: 0.10176366624382634, test loss: 0.26685060338493666\n",
      "epoch 18477: train loss: 0.10176231549828174, test loss: 0.26685127588990415\n",
      "epoch 18478: train loss: 0.10176096484767569, test loss: 0.2668519484109197\n",
      "epoch 18479: train loss: 0.10175961429199573, test loss: 0.2668526209479792\n",
      "epoch 18480: train loss: 0.10175826383122942, test loss: 0.2668532935010788\n",
      "epoch 18481: train loss: 0.10175691346536432, test loss: 0.2668539660702144\n",
      "epoch 18482: train loss: 0.10175556319438794, test loss: 0.2668546386553823\n",
      "epoch 18483: train loss: 0.10175421301828784, test loss: 0.2668553112565783\n",
      "epoch 18484: train loss: 0.10175286293705155, test loss: 0.2668559838737986\n",
      "epoch 18485: train loss: 0.10175151295066662, test loss: 0.2668566565070394\n",
      "epoch 18486: train loss: 0.10175016305912063, test loss: 0.26685732915629645\n",
      "epoch 18487: train loss: 0.10174881326240111, test loss: 0.2668580018215661\n",
      "epoch 18488: train loss: 0.10174746356049562, test loss: 0.2668586745028442\n",
      "epoch 18489: train loss: 0.10174611395339174, test loss: 0.26685934720012694\n",
      "epoch 18490: train loss: 0.10174476444107698, test loss: 0.26686001991341035\n",
      "epoch 18491: train loss: 0.101743415023539, test loss: 0.2668606926426906\n",
      "epoch 18492: train loss: 0.10174206570076524, test loss: 0.26686136538796357\n",
      "epoch 18493: train loss: 0.10174071647274337, test loss: 0.26686203814922543\n",
      "epoch 18494: train loss: 0.10173936733946093, test loss: 0.2668627109264723\n",
      "epoch 18495: train loss: 0.10173801830090547, test loss: 0.2668633837197001\n",
      "epoch 18496: train loss: 0.10173666935706459, test loss: 0.266864056528905\n",
      "epoch 18497: train loss: 0.10173532050792584, test loss: 0.2668647293540831\n",
      "epoch 18498: train loss: 0.10173397175347684, test loss: 0.26686540219523036\n",
      "epoch 18499: train loss: 0.10173262309370518, test loss: 0.26686607505234305\n",
      "epoch 18500: train loss: 0.10173127452859837, test loss: 0.266866747925417\n",
      "epoch 18501: train loss: 0.10172992605814407, test loss: 0.2668674208144485\n",
      "epoch 18502: train loss: 0.10172857768232985, test loss: 0.2668680937194335\n",
      "epoch 18503: train loss: 0.1017272294011433, test loss: 0.2668687666403681\n",
      "epoch 18504: train loss: 0.10172588121457199, test loss: 0.2668694395772485\n",
      "epoch 18505: train loss: 0.10172453312260356, test loss: 0.2668701125300706\n",
      "epoch 18506: train loss: 0.10172318512522559, test loss: 0.26687078549883053\n",
      "epoch 18507: train loss: 0.10172183722242564, test loss: 0.26687145848352434\n",
      "epoch 18508: train loss: 0.10172048941419139, test loss: 0.26687213148414823\n",
      "epoch 18509: train loss: 0.10171914170051038, test loss: 0.2668728045006982\n",
      "epoch 18510: train loss: 0.10171779408137027, test loss: 0.2668734775331704\n",
      "epoch 18511: train loss: 0.10171644655675863, test loss: 0.26687415058156083\n",
      "epoch 18512: train loss: 0.10171509912666311, test loss: 0.26687482364586557\n",
      "epoch 18513: train loss: 0.10171375179107128, test loss: 0.26687549672608074\n",
      "epoch 18514: train loss: 0.1017124045499708, test loss: 0.26687616982220247\n",
      "epoch 18515: train loss: 0.10171105740334925, test loss: 0.26687684293422687\n",
      "epoch 18516: train loss: 0.10170971035119428, test loss: 0.2668775160621499\n",
      "epoch 18517: train loss: 0.1017083633934935, test loss: 0.2668781892059677\n",
      "epoch 18518: train loss: 0.10170701653023455, test loss: 0.2668788623656765\n",
      "epoch 18519: train loss: 0.10170566976140502, test loss: 0.26687953554127214\n",
      "epoch 18520: train loss: 0.10170432308699262, test loss: 0.2668802087327508\n",
      "epoch 18521: train loss: 0.10170297650698494, test loss: 0.2668808819401087\n",
      "epoch 18522: train loss: 0.1017016300213696, test loss: 0.26688155516334183\n",
      "epoch 18523: train loss: 0.10170028363013424, test loss: 0.26688222840244635\n",
      "epoch 18524: train loss: 0.10169893733326649, test loss: 0.26688290165741824\n",
      "epoch 18525: train loss: 0.10169759113075408, test loss: 0.26688357492825376\n",
      "epoch 18526: train loss: 0.10169624502258452, test loss: 0.2668842482149488\n",
      "epoch 18527: train loss: 0.10169489900874557, test loss: 0.2668849215174996\n",
      "epoch 18528: train loss: 0.10169355308922484, test loss: 0.2668855948359022\n",
      "epoch 18529: train loss: 0.10169220726400996, test loss: 0.26688626817015276\n",
      "epoch 18530: train loss: 0.10169086153308861, test loss: 0.26688694152024744\n",
      "epoch 18531: train loss: 0.10168951589644845, test loss: 0.2668876148861821\n",
      "epoch 18532: train loss: 0.1016881703540771, test loss: 0.2668882882679531\n",
      "epoch 18533: train loss: 0.10168682490596226, test loss: 0.2668889616655564\n",
      "epoch 18534: train loss: 0.1016854795520916, test loss: 0.26688963507898805\n",
      "epoch 18535: train loss: 0.10168413429245277, test loss: 0.26689030850824436\n",
      "epoch 18536: train loss: 0.10168278912703343, test loss: 0.2668909819533213\n",
      "epoch 18537: train loss: 0.10168144405582126, test loss: 0.26689165541421495\n",
      "epoch 18538: train loss: 0.10168009907880393, test loss: 0.26689232889092146\n",
      "epoch 18539: train loss: 0.10167875419596911, test loss: 0.266893002383437\n",
      "epoch 18540: train loss: 0.10167740940730448, test loss: 0.2668936758917576\n",
      "epoch 18541: train loss: 0.10167606471279775, test loss: 0.26689434941587936\n",
      "epoch 18542: train loss: 0.10167472011243657, test loss: 0.2668950229557985\n",
      "epoch 18543: train loss: 0.10167337560620862, test loss: 0.266895696511511\n",
      "epoch 18544: train loss: 0.1016720311941016, test loss: 0.2668963700830129\n",
      "epoch 18545: train loss: 0.1016706868761032, test loss: 0.2668970436703006\n",
      "epoch 18546: train loss: 0.10166934265220111, test loss: 0.26689771727337\n",
      "epoch 18547: train loss: 0.10166799852238301, test loss: 0.26689839089221734\n",
      "epoch 18548: train loss: 0.10166665448663662, test loss: 0.2668990645268386\n",
      "epoch 18549: train loss: 0.10166531054494961, test loss: 0.26689973817722995\n",
      "epoch 18550: train loss: 0.10166396669730968, test loss: 0.2669004118433875\n",
      "epoch 18551: train loss: 0.10166262294370458, test loss: 0.2669010855253075\n",
      "epoch 18552: train loss: 0.10166127928412198, test loss: 0.2669017592229858\n",
      "epoch 18553: train loss: 0.10165993571854959, test loss: 0.26690243293641885\n",
      "epoch 18554: train loss: 0.10165859224697511, test loss: 0.26690310666560246\n",
      "epoch 18555: train loss: 0.10165724886938628, test loss: 0.2669037804105329\n",
      "epoch 18556: train loss: 0.1016559055857708, test loss: 0.26690445417120634\n",
      "epoch 18557: train loss: 0.10165456239611637, test loss: 0.2669051279476188\n",
      "epoch 18558: train loss: 0.1016532193004107, test loss: 0.26690580173976647\n",
      "epoch 18559: train loss: 0.10165187629864157, test loss: 0.26690647554764546\n",
      "epoch 18560: train loss: 0.10165053339079666, test loss: 0.2669071493712519\n",
      "epoch 18561: train loss: 0.10164919057686368, test loss: 0.2669078232105819\n",
      "epoch 18562: train loss: 0.1016478478568304, test loss: 0.26690849706563147\n",
      "epoch 18563: train loss: 0.10164650523068454, test loss: 0.2669091709363971\n",
      "epoch 18564: train loss: 0.10164516269841382, test loss: 0.26690984482287455\n",
      "epoch 18565: train loss: 0.10164382026000596, test loss: 0.2669105187250601\n",
      "epoch 18566: train loss: 0.10164247791544873, test loss: 0.2669111926429498\n",
      "epoch 18567: train loss: 0.10164113566472985, test loss: 0.2669118665765399\n",
      "epoch 18568: train loss: 0.10163979350783706, test loss: 0.2669125405258265\n",
      "epoch 18569: train loss: 0.10163845144475812, test loss: 0.26691321449080574\n",
      "epoch 18570: train loss: 0.1016371094754808, test loss: 0.2669138884714737\n",
      "epoch 18571: train loss: 0.10163576759999275, test loss: 0.2669145624678265\n",
      "epoch 18572: train loss: 0.10163442581828183, test loss: 0.2669152364798604\n",
      "epoch 18573: train loss: 0.10163308413033574, test loss: 0.2669159105075714\n",
      "epoch 18574: train loss: 0.10163174253614227, test loss: 0.2669165845509558\n",
      "epoch 18575: train loss: 0.1016304010356891, test loss: 0.2669172586100095\n",
      "epoch 18576: train loss: 0.1016290596289641, test loss: 0.2669179326847288\n",
      "epoch 18577: train loss: 0.10162771831595493, test loss: 0.26691860677510987\n",
      "epoch 18578: train loss: 0.10162637709664946, test loss: 0.2669192808811488\n",
      "epoch 18579: train loss: 0.10162503597103534, test loss: 0.2669199550028416\n",
      "epoch 18580: train loss: 0.10162369493910042, test loss: 0.26692062914018466\n",
      "epoch 18581: train loss: 0.10162235400083244, test loss: 0.26692130329317393\n",
      "epoch 18582: train loss: 0.1016210131562192, test loss: 0.2669219774618058\n",
      "epoch 18583: train loss: 0.10161967240524844, test loss: 0.2669226516460761\n",
      "epoch 18584: train loss: 0.10161833174790796, test loss: 0.2669233258459811\n",
      "epoch 18585: train loss: 0.10161699118418553, test loss: 0.26692400006151706\n",
      "epoch 18586: train loss: 0.10161565071406894, test loss: 0.2669246742926799\n",
      "epoch 18587: train loss: 0.10161431033754599, test loss: 0.26692534853946603\n",
      "epoch 18588: train loss: 0.10161297005460447, test loss: 0.26692602280187144\n",
      "epoch 18589: train loss: 0.10161162986523213, test loss: 0.26692669707989236\n",
      "epoch 18590: train loss: 0.1016102897694168, test loss: 0.26692737137352485\n",
      "epoch 18591: train loss: 0.10160894976714623, test loss: 0.2669280456827651\n",
      "epoch 18592: train loss: 0.10160760985840826, test loss: 0.2669287200076093\n",
      "epoch 18593: train loss: 0.10160627004319067, test loss: 0.2669293943480535\n",
      "epoch 18594: train loss: 0.10160493032148127, test loss: 0.2669300687040941\n",
      "epoch 18595: train loss: 0.10160359069326785, test loss: 0.266930743075727\n",
      "epoch 18596: train loss: 0.10160225115853823, test loss: 0.2669314174629484\n",
      "epoch 18597: train loss: 0.10160091171728024, test loss: 0.26693209186575456\n",
      "epoch 18598: train loss: 0.10159957236948162, test loss: 0.2669327662841416\n",
      "epoch 18599: train loss: 0.10159823311513025, test loss: 0.2669334407181056\n",
      "epoch 18600: train loss: 0.10159689395421392, test loss: 0.26693411516764276\n",
      "epoch 18601: train loss: 0.10159555488672044, test loss: 0.26693478963274925\n",
      "epoch 18602: train loss: 0.10159421591263763, test loss: 0.26693546411342134\n",
      "epoch 18603: train loss: 0.10159287703195333, test loss: 0.26693613860965504\n",
      "epoch 18604: train loss: 0.10159153824465532, test loss: 0.26693681312144657\n",
      "epoch 18605: train loss: 0.10159019955073151, test loss: 0.2669374876487921\n",
      "epoch 18606: train loss: 0.10158886095016963, test loss: 0.26693816219168776\n",
      "epoch 18607: train loss: 0.10158752244295759, test loss: 0.2669388367501298\n",
      "epoch 18608: train loss: 0.10158618402908318, test loss: 0.2669395113241142\n",
      "epoch 18609: train loss: 0.10158484570853422, test loss: 0.2669401859136374\n",
      "epoch 18610: train loss: 0.1015835074812986, test loss: 0.2669408605186954\n",
      "epoch 18611: train loss: 0.10158216934736412, test loss: 0.2669415351392844\n",
      "epoch 18612: train loss: 0.10158083130671863, test loss: 0.2669422097754005\n",
      "epoch 18613: train loss: 0.10157949335935001, test loss: 0.26694288442703995\n",
      "epoch 18614: train loss: 0.10157815550524604, test loss: 0.2669435590941988\n",
      "epoch 18615: train loss: 0.1015768177443946, test loss: 0.26694423377687354\n",
      "epoch 18616: train loss: 0.10157548007678358, test loss: 0.2669449084750601\n",
      "epoch 18617: train loss: 0.10157414250240077, test loss: 0.2669455831887546\n",
      "epoch 18618: train loss: 0.10157280502123406, test loss: 0.26694625791795334\n",
      "epoch 18619: train loss: 0.10157146763327128, test loss: 0.2669469326626524\n",
      "epoch 18620: train loss: 0.10157013033850035, test loss: 0.26694760742284807\n",
      "epoch 18621: train loss: 0.10156879313690909, test loss: 0.2669482821985365\n",
      "epoch 18622: train loss: 0.10156745602848534, test loss: 0.2669489569897137\n",
      "epoch 18623: train loss: 0.101566119013217, test loss: 0.26694963179637615\n",
      "epoch 18624: train loss: 0.10156478209109196, test loss: 0.2669503066185198\n",
      "epoch 18625: train loss: 0.10156344526209804, test loss: 0.2669509814561409\n",
      "epoch 18626: train loss: 0.10156210852622313, test loss: 0.2669516563092357\n",
      "epoch 18627: train loss: 0.10156077188345515, test loss: 0.2669523311778002\n",
      "epoch 18628: train loss: 0.10155943533378192, test loss: 0.26695300606183076\n",
      "epoch 18629: train loss: 0.10155809887719137, test loss: 0.26695368096132344\n",
      "epoch 18630: train loss: 0.10155676251367134, test loss: 0.26695435587627464\n",
      "epoch 18631: train loss: 0.10155542624320972, test loss: 0.26695503080668026\n",
      "epoch 18632: train loss: 0.1015540900657944, test loss: 0.2669557057525366\n",
      "epoch 18633: train loss: 0.10155275398141328, test loss: 0.26695638071383987\n",
      "epoch 18634: train loss: 0.10155141799005428, test loss: 0.2669570556905863\n",
      "epoch 18635: train loss: 0.10155008209170525, test loss: 0.26695773068277207\n",
      "epoch 18636: train loss: 0.10154874628635407, test loss: 0.2669584056903933\n",
      "epoch 18637: train loss: 0.10154741057398871, test loss: 0.2669590807134462\n",
      "epoch 18638: train loss: 0.101546074954597, test loss: 0.266959755751927\n",
      "epoch 18639: train loss: 0.1015447394281669, test loss: 0.26696043080583187\n",
      "epoch 18640: train loss: 0.10154340399468627, test loss: 0.266961105875157\n",
      "epoch 18641: train loss: 0.10154206865414302, test loss: 0.2669617809598986\n",
      "epoch 18642: train loss: 0.10154073340652509, test loss: 0.26696245606005276\n",
      "epoch 18643: train loss: 0.10153939825182037, test loss: 0.2669631311756159\n",
      "epoch 18644: train loss: 0.10153806319001679, test loss: 0.266963806306584\n",
      "epoch 18645: train loss: 0.10153672822110225, test loss: 0.2669644814529534\n",
      "epoch 18646: train loss: 0.10153539334506469, test loss: 0.26696515661472014\n",
      "epoch 18647: train loss: 0.10153405856189199, test loss: 0.26696583179188055\n",
      "epoch 18648: train loss: 0.10153272387157208, test loss: 0.2669665069844309\n",
      "epoch 18649: train loss: 0.10153138927409296, test loss: 0.2669671821923672\n",
      "epoch 18650: train loss: 0.10153005476944246, test loss: 0.26696785741568574\n",
      "epoch 18651: train loss: 0.10152872035760857, test loss: 0.2669685326543828\n",
      "epoch 18652: train loss: 0.10152738603857923, test loss: 0.2669692079084544\n",
      "epoch 18653: train loss: 0.10152605181234231, test loss: 0.26696988317789694\n",
      "epoch 18654: train loss: 0.10152471767888581, test loss: 0.2669705584627065\n",
      "epoch 18655: train loss: 0.10152338363819763, test loss: 0.26697123376287935\n",
      "epoch 18656: train loss: 0.10152204969026571, test loss: 0.2669719090784117\n",
      "epoch 18657: train loss: 0.10152071583507803, test loss: 0.26697258440929966\n",
      "epoch 18658: train loss: 0.10151938207262251, test loss: 0.2669732597555394\n",
      "epoch 18659: train loss: 0.1015180484028871, test loss: 0.26697393511712747\n",
      "epoch 18660: train loss: 0.10151671482585975, test loss: 0.2669746104940597\n",
      "epoch 18661: train loss: 0.10151538134152842, test loss: 0.26697528588633246\n",
      "epoch 18662: train loss: 0.10151404794988104, test loss: 0.2669759612939419\n",
      "epoch 18663: train loss: 0.10151271465090561, test loss: 0.26697663671688443\n",
      "epoch 18664: train loss: 0.10151138144459008, test loss: 0.266977312155156\n",
      "epoch 18665: train loss: 0.10151004833092238, test loss: 0.266977987608753\n",
      "epoch 18666: train loss: 0.10150871530989045, test loss: 0.26697866307767154\n",
      "epoch 18667: train loss: 0.10150738238148233, test loss: 0.26697933856190786\n",
      "epoch 18668: train loss: 0.10150604954568596, test loss: 0.2669800140614582\n",
      "epoch 18669: train loss: 0.1015047168024893, test loss: 0.2669806895763189\n",
      "epoch 18670: train loss: 0.10150338415188032, test loss: 0.26698136510648585\n",
      "epoch 18671: train loss: 0.101502051593847, test loss: 0.2669820406519555\n",
      "epoch 18672: train loss: 0.10150071912837731, test loss: 0.2669827162127242\n",
      "epoch 18673: train loss: 0.10149938675545926, test loss: 0.2669833917887879\n",
      "epoch 18674: train loss: 0.10149805447508077, test loss: 0.2669840673801429\n",
      "epoch 18675: train loss: 0.10149672228722988, test loss: 0.2669847429867855\n",
      "epoch 18676: train loss: 0.10149539019189457, test loss: 0.2669854186087119\n",
      "epoch 18677: train loss: 0.10149405818906282, test loss: 0.26698609424591824\n",
      "epoch 18678: train loss: 0.10149272627872259, test loss: 0.2669867698984008\n",
      "epoch 18679: train loss: 0.10149139446086192, test loss: 0.26698744556615583\n",
      "epoch 18680: train loss: 0.10149006273546876, test loss: 0.2669881212491796\n",
      "epoch 18681: train loss: 0.10148873110253115, test loss: 0.26698879694746824\n",
      "epoch 18682: train loss: 0.10148739956203708, test loss: 0.2669894726610179\n",
      "epoch 18683: train loss: 0.1014860681139745, test loss: 0.2669901483898251\n",
      "epoch 18684: train loss: 0.10148473675833149, test loss: 0.2669908241338858\n",
      "epoch 18685: train loss: 0.10148340549509602, test loss: 0.2669914998931963\n",
      "epoch 18686: train loss: 0.10148207432425607, test loss: 0.26699217566775296\n",
      "epoch 18687: train loss: 0.10148074324579971, test loss: 0.26699285145755186\n",
      "epoch 18688: train loss: 0.1014794122597149, test loss: 0.2669935272625893\n",
      "epoch 18689: train loss: 0.10147808136598968, test loss: 0.2669942030828614\n",
      "epoch 18690: train loss: 0.10147675056461207, test loss: 0.26699487891836454\n",
      "epoch 18691: train loss: 0.10147541985557007, test loss: 0.266995554769095\n",
      "epoch 18692: train loss: 0.10147408923885173, test loss: 0.26699623063504874\n",
      "epoch 18693: train loss: 0.10147275871444504, test loss: 0.2669969065162222\n",
      "epoch 18694: train loss: 0.10147142828233804, test loss: 0.26699758241261173\n",
      "epoch 18695: train loss: 0.10147009794251877, test loss: 0.2669982583242134\n",
      "epoch 18696: train loss: 0.10146876769497527, test loss: 0.2669989342510235\n",
      "epoch 18697: train loss: 0.10146743753969552, test loss: 0.2669996101930381\n",
      "epoch 18698: train loss: 0.10146610747666764, test loss: 0.26700028615025373\n",
      "epoch 18699: train loss: 0.10146477750587955, test loss: 0.26700096212266655\n",
      "epoch 18700: train loss: 0.1014634476273194, test loss: 0.2670016381102726\n",
      "epoch 18701: train loss: 0.10146211784097517, test loss: 0.26700231411306846\n",
      "epoch 18702: train loss: 0.10146078814683496, test loss: 0.26700299013105\n",
      "epoch 18703: train loss: 0.10145945854488671, test loss: 0.2670036661642138\n",
      "epoch 18704: train loss: 0.10145812903511857, test loss: 0.26700434221255587\n",
      "epoch 18705: train loss: 0.10145679961751856, test loss: 0.26700501827607254\n",
      "epoch 18706: train loss: 0.1014554702920747, test loss: 0.2670056943547601\n",
      "epoch 18707: train loss: 0.1014541410587751, test loss: 0.26700637044861475\n",
      "epoch 18708: train loss: 0.10145281191760777, test loss: 0.2670070465576328\n",
      "epoch 18709: train loss: 0.10145148286856082, test loss: 0.2670077226818104\n",
      "epoch 18710: train loss: 0.10145015391162221, test loss: 0.2670083988211439\n",
      "epoch 18711: train loss: 0.10144882504678014, test loss: 0.2670090749756295\n",
      "epoch 18712: train loss: 0.1014474962740226, test loss: 0.2670097511452635\n",
      "epoch 18713: train loss: 0.10144616759333763, test loss: 0.26701042733004204\n",
      "epoch 18714: train loss: 0.10144483900471336, test loss: 0.26701110352996144\n",
      "epoch 18715: train loss: 0.10144351050813784, test loss: 0.26701177974501794\n",
      "epoch 18716: train loss: 0.10144218210359915, test loss: 0.26701245597520795\n",
      "epoch 18717: train loss: 0.10144085379108535, test loss: 0.26701313222052747\n",
      "epoch 18718: train loss: 0.10143952557058451, test loss: 0.267013808480973\n",
      "epoch 18719: train loss: 0.10143819744208478, test loss: 0.26701448475654066\n",
      "epoch 18720: train loss: 0.10143686940557414, test loss: 0.26701516104722667\n",
      "epoch 18721: train loss: 0.10143554146104077, test loss: 0.2670158373530273\n",
      "epoch 18722: train loss: 0.10143421360847268, test loss: 0.267016513673939\n",
      "epoch 18723: train loss: 0.10143288584785803, test loss: 0.2670171900099578\n",
      "epoch 18724: train loss: 0.10143155817918487, test loss: 0.2670178663610801\n",
      "epoch 18725: train loss: 0.10143023060244132, test loss: 0.2670185427273022\n",
      "epoch 18726: train loss: 0.10142890311761545, test loss: 0.2670192191086202\n",
      "epoch 18727: train loss: 0.10142757572469537, test loss: 0.2670198955050304\n",
      "epoch 18728: train loss: 0.10142624842366917, test loss: 0.26702057191652917\n",
      "epoch 18729: train loss: 0.10142492121452498, test loss: 0.26702124834311275\n",
      "epoch 18730: train loss: 0.10142359409725088, test loss: 0.26702192478477743\n",
      "epoch 18731: train loss: 0.10142226707183498, test loss: 0.2670226012415193\n",
      "epoch 18732: train loss: 0.10142094013826541, test loss: 0.26702327771333484\n",
      "epoch 18733: train loss: 0.10141961329653029, test loss: 0.2670239542002202\n",
      "epoch 18734: train loss: 0.10141828654661769, test loss: 0.2670246307021717\n",
      "epoch 18735: train loss: 0.10141695988851578, test loss: 0.26702530721918566\n",
      "epoch 18736: train loss: 0.10141563332221258, test loss: 0.26702598375125824\n",
      "epoch 18737: train loss: 0.10141430684769634, test loss: 0.26702666029838573\n",
      "epoch 18738: train loss: 0.1014129804649551, test loss: 0.26702733686056446\n",
      "epoch 18739: train loss: 0.10141165417397703, test loss: 0.2670280134377907\n",
      "epoch 18740: train loss: 0.1014103279747502, test loss: 0.2670286900300607\n",
      "epoch 18741: train loss: 0.10140900186726283, test loss: 0.2670293666373707\n",
      "epoch 18742: train loss: 0.10140767585150297, test loss: 0.26703004325971724\n",
      "epoch 18743: train loss: 0.10140634992745877, test loss: 0.26703071989709615\n",
      "epoch 18744: train loss: 0.10140502409511835, test loss: 0.26703139654950403\n",
      "epoch 18745: train loss: 0.10140369835446993, test loss: 0.267032073216937\n",
      "epoch 18746: train loss: 0.10140237270550159, test loss: 0.2670327498993915\n",
      "epoch 18747: train loss: 0.10140104714820147, test loss: 0.2670334265968638\n",
      "epoch 18748: train loss: 0.10139972168255772, test loss: 0.26703410330934996\n",
      "epoch 18749: train loss: 0.1013983963085585, test loss: 0.2670347800368464\n",
      "epoch 18750: train loss: 0.10139707102619194, test loss: 0.2670354567793495\n",
      "epoch 18751: train loss: 0.10139574583544622, test loss: 0.26703613353685546\n",
      "epoch 18752: train loss: 0.10139442073630947, test loss: 0.26703681030936044\n",
      "epoch 18753: train loss: 0.10139309572876985, test loss: 0.267037487096861\n",
      "epoch 18754: train loss: 0.10139177081281553, test loss: 0.2670381638993532\n",
      "epoch 18755: train loss: 0.10139044598843466, test loss: 0.2670388407168334\n",
      "epoch 18756: train loss: 0.10138912125561539, test loss: 0.26703951754929794\n",
      "epoch 18757: train loss: 0.10138779661434592, test loss: 0.267040194396743\n",
      "epoch 18758: train loss: 0.10138647206461439, test loss: 0.2670408712591649\n",
      "epoch 18759: train loss: 0.10138514760640897, test loss: 0.26704154813656006\n",
      "epoch 18760: train loss: 0.10138382323971783, test loss: 0.26704222502892455\n",
      "epoch 18761: train loss: 0.10138249896452917, test loss: 0.2670429019362549\n",
      "epoch 18762: train loss: 0.10138117478083113, test loss: 0.26704357885854735\n",
      "epoch 18763: train loss: 0.10137985068861191, test loss: 0.26704425579579794\n",
      "epoch 18764: train loss: 0.10137852668785968, test loss: 0.26704493274800334\n",
      "epoch 18765: train loss: 0.10137720277856262, test loss: 0.2670456097151596\n",
      "epoch 18766: train loss: 0.10137587896070893, test loss: 0.2670462866972631\n",
      "epoch 18767: train loss: 0.10137455523428679, test loss: 0.26704696369431\n",
      "epoch 18768: train loss: 0.1013732315992844, test loss: 0.2670476407062969\n",
      "epoch 18769: train loss: 0.1013719080556899, test loss: 0.2670483177332197\n",
      "epoch 18770: train loss: 0.10137058460349156, test loss: 0.2670489947750751\n",
      "epoch 18771: train loss: 0.10136926124267749, test loss: 0.2670496718318591\n",
      "epoch 18772: train loss: 0.10136793797323598, test loss: 0.26705034890356827\n",
      "epoch 18773: train loss: 0.10136661479515513, test loss: 0.26705102599019864\n",
      "epoch 18774: train loss: 0.10136529170842323, test loss: 0.2670517030917467\n",
      "epoch 18775: train loss: 0.10136396871302844, test loss: 0.26705238020820865\n",
      "epoch 18776: train loss: 0.10136264580895898, test loss: 0.2670530573395809\n",
      "epoch 18777: train loss: 0.10136132299620305, test loss: 0.26705373448585956\n",
      "epoch 18778: train loss: 0.10136000027474887, test loss: 0.2670544116470412\n",
      "epoch 18779: train loss: 0.10135867764458464, test loss: 0.26705508882312196\n",
      "epoch 18780: train loss: 0.10135735510569858, test loss: 0.26705576601409814\n",
      "epoch 18781: train loss: 0.10135603265807891, test loss: 0.267056443219966\n",
      "epoch 18782: train loss: 0.10135471030171385, test loss: 0.26705712044072205\n",
      "epoch 18783: train loss: 0.10135338803659164, test loss: 0.2670577976763625\n",
      "epoch 18784: train loss: 0.10135206586270044, test loss: 0.2670584749268837\n",
      "epoch 18785: train loss: 0.10135074378002855, test loss: 0.26705915219228177\n",
      "epoch 18786: train loss: 0.10134942178856415, test loss: 0.26705982947255336\n",
      "epoch 18787: train loss: 0.10134809988829549, test loss: 0.26706050676769444\n",
      "epoch 18788: train loss: 0.10134677807921079, test loss: 0.2670611840777014\n",
      "epoch 18789: train loss: 0.10134545636129833, test loss: 0.2670618614025708\n",
      "epoch 18790: train loss: 0.10134413473454627, test loss: 0.26706253874229874\n",
      "epoch 18791: train loss: 0.10134281319894292, test loss: 0.26706321609688155\n",
      "epoch 18792: train loss: 0.10134149175447646, test loss: 0.2670638934663156\n",
      "epoch 18793: train loss: 0.1013401704011352, test loss: 0.2670645708505972\n",
      "epoch 18794: train loss: 0.10133884913890732, test loss: 0.2670652482497226\n",
      "epoch 18795: train loss: 0.10133752796778112, test loss: 0.26706592566368825\n",
      "epoch 18796: train loss: 0.10133620688774482, test loss: 0.26706660309249036\n",
      "epoch 18797: train loss: 0.10133488589878667, test loss: 0.2670672805361253\n",
      "epoch 18798: train loss: 0.10133356500089492, test loss: 0.26706795799458943\n",
      "epoch 18799: train loss: 0.10133224419405785, test loss: 0.267068635467879\n",
      "epoch 18800: train loss: 0.1013309234782637, test loss: 0.2670693129559903\n",
      "epoch 18801: train loss: 0.10132960285350076, test loss: 0.2670699904589197\n",
      "epoch 18802: train loss: 0.10132828231975725, test loss: 0.2670706679766636\n",
      "epoch 18803: train loss: 0.10132696187702145, test loss: 0.2670713455092183\n",
      "epoch 18804: train loss: 0.10132564152528165, test loss: 0.26707202305658007\n",
      "epoch 18805: train loss: 0.1013243212645261, test loss: 0.2670727006187452\n",
      "epoch 18806: train loss: 0.10132300109474304, test loss: 0.2670733781957102\n",
      "epoch 18807: train loss: 0.1013216810159208, test loss: 0.2670740557874711\n",
      "epoch 18808: train loss: 0.10132036102804765, test loss: 0.26707473339402454\n",
      "epoch 18809: train loss: 0.10131904113111181, test loss: 0.26707541101536675\n",
      "epoch 18810: train loss: 0.10131772132510164, test loss: 0.267076088651494\n",
      "epoch 18811: train loss: 0.10131640161000535, test loss: 0.26707676630240257\n",
      "epoch 18812: train loss: 0.10131508198581127, test loss: 0.26707744396808897\n",
      "epoch 18813: train loss: 0.10131376245250767, test loss: 0.26707812164854944\n",
      "epoch 18814: train loss: 0.10131244301008284, test loss: 0.26707879934378037\n",
      "epoch 18815: train loss: 0.10131112365852506, test loss: 0.267079477053778\n",
      "epoch 18816: train loss: 0.10130980439782265, test loss: 0.26708015477853864\n",
      "epoch 18817: train loss: 0.10130848522796387, test loss: 0.26708083251805875\n",
      "epoch 18818: train loss: 0.10130716614893708, test loss: 0.2670815102723347\n",
      "epoch 18819: train loss: 0.10130584716073049, test loss: 0.26708218804136263\n",
      "epoch 18820: train loss: 0.10130452826333246, test loss: 0.26708286582513907\n",
      "epoch 18821: train loss: 0.10130320945673127, test loss: 0.2670835436236603\n",
      "epoch 18822: train loss: 0.10130189074091528, test loss: 0.2670842214369226\n",
      "epoch 18823: train loss: 0.1013005721158727, test loss: 0.2670848992649224\n",
      "epoch 18824: train loss: 0.10129925358159193, test loss: 0.267085577107656\n",
      "epoch 18825: train loss: 0.10129793513806123, test loss: 0.26708625496511984\n",
      "epoch 18826: train loss: 0.10129661678526897, test loss: 0.26708693283731\n",
      "epoch 18827: train loss: 0.1012952985232034, test loss: 0.26708761072422305\n",
      "epoch 18828: train loss: 0.10129398035185286, test loss: 0.26708828862585543\n",
      "epoch 18829: train loss: 0.10129266227120569, test loss: 0.26708896654220315\n",
      "epoch 18830: train loss: 0.10129134428125021, test loss: 0.2670896444732629\n",
      "epoch 18831: train loss: 0.10129002638197473, test loss: 0.26709032241903086\n",
      "epoch 18832: train loss: 0.10128870857336757, test loss: 0.26709100037950334\n",
      "epoch 18833: train loss: 0.1012873908554171, test loss: 0.26709167835467684\n",
      "epoch 18834: train loss: 0.10128607322811162, test loss: 0.2670923563445475\n",
      "epoch 18835: train loss: 0.10128475569143944, test loss: 0.2670930343491119\n",
      "epoch 18836: train loss: 0.10128343824538895, test loss: 0.26709371236836627\n",
      "epoch 18837: train loss: 0.10128212088994847, test loss: 0.26709439040230687\n",
      "epoch 18838: train loss: 0.10128080362510632, test loss: 0.26709506845093034\n",
      "epoch 18839: train loss: 0.10127948645085086, test loss: 0.26709574651423273\n",
      "epoch 18840: train loss: 0.10127816936717043, test loss: 0.26709642459221056\n",
      "epoch 18841: train loss: 0.1012768523740534, test loss: 0.26709710268486014\n",
      "epoch 18842: train loss: 0.10127553547148807, test loss: 0.267097780792178\n",
      "epoch 18843: train loss: 0.10127421865946282, test loss: 0.2670984589141602\n",
      "epoch 18844: train loss: 0.10127290193796598, test loss: 0.26709913705080324\n",
      "epoch 18845: train loss: 0.10127158530698598, test loss: 0.26709981520210346\n",
      "epoch 18846: train loss: 0.10127026876651107, test loss: 0.2671004933680573\n",
      "epoch 18847: train loss: 0.1012689523165297, test loss: 0.26710117154866103\n",
      "epoch 18848: train loss: 0.10126763595703019, test loss: 0.2671018497439111\n",
      "epoch 18849: train loss: 0.10126631968800089, test loss: 0.2671025279538038\n",
      "epoch 18850: train loss: 0.10126500350943018, test loss: 0.26710320617833544\n",
      "epoch 18851: train loss: 0.10126368742130644, test loss: 0.2671038844175025\n",
      "epoch 18852: train loss: 0.10126237142361806, test loss: 0.2671045626713012\n",
      "epoch 18853: train loss: 0.10126105551635335, test loss: 0.2671052409397282\n",
      "epoch 18854: train loss: 0.10125973969950074, test loss: 0.2671059192227795\n",
      "epoch 18855: train loss: 0.10125842397304857, test loss: 0.26710659752045174\n",
      "epoch 18856: train loss: 0.10125710833698526, test loss: 0.26710727583274124\n",
      "epoch 18857: train loss: 0.10125579279129916, test loss: 0.26710795415964417\n",
      "epoch 18858: train loss: 0.10125447733597863, test loss: 0.26710863250115713\n",
      "epoch 18859: train loss: 0.10125316197101211, test loss: 0.26710931085727646\n",
      "epoch 18860: train loss: 0.10125184669638797, test loss: 0.26710998922799845\n",
      "epoch 18861: train loss: 0.10125053151209457, test loss: 0.2671106676133195\n",
      "epoch 18862: train loss: 0.10124921641812033, test loss: 0.26711134601323594\n",
      "epoch 18863: train loss: 0.10124790141445363, test loss: 0.2671120244277443\n",
      "epoch 18864: train loss: 0.10124658650108291, test loss: 0.2671127028568407\n",
      "epoch 18865: train loss: 0.10124527167799649, test loss: 0.2671133813005217\n",
      "epoch 18866: train loss: 0.1012439569451828, test loss: 0.2671140597587837\n",
      "epoch 18867: train loss: 0.1012426423026303, test loss: 0.267114738231623\n",
      "epoch 18868: train loss: 0.10124132775032732, test loss: 0.26711541671903605\n",
      "epoch 18869: train loss: 0.1012400132882623, test loss: 0.26711609522101915\n",
      "epoch 18870: train loss: 0.10123869891642365, test loss: 0.26711677373756865\n",
      "epoch 18871: train loss: 0.10123738463479975, test loss: 0.2671174522686811\n",
      "epoch 18872: train loss: 0.10123607044337905, test loss: 0.26711813081435265\n",
      "epoch 18873: train loss: 0.10123475634214994, test loss: 0.2671188093745797\n",
      "epoch 18874: train loss: 0.10123344233110089, test loss: 0.2671194879493589\n",
      "epoch 18875: train loss: 0.10123212841022024, test loss: 0.2671201665386864\n",
      "epoch 18876: train loss: 0.10123081457949645, test loss: 0.2671208451425588\n",
      "epoch 18877: train loss: 0.10122950083891796, test loss: 0.2671215237609722\n",
      "epoch 18878: train loss: 0.10122818718847316, test loss: 0.2671222023939231\n",
      "epoch 18879: train loss: 0.10122687362815051, test loss: 0.2671228810414079\n",
      "epoch 18880: train loss: 0.10122556015793843, test loss: 0.2671235597034231\n",
      "epoch 18881: train loss: 0.10122424677782535, test loss: 0.26712423837996485\n",
      "epoch 18882: train loss: 0.10122293348779973, test loss: 0.26712491707102975\n",
      "epoch 18883: train loss: 0.10122162028784992, test loss: 0.2671255957766141\n",
      "epoch 18884: train loss: 0.10122030717796444, test loss: 0.2671262744967144\n",
      "epoch 18885: train loss: 0.10121899415813171, test loss: 0.2671269532313268\n",
      "epoch 18886: train loss: 0.10121768122834016, test loss: 0.2671276319804479\n",
      "epoch 18887: train loss: 0.10121636838857828, test loss: 0.267128310744074\n",
      "epoch 18888: train loss: 0.10121505563883447, test loss: 0.2671289895222015\n",
      "epoch 18889: train loss: 0.10121374297909717, test loss: 0.2671296683148269\n",
      "epoch 18890: train loss: 0.10121243040935488, test loss: 0.2671303471219464\n",
      "epoch 18891: train loss: 0.10121111792959603, test loss: 0.2671310259435566\n",
      "epoch 18892: train loss: 0.10120980553980904, test loss: 0.2671317047796538\n",
      "epoch 18893: train loss: 0.10120849323998242, test loss: 0.2671323836302344\n",
      "epoch 18894: train loss: 0.1012071810301046, test loss: 0.2671330624952947\n",
      "epoch 18895: train loss: 0.10120586891016405, test loss: 0.2671337413748313\n",
      "epoch 18896: train loss: 0.10120455688014925, test loss: 0.26713442026884054\n",
      "epoch 18897: train loss: 0.10120324494004866, test loss: 0.26713509917731876\n",
      "epoch 18898: train loss: 0.10120193308985069, test loss: 0.26713577810026234\n",
      "epoch 18899: train loss: 0.10120062132954391, test loss: 0.26713645703766775\n",
      "epoch 18900: train loss: 0.1011993096591167, test loss: 0.2671371359895314\n",
      "epoch 18901: train loss: 0.10119799807855763, test loss: 0.2671378149558496\n",
      "epoch 18902: train loss: 0.10119668658785509, test loss: 0.2671384939366189\n",
      "epoch 18903: train loss: 0.10119537518699757, test loss: 0.2671391729318356\n",
      "epoch 18904: train loss: 0.1011940638759736, test loss: 0.2671398519414961\n",
      "epoch 18905: train loss: 0.10119275265477162, test loss: 0.2671405309655968\n",
      "epoch 18906: train loss: 0.10119144152338015, test loss: 0.2671412100041342\n",
      "epoch 18907: train loss: 0.10119013048178765, test loss: 0.2671418890571047\n",
      "epoch 18908: train loss: 0.10118881952998261, test loss: 0.26714256812450465\n",
      "epoch 18909: train loss: 0.10118750866795352, test loss: 0.26714324720633037\n",
      "epoch 18910: train loss: 0.10118619789568889, test loss: 0.26714392630257844\n",
      "epoch 18911: train loss: 0.10118488721317721, test loss: 0.2671446054132453\n",
      "epoch 18912: train loss: 0.10118357662040697, test loss: 0.2671452845383271\n",
      "epoch 18913: train loss: 0.10118226611736666, test loss: 0.26714596367782056\n",
      "epoch 18914: train loss: 0.10118095570404485, test loss: 0.26714664283172185\n",
      "epoch 18915: train loss: 0.10117964538042994, test loss: 0.26714732200002755\n",
      "epoch 18916: train loss: 0.10117833514651049, test loss: 0.2671480011827341\n",
      "epoch 18917: train loss: 0.10117702500227502, test loss: 0.2671486803798377\n",
      "epoch 18918: train loss: 0.10117571494771202, test loss: 0.267149359591335\n",
      "epoch 18919: train loss: 0.10117440498281, test loss: 0.26715003881722227\n",
      "epoch 18920: train loss: 0.10117309510755751, test loss: 0.2671507180574959\n",
      "epoch 18921: train loss: 0.10117178532194301, test loss: 0.26715139731215254\n",
      "epoch 18922: train loss: 0.10117047562595506, test loss: 0.26715207658118845\n",
      "epoch 18923: train loss: 0.10116916601958216, test loss: 0.26715275586459986\n",
      "epoch 18924: train loss: 0.10116785650281283, test loss: 0.26715343516238355\n",
      "epoch 18925: train loss: 0.10116654707563562, test loss: 0.26715411447453585\n",
      "epoch 18926: train loss: 0.10116523773803905, test loss: 0.267154793801053\n",
      "epoch 18927: train loss: 0.10116392849001164, test loss: 0.2671554731419316\n",
      "epoch 18928: train loss: 0.10116261933154191, test loss: 0.26715615249716806\n",
      "epoch 18929: train loss: 0.10116131026261843, test loss: 0.2671568318667587\n",
      "epoch 18930: train loss: 0.1011600012832297, test loss: 0.2671575112507\n",
      "epoch 18931: train loss: 0.10115869239336427, test loss: 0.2671581906489884\n",
      "epoch 18932: train loss: 0.10115738359301069, test loss: 0.26715887006162037\n",
      "epoch 18933: train loss: 0.10115607488215747, test loss: 0.26715954948859216\n",
      "epoch 18934: train loss: 0.10115476626079321, test loss: 0.26716022892990055\n",
      "epoch 18935: train loss: 0.1011534577289064, test loss: 0.26716090838554163\n",
      "epoch 18936: train loss: 0.10115214928648562, test loss: 0.2671615878555119\n",
      "epoch 18937: train loss: 0.10115084093351939, test loss: 0.2671622673398079\n",
      "epoch 18938: train loss: 0.1011495326699963, test loss: 0.26716294683842606\n",
      "epoch 18939: train loss: 0.1011482244959049, test loss: 0.2671636263513627\n",
      "epoch 18940: train loss: 0.1011469164112337, test loss: 0.26716430587861445\n",
      "epoch 18941: train loss: 0.10114560841597132, test loss: 0.2671649854201776\n",
      "epoch 18942: train loss: 0.1011443005101063, test loss: 0.26716566497604843\n",
      "epoch 18943: train loss: 0.10114299269362718, test loss: 0.2671663445462236\n",
      "epoch 18944: train loss: 0.10114168496652255, test loss: 0.26716702413069965\n",
      "epoch 18945: train loss: 0.10114037732878095, test loss: 0.2671677037294727\n",
      "epoch 18946: train loss: 0.10113906978039097, test loss: 0.26716838334253945\n",
      "epoch 18947: train loss: 0.1011377623213412, test loss: 0.2671690629698962\n",
      "epoch 18948: train loss: 0.10113645495162019, test loss: 0.26716974261153936\n",
      "epoch 18949: train loss: 0.10113514767121651, test loss: 0.26717042226746557\n",
      "epoch 18950: train loss: 0.10113384048011874, test loss: 0.267171101937671\n",
      "epoch 18951: train loss: 0.10113253337831546, test loss: 0.2671717816221523\n",
      "epoch 18952: train loss: 0.10113122636579526, test loss: 0.26717246132090594\n",
      "epoch 18953: train loss: 0.10112991944254673, test loss: 0.26717314103392825\n",
      "epoch 18954: train loss: 0.10112861260855845, test loss: 0.2671738207612156\n",
      "epoch 18955: train loss: 0.10112730586381899, test loss: 0.2671745005027646\n",
      "epoch 18956: train loss: 0.10112599920831694, test loss: 0.2671751802585716\n",
      "epoch 18957: train loss: 0.10112469264204091, test loss: 0.26717586002863314\n",
      "epoch 18958: train loss: 0.10112338616497951, test loss: 0.2671765398129456\n",
      "epoch 18959: train loss: 0.10112207977712132, test loss: 0.26717721961150553\n",
      "epoch 18960: train loss: 0.10112077347845493, test loss: 0.26717789942430914\n",
      "epoch 18961: train loss: 0.10111946726896892, test loss: 0.2671785792513531\n",
      "epoch 18962: train loss: 0.10111816114865194, test loss: 0.26717925909263374\n",
      "epoch 18963: train loss: 0.10111685511749256, test loss: 0.2671799389481476\n",
      "epoch 18964: train loss: 0.10111554917547942, test loss: 0.26718061881789107\n",
      "epoch 18965: train loss: 0.10111424332260106, test loss: 0.2671812987018607\n",
      "epoch 18966: train loss: 0.10111293755884619, test loss: 0.26718197860005277\n",
      "epoch 18967: train loss: 0.10111163188420334, test loss: 0.2671826585124639\n",
      "epoch 18968: train loss: 0.10111032629866117, test loss: 0.2671833384390905\n",
      "epoch 18969: train loss: 0.10110902080220828, test loss: 0.26718401837992894\n",
      "epoch 18970: train loss: 0.1011077153948333, test loss: 0.2671846983349758\n",
      "epoch 18971: train loss: 0.10110641007652482, test loss: 0.26718537830422756\n",
      "epoch 18972: train loss: 0.10110510484727152, test loss: 0.26718605828768055\n",
      "epoch 18973: train loss: 0.10110379970706196, test loss: 0.2671867382853312\n",
      "epoch 18974: train loss: 0.10110249465588485, test loss: 0.2671874182971761\n",
      "epoch 18975: train loss: 0.1011011896937287, test loss: 0.2671880983232118\n",
      "epoch 18976: train loss: 0.10109988482058224, test loss: 0.26718877836343446\n",
      "epoch 18977: train loss: 0.1010985800364341, test loss: 0.26718945841784086\n",
      "epoch 18978: train loss: 0.10109727534127287, test loss: 0.26719013848642725\n",
      "epoch 18979: train loss: 0.10109597073508719, test loss: 0.26719081856919014\n",
      "epoch 18980: train loss: 0.10109466621786574, test loss: 0.267191498666126\n",
      "epoch 18981: train loss: 0.10109336178959716, test loss: 0.26719217877723134\n",
      "epoch 18982: train loss: 0.10109205745027004, test loss: 0.2671928589025027\n",
      "epoch 18983: train loss: 0.10109075319987308, test loss: 0.2671935390419364\n",
      "epoch 18984: train loss: 0.1010894490383949, test loss: 0.2671942191955289\n",
      "epoch 18985: train loss: 0.10108814496582415, test loss: 0.26719489936327667\n",
      "epoch 18986: train loss: 0.10108684098214951, test loss: 0.2671955795451763\n",
      "epoch 18987: train loss: 0.10108553708735961, test loss: 0.2671962597412243\n",
      "epoch 18988: train loss: 0.10108423328144311, test loss: 0.2671969399514169\n",
      "epoch 18989: train loss: 0.10108292956438868, test loss: 0.26719762017575077\n",
      "epoch 18990: train loss: 0.10108162593618494, test loss: 0.26719830041422227\n",
      "epoch 18991: train loss: 0.10108032239682062, test loss: 0.267198980666828\n",
      "epoch 18992: train loss: 0.10107901894628435, test loss: 0.26719966093356434\n",
      "epoch 18993: train loss: 0.10107771558456477, test loss: 0.26720034121442776\n",
      "epoch 18994: train loss: 0.10107641231165058, test loss: 0.2672010215094147\n",
      "epoch 18995: train loss: 0.10107510912753045, test loss: 0.2672017018185218\n",
      "epoch 18996: train loss: 0.10107380603219308, test loss: 0.2672023821417454\n",
      "epoch 18997: train loss: 0.10107250302562708, test loss: 0.26720306247908204\n",
      "epoch 18998: train loss: 0.10107120010782118, test loss: 0.2672037428305282\n",
      "epoch 18999: train loss: 0.10106989727876403, test loss: 0.26720442319608023\n",
      "epoch 19000: train loss: 0.10106859453844433, test loss: 0.26720510357573474\n",
      "epoch 19001: train loss: 0.10106729188685075, test loss: 0.26720578396948824\n",
      "epoch 19002: train loss: 0.10106598932397198, test loss: 0.2672064643773372\n",
      "epoch 19003: train loss: 0.10106468684979669, test loss: 0.26720714479927793\n",
      "epoch 19004: train loss: 0.10106338446431362, test loss: 0.26720782523530706\n",
      "epoch 19005: train loss: 0.10106208216751142, test loss: 0.2672085056854211\n",
      "epoch 19006: train loss: 0.10106077995937877, test loss: 0.26720918614961653\n",
      "epoch 19007: train loss: 0.1010594778399044, test loss: 0.26720986662788976\n",
      "epoch 19008: train loss: 0.10105817580907703, test loss: 0.2672105471202373\n",
      "epoch 19009: train loss: 0.10105687386688532, test loss: 0.2672112276266557\n",
      "epoch 19010: train loss: 0.10105557201331795, test loss: 0.2672119081471413\n",
      "epoch 19011: train loss: 0.10105427024836366, test loss: 0.2672125886816908\n",
      "epoch 19012: train loss: 0.10105296857201117, test loss: 0.26721326923030053\n",
      "epoch 19013: train loss: 0.10105166698424915, test loss: 0.267213949792967\n",
      "epoch 19014: train loss: 0.10105036548506634, test loss: 0.2672146303696868\n",
      "epoch 19015: train loss: 0.10104906407445143, test loss: 0.2672153109604563\n",
      "epoch 19016: train loss: 0.10104776275239315, test loss: 0.2672159915652721\n",
      "epoch 19017: train loss: 0.10104646151888019, test loss: 0.26721667218413064\n",
      "epoch 19018: train loss: 0.10104516037390131, test loss: 0.2672173528170283\n",
      "epoch 19019: train loss: 0.10104385931744522, test loss: 0.26721803346396183\n",
      "epoch 19020: train loss: 0.1010425583495006, test loss: 0.26721871412492754\n",
      "epoch 19021: train loss: 0.10104125747005624, test loss: 0.26721939479992196\n",
      "epoch 19022: train loss: 0.10103995667910082, test loss: 0.2672200754889415\n",
      "epoch 19023: train loss: 0.10103865597662307, test loss: 0.2672207561919829\n",
      "epoch 19024: train loss: 0.10103735536261177, test loss: 0.2672214369090424\n",
      "epoch 19025: train loss: 0.10103605483705558, test loss: 0.2672221176401167\n",
      "epoch 19026: train loss: 0.10103475439994326, test loss: 0.2672227983852022\n",
      "epoch 19027: train loss: 0.10103345405126359, test loss: 0.2672234791442954\n",
      "epoch 19028: train loss: 0.10103215379100526, test loss: 0.26722415991739285\n",
      "epoch 19029: train loss: 0.10103085361915705, test loss: 0.26722484070449104\n",
      "epoch 19030: train loss: 0.10102955353570767, test loss: 0.2672255215055863\n",
      "epoch 19031: train loss: 0.10102825354064585, test loss: 0.26722620232067545\n",
      "epoch 19032: train loss: 0.10102695363396041, test loss: 0.2672268831497548\n",
      "epoch 19033: train loss: 0.10102565381564, test loss: 0.26722756399282077\n",
      "epoch 19034: train loss: 0.10102435408567345, test loss: 0.2672282448498701\n",
      "epoch 19035: train loss: 0.10102305444404952, test loss: 0.26722892572089924\n",
      "epoch 19036: train loss: 0.10102175489075688, test loss: 0.2672296066059046\n",
      "epoch 19037: train loss: 0.10102045542578436, test loss: 0.2672302875048826\n",
      "epoch 19038: train loss: 0.10101915604912069, test loss: 0.26723096841782995\n",
      "epoch 19039: train loss: 0.10101785676075466, test loss: 0.2672316493447432\n",
      "epoch 19040: train loss: 0.10101655756067501, test loss: 0.26723233028561855\n",
      "epoch 19041: train loss: 0.10101525844887049, test loss: 0.26723301124045284\n",
      "epoch 19042: train loss: 0.10101395942532992, test loss: 0.2672336922092425\n",
      "epoch 19043: train loss: 0.10101266049004201, test loss: 0.2672343731919839\n",
      "epoch 19044: train loss: 0.10101136164299555, test loss: 0.26723505418867366\n",
      "epoch 19045: train loss: 0.10101006288417935, test loss: 0.2672357351993083\n",
      "epoch 19046: train loss: 0.10100876421358215, test loss: 0.26723641622388433\n",
      "epoch 19047: train loss: 0.10100746563119273, test loss: 0.2672370972623982\n",
      "epoch 19048: train loss: 0.10100616713699989, test loss: 0.26723777831484663\n",
      "epoch 19049: train loss: 0.10100486873099239, test loss: 0.26723845938122587\n",
      "epoch 19050: train loss: 0.10100357041315905, test loss: 0.2672391404615326\n",
      "epoch 19051: train loss: 0.10100227218348859, test loss: 0.2672398215557633\n",
      "epoch 19052: train loss: 0.10100097404196985, test loss: 0.26724050266391436\n",
      "epoch 19053: train loss: 0.10099967598859161, test loss: 0.26724118378598255\n",
      "epoch 19054: train loss: 0.10099837802334266, test loss: 0.2672418649219644\n",
      "epoch 19055: train loss: 0.10099708014621178, test loss: 0.26724254607185616\n",
      "epoch 19056: train loss: 0.10099578235718779, test loss: 0.26724322723565447\n",
      "epoch 19057: train loss: 0.10099448465625947, test loss: 0.26724390841335594\n",
      "epoch 19058: train loss: 0.10099318704341564, test loss: 0.26724458960495695\n",
      "epoch 19059: train loss: 0.10099188951864509, test loss: 0.2672452708104542\n",
      "epoch 19060: train loss: 0.10099059208193661, test loss: 0.2672459520298441\n",
      "epoch 19061: train loss: 0.10098929473327907, test loss: 0.26724663326312315\n",
      "epoch 19062: train loss: 0.10098799747266118, test loss: 0.26724731451028805\n",
      "epoch 19063: train loss: 0.10098670030007181, test loss: 0.2672479957713352\n",
      "epoch 19064: train loss: 0.10098540321549977, test loss: 0.267248677046261\n",
      "epoch 19065: train loss: 0.10098410621893386, test loss: 0.26724935833506225\n",
      "epoch 19066: train loss: 0.10098280931036291, test loss: 0.26725003963773525\n",
      "epoch 19067: train loss: 0.10098151248977573, test loss: 0.26725072095427665\n",
      "epoch 19068: train loss: 0.10098021575716114, test loss: 0.26725140228468297\n",
      "epoch 19069: train loss: 0.10097891911250796, test loss: 0.2672520836289508\n",
      "epoch 19070: train loss: 0.10097762255580502, test loss: 0.26725276498707656\n",
      "epoch 19071: train loss: 0.10097632608704117, test loss: 0.2672534463590569\n",
      "epoch 19072: train loss: 0.10097502970620521, test loss: 0.2672541277448882\n",
      "epoch 19073: train loss: 0.10097373341328598, test loss: 0.26725480914456706\n",
      "epoch 19074: train loss: 0.10097243720827231, test loss: 0.26725549055809017\n",
      "epoch 19075: train loss: 0.10097114109115302, test loss: 0.2672561719854538\n",
      "epoch 19076: train loss: 0.10096984506191697, test loss: 0.26725685342665473\n",
      "epoch 19077: train loss: 0.10096854912055302, test loss: 0.26725753488168924\n",
      "epoch 19078: train loss: 0.10096725326704992, test loss: 0.2672582163505542\n",
      "epoch 19079: train loss: 0.10096595750139661, test loss: 0.26725889783324597\n",
      "epoch 19080: train loss: 0.10096466182358191, test loss: 0.26725957932976097\n",
      "epoch 19081: train loss: 0.10096336623359464, test loss: 0.267260260840096\n",
      "epoch 19082: train loss: 0.10096207073142366, test loss: 0.2672609423642473\n",
      "epoch 19083: train loss: 0.10096077531705784, test loss: 0.2672616239022117\n",
      "epoch 19084: train loss: 0.10095947999048605, test loss: 0.26726230545398555\n",
      "epoch 19085: train loss: 0.10095818475169707, test loss: 0.2672629870195656\n",
      "epoch 19086: train loss: 0.10095688960067982, test loss: 0.2672636685989481\n",
      "epoch 19087: train loss: 0.10095559453742314, test loss: 0.26726435019212996\n",
      "epoch 19088: train loss: 0.10095429956191589, test loss: 0.2672650317991074\n",
      "epoch 19089: train loss: 0.10095300467414696, test loss: 0.26726571341987715\n",
      "epoch 19090: train loss: 0.10095170987410515, test loss: 0.26726639505443567\n",
      "epoch 19091: train loss: 0.1009504151617794, test loss: 0.26726707670277955\n",
      "epoch 19092: train loss: 0.10094912053715854, test loss: 0.2672677583649053\n",
      "epoch 19093: train loss: 0.10094782600023144, test loss: 0.2672684400408096\n",
      "epoch 19094: train loss: 0.100946531550987, test loss: 0.2672691217304889\n",
      "epoch 19095: train loss: 0.10094523718941405, test loss: 0.2672698034339397\n",
      "epoch 19096: train loss: 0.10094394291550152, test loss: 0.26727048515115864\n",
      "epoch 19097: train loss: 0.10094264872923826, test loss: 0.26727116688214225\n",
      "epoch 19098: train loss: 0.10094135463061317, test loss: 0.26727184862688713\n",
      "epoch 19099: train loss: 0.1009400606196151, test loss: 0.26727253038538973\n",
      "epoch 19100: train loss: 0.10093876669623297, test loss: 0.2672732121576466\n",
      "epoch 19101: train loss: 0.10093747286045565, test loss: 0.2672738939436545\n",
      "epoch 19102: train loss: 0.10093617911227203, test loss: 0.2672745757434098\n",
      "epoch 19103: train loss: 0.100934885451671, test loss: 0.26727525755690906\n",
      "epoch 19104: train loss: 0.10093359187864147, test loss: 0.26727593938414884\n",
      "epoch 19105: train loss: 0.10093229839317232, test loss: 0.26727662122512585\n",
      "epoch 19106: train loss: 0.10093100499525243, test loss: 0.2672773030798365\n",
      "epoch 19107: train loss: 0.10092971168487075, test loss: 0.2672779849482774\n",
      "epoch 19108: train loss: 0.10092841846201611, test loss: 0.26727866683044516\n",
      "epoch 19109: train loss: 0.10092712532667751, test loss: 0.2672793487263362\n",
      "epoch 19110: train loss: 0.10092583227884377, test loss: 0.2672800306359473\n",
      "epoch 19111: train loss: 0.10092453931850383, test loss: 0.2672807125592748\n",
      "epoch 19112: train loss: 0.10092324644564661, test loss: 0.2672813944963153\n",
      "epoch 19113: train loss: 0.10092195366026102, test loss: 0.2672820764470656\n",
      "epoch 19114: train loss: 0.10092066096233594, test loss: 0.26728275841152205\n",
      "epoch 19115: train loss: 0.10091936835186033, test loss: 0.26728344038968116\n",
      "epoch 19116: train loss: 0.10091807582882305, test loss: 0.26728412238153965\n",
      "epoch 19117: train loss: 0.10091678339321308, test loss: 0.26728480438709407\n",
      "epoch 19118: train loss: 0.10091549104501933, test loss: 0.267285486406341\n",
      "epoch 19119: train loss: 0.10091419878423072, test loss: 0.2672861684392769\n",
      "epoch 19120: train loss: 0.10091290661083616, test loss: 0.2672868504858984\n",
      "epoch 19121: train loss: 0.10091161452482457, test loss: 0.2672875325462021\n",
      "epoch 19122: train loss: 0.1009103225261849, test loss: 0.2672882146201846\n",
      "epoch 19123: train loss: 0.1009090306149061, test loss: 0.2672888967078424\n",
      "epoch 19124: train loss: 0.10090773879097703, test loss: 0.26728957880917203\n",
      "epoch 19125: train loss: 0.10090644705438674, test loss: 0.26729026092417024\n",
      "epoch 19126: train loss: 0.10090515540512408, test loss: 0.26729094305283346\n",
      "epoch 19127: train loss: 0.100903863843178, test loss: 0.2672916251951583\n",
      "epoch 19128: train loss: 0.10090257236853746, test loss: 0.26729230735114134\n",
      "epoch 19129: train loss: 0.10090128098119142, test loss: 0.2672929895207792\n",
      "epoch 19130: train loss: 0.10089998968112879, test loss: 0.2672936717040684\n",
      "epoch 19131: train loss: 0.10089869846833854, test loss: 0.2672943539010055\n",
      "epoch 19132: train loss: 0.10089740734280961, test loss: 0.2672950361115872\n",
      "epoch 19133: train loss: 0.10089611630453095, test loss: 0.26729571833580995\n",
      "epoch 19134: train loss: 0.10089482535349152, test loss: 0.2672964005736704\n",
      "epoch 19135: train loss: 0.10089353448968028, test loss: 0.26729708282516507\n",
      "epoch 19136: train loss: 0.1008922437130862, test loss: 0.2672977650902906\n",
      "epoch 19137: train loss: 0.10089095302369819, test loss: 0.26729844736904357\n",
      "epoch 19138: train loss: 0.10088966242150525, test loss: 0.2672991296614205\n",
      "epoch 19139: train loss: 0.10088837190649634, test loss: 0.2672998119674181\n",
      "epoch 19140: train loss: 0.10088708147866048, test loss: 0.26730049428703295\n",
      "epoch 19141: train loss: 0.10088579113798651, test loss: 0.2673011766202615\n",
      "epoch 19142: train loss: 0.10088450088446349, test loss: 0.26730185896710035\n",
      "epoch 19143: train loss: 0.10088321071808039, test loss: 0.26730254132754616\n",
      "epoch 19144: train loss: 0.10088192063882616, test loss: 0.26730322370159554\n",
      "epoch 19145: train loss: 0.10088063064668978, test loss: 0.2673039060892451\n",
      "epoch 19146: train loss: 0.10087934074166023, test loss: 0.26730458849049127\n",
      "epoch 19147: train loss: 0.10087805092372648, test loss: 0.26730527090533074\n",
      "epoch 19148: train loss: 0.10087676119287753, test loss: 0.26730595333376017\n",
      "epoch 19149: train loss: 0.10087547154910237, test loss: 0.26730663577577607\n",
      "epoch 19150: train loss: 0.10087418199238997, test loss: 0.2673073182313751\n",
      "epoch 19151: train loss: 0.10087289252272928, test loss: 0.2673080007005537\n",
      "epoch 19152: train loss: 0.10087160314010937, test loss: 0.2673086831833087\n",
      "epoch 19153: train loss: 0.1008703138445192, test loss: 0.2673093656796365\n",
      "epoch 19154: train loss: 0.10086902463594774, test loss: 0.26731004818953374\n",
      "epoch 19155: train loss: 0.10086773551438398, test loss: 0.2673107307129971\n",
      "epoch 19156: train loss: 0.10086644647981696, test loss: 0.26731141325002306\n",
      "epoch 19157: train loss: 0.10086515753223566, test loss: 0.2673120958006083\n",
      "epoch 19158: train loss: 0.10086386867162908, test loss: 0.2673127783647493\n",
      "epoch 19159: train loss: 0.10086257989798622, test loss: 0.2673134609424428\n",
      "epoch 19160: train loss: 0.10086129121129607, test loss: 0.2673141435336854\n",
      "epoch 19161: train loss: 0.10086000261154769, test loss: 0.2673148261384737\n",
      "epoch 19162: train loss: 0.10085871409873004, test loss: 0.2673155087568042\n",
      "epoch 19163: train loss: 0.10085742567283214, test loss: 0.26731619138867346\n",
      "epoch 19164: train loss: 0.10085613733384304, test loss: 0.2673168740340783\n",
      "epoch 19165: train loss: 0.1008548490817517, test loss: 0.26731755669301516\n",
      "epoch 19166: train loss: 0.10085356091654717, test loss: 0.2673182393654807\n",
      "epoch 19167: train loss: 0.10085227283821846, test loss: 0.26731892205147156\n",
      "epoch 19168: train loss: 0.10085098484675459, test loss: 0.26731960475098426\n",
      "epoch 19169: train loss: 0.10084969694214461, test loss: 0.2673202874640155\n",
      "epoch 19170: train loss: 0.10084840912437752, test loss: 0.26732097019056184\n",
      "epoch 19171: train loss: 0.10084712139344236, test loss: 0.26732165293061977\n",
      "epoch 19172: train loss: 0.10084583374932814, test loss: 0.26732233568418606\n",
      "epoch 19173: train loss: 0.10084454619202392, test loss: 0.2673230184512574\n",
      "epoch 19174: train loss: 0.10084325872151868, test loss: 0.2673237012318302\n",
      "epoch 19175: train loss: 0.10084197133780153, test loss: 0.2673243840259011\n",
      "epoch 19176: train loss: 0.10084068404086147, test loss: 0.2673250668334668\n",
      "epoch 19177: train loss: 0.10083939683068753, test loss: 0.2673257496545239\n",
      "epoch 19178: train loss: 0.10083810970726875, test loss: 0.26732643248906895\n",
      "epoch 19179: train loss: 0.1008368226705942, test loss: 0.26732711533709863\n",
      "epoch 19180: train loss: 0.1008355357206529, test loss: 0.26732779819860947\n",
      "epoch 19181: train loss: 0.1008342488574339, test loss: 0.2673284810735981\n",
      "epoch 19182: train loss: 0.10083296208092626, test loss: 0.26732916396206124\n",
      "epoch 19183: train loss: 0.10083167539111904, test loss: 0.26732984686399547\n",
      "epoch 19184: train loss: 0.10083038878800128, test loss: 0.26733052977939736\n",
      "epoch 19185: train loss: 0.10082910227156205, test loss: 0.26733121270826365\n",
      "epoch 19186: train loss: 0.10082781584179039, test loss: 0.26733189565059073\n",
      "epoch 19187: train loss: 0.10082652949867534, test loss: 0.2673325786063754\n",
      "epoch 19188: train loss: 0.10082524324220599, test loss: 0.26733326157561427\n",
      "epoch 19189: train loss: 0.1008239570723714, test loss: 0.26733394455830384\n",
      "epoch 19190: train loss: 0.10082267098916063, test loss: 0.2673346275544409\n",
      "epoch 19191: train loss: 0.10082138499256274, test loss: 0.26733531056402193\n",
      "epoch 19192: train loss: 0.10082009908256685, test loss: 0.2673359935870437\n",
      "epoch 19193: train loss: 0.10081881325916196, test loss: 0.26733667662350263\n",
      "epoch 19194: train loss: 0.10081752752233716, test loss: 0.2673373596733956\n",
      "epoch 19195: train loss: 0.10081624187208156, test loss: 0.267338042736719\n",
      "epoch 19196: train loss: 0.10081495630838422, test loss: 0.26733872581346957\n",
      "epoch 19197: train loss: 0.1008136708312342, test loss: 0.267339408903644\n",
      "epoch 19198: train loss: 0.10081238544062059, test loss: 0.26734009200723874\n",
      "epoch 19199: train loss: 0.10081110013653248, test loss: 0.26734077512425053\n",
      "epoch 19200: train loss: 0.10080981491895896, test loss: 0.2673414582546761\n",
      "epoch 19201: train loss: 0.10080852978788911, test loss: 0.2673421413985119\n",
      "epoch 19202: train loss: 0.10080724474331201, test loss: 0.26734282455575464\n",
      "epoch 19203: train loss: 0.10080595978521677, test loss: 0.26734350772640086\n",
      "epoch 19204: train loss: 0.10080467491359248, test loss: 0.2673441909104474\n",
      "epoch 19205: train loss: 0.1008033901284282, test loss: 0.26734487410789076\n",
      "epoch 19206: train loss: 0.10080210542971306, test loss: 0.26734555731872756\n",
      "epoch 19207: train loss: 0.10080082081743616, test loss: 0.26734624054295447\n",
      "epoch 19208: train loss: 0.10079953629158661, test loss: 0.2673469237805681\n",
      "epoch 19209: train loss: 0.10079825185215345, test loss: 0.2673476070315651\n",
      "epoch 19210: train loss: 0.10079696749912585, test loss: 0.26734829029594215\n",
      "epoch 19211: train loss: 0.10079568323249292, test loss: 0.26734897357369575\n",
      "epoch 19212: train loss: 0.10079439905224373, test loss: 0.2673496568648227\n",
      "epoch 19213: train loss: 0.10079311495836739, test loss: 0.2673503401693196\n",
      "epoch 19214: train loss: 0.10079183095085305, test loss: 0.26735102348718315\n",
      "epoch 19215: train loss: 0.1007905470296898, test loss: 0.2673517068184097\n",
      "epoch 19216: train loss: 0.10078926319486677, test loss: 0.2673523901629962\n",
      "epoch 19217: train loss: 0.10078797944637304, test loss: 0.26735307352093923\n",
      "epoch 19218: train loss: 0.10078669578419774, test loss: 0.2673537568922354\n",
      "epoch 19219: train loss: 0.10078541220833007, test loss: 0.26735444027688116\n",
      "epoch 19220: train loss: 0.10078412871875905, test loss: 0.26735512367487346\n",
      "epoch 19221: train loss: 0.10078284531547385, test loss: 0.26735580708620893\n",
      "epoch 19222: train loss: 0.10078156199846361, test loss: 0.267356490510884\n",
      "epoch 19223: train loss: 0.10078027876771743, test loss: 0.26735717394889547\n",
      "epoch 19224: train loss: 0.10077899562322451, test loss: 0.2673578574002399\n",
      "epoch 19225: train loss: 0.10077771256497389, test loss: 0.26735854086491395\n",
      "epoch 19226: train loss: 0.10077642959295474, test loss: 0.26735922434291437\n",
      "epoch 19227: train loss: 0.10077514670715625, test loss: 0.2673599078342377\n",
      "epoch 19228: train loss: 0.1007738639075675, test loss: 0.2673605913388807\n",
      "epoch 19229: train loss: 0.10077258119417763, test loss: 0.26736127485683986\n",
      "epoch 19230: train loss: 0.10077129856697581, test loss: 0.267361958388112\n",
      "epoch 19231: train loss: 0.1007700160259512, test loss: 0.26736264193269366\n",
      "epoch 19232: train loss: 0.10076873357109291, test loss: 0.2673633254905814\n",
      "epoch 19233: train loss: 0.10076745120239011, test loss: 0.26736400906177216\n",
      "epoch 19234: train loss: 0.10076616891983196, test loss: 0.2673646926462624\n",
      "epoch 19235: train loss: 0.10076488672340758, test loss: 0.26736537624404877\n",
      "epoch 19236: train loss: 0.10076360461310616, test loss: 0.26736605985512807\n",
      "epoch 19237: train loss: 0.10076232258891686, test loss: 0.26736674347949685\n",
      "epoch 19238: train loss: 0.1007610406508288, test loss: 0.2673674271171516\n",
      "epoch 19239: train loss: 0.10075975879883117, test loss: 0.26736811076808925\n",
      "epoch 19240: train loss: 0.10075847703291316, test loss: 0.2673687944323063\n",
      "epoch 19241: train loss: 0.10075719535306388, test loss: 0.2673694781097996\n",
      "epoch 19242: train loss: 0.1007559137592725, test loss: 0.26737016180056566\n",
      "epoch 19243: train loss: 0.10075463225152824, test loss: 0.26737084550460105\n",
      "epoch 19244: train loss: 0.10075335082982023, test loss: 0.2673715292219026\n",
      "epoch 19245: train loss: 0.10075206949413765, test loss: 0.2673722129524669\n",
      "epoch 19246: train loss: 0.1007507882444697, test loss: 0.26737289669629066\n",
      "epoch 19247: train loss: 0.10074950708080553, test loss: 0.26737358045337045\n",
      "epoch 19248: train loss: 0.10074822600313432, test loss: 0.2673742642237029\n",
      "epoch 19249: train loss: 0.10074694501144528, test loss: 0.267374948007285\n",
      "epoch 19250: train loss: 0.10074566410572754, test loss: 0.26737563180411306\n",
      "epoch 19251: train loss: 0.10074438328597035, test loss: 0.26737631561418385\n",
      "epoch 19252: train loss: 0.10074310255216283, test loss: 0.2673769994374941\n",
      "epoch 19253: train loss: 0.1007418219042942, test loss: 0.2673776832740404\n",
      "epoch 19254: train loss: 0.10074054134235366, test loss: 0.26737836712381957\n",
      "epoch 19255: train loss: 0.1007392608663304, test loss: 0.26737905098682796\n",
      "epoch 19256: train loss: 0.10073798047621359, test loss: 0.2673797348630626\n",
      "epoch 19257: train loss: 0.10073670017199246, test loss: 0.26738041875251994\n",
      "epoch 19258: train loss: 0.1007354199536562, test loss: 0.26738110265519677\n",
      "epoch 19259: train loss: 0.100734139821194, test loss: 0.2673817865710897\n",
      "epoch 19260: train loss: 0.10073285977459506, test loss: 0.2673824705001953\n",
      "epoch 19261: train loss: 0.10073157981384857, test loss: 0.26738315444251054\n",
      "epoch 19262: train loss: 0.10073029993894377, test loss: 0.26738383839803176\n",
      "epoch 19263: train loss: 0.10072902014986987, test loss: 0.26738452236675586\n",
      "epoch 19264: train loss: 0.10072774044661605, test loss: 0.2673852063486794\n",
      "epoch 19265: train loss: 0.10072646082917154, test loss: 0.26738589034379917\n",
      "epoch 19266: train loss: 0.10072518129752556, test loss: 0.2673865743521117\n",
      "epoch 19267: train loss: 0.1007239018516673, test loss: 0.26738725837361377\n",
      "epoch 19268: train loss: 0.10072262249158602, test loss: 0.26738794240830205\n",
      "epoch 19269: train loss: 0.10072134321727087, test loss: 0.2673886264561732\n",
      "epoch 19270: train loss: 0.10072006402871118, test loss: 0.26738931051722387\n",
      "epoch 19271: train loss: 0.10071878492589606, test loss: 0.26738999459145085\n",
      "epoch 19272: train loss: 0.10071750590881479, test loss: 0.2673906786788507\n",
      "epoch 19273: train loss: 0.1007162269774566, test loss: 0.2673913627794201\n",
      "epoch 19274: train loss: 0.1007149481318107, test loss: 0.2673920468931558\n",
      "epoch 19275: train loss: 0.10071366937186635, test loss: 0.26739273102005456\n",
      "epoch 19276: train loss: 0.10071239069761276, test loss: 0.2673934151601129\n",
      "epoch 19277: train loss: 0.10071111210903919, test loss: 0.2673940993133275\n",
      "epoch 19278: train loss: 0.10070983360613482, test loss: 0.26739478347969525\n",
      "epoch 19279: train loss: 0.10070855518888898, test loss: 0.2673954676592126\n",
      "epoch 19280: train loss: 0.10070727685729082, test loss: 0.26739615185187643\n",
      "epoch 19281: train loss: 0.10070599861132966, test loss: 0.2673968360576833\n",
      "epoch 19282: train loss: 0.10070472045099466, test loss: 0.26739752027662994\n",
      "epoch 19283: train loss: 0.10070344237627514, test loss: 0.2673982045087131\n",
      "epoch 19284: train loss: 0.10070216438716033, test loss: 0.2673988887539293\n",
      "epoch 19285: train loss: 0.10070088648363945, test loss: 0.26739957301227546\n",
      "epoch 19286: train loss: 0.10069960866570181, test loss: 0.26740025728374817\n",
      "epoch 19287: train loss: 0.1006983309333366, test loss: 0.26740094156834404\n",
      "epoch 19288: train loss: 0.10069705328653314, test loss: 0.2674016258660598\n",
      "epoch 19289: train loss: 0.10069577572528061, test loss: 0.26740231017689226\n",
      "epoch 19290: train loss: 0.10069449824956836, test loss: 0.267402994500838\n",
      "epoch 19291: train loss: 0.10069322085938559, test loss: 0.2674036788378937\n",
      "epoch 19292: train loss: 0.1006919435547216, test loss: 0.26740436318805616\n",
      "epoch 19293: train loss: 0.10069066633556563, test loss: 0.26740504755132194\n",
      "epoch 19294: train loss: 0.10068938920190697, test loss: 0.2674057319276879\n",
      "epoch 19295: train loss: 0.10068811215373484, test loss: 0.26740641631715056\n",
      "epoch 19296: train loss: 0.10068683519103859, test loss: 0.2674071007197068\n",
      "epoch 19297: train loss: 0.10068555831380743, test loss: 0.2674077851353532\n",
      "epoch 19298: train loss: 0.10068428152203067, test loss: 0.2674084695640865\n",
      "epoch 19299: train loss: 0.10068300481569757, test loss: 0.2674091540059033\n",
      "epoch 19300: train loss: 0.10068172819479745, test loss: 0.2674098384608005\n",
      "epoch 19301: train loss: 0.10068045165931952, test loss: 0.26741052292877476\n",
      "epoch 19302: train loss: 0.10067917520925311, test loss: 0.26741120740982255\n",
      "epoch 19303: train loss: 0.10067789884458751, test loss: 0.2674118919039408\n",
      "epoch 19304: train loss: 0.100676622565312, test loss: 0.2674125764111262\n",
      "epoch 19305: train loss: 0.10067534637141586, test loss: 0.26741326093137535\n",
      "epoch 19306: train loss: 0.10067407026288837, test loss: 0.2674139454646851\n",
      "epoch 19307: train loss: 0.10067279423971884, test loss: 0.26741463001105203\n",
      "epoch 19308: train loss: 0.10067151830189658, test loss: 0.26741531457047296\n",
      "epoch 19309: train loss: 0.10067024244941088, test loss: 0.26741599914294445\n",
      "epoch 19310: train loss: 0.10066896668225102, test loss: 0.26741668372846333\n",
      "epoch 19311: train loss: 0.1006676910004063, test loss: 0.26741736832702623\n",
      "epoch 19312: train loss: 0.10066641540386603, test loss: 0.26741805293862997\n",
      "epoch 19313: train loss: 0.10066513989261955, test loss: 0.26741873756327117\n",
      "epoch 19314: train loss: 0.10066386446665611, test loss: 0.26741942220094655\n",
      "epoch 19315: train loss: 0.10066258912596504, test loss: 0.26742010685165285\n",
      "epoch 19316: train loss: 0.10066131387053566, test loss: 0.2674207915153867\n",
      "epoch 19317: train loss: 0.1006600387003573, test loss: 0.26742147619214496\n",
      "epoch 19318: train loss: 0.10065876361541921, test loss: 0.2674221608819243\n",
      "epoch 19319: train loss: 0.10065748861571076, test loss: 0.2674228455847213\n",
      "epoch 19320: train loss: 0.10065621370122127, test loss: 0.2674235303005328\n",
      "epoch 19321: train loss: 0.10065493887194005, test loss: 0.2674242150293555\n",
      "epoch 19322: train loss: 0.10065366412785641, test loss: 0.2674248997711861\n",
      "epoch 19323: train loss: 0.10065238946895967, test loss: 0.2674255845260214\n",
      "epoch 19324: train loss: 0.10065111489523919, test loss: 0.26742626929385793\n",
      "epoch 19325: train loss: 0.10064984040668425, test loss: 0.26742695407469264\n",
      "epoch 19326: train loss: 0.10064856600328423, test loss: 0.26742763886852206\n",
      "epoch 19327: train loss: 0.1006472916850284, test loss: 0.2674283236753431\n",
      "epoch 19328: train loss: 0.10064601745190614, test loss: 0.2674290084951523\n",
      "epoch 19329: train loss: 0.10064474330390677, test loss: 0.2674296933279464\n",
      "epoch 19330: train loss: 0.10064346924101965, test loss: 0.2674303781737223\n",
      "epoch 19331: train loss: 0.10064219526323409, test loss: 0.2674310630324765\n",
      "epoch 19332: train loss: 0.10064092137053947, test loss: 0.2674317479042059\n",
      "epoch 19333: train loss: 0.10063964756292508, test loss: 0.26743243278890705\n",
      "epoch 19334: train loss: 0.10063837384038027, test loss: 0.2674331176865768\n",
      "epoch 19335: train loss: 0.10063710020289446, test loss: 0.26743380259721183\n",
      "epoch 19336: train loss: 0.1006358266504569, test loss: 0.26743448752080895\n",
      "epoch 19337: train loss: 0.10063455318305702, test loss: 0.26743517245736487\n",
      "epoch 19338: train loss: 0.10063327980068411, test loss: 0.2674358574068762\n",
      "epoch 19339: train loss: 0.10063200650332757, test loss: 0.2674365423693397\n",
      "epoch 19340: train loss: 0.10063073329097673, test loss: 0.2674372273447522\n",
      "epoch 19341: train loss: 0.10062946016362094, test loss: 0.26743791233311043\n",
      "epoch 19342: train loss: 0.10062818712124962, test loss: 0.26743859733441105\n",
      "epoch 19343: train loss: 0.10062691416385207, test loss: 0.2674392823486507\n",
      "epoch 19344: train loss: 0.10062564129141767, test loss: 0.2674399673758263\n",
      "epoch 19345: train loss: 0.10062436850393577, test loss: 0.2674406524159345\n",
      "epoch 19346: train loss: 0.10062309580139579, test loss: 0.26744133746897214\n",
      "epoch 19347: train loss: 0.10062182318378707, test loss: 0.2674420225349357\n",
      "epoch 19348: train loss: 0.10062055065109897, test loss: 0.2674427076138221\n",
      "epoch 19349: train loss: 0.10061927820332088, test loss: 0.2674433927056281\n",
      "epoch 19350: train loss: 0.10061800584044216, test loss: 0.26744407781035034\n",
      "epoch 19351: train loss: 0.1006167335624522, test loss: 0.26744476292798564\n",
      "epoch 19352: train loss: 0.10061546136934035, test loss: 0.2674454480585306\n",
      "epoch 19353: train loss: 0.10061418926109605, test loss: 0.26744613320198224\n",
      "epoch 19354: train loss: 0.10061291723770864, test loss: 0.267446818358337\n",
      "epoch 19355: train loss: 0.10061164529916748, test loss: 0.2674475035275918\n",
      "epoch 19356: train loss: 0.10061037344546203, test loss: 0.2674481887097432\n",
      "epoch 19357: train loss: 0.10060910167658164, test loss: 0.26744887390478816\n",
      "epoch 19358: train loss: 0.10060782999251568, test loss: 0.26744955911272333\n",
      "epoch 19359: train loss: 0.10060655839325358, test loss: 0.26745024433354536\n",
      "epoch 19360: train loss: 0.10060528687878473, test loss: 0.26745092956725114\n",
      "epoch 19361: train loss: 0.10060401544909847, test loss: 0.26745161481383745\n",
      "epoch 19362: train loss: 0.10060274410418428, test loss: 0.2674523000733008\n",
      "epoch 19363: train loss: 0.10060147284403148, test loss: 0.2674529853456381\n",
      "epoch 19364: train loss: 0.10060020166862957, test loss: 0.26745367063084613\n",
      "epoch 19365: train loss: 0.10059893057796784, test loss: 0.26745435592892153\n",
      "epoch 19366: train loss: 0.10059765957203579, test loss: 0.26745504123986114\n",
      "epoch 19367: train loss: 0.10059638865082281, test loss: 0.26745572656366173\n",
      "epoch 19368: train loss: 0.10059511781431826, test loss: 0.2674564119003199\n",
      "epoch 19369: train loss: 0.10059384706251158, test loss: 0.2674570972498325\n",
      "epoch 19370: train loss: 0.10059257639539218, test loss: 0.2674577826121963\n",
      "epoch 19371: train loss: 0.10059130581294952, test loss: 0.2674584679874081\n",
      "epoch 19372: train loss: 0.10059003531517295, test loss: 0.26745915337546444\n",
      "epoch 19373: train loss: 0.1005887649020519, test loss: 0.26745983877636226\n",
      "epoch 19374: train loss: 0.10058749457357585, test loss: 0.26746052419009825\n",
      "epoch 19375: train loss: 0.10058622432973417, test loss: 0.2674612096166692\n",
      "epoch 19376: train loss: 0.10058495417051627, test loss: 0.2674618950560718\n",
      "epoch 19377: train loss: 0.10058368409591162, test loss: 0.2674625805083029\n",
      "epoch 19378: train loss: 0.10058241410590962, test loss: 0.2674632659733593\n",
      "epoch 19379: train loss: 0.10058114420049975, test loss: 0.26746395145123747\n",
      "epoch 19380: train loss: 0.10057987437967136, test loss: 0.26746463694193445\n",
      "epoch 19381: train loss: 0.10057860464341395, test loss: 0.26746532244544696\n",
      "epoch 19382: train loss: 0.10057733499171694, test loss: 0.26746600796177167\n",
      "epoch 19383: train loss: 0.10057606542456977, test loss: 0.2674666934909054\n",
      "epoch 19384: train loss: 0.10057479594196185, test loss: 0.2674673790328448\n",
      "epoch 19385: train loss: 0.10057352654388264, test loss: 0.2674680645875868\n",
      "epoch 19386: train loss: 0.10057225723032161, test loss: 0.2674687501551281\n",
      "epoch 19387: train loss: 0.10057098800126817, test loss: 0.26746943573546544\n",
      "epoch 19388: train loss: 0.10056971885671179, test loss: 0.26747012132859554\n",
      "epoch 19389: train loss: 0.1005684497966419, test loss: 0.26747080693451525\n",
      "epoch 19390: train loss: 0.10056718082104794, test loss: 0.26747149255322133\n",
      "epoch 19391: train loss: 0.10056591192991941, test loss: 0.2674721781847105\n",
      "epoch 19392: train loss: 0.10056464312324576, test loss: 0.26747286382897945\n",
      "epoch 19393: train loss: 0.1005633744010164, test loss: 0.2674735494860252\n",
      "epoch 19394: train loss: 0.10056210576322082, test loss: 0.26747423515584423\n",
      "epoch 19395: train loss: 0.10056083720984849, test loss: 0.2674749208384335\n",
      "epoch 19396: train loss: 0.10055956874088884, test loss: 0.2674756065337897\n",
      "epoch 19397: train loss: 0.10055830035633136, test loss: 0.2674762922419096\n",
      "epoch 19398: train loss: 0.10055703205616551, test loss: 0.2674769779627901\n",
      "epoch 19399: train loss: 0.10055576384038074, test loss: 0.26747766369642767\n",
      "epoch 19400: train loss: 0.10055449570896656, test loss: 0.2674783494428194\n",
      "epoch 19401: train loss: 0.10055322766191242, test loss: 0.26747903520196187\n",
      "epoch 19402: train loss: 0.10055195969920777, test loss: 0.26747972097385203\n",
      "epoch 19403: train loss: 0.10055069182084214, test loss: 0.26748040675848633\n",
      "epoch 19404: train loss: 0.10054942402680496, test loss: 0.26748109255586194\n",
      "epoch 19405: train loss: 0.10054815631708572, test loss: 0.26748177836597536\n",
      "epoch 19406: train loss: 0.10054688869167391, test loss: 0.26748246418882343\n",
      "epoch 19407: train loss: 0.10054562115055901, test loss: 0.2674831500244031\n",
      "epoch 19408: train loss: 0.1005443536937305, test loss: 0.26748383587271085\n",
      "epoch 19409: train loss: 0.10054308632117788, test loss: 0.26748452173374365\n",
      "epoch 19410: train loss: 0.10054181903289065, test loss: 0.2674852076074983\n",
      "epoch 19411: train loss: 0.10054055182885825, test loss: 0.26748589349397156\n",
      "epoch 19412: train loss: 0.10053928470907023, test loss: 0.26748657939315995\n",
      "epoch 19413: train loss: 0.10053801767351604, test loss: 0.2674872653050607\n",
      "epoch 19414: train loss: 0.10053675072218521, test loss: 0.26748795122967023\n",
      "epoch 19415: train loss: 0.10053548385506722, test loss: 0.2674886371669855\n",
      "epoch 19416: train loss: 0.10053421707215157, test loss: 0.26748932311700324\n",
      "epoch 19417: train loss: 0.10053295037342778, test loss: 0.2674900090797202\n",
      "epoch 19418: train loss: 0.10053168375888533, test loss: 0.26749069505513323\n",
      "epoch 19419: train loss: 0.10053041722851372, test loss: 0.26749138104323916\n",
      "epoch 19420: train loss: 0.1005291507823025, test loss: 0.2674920670440346\n",
      "epoch 19421: train loss: 0.10052788442024112, test loss: 0.2674927530575165\n",
      "epoch 19422: train loss: 0.10052661814231914, test loss: 0.26749343908368156\n",
      "epoch 19423: train loss: 0.10052535194852606, test loss: 0.2674941251225266\n",
      "epoch 19424: train loss: 0.10052408583885139, test loss: 0.2674948111740485\n",
      "epoch 19425: train loss: 0.10052281981328466, test loss: 0.2674954972382439\n",
      "epoch 19426: train loss: 0.10052155387181536, test loss: 0.2674961833151095\n",
      "epoch 19427: train loss: 0.10052028801443305, test loss: 0.2674968694046425\n",
      "epoch 19428: train loss: 0.1005190222411272, test loss: 0.26749755550683924\n",
      "epoch 19429: train loss: 0.1005177565518874, test loss: 0.26749824162169683\n",
      "epoch 19430: train loss: 0.1005164909467031, test loss: 0.2674989277492118\n",
      "epoch 19431: train loss: 0.10051522542556392, test loss: 0.2674996138893811\n",
      "epoch 19432: train loss: 0.10051395998845931, test loss: 0.2675003000422016\n",
      "epoch 19433: train loss: 0.10051269463537885, test loss: 0.26750098620766993\n",
      "epoch 19434: train loss: 0.10051142936631204, test loss: 0.267501672385783\n",
      "epoch 19435: train loss: 0.10051016418124843, test loss: 0.2675023585765374\n",
      "epoch 19436: train loss: 0.10050889908017757, test loss: 0.2675030447799302\n",
      "epoch 19437: train loss: 0.10050763406308898, test loss: 0.26750373099595814\n",
      "epoch 19438: train loss: 0.1005063691299722, test loss: 0.2675044172246179\n",
      "epoch 19439: train loss: 0.1005051042808168, test loss: 0.2675051034659063\n",
      "epoch 19440: train loss: 0.10050383951561231, test loss: 0.26750578971982025\n",
      "epoch 19441: train loss: 0.10050257483434827, test loss: 0.26750647598635635\n",
      "epoch 19442: train loss: 0.10050131023701421, test loss: 0.26750716226551163\n",
      "epoch 19443: train loss: 0.10050004572359975, test loss: 0.2675078485572828\n",
      "epoch 19444: train loss: 0.10049878129409436, test loss: 0.2675085348616667\n",
      "epoch 19445: train loss: 0.10049751694848766, test loss: 0.26750922117866\n",
      "epoch 19446: train loss: 0.10049625268676916, test loss: 0.2675099075082596\n",
      "epoch 19447: train loss: 0.10049498850892842, test loss: 0.26751059385046233\n",
      "epoch 19448: train loss: 0.10049372441495504, test loss: 0.26751128020526493\n",
      "epoch 19449: train loss: 0.10049246040483854, test loss: 0.26751196657266424\n",
      "epoch 19450: train loss: 0.1004911964785685, test loss: 0.2675126529526571\n",
      "epoch 19451: train loss: 0.10048993263613451, test loss: 0.2675133393452402\n",
      "epoch 19452: train loss: 0.1004886688775261, test loss: 0.2675140257504105\n",
      "epoch 19453: train loss: 0.1004874052027328, test loss: 0.26751471216816475\n",
      "epoch 19454: train loss: 0.10048614161174431, test loss: 0.2675153985984998\n",
      "epoch 19455: train loss: 0.10048487810455008, test loss: 0.2675160850414123\n",
      "epoch 19456: train loss: 0.10048361468113974, test loss: 0.2675167714968992\n",
      "epoch 19457: train loss: 0.10048235134150288, test loss: 0.26751745796495724\n",
      "epoch 19458: train loss: 0.10048108808562903, test loss: 0.2675181444455833\n",
      "epoch 19459: train loss: 0.10047982491350779, test loss: 0.26751883093877415\n",
      "epoch 19460: train loss: 0.10047856182512879, test loss: 0.2675195174445267\n",
      "epoch 19461: train loss: 0.10047729882048156, test loss: 0.2675202039628376\n",
      "epoch 19462: train loss: 0.10047603589955567, test loss: 0.26752089049370376\n",
      "epoch 19463: train loss: 0.10047477306234075, test loss: 0.2675215770371219\n",
      "epoch 19464: train loss: 0.10047351030882636, test loss: 0.26752226359308906\n",
      "epoch 19465: train loss: 0.10047224763900214, test loss: 0.2675229501616018\n",
      "epoch 19466: train loss: 0.10047098505285762, test loss: 0.26752363674265717\n",
      "epoch 19467: train loss: 0.10046972255038243, test loss: 0.26752432333625176\n",
      "epoch 19468: train loss: 0.10046846013156617, test loss: 0.2675250099423826\n",
      "epoch 19469: train loss: 0.10046719779639844, test loss: 0.26752569656104636\n",
      "epoch 19470: train loss: 0.10046593554486881, test loss: 0.26752638319223987\n",
      "epoch 19471: train loss: 0.10046467337696693, test loss: 0.2675270698359601\n",
      "epoch 19472: train loss: 0.10046341129268235, test loss: 0.26752775649220367\n",
      "epoch 19473: train loss: 0.10046214929200474, test loss: 0.26752844316096747\n",
      "epoch 19474: train loss: 0.10046088737492365, test loss: 0.2675291298422484\n",
      "epoch 19475: train loss: 0.10045962554142873, test loss: 0.26752981653604324\n",
      "epoch 19476: train loss: 0.10045836379150956, test loss: 0.2675305032423488\n",
      "epoch 19477: train loss: 0.10045710212515578, test loss: 0.2675311899611619\n",
      "epoch 19478: train loss: 0.100455840542357, test loss: 0.2675318766924793\n",
      "epoch 19479: train loss: 0.1004545790431028, test loss: 0.267532563436298\n",
      "epoch 19480: train loss: 0.10045331762738285, test loss: 0.26753325019261465\n",
      "epoch 19481: train loss: 0.10045205629518676, test loss: 0.26753393696142624\n",
      "epoch 19482: train loss: 0.10045079504650413, test loss: 0.26753462374272946\n",
      "epoch 19483: train loss: 0.10044953388132463, test loss: 0.26753531053652124\n",
      "epoch 19484: train loss: 0.10044827279963782, test loss: 0.2675359973427983\n",
      "epoch 19485: train loss: 0.10044701180143338, test loss: 0.2675366841615576\n",
      "epoch 19486: train loss: 0.10044575088670092, test loss: 0.26753737099279584\n",
      "epoch 19487: train loss: 0.10044449005543007, test loss: 0.26753805783651\n",
      "epoch 19488: train loss: 0.10044322930761047, test loss: 0.26753874469269673\n",
      "epoch 19489: train loss: 0.1004419686432318, test loss: 0.26753943156135307\n",
      "epoch 19490: train loss: 0.10044070806228361, test loss: 0.2675401184424757\n",
      "epoch 19491: train loss: 0.1004394475647556, test loss: 0.26754080533606145\n",
      "epoch 19492: train loss: 0.1004381871506374, test loss: 0.2675414922421073\n",
      "epoch 19493: train loss: 0.10043692681991864, test loss: 0.26754217916060985\n",
      "epoch 19494: train loss: 0.10043566657258897, test loss: 0.2675428660915663\n",
      "epoch 19495: train loss: 0.10043440640863803, test loss: 0.2675435530349731\n",
      "epoch 19496: train loss: 0.1004331463280555, test loss: 0.2675442399908273\n",
      "epoch 19497: train loss: 0.10043188633083101, test loss: 0.2675449269591257\n",
      "epoch 19498: train loss: 0.10043062641695422, test loss: 0.26754561393986515\n",
      "epoch 19499: train loss: 0.10042936658641474, test loss: 0.26754630093304244\n",
      "epoch 19500: train loss: 0.1004281068392023, test loss: 0.26754698793865445\n",
      "epoch 19501: train loss: 0.10042684717530649, test loss: 0.2675476749566981\n",
      "epoch 19502: train loss: 0.10042558759471704, test loss: 0.26754836198717\n",
      "epoch 19503: train loss: 0.10042432809742355, test loss: 0.2675490490300672\n",
      "epoch 19504: train loss: 0.10042306868341569, test loss: 0.26754973608538657\n",
      "epoch 19505: train loss: 0.10042180935268316, test loss: 0.2675504231531247\n",
      "epoch 19506: train loss: 0.1004205501052156, test loss: 0.2675511102332787\n",
      "epoch 19507: train loss: 0.10041929094100269, test loss: 0.2675517973258454\n",
      "epoch 19508: train loss: 0.10041803186003408, test loss: 0.26755248443082147\n",
      "epoch 19509: train loss: 0.10041677286229948, test loss: 0.26755317154820385\n",
      "epoch 19510: train loss: 0.10041551394778855, test loss: 0.2675538586779894\n",
      "epoch 19511: train loss: 0.10041425511649094, test loss: 0.2675545458201749\n",
      "epoch 19512: train loss: 0.10041299636839636, test loss: 0.26755523297475725\n",
      "epoch 19513: train loss: 0.10041173770349449, test loss: 0.26755592014173346\n",
      "epoch 19514: train loss: 0.10041047912177498, test loss: 0.26755660732110004\n",
      "epoch 19515: train loss: 0.10040922062322753, test loss: 0.26755729451285415\n",
      "epoch 19516: train loss: 0.10040796220784184, test loss: 0.26755798171699247\n",
      "epoch 19517: train loss: 0.10040670387560759, test loss: 0.26755866893351193\n",
      "epoch 19518: train loss: 0.10040544562651446, test loss: 0.26755935616240933\n",
      "epoch 19519: train loss: 0.10040418746055217, test loss: 0.2675600434036815\n",
      "epoch 19520: train loss: 0.10040292937771035, test loss: 0.26756073065732544\n",
      "epoch 19521: train loss: 0.10040167137797874, test loss: 0.26756141792333776\n",
      "epoch 19522: train loss: 0.10040041346134707, test loss: 0.26756210520171564\n",
      "epoch 19523: train loss: 0.10039915562780496, test loss: 0.2675627924924557\n",
      "epoch 19524: train loss: 0.10039789787734218, test loss: 0.26756347979555484\n",
      "epoch 19525: train loss: 0.10039664020994837, test loss: 0.2675641671110099\n",
      "epoch 19526: train loss: 0.10039538262561326, test loss: 0.2675648544388178\n",
      "epoch 19527: train loss: 0.1003941251243266, test loss: 0.2675655417789753\n",
      "epoch 19528: train loss: 0.10039286770607803, test loss: 0.2675662291314794\n",
      "epoch 19529: train loss: 0.1003916103708573, test loss: 0.2675669164963268\n",
      "epoch 19530: train loss: 0.10039035311865409, test loss: 0.2675676038735145\n",
      "epoch 19531: train loss: 0.10038909594945813, test loss: 0.26756829126303944\n",
      "epoch 19532: train loss: 0.10038783886325914, test loss: 0.2675689786648982\n",
      "epoch 19533: train loss: 0.10038658186004686, test loss: 0.2675696660790878\n",
      "epoch 19534: train loss: 0.10038532493981095, test loss: 0.2675703535056052\n",
      "epoch 19535: train loss: 0.10038406810254116, test loss: 0.2675710409444471\n",
      "epoch 19536: train loss: 0.10038281134822721, test loss: 0.26757172839561044\n",
      "epoch 19537: train loss: 0.10038155467685884, test loss: 0.2675724158590921\n",
      "epoch 19538: train loss: 0.10038029808842577, test loss: 0.2675731033348889\n",
      "epoch 19539: train loss: 0.1003790415829177, test loss: 0.2675737908229976\n",
      "epoch 19540: train loss: 0.10037778516032439, test loss: 0.2675744783234154\n",
      "epoch 19541: train loss: 0.10037652882063557, test loss: 0.26757516583613883\n",
      "epoch 19542: train loss: 0.10037527256384093, test loss: 0.2675758533611649\n",
      "epoch 19543: train loss: 0.10037401638993025, test loss: 0.2675765408984906\n",
      "epoch 19544: train loss: 0.10037276029889326, test loss: 0.26757722844811255\n",
      "epoch 19545: train loss: 0.10037150429071967, test loss: 0.26757791601002784\n",
      "epoch 19546: train loss: 0.10037024836539929, test loss: 0.2675786035842332\n",
      "epoch 19547: train loss: 0.10036899252292178, test loss: 0.2675792911707255\n",
      "epoch 19548: train loss: 0.10036773676327694, test loss: 0.2675799787695017\n",
      "epoch 19549: train loss: 0.10036648108645448, test loss: 0.26758066638055866\n",
      "epoch 19550: train loss: 0.10036522549244417, test loss: 0.2675813540038932\n",
      "epoch 19551: train loss: 0.10036396998123574, test loss: 0.2675820416395023\n",
      "epoch 19552: train loss: 0.10036271455281895, test loss: 0.2675827292873827\n",
      "epoch 19553: train loss: 0.10036145920718355, test loss: 0.2675834169475313\n",
      "epoch 19554: train loss: 0.10036020394431933, test loss: 0.26758410461994503\n",
      "epoch 19555: train loss: 0.10035894876421599, test loss: 0.2675847923046208\n",
      "epoch 19556: train loss: 0.10035769366686333, test loss: 0.2675854800015554\n",
      "epoch 19557: train loss: 0.10035643865225108, test loss: 0.26758616771074584\n",
      "epoch 19558: train loss: 0.100355183720369, test loss: 0.2675868554321888\n",
      "epoch 19559: train loss: 0.10035392887120692, test loss: 0.2675875431658813\n",
      "epoch 19560: train loss: 0.10035267410475451, test loss: 0.26758823091182027\n",
      "epoch 19561: train loss: 0.10035141942100162, test loss: 0.26758891867000245\n",
      "epoch 19562: train loss: 0.10035016481993797, test loss: 0.26758960644042473\n",
      "epoch 19563: train loss: 0.10034891030155334, test loss: 0.2675902942230841\n",
      "epoch 19564: train loss: 0.1003476558658375, test loss: 0.2675909820179775\n",
      "epoch 19565: train loss: 0.10034640151278026, test loss: 0.2675916698251016\n",
      "epoch 19566: train loss: 0.10034514724237134, test loss: 0.2675923576444534\n",
      "epoch 19567: train loss: 0.10034389305460056, test loss: 0.26759304547602975\n",
      "epoch 19568: train loss: 0.10034263894945768, test loss: 0.26759373331982755\n",
      "epoch 19569: train loss: 0.10034138492693251, test loss: 0.2675944211758437\n",
      "epoch 19570: train loss: 0.10034013098701479, test loss: 0.2675951090440752\n",
      "epoch 19571: train loss: 0.10033887712969433, test loss: 0.26759579692451874\n",
      "epoch 19572: train loss: 0.10033762335496092, test loss: 0.26759648481717135\n",
      "epoch 19573: train loss: 0.10033636966280435, test loss: 0.2675971727220298\n",
      "epoch 19574: train loss: 0.1003351160532144, test loss: 0.26759786063909113\n",
      "epoch 19575: train loss: 0.10033386252618087, test loss: 0.26759854856835213\n",
      "epoch 19576: train loss: 0.10033260908169353, test loss: 0.2675992365098096\n",
      "epoch 19577: train loss: 0.10033135571974222, test loss: 0.2675999244634607\n",
      "epoch 19578: train loss: 0.10033010244031672, test loss: 0.26760061242930205\n",
      "epoch 19579: train loss: 0.10032884924340682, test loss: 0.2676013004073307\n",
      "epoch 19580: train loss: 0.10032759612900236, test loss: 0.26760198839754357\n",
      "epoch 19581: train loss: 0.10032634309709308, test loss: 0.26760267639993734\n",
      "epoch 19582: train loss: 0.10032509014766884, test loss: 0.26760336441450916\n",
      "epoch 19583: train loss: 0.1003238372807194, test loss: 0.26760405244125585\n",
      "epoch 19584: train loss: 0.1003225844962346, test loss: 0.2676047404801743\n",
      "epoch 19585: train loss: 0.10032133179420427, test loss: 0.2676054285312613\n",
      "epoch 19586: train loss: 0.10032007917461819, test loss: 0.2676061165945138\n",
      "epoch 19587: train loss: 0.10031882663746618, test loss: 0.2676068046699288\n",
      "epoch 19588: train loss: 0.10031757418273805, test loss: 0.26760749275750323\n",
      "epoch 19589: train loss: 0.10031632181042366, test loss: 0.2676081808572337\n",
      "epoch 19590: train loss: 0.10031506952051278, test loss: 0.2676088689691174\n",
      "epoch 19591: train loss: 0.10031381731299521, test loss: 0.26760955709315115\n",
      "epoch 19592: train loss: 0.10031256518786083, test loss: 0.26761024522933186\n",
      "epoch 19593: train loss: 0.10031131314509947, test loss: 0.26761093337765646\n",
      "epoch 19594: train loss: 0.10031006118470093, test loss: 0.2676116215381217\n",
      "epoch 19595: train loss: 0.10030880930665503, test loss: 0.2676123097107246\n",
      "epoch 19596: train loss: 0.10030755751095163, test loss: 0.26761299789546206\n",
      "epoch 19597: train loss: 0.10030630579758056, test loss: 0.26761368609233105\n",
      "epoch 19598: train loss: 0.10030505416653163, test loss: 0.2676143743013284\n",
      "epoch 19599: train loss: 0.10030380261779466, test loss: 0.267615062522451\n",
      "epoch 19600: train loss: 0.10030255115135953, test loss: 0.26761575075569577\n",
      "epoch 19601: train loss: 0.10030129976721608, test loss: 0.26761643900105964\n",
      "epoch 19602: train loss: 0.10030004846535412, test loss: 0.2676171272585395\n",
      "epoch 19603: train loss: 0.10029879724576352, test loss: 0.2676178155281323\n",
      "epoch 19604: train loss: 0.10029754610843412, test loss: 0.267618503809835\n",
      "epoch 19605: train loss: 0.10029629505335574, test loss: 0.26761919210364427\n",
      "epoch 19606: train loss: 0.10029504408051824, test loss: 0.2676198804095573\n",
      "epoch 19607: train loss: 0.10029379318991152, test loss: 0.2676205687275708\n",
      "epoch 19608: train loss: 0.10029254238152538, test loss: 0.26762125705768175\n",
      "epoch 19609: train loss: 0.10029129165534968, test loss: 0.26762194539988726\n",
      "epoch 19610: train loss: 0.10029004101137427, test loss: 0.2676226337541839\n",
      "epoch 19611: train loss: 0.10028879044958906, test loss: 0.2676233221205689\n",
      "epoch 19612: train loss: 0.10028753996998382, test loss: 0.26762401049903883\n",
      "epoch 19613: train loss: 0.1002862895725485, test loss: 0.26762469888959095\n",
      "epoch 19614: train loss: 0.1002850392572729, test loss: 0.26762538729222207\n",
      "epoch 19615: train loss: 0.10028378902414692, test loss: 0.267626075706929\n",
      "epoch 19616: train loss: 0.10028253887316041, test loss: 0.2676267641337087\n",
      "epoch 19617: train loss: 0.10028128880430325, test loss: 0.2676274525725582\n",
      "epoch 19618: train loss: 0.1002800388175653, test loss: 0.26762814102347426\n",
      "epoch 19619: train loss: 0.10027878891293643, test loss: 0.2676288294864539\n",
      "epoch 19620: train loss: 0.10027753909040651, test loss: 0.2676295179614941\n",
      "epoch 19621: train loss: 0.10027628934996546, test loss: 0.2676302064485916\n",
      "epoch 19622: train loss: 0.10027503969160309, test loss: 0.2676308949477434\n",
      "epoch 19623: train loss: 0.10027379011530932, test loss: 0.26763158345894655\n",
      "epoch 19624: train loss: 0.10027254062107403, test loss: 0.26763227198219774\n",
      "epoch 19625: train loss: 0.10027129120888709, test loss: 0.2676329605174942\n",
      "epoch 19626: train loss: 0.1002700418787384, test loss: 0.2676336490648326\n",
      "epoch 19627: train loss: 0.10026879263061782, test loss: 0.26763433762420985\n",
      "epoch 19628: train loss: 0.10026754346451526, test loss: 0.267635026195623\n",
      "epoch 19629: train loss: 0.10026629438042058, test loss: 0.2676357147790691\n",
      "epoch 19630: train loss: 0.10026504537832372, test loss: 0.2676364033745448\n",
      "epoch 19631: train loss: 0.10026379645821455, test loss: 0.2676370919820472\n",
      "epoch 19632: train loss: 0.10026254762008295, test loss: 0.2676377806015731\n",
      "epoch 19633: train loss: 0.10026129886391885, test loss: 0.26763846923311957\n",
      "epoch 19634: train loss: 0.10026005018971212, test loss: 0.2676391578766835\n",
      "epoch 19635: train loss: 0.10025880159745267, test loss: 0.26763984653226186\n",
      "epoch 19636: train loss: 0.10025755308713037, test loss: 0.2676405351998514\n",
      "epoch 19637: train loss: 0.10025630465873522, test loss: 0.2676412238794492\n",
      "epoch 19638: train loss: 0.10025505631225702, test loss: 0.2676419125710523\n",
      "epoch 19639: train loss: 0.10025380804768574, test loss: 0.26764260127465733\n",
      "epoch 19640: train loss: 0.10025255986501126, test loss: 0.26764328999026155\n",
      "epoch 19641: train loss: 0.10025131176422347, test loss: 0.2676439787178617\n",
      "epoch 19642: train loss: 0.10025006374531234, test loss: 0.26764466745745463\n",
      "epoch 19643: train loss: 0.10024881580826778, test loss: 0.2676453562090375\n",
      "epoch 19644: train loss: 0.10024756795307964, test loss: 0.2676460449726072\n",
      "epoch 19645: train loss: 0.10024632017973792, test loss: 0.26764673374816056\n",
      "epoch 19646: train loss: 0.10024507248823249, test loss: 0.26764742253569457\n",
      "epoch 19647: train loss: 0.10024382487855327, test loss: 0.26764811133520616\n",
      "epoch 19648: train loss: 0.10024257735069021, test loss: 0.26764880014669234\n",
      "epoch 19649: train loss: 0.10024132990463322, test loss: 0.26764948897015\n",
      "epoch 19650: train loss: 0.10024008254037226, test loss: 0.26765017780557604\n",
      "epoch 19651: train loss: 0.1002388352578972, test loss: 0.26765086665296745\n",
      "epoch 19652: train loss: 0.10023758805719801, test loss: 0.26765155551232117\n",
      "epoch 19653: train loss: 0.10023634093826461, test loss: 0.26765224438363405\n",
      "epoch 19654: train loss: 0.10023509390108694, test loss: 0.2676529332669032\n",
      "epoch 19655: train loss: 0.10023384694565493, test loss: 0.2676536221621255\n",
      "epoch 19656: train loss: 0.10023260007195853, test loss: 0.2676543110692978\n",
      "epoch 19657: train loss: 0.10023135327998768, test loss: 0.2676549999884172\n",
      "epoch 19658: train loss: 0.10023010656973229, test loss: 0.26765568891948055\n",
      "epoch 19659: train loss: 0.10022885994118233, test loss: 0.26765637786248475\n",
      "epoch 19660: train loss: 0.10022761339432776, test loss: 0.2676570668174268\n",
      "epoch 19661: train loss: 0.1002263669291585, test loss: 0.2676577557843038\n",
      "epoch 19662: train loss: 0.1002251205456645, test loss: 0.26765844476311246\n",
      "epoch 19663: train loss: 0.1002238742438357, test loss: 0.26765913375384975\n",
      "epoch 19664: train loss: 0.10022262802366211, test loss: 0.26765982275651284\n",
      "epoch 19665: train loss: 0.10022138188513362, test loss: 0.2676605117710984\n",
      "epoch 19666: train loss: 0.1002201358282402, test loss: 0.26766120079760364\n",
      "epoch 19667: train loss: 0.10021888985297181, test loss: 0.2676618898360253\n",
      "epoch 19668: train loss: 0.10021764395931845, test loss: 0.26766257888636047\n",
      "epoch 19669: train loss: 0.10021639814727001, test loss: 0.26766326794860595\n",
      "epoch 19670: train loss: 0.1002151524168165, test loss: 0.2676639570227589\n",
      "epoch 19671: train loss: 0.10021390676794788, test loss: 0.2676646461088163\n",
      "epoch 19672: train loss: 0.10021266120065409, test loss: 0.26766533520677477\n",
      "epoch 19673: train loss: 0.10021141571492514, test loss: 0.2676660243166315\n",
      "epoch 19674: train loss: 0.10021017031075097, test loss: 0.26766671343838344\n",
      "epoch 19675: train loss: 0.10020892498812156, test loss: 0.26766740257202754\n",
      "epoch 19676: train loss: 0.10020767974702688, test loss: 0.26766809171756073\n",
      "epoch 19677: train loss: 0.1002064345874569, test loss: 0.26766878087498003\n",
      "epoch 19678: train loss: 0.10020518950940162, test loss: 0.2676694700442824\n",
      "epoch 19679: train loss: 0.10020394451285097, test loss: 0.26767015922546467\n",
      "epoch 19680: train loss: 0.10020269959779499, test loss: 0.2676708484185238\n",
      "epoch 19681: train loss: 0.10020145476422362, test loss: 0.2676715376234569\n",
      "epoch 19682: train loss: 0.10020021001212687, test loss: 0.2676722268402609\n",
      "epoch 19683: train loss: 0.10019896534149471, test loss: 0.26767291606893273\n",
      "epoch 19684: train loss: 0.10019772075231714, test loss: 0.2676736053094694\n",
      "epoch 19685: train loss: 0.10019647624458412, test loss: 0.2676742945618677\n",
      "epoch 19686: train loss: 0.10019523181828567, test loss: 0.2676749838261248\n",
      "epoch 19687: train loss: 0.10019398747341177, test loss: 0.2676756731022376\n",
      "epoch 19688: train loss: 0.10019274320995245, test loss: 0.26767636239020304\n",
      "epoch 19689: train loss: 0.10019149902789763, test loss: 0.267677051690018\n",
      "epoch 19690: train loss: 0.10019025492723738, test loss: 0.2676777410016796\n",
      "epoch 19691: train loss: 0.10018901090796165, test loss: 0.2676784303251848\n",
      "epoch 19692: train loss: 0.10018776697006049, test loss: 0.2676791196605304\n",
      "epoch 19693: train loss: 0.10018652311352387, test loss: 0.26767980900771354\n",
      "epoch 19694: train loss: 0.10018527933834179, test loss: 0.2676804983667312\n",
      "epoch 19695: train loss: 0.10018403564450427, test loss: 0.2676811877375802\n",
      "epoch 19696: train loss: 0.10018279203200132, test loss: 0.2676818771202577\n",
      "epoch 19697: train loss: 0.10018154850082298, test loss: 0.2676825665147604\n",
      "epoch 19698: train loss: 0.10018030505095918, test loss: 0.26768325592108566\n",
      "epoch 19699: train loss: 0.1001790616824, test loss: 0.26768394533923007\n",
      "epoch 19700: train loss: 0.10017781839513544, test loss: 0.2676846347691908\n",
      "epoch 19701: train loss: 0.10017657518915551, test loss: 0.2676853242109648\n",
      "epoch 19702: train loss: 0.10017533206445027, test loss: 0.26768601366454897\n",
      "epoch 19703: train loss: 0.10017408902100967, test loss: 0.2676867031299405\n",
      "epoch 19704: train loss: 0.10017284605882378, test loss: 0.26768739260713614\n",
      "epoch 19705: train loss: 0.10017160317788262, test loss: 0.26768808209613293\n",
      "epoch 19706: train loss: 0.10017036037817623, test loss: 0.2676887715969278\n",
      "epoch 19707: train loss: 0.10016911765969456, test loss: 0.26768946110951786\n",
      "epoch 19708: train loss: 0.10016787502242774, test loss: 0.2676901506339\n",
      "epoch 19709: train loss: 0.10016663246636576, test loss: 0.2676908401700712\n",
      "epoch 19710: train loss: 0.10016538999149864, test loss: 0.26769152971802845\n",
      "epoch 19711: train loss: 0.10016414759781643, test loss: 0.26769221927776876\n",
      "epoch 19712: train loss: 0.10016290528530915, test loss: 0.2676929088492891\n",
      "epoch 19713: train loss: 0.10016166305396684, test loss: 0.2676935984325865\n",
      "epoch 19714: train loss: 0.10016042090377958, test loss: 0.2676942880276577\n",
      "epoch 19715: train loss: 0.10015917883473739, test loss: 0.2676949776345\n",
      "epoch 19716: train loss: 0.10015793684683029, test loss: 0.2676956672531102\n",
      "epoch 19717: train loss: 0.10015669494004832, test loss: 0.2676963568834853\n",
      "epoch 19718: train loss: 0.10015545311438158, test loss: 0.2676970465256224\n",
      "epoch 19719: train loss: 0.1001542113698201, test loss: 0.26769773617951836\n",
      "epoch 19720: train loss: 0.1001529697063539, test loss: 0.2676984258451702\n",
      "epoch 19721: train loss: 0.10015172812397304, test loss: 0.26769911552257497\n",
      "epoch 19722: train loss: 0.10015048662266761, test loss: 0.26769980521172954\n",
      "epoch 19723: train loss: 0.10014924520242761, test loss: 0.2677004949126309\n",
      "epoch 19724: train loss: 0.10014800386324314, test loss: 0.2677011846252762\n",
      "epoch 19725: train loss: 0.10014676260510426, test loss: 0.2677018743496623\n",
      "epoch 19726: train loss: 0.10014552142800102, test loss: 0.26770256408578624\n",
      "epoch 19727: train loss: 0.10014428033192348, test loss: 0.267703253833645\n",
      "epoch 19728: train loss: 0.10014303931686168, test loss: 0.2677039435932354\n",
      "epoch 19729: train loss: 0.10014179838280576, test loss: 0.2677046333645548\n",
      "epoch 19730: train loss: 0.10014055752974571, test loss: 0.2677053231476\n",
      "epoch 19731: train loss: 0.10013931675767164, test loss: 0.2677060129423678\n",
      "epoch 19732: train loss: 0.10013807606657359, test loss: 0.26770670274885544\n",
      "epoch 19733: train loss: 0.1001368354564417, test loss: 0.2677073925670599\n",
      "epoch 19734: train loss: 0.10013559492726597, test loss: 0.2677080823969781\n",
      "epoch 19735: train loss: 0.10013435447903651, test loss: 0.26770877223860695\n",
      "epoch 19736: train loss: 0.10013311411174343, test loss: 0.26770946209194374\n",
      "epoch 19737: train loss: 0.10013187382537674, test loss: 0.26771015195698517\n",
      "epoch 19738: train loss: 0.10013063361992655, test loss: 0.2677108418337284\n",
      "epoch 19739: train loss: 0.10012939349538298, test loss: 0.2677115317221703\n",
      "epoch 19740: train loss: 0.10012815345173606, test loss: 0.26771222162230807\n",
      "epoch 19741: train loss: 0.10012691348897594, test loss: 0.26771291153413856\n",
      "epoch 19742: train loss: 0.10012567360709265, test loss: 0.26771360145765866\n",
      "epoch 19743: train loss: 0.1001244338060763, test loss: 0.2677142913928656\n",
      "epoch 19744: train loss: 0.10012319408591701, test loss: 0.2677149813397564\n",
      "epoch 19745: train loss: 0.10012195444660485, test loss: 0.26771567129832785\n",
      "epoch 19746: train loss: 0.1001207148881299, test loss: 0.26771636126857706\n",
      "epoch 19747: train loss: 0.10011947541048227, test loss: 0.267717051250501\n",
      "epoch 19748: train loss: 0.10011823601365208, test loss: 0.26771774124409675\n",
      "epoch 19749: train loss: 0.10011699669762943, test loss: 0.26771843124936123\n",
      "epoch 19750: train loss: 0.10011575746240439, test loss: 0.26771912126629155\n",
      "epoch 19751: train loss: 0.10011451830796708, test loss: 0.26771981129488454\n",
      "epoch 19752: train loss: 0.10011327923430763, test loss: 0.2677205013351374\n",
      "epoch 19753: train loss: 0.1001120402414161, test loss: 0.2677211913870471\n",
      "epoch 19754: train loss: 0.10011080132928264, test loss: 0.2677218814506105\n",
      "epoch 19755: train loss: 0.10010956249789736, test loss: 0.2677225715258248\n",
      "epoch 19756: train loss: 0.10010832374725034, test loss: 0.2677232616126868\n",
      "epoch 19757: train loss: 0.10010708507733174, test loss: 0.26772395171119373\n",
      "epoch 19758: train loss: 0.10010584648813164, test loss: 0.2677246418213424\n",
      "epoch 19759: train loss: 0.10010460797964019, test loss: 0.26772533194313014\n",
      "epoch 19760: train loss: 0.10010336955184748, test loss: 0.2677260220765536\n",
      "epoch 19761: train loss: 0.10010213120474365, test loss: 0.26772671222160993\n",
      "epoch 19762: train loss: 0.10010089293831882, test loss: 0.2677274023782962\n",
      "epoch 19763: train loss: 0.1000996547525631, test loss: 0.2677280925466093\n",
      "epoch 19764: train loss: 0.10009841664746665, test loss: 0.2677287827265464\n",
      "epoch 19765: train loss: 0.10009717862301956, test loss: 0.2677294729181044\n",
      "epoch 19766: train loss: 0.10009594067921199, test loss: 0.2677301631212803\n",
      "epoch 19767: train loss: 0.10009470281603408, test loss: 0.2677308533360712\n",
      "epoch 19768: train loss: 0.10009346503347592, test loss: 0.26773154356247414\n",
      "epoch 19769: train loss: 0.1000922273315277, test loss: 0.267732233800486\n",
      "epoch 19770: train loss: 0.10009098971017952, test loss: 0.26773292405010396\n",
      "epoch 19771: train loss: 0.10008975216942152, test loss: 0.2677336143113249\n",
      "epoch 19772: train loss: 0.10008851470924388, test loss: 0.2677343045841459\n",
      "epoch 19773: train loss: 0.10008727732963671, test loss: 0.267734994868564\n",
      "epoch 19774: train loss: 0.10008604003059013, test loss: 0.26773568516457613\n",
      "epoch 19775: train loss: 0.10008480281209434, test loss: 0.26773637547217954\n",
      "epoch 19776: train loss: 0.10008356567413944, test loss: 0.26773706579137097\n",
      "epoch 19777: train loss: 0.10008232861671562, test loss: 0.2677377561221475\n",
      "epoch 19778: train loss: 0.100081091639813, test loss: 0.26773844646450634\n",
      "epoch 19779: train loss: 0.10007985474342178, test loss: 0.26773913681844436\n",
      "epoch 19780: train loss: 0.10007861792753203, test loss: 0.26773982718395856\n",
      "epoch 19781: train loss: 0.100077381192134, test loss: 0.2677405175610461\n",
      "epoch 19782: train loss: 0.1000761445372178, test loss: 0.267741207949704\n",
      "epoch 19783: train loss: 0.10007490796277359, test loss: 0.2677418983499291\n",
      "epoch 19784: train loss: 0.10007367146879155, test loss: 0.2677425887617185\n",
      "epoch 19785: train loss: 0.10007243505526181, test loss: 0.26774327918506935\n",
      "epoch 19786: train loss: 0.10007119872217457, test loss: 0.26774396961997865\n",
      "epoch 19787: train loss: 0.10006996246951998, test loss: 0.2677446600664432\n",
      "epoch 19788: train loss: 0.10006872629728822, test loss: 0.26774535052446036\n",
      "epoch 19789: train loss: 0.10006749020546943, test loss: 0.267746040994027\n",
      "epoch 19790: train loss: 0.10006625419405384, test loss: 0.2677467314751401\n",
      "epoch 19791: train loss: 0.10006501826303157, test loss: 0.2677474219677968\n",
      "epoch 19792: train loss: 0.10006378241239283, test loss: 0.2677481124719941\n",
      "epoch 19793: train loss: 0.10006254664212776, test loss: 0.26774880298772885\n",
      "epoch 19794: train loss: 0.10006131095222656, test loss: 0.26774949351499844\n",
      "epoch 19795: train loss: 0.10006007534267941, test loss: 0.2677501840537997\n",
      "epoch 19796: train loss: 0.1000588398134765, test loss: 0.26775087460412955\n",
      "epoch 19797: train loss: 0.10005760436460802, test loss: 0.2677515651659852\n",
      "epoch 19798: train loss: 0.10005636899606411, test loss: 0.26775225573936373\n",
      "epoch 19799: train loss: 0.10005513370783505, test loss: 0.26775294632426205\n",
      "epoch 19800: train loss: 0.10005389849991093, test loss: 0.2677536369206772\n",
      "epoch 19801: train loss: 0.10005266337228198, test loss: 0.2677543275286062\n",
      "epoch 19802: train loss: 0.10005142832493841, test loss: 0.2677550181480462\n",
      "epoch 19803: train loss: 0.10005019335787038, test loss: 0.2677557087789942\n",
      "epoch 19804: train loss: 0.10004895847106812, test loss: 0.2677563994214472\n",
      "epoch 19805: train loss: 0.1000477236645218, test loss: 0.2677570900754022\n",
      "epoch 19806: train loss: 0.10004648893822164, test loss: 0.2677577807408564\n",
      "epoch 19807: train loss: 0.10004525429215784, test loss: 0.2677584714178068\n",
      "epoch 19808: train loss: 0.10004401972632058, test loss: 0.2677591621062502\n",
      "epoch 19809: train loss: 0.1000427852407001, test loss: 0.2677598528061839\n",
      "epoch 19810: train loss: 0.10004155083528656, test loss: 0.2677605435176049\n",
      "epoch 19811: train loss: 0.10004031651007023, test loss: 0.2677612342405102\n",
      "epoch 19812: train loss: 0.10003908226504125, test loss: 0.26776192497489687\n",
      "epoch 19813: train loss: 0.1000378481001899, test loss: 0.26776261572076193\n",
      "epoch 19814: train loss: 0.10003661401550634, test loss: 0.26776330647810254\n",
      "epoch 19815: train loss: 0.1000353800109808, test loss: 0.2677639972469156\n",
      "epoch 19816: train loss: 0.10003414608660352, test loss: 0.2677646880271981\n",
      "epoch 19817: train loss: 0.1000329122423647, test loss: 0.26776537881894724\n",
      "epoch 19818: train loss: 0.10003167847825453, test loss: 0.26776606962216015\n",
      "epoch 19819: train loss: 0.1000304447942633, test loss: 0.2677667604368336\n",
      "epoch 19820: train loss: 0.10002921119038119, test loss: 0.2677674512629649\n",
      "epoch 19821: train loss: 0.10002797766659842, test loss: 0.26776814210055094\n",
      "epoch 19822: train loss: 0.10002674422290521, test loss: 0.2677688329495889\n",
      "epoch 19823: train loss: 0.10002551085929184, test loss: 0.26776952381007557\n",
      "epoch 19824: train loss: 0.1000242775757485, test loss: 0.26777021468200835\n",
      "epoch 19825: train loss: 0.10002304437226542, test loss: 0.26777090556538413\n",
      "epoch 19826: train loss: 0.10002181124883286, test loss: 0.26777159646019993\n",
      "epoch 19827: train loss: 0.10002057820544102, test loss: 0.2677722873664529\n",
      "epoch 19828: train loss: 0.10001934524208016, test loss: 0.26777297828414\n",
      "epoch 19829: train loss: 0.10001811235874052, test loss: 0.2677736692132583\n",
      "epoch 19830: train loss: 0.10001687955541233, test loss: 0.26777436015380485\n",
      "epoch 19831: train loss: 0.10001564683208583, test loss: 0.26777505110577676\n",
      "epoch 19832: train loss: 0.10001441418875129, test loss: 0.26777574206917104\n",
      "epoch 19833: train loss: 0.10001318162539892, test loss: 0.2677764330439848\n",
      "epoch 19834: train loss: 0.100011949142019, test loss: 0.2677771240302151\n",
      "epoch 19835: train loss: 0.10001071673860175, test loss: 0.26777781502785897\n",
      "epoch 19836: train loss: 0.10000948441513746, test loss: 0.2677785060369134\n",
      "epoch 19837: train loss: 0.10000825217161634, test loss: 0.2677791970573756\n",
      "epoch 19838: train loss: 0.10000702000802864, test loss: 0.2677798880892425\n",
      "epoch 19839: train loss: 0.10000578792436465, test loss: 0.2677805791325113\n",
      "epoch 19840: train loss: 0.10000455592061464, test loss: 0.26778127018717884\n",
      "epoch 19841: train loss: 0.10000332399676883, test loss: 0.26778196125324244\n",
      "epoch 19842: train loss: 0.1000020921528175, test loss: 0.2677826523306989\n",
      "epoch 19843: train loss: 0.10000086038875089, test loss: 0.26778334341954557\n",
      "epoch 19844: train loss: 0.09999962870455928, test loss: 0.2677840345197793\n",
      "epoch 19845: train loss: 0.09999839710023295, test loss: 0.2677847256313972\n",
      "epoch 19846: train loss: 0.09999716557576216, test loss: 0.26778541675439643\n",
      "epoch 19847: train loss: 0.09999593413113717, test loss: 0.26778610788877394\n",
      "epoch 19848: train loss: 0.09999470276634828, test loss: 0.2677867990345269\n",
      "epoch 19849: train loss: 0.09999347148138572, test loss: 0.2677874901916523\n",
      "epoch 19850: train loss: 0.09999224027623978, test loss: 0.2677881813601472\n",
      "epoch 19851: train loss: 0.09999100915090077, test loss: 0.2677888725400087\n",
      "epoch 19852: train loss: 0.0999897781053589, test loss: 0.26778956373123397\n",
      "epoch 19853: train loss: 0.09998854713960452, test loss: 0.26779025493381997\n",
      "epoch 19854: train loss: 0.09998731625362786, test loss: 0.2677909461477638\n",
      "epoch 19855: train loss: 0.09998608544741923, test loss: 0.26779163737306233\n",
      "epoch 19856: train loss: 0.09998485472096892, test loss: 0.267792328609713\n",
      "epoch 19857: train loss: 0.09998362407426722, test loss: 0.26779301985771264\n",
      "epoch 19858: train loss: 0.09998239350730437, test loss: 0.26779371111705846\n",
      "epoch 19859: train loss: 0.0999811630200707, test loss: 0.26779440238774743\n",
      "epoch 19860: train loss: 0.09997993261255651, test loss: 0.26779509366977666\n",
      "epoch 19861: train loss: 0.09997870228475207, test loss: 0.26779578496314316\n",
      "epoch 19862: train loss: 0.09997747203664767, test loss: 0.26779647626784414\n",
      "epoch 19863: train loss: 0.09997624186823365, test loss: 0.26779716758387656\n",
      "epoch 19864: train loss: 0.09997501177950024, test loss: 0.26779785891123764\n",
      "epoch 19865: train loss: 0.09997378177043782, test loss: 0.2677985502499242\n",
      "epoch 19866: train loss: 0.09997255184103661, test loss: 0.2677992415999336\n",
      "epoch 19867: train loss: 0.09997132199128698, test loss: 0.2677999329612628\n",
      "epoch 19868: train loss: 0.09997009222117921, test loss: 0.2678006243339089\n",
      "epoch 19869: train loss: 0.09996886253070361, test loss: 0.2678013157178689\n",
      "epoch 19870: train loss: 0.09996763291985046, test loss: 0.26780200711313995\n",
      "epoch 19871: train loss: 0.0999664033886101, test loss: 0.26780269851971916\n",
      "epoch 19872: train loss: 0.09996517393697286, test loss: 0.26780338993760355\n",
      "epoch 19873: train loss: 0.099963944564929, test loss: 0.26780408136679024\n",
      "epoch 19874: train loss: 0.0999627152724689, test loss: 0.2678047728072764\n",
      "epoch 19875: train loss: 0.09996148605958283, test loss: 0.2678054642590588\n",
      "epoch 19876: train loss: 0.0999602569262611, test loss: 0.2678061557221349\n",
      "epoch 19877: train loss: 0.09995902787249406, test loss: 0.26780684719650155\n",
      "epoch 19878: train loss: 0.09995779889827204, test loss: 0.26780753868215595\n",
      "epoch 19879: train loss: 0.09995657000358533, test loss: 0.2678082301790952\n",
      "epoch 19880: train loss: 0.09995534118842431, test loss: 0.2678089216873163\n",
      "epoch 19881: train loss: 0.09995411245277921, test loss: 0.2678096132068164\n",
      "epoch 19882: train loss: 0.09995288379664044, test loss: 0.26781030473759254\n",
      "epoch 19883: train loss: 0.09995165521999834, test loss: 0.2678109962796419\n",
      "epoch 19884: train loss: 0.09995042672284318, test loss: 0.26781168783296144\n",
      "epoch 19885: train loss: 0.09994919830516533, test loss: 0.2678123793975483\n",
      "epoch 19886: train loss: 0.09994796996695515, test loss: 0.2678130709733997\n",
      "epoch 19887: train loss: 0.09994674170820293, test loss: 0.2678137625605125\n",
      "epoch 19888: train loss: 0.099945513528899, test loss: 0.26781445415888394\n",
      "epoch 19889: train loss: 0.09994428542903375, test loss: 0.26781514576851106\n",
      "epoch 19890: train loss: 0.0999430574085975, test loss: 0.267815837389391\n",
      "epoch 19891: train loss: 0.0999418294675806, test loss: 0.26781652902152087\n",
      "epoch 19892: train loss: 0.09994060160597339, test loss: 0.26781722066489777\n",
      "epoch 19893: train loss: 0.0999393738237662, test loss: 0.26781791231951874\n",
      "epoch 19894: train loss: 0.09993814612094941, test loss: 0.2678186039853808\n",
      "epoch 19895: train loss: 0.09993691849751336, test loss: 0.26781929566248125\n",
      "epoch 19896: train loss: 0.09993569095344838, test loss: 0.267819987350817\n",
      "epoch 19897: train loss: 0.09993446348874485, test loss: 0.2678206790503852\n",
      "epoch 19898: train loss: 0.09993323610339312, test loss: 0.267821370761183\n",
      "epoch 19899: train loss: 0.09993200879738355, test loss: 0.2678220624832075\n",
      "epoch 19900: train loss: 0.09993078157070649, test loss: 0.26782275421645574\n",
      "epoch 19901: train loss: 0.0999295544233523, test loss: 0.26782344596092483\n",
      "epoch 19902: train loss: 0.09992832735531137, test loss: 0.26782413771661195\n",
      "epoch 19903: train loss: 0.09992710036657401, test loss: 0.26782482948351416\n",
      "epoch 19904: train loss: 0.09992587345713065, test loss: 0.2678255212616285\n",
      "epoch 19905: train loss: 0.09992464662697163, test loss: 0.26782621305095217\n",
      "epoch 19906: train loss: 0.09992341987608729, test loss: 0.26782690485148214\n",
      "epoch 19907: train loss: 0.09992219320446803, test loss: 0.2678275966632156\n",
      "epoch 19908: train loss: 0.09992096661210421, test loss: 0.26782828848614976\n",
      "epoch 19909: train loss: 0.09991974009898623, test loss: 0.2678289803202815\n",
      "epoch 19910: train loss: 0.09991851366510446, test loss: 0.26782967216560816\n",
      "epoch 19911: train loss: 0.09991728731044924, test loss: 0.2678303640221266\n",
      "epoch 19912: train loss: 0.09991606103501098, test loss: 0.2678310558898342\n",
      "epoch 19913: train loss: 0.09991483483878007, test loss: 0.2678317477687278\n",
      "epoch 19914: train loss: 0.09991360872174689, test loss: 0.26783243965880466\n",
      "epoch 19915: train loss: 0.09991238268390176, test loss: 0.267833131560062\n",
      "epoch 19916: train loss: 0.09991115672523515, test loss: 0.2678338234724966\n",
      "epoch 19917: train loss: 0.09990993084573743, test loss: 0.26783451539610587\n",
      "epoch 19918: train loss: 0.09990870504539895, test loss: 0.26783520733088684\n",
      "epoch 19919: train loss: 0.09990747932421015, test loss: 0.2678358992768365\n",
      "epoch 19920: train loss: 0.09990625368216138, test loss: 0.2678365912339522\n",
      "epoch 19921: train loss: 0.09990502811924307, test loss: 0.2678372832022307\n",
      "epoch 19922: train loss: 0.09990380263544558, test loss: 0.2678379751816695\n",
      "epoch 19923: train loss: 0.09990257723075935, test loss: 0.2678386671722655\n",
      "epoch 19924: train loss: 0.09990135190517474, test loss: 0.26783935917401586\n",
      "epoch 19925: train loss: 0.09990012665868216, test loss: 0.2678400511869177\n",
      "epoch 19926: train loss: 0.09989890149127204, test loss: 0.2678407432109681\n",
      "epoch 19927: train loss: 0.09989767640293476, test loss: 0.26784143524616416\n",
      "epoch 19928: train loss: 0.09989645139366071, test loss: 0.2678421272925032\n",
      "epoch 19929: train loss: 0.09989522646344033, test loss: 0.26784281934998205\n",
      "epoch 19930: train loss: 0.09989400161226403, test loss: 0.26784351141859786\n",
      "epoch 19931: train loss: 0.09989277684012218, test loss: 0.26784420349834803\n",
      "epoch 19932: train loss: 0.09989155214700524, test loss: 0.26784489558922936\n",
      "epoch 19933: train loss: 0.09989032753290364, test loss: 0.2678455876912391\n",
      "epoch 19934: train loss: 0.09988910299780769, test loss: 0.2678462798043745\n",
      "epoch 19935: train loss: 0.09988787854170794, test loss: 0.26784697192863255\n",
      "epoch 19936: train loss: 0.09988665416459469, test loss: 0.2678476640640103\n",
      "epoch 19937: train loss: 0.09988542986645847, test loss: 0.26784835621050496\n",
      "epoch 19938: train loss: 0.09988420564728961, test loss: 0.26784904836811363\n",
      "epoch 19939: train loss: 0.09988298150707861, test loss: 0.26784974053683347\n",
      "epoch 19940: train loss: 0.09988175744581584, test loss: 0.2678504327166617\n",
      "epoch 19941: train loss: 0.09988053346349175, test loss: 0.2678511249075951\n",
      "epoch 19942: train loss: 0.09987930956009679, test loss: 0.26785181710963124\n",
      "epoch 19943: train loss: 0.09987808573562136, test loss: 0.2678525093227669\n",
      "epoch 19944: train loss: 0.0998768619900559, test loss: 0.2678532015469994\n",
      "epoch 19945: train loss: 0.09987563832339083, test loss: 0.2678538937823258\n",
      "epoch 19946: train loss: 0.09987441473561662, test loss: 0.26785458602874324\n",
      "epoch 19947: train loss: 0.09987319122672368, test loss: 0.2678552782862488\n",
      "epoch 19948: train loss: 0.09987196779670246, test loss: 0.2678559705548397\n",
      "epoch 19949: train loss: 0.09987074444554339, test loss: 0.2678566628345129\n",
      "epoch 19950: train loss: 0.09986952117323696, test loss: 0.26785735512526576\n",
      "epoch 19951: train loss: 0.09986829797977353, test loss: 0.26785804742709535\n",
      "epoch 19952: train loss: 0.09986707486514364, test loss: 0.26785873973999863\n",
      "epoch 19953: train loss: 0.09986585182933766, test loss: 0.267859432063973\n",
      "epoch 19954: train loss: 0.09986462887234607, test loss: 0.26786012439901535\n",
      "epoch 19955: train loss: 0.09986340599415933, test loss: 0.267860816745123\n",
      "epoch 19956: train loss: 0.09986218319476788, test loss: 0.2678615091022929\n",
      "epoch 19957: train loss: 0.09986096047416218, test loss: 0.2678622014705223\n",
      "epoch 19958: train loss: 0.09985973783233268, test loss: 0.2678628938498083\n",
      "epoch 19959: train loss: 0.09985851526926982, test loss: 0.26786358624014817\n",
      "epoch 19960: train loss: 0.09985729278496412, test loss: 0.26786427864153883\n",
      "epoch 19961: train loss: 0.09985607037940597, test loss: 0.26786497105397755\n",
      "epoch 19962: train loss: 0.09985484805258588, test loss: 0.26786566347746144\n",
      "epoch 19963: train loss: 0.09985362580449429, test loss: 0.26786635591198765\n",
      "epoch 19964: train loss: 0.09985240363512168, test loss: 0.2678670483575533\n",
      "epoch 19965: train loss: 0.0998511815444585, test loss: 0.2678677408141555\n",
      "epoch 19966: train loss: 0.09984995953249523, test loss: 0.26786843328179155\n",
      "epoch 19967: train loss: 0.09984873759922235, test loss: 0.26786912576045835\n",
      "epoch 19968: train loss: 0.09984751574463033, test loss: 0.26786981825015316\n",
      "epoch 19969: train loss: 0.09984629396870962, test loss: 0.26787051075087315\n",
      "epoch 19970: train loss: 0.0998450722714507, test loss: 0.26787120326261543\n",
      "epoch 19971: train loss: 0.09984385065284407, test loss: 0.2678718957853772\n",
      "epoch 19972: train loss: 0.09984262911288021, test loss: 0.26787258831915545\n",
      "epoch 19973: train loss: 0.09984140765154959, test loss: 0.2678732808639475\n",
      "epoch 19974: train loss: 0.09984018626884268, test loss: 0.26787397341975044\n",
      "epoch 19975: train loss: 0.09983896496474998, test loss: 0.26787466598656134\n",
      "epoch 19976: train loss: 0.09983774373926196, test loss: 0.26787535856437744\n",
      "epoch 19977: train loss: 0.09983652259236915, test loss: 0.26787605115319585\n",
      "epoch 19978: train loss: 0.09983530152406198, test loss: 0.2678767437530136\n",
      "epoch 19979: train loss: 0.09983408053433099, test loss: 0.2678774363638282\n",
      "epoch 19980: train loss: 0.09983285962316663, test loss: 0.26787812898563634\n",
      "epoch 19981: train loss: 0.09983163879055942, test loss: 0.26787882161843546\n",
      "epoch 19982: train loss: 0.09983041803649986, test loss: 0.2678795142622226\n",
      "epoch 19983: train loss: 0.09982919736097841, test loss: 0.267880206916995\n",
      "epoch 19984: train loss: 0.09982797676398562, test loss: 0.26788089958274974\n",
      "epoch 19985: train loss: 0.09982675624551197, test loss: 0.26788159225948405\n",
      "epoch 19986: train loss: 0.09982553580554797, test loss: 0.26788228494719496\n",
      "epoch 19987: train loss: 0.09982431544408406, test loss: 0.26788297764587976\n",
      "epoch 19988: train loss: 0.09982309516111083, test loss: 0.2678836703555354\n",
      "epoch 19989: train loss: 0.09982187495661876, test loss: 0.2678843630761592\n",
      "epoch 19990: train loss: 0.09982065483059835, test loss: 0.2678850558077484\n",
      "epoch 19991: train loss: 0.0998194347830401, test loss: 0.26788574855029984\n",
      "epoch 19992: train loss: 0.09981821481393455, test loss: 0.26788644130381106\n",
      "epoch 19993: train loss: 0.0998169949232722, test loss: 0.2678871340682789\n",
      "epoch 19994: train loss: 0.09981577511104354, test loss: 0.2678878268437007\n",
      "epoch 19995: train loss: 0.09981455537723914, test loss: 0.26788851963007354\n",
      "epoch 19996: train loss: 0.09981333572184947, test loss: 0.2678892124273946\n",
      "epoch 19997: train loss: 0.0998121161448651, test loss: 0.2678899052356611\n",
      "epoch 19998: train loss: 0.0998108966462765, test loss: 0.2678905980548701\n",
      "epoch 19999: train loss: 0.09980967722607423, test loss: 0.26789129088501873\n",
      "epoch 20000: train loss: 0.09980845788424877, test loss: 0.26789198372610434\n",
      "Confusion Matrix:\n",
      "[[95  8]\n",
      " [10 87]]\n",
      "Accuracy: 0.91\n",
      "Classification Report (scikit-learn):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.92      0.91       103\n",
      "         1.0       0.92      0.90      0.91        97\n",
      "\n",
      "    accuracy                           0.91       200\n",
      "   macro avg       0.91      0.91      0.91       200\n",
      "weighted avg       0.91      0.91      0.91       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAHFCAYAAAD/kYOsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOydd5gU9f3HXzOzvVzvjbuj92YBAQV7QxTQ2EssSexKYglqrNGYZvlZgrEkFjQgdkVRQbCgKL23673v3fadmd8fc7flGoeCFOf1PPvc7s53Zr4zu7fznk8VVFVV0dHR0dHR0dE5jBAP9AR0dHR0dHR0dPY1usDR0dHR0dHROezQBY6Ojo6Ojo7OYYcucHR0dHR0dHQOO3SBo6Ojo6Ojo3PYoQscHR0dHR0dncMOXeDo6Ojo6OjoHHboAkdHR0dHR0fnsEMXODo6Ojo6OjqHHbrA0dHR+dG89NJLCIIQfhgMBnJycrjiiiuoqKgIj1u2bBmCILBs2bK93sfXX3/NvffeS3Nz80+a6+WXX05+fv6PWvfpp5/mpZde+kn719HR+XnRBY6Ojs5P5sUXX+Sbb75hyZIlXH311cyfP58pU6bgdrt/8ra//vpr7rvvvp8scH4KusDR0Tn0MBzoCejo6Bz6jBgxgiOOOAKAadOmIcsyDzzwAG+//TYXXXTRAZ6djo7OLxHdgqOjo7PPmTBhAgAlJSW9jnv33XeZOHEiNpsNp9PJSSedxDfffBNefu+99/KHP/wBgIKCgrArbE+urpdeeonBgwdjNpsZOnQo//3vf7sdd99993H00UeTlJREXFwc48aN4/nnnye6B3F+fj6bNm3iiy++CO+/w9Xl8/mYM2cOY8aMIT4+nqSkJCZOnMg777yzp1Oko6Ozn9EtODo6OvucnTt3ApCamtrjmNdee42LLrqIk08+mfnz5+P3+3n00UeZOnUqn332GZMnT+aqq66isbGRJ598kkWLFpGZmQnAsGHDetzuSy+9xBVXXMGMGTP4+9//TktLC/feey9+vx9RjL2nKy4u5je/+Q15eXkArFy5khtuuIGKigruueceAN566y1mz55NfHw8Tz/9NABmsxkAv99PY2Mjv//978nOziYQCPDpp58yc+ZMXnzxRS699NIfeQZ1dHR+MqqOjo7Oj+TFF19UAXXlypVqMBhUW1tb1ffff19NTU1VnU6nWl1draqqqi5dulQF1KVLl6qqqqqyLKtZWVnqyJEjVVmWw9trbW1V09LS1GOOOSb83l//+lcVUIuKivY4n47tjhs3TlUUJfx+cXGxajQa1X79+vW6bjAYVO+//341OTk5Zv3hw4erxx133B73HwqF1GAwqF555ZXq2LFj9zheR0dn/6G7qHR0dH4yEyZMwGg04nQ6OfPMM8nIyOCjjz4iPT292/Hbtm2jsrKSSy65JMaq4nA4mDVrFitXrsTj8ez1PDq2e+GFFyIIQvj9fv36ccwxx3QZ//nnn3PiiScSHx+PJEkYjUbuueceGhoaqK2t7dM+FyxYwKRJk3A4HBgMBoxGI88//zxbtmzZ6/nr6OjsO3SBo6Oj85P573//y6pVq1izZg2VlZWsX7+eSZMm9Ti+oaEBIOxyiiYrKwtFUWhqatrreXRsNyMjo8uyzu999913nHzyyQA899xzfPXVV6xatYq5c+cC4PV697i/RYsWcd5555Gdnc0rr7zCN998w6pVq/j1r3+Nz+fb6/nr6OjsO/QYHB0dnZ/M0KFDw1lUfSE5ORmAqqqqLssqKysRRZHExMS9nkfHdqurq7ss6/ze66+/jtFo5P3338disYTff/vtt/u8v1deeYWCggLeeOONGIuR3+/fy5nr6Ojsa3QLjo6Ozs/O4MGDyc7O5rXXXovJWHK73bz55pvhzCqIBPT2xaIyePBgMjMzmT9/fsx2S0pK+Prrr2PGdhQmlCQp/J7X6+Xll1/usl2z2dzt/gVBwGQyxYib6upqPYtKR+cgQBc4Ojo6PzuiKPLoo4+ydu1azjzzTN59910WLFjAtGnTaG5u5pFHHgmPHTlyJACPP/4433zzDd9//z2tra09bveBBx7ghx9+4JxzzuGDDz7g1Vdf5cQTT+ziojrjjDNoa2vjwgsvZMmSJbz++utMmTIlLKiiGTlyJOvWreONN95g1apVbNiwAYAzzzyTbdu2ce211/L555/zn//8h8mTJ3fretPR0fmZOdBRzjo6OocuHVlUq1at6nVc5yyqDt5++2316KOPVi0Wi2q329UTTjhB/eqrr7qsf+edd6pZWVmqKIrdbqcz//73v9WBAweqJpNJHTRokPrCCy+ol112WZcsqhdeeEEdPHiwajab1cLCQvXhhx9Wn3/++S5ZW8XFxerJJ5+sOp1OFYjZziOPPKLm5+erZrNZHTp0qPrcc8+pf/rTn1T951VH58AiqGqUHVdHR0dHR0dH5zBAd1Hp6Ojo6OjoHHboAkdHR0dHR0fnsEMXODo6Ojo6OjqHHbrA0dHR0dHR0Tns0AWOjo6Ojo6OzmGHLnB0dHR0dHR0Djt+Ua0aFEWhsrISp9MZU3lUR0dHR0dH5+BFVVVaW1vJysqKadDbG78ogVNZWUlubu6BnoaOjo6Ojo7Oj6CsrIycnJw+jf1FCRyn0wloJyguLu4Az0ZHR0dHR0enL7hcLnJzc8PX8b7wixI4HW6puLg4XeDo6Ojo6OgcYuxNeIkeZKyjo6Ojo6Nz2KELHB0dHR0dHZ3DDl3g6Ojo6Ojo6Bx2/KJicPqKLMsEg8EDPY1fLCaTqc9pgDo6Ojo6Ot2hC5woVFWlurqa5ubmAz2VXzSiKFJQUIDJZDrQU9HR0dHROUTRBU4UHeImLS0Nm82mFwM8AHQUY6yqqiIvL0//DHR0dHR0fhS6wGlHluWwuElOTj7Q0/lFk5qaSmVlJaFQCKPReKCno6Ojo6NzCHLIBDo8/PDDHHnkkTidTtLS0jj77LPZtm3bPtt+R8yNzWbbZ9vU+XF0uKZkWT7AM9HR0dHROVQ5ZATOF198wXXXXcfKlStZsmQJoVCIk08+GbfbvU/3o7tEDjz6Z6Cjo6Oj81M5ZFxUixcvjnn94osvkpaWxg8//MCxxx57gGalo6Ojo6OjczByyFhwOtPS0gJAUlJSj2P8fj8ulyvm8UtFEATefvvtAz0NHR0dHR2dn4VDUuCoqsqtt97K5MmTGTFiRI/jHn74YeLj48OPn6WTuCzDsmUwf77292eII6muruaGG26gsLAQs9lMbm4u06dP57PPPtvv++4LixYt4pRTTiElJQVBEFi7du2BnpKOjo6OzmHOISlwrr/+etavX8/8+fN7HXfnnXfS0tISfpSVle3fiS1aBPn5MG0aXHih9jc/X3t/P1FcXMz48eP5/PPPefTRR9mwYQOLFy9m2rRpXHfddfttv3uD2+1m0qRJPPLIIwd6Kjo6Ojo6vxAOOYFzww038O6777J06VJycnJ6HWs2m8Odw/d7B/FFi2D2bCgvj32/okJ7fz+JnGuvvRZBEPjuu++YPXs2gwYNYvjw4dx6662sXLmyx/Vuv/12Bg0ahM1mo7CwkLvvvjumevO6deuYNm0aTqeTuLg4xo8fz/fffw9ASUkJ06dPJzExEbvdzvDhw/nwww973Ncll1zCPffcw4knnrjvDlxHR0dHR6cXDpkgY1VVueGGG3jrrbdYtmwZBQUFB3pKEWQZbroJVLXrMlUFQYCbb4YZM0CS9tluGxsbWbx4MQ899BB2u73L8oSEhB7XdTqdvPTSS2RlZbFhwwauvvpqnE4nt912GwAXXXQRY8eO5ZlnnkGSJNauXRuuSXPdddcRCARYvnw5drudzZs343A49tlx6ejo6Ojo/FQOGYFz3XXX8dprr/HOO+/gdDqprq4GID4+HqvVemAnt2JFV8tNNKoKZWXauKlT99lud+7ciaqqDBkyZK/Xveuuu8LP8/PzmTNnDm+88UZY4JSWlvKHP/whvO2BAweGx5eWljJr1ixGjhwJQGFh4U85DB0dHR0dnX3OIeOieuaZZ2hpaWHq1KlkZmaGH2+88caBnhpUVe3bcX1EbbcY/Zi6MQsXLmTy5MlkZGTgcDi4++67KS0tDS+/9dZbueqqqzjxxBN55JFH2LVrV3jZjTfeyIMPPsikSZP405/+xPr163/6wejo6Ojo6OxDDhmBo6pqt4/LL7/8QE8NMjP37bg+MnDgQARBYMuWLXu13sqVKzn//PM57bTTeP/991mzZg1z584lEAiEx9x7771s2rSJM844g88//5xhw4bx1ltvAXDVVVexe/duLrnkEjZs2MARRxzBk08+uU+PTUdHR0dH56dwyAicg5opUyAnR4u16Q5BgNxcbdw+JCkpiVNOOYWnnnqq24rOPXVF/+qrr+jXrx9z587liCOOYODAgZSUlHQZN2jQIG655RY++eQTZs6cyYsvvhhelpuby29/+1sWLVrEnDlzeO655/bZceno6Ojo6PxUdIGzL5AkePxx7XlnkdPx+rHH9mmAcQdPP/00sixz1FFH8eabb7Jjxw62bNnCE088wcSJE7tdZ8CAAZSWlvL666+za9cunnjiibB1BsDr9XL99dezbNkySkpK+Oqrr1i1ahVDhw4F4Oabb+bjjz+mqKiI1atX8/nnn4eXdUdjYyNr165l8+bNAGzbto21a9eG46h0dHR0dHT2NbrA2VfMnAkLF0J2duz7OTna+zNn7pfdFhQUsHr1aqZNm8acOXMYMWIEJ510Ep999hnPPPNMt+vMmDGDW265heuvv54xY8bw9ddfc/fdd4eXS5JEQ0MDl156KYMGDeK8887jtNNO47777gO0JpjXXXcdQ4cO5dRTT2Xw4ME8/fTTPc7x3XffZezYsZxxxhkAnH/++YwdO5Znn312H54JHR2d3lAVGbm5DrmpJvbhagjH8+noHE4I6i/om+1yuYiPj6elpaVLTRyfz0dRUREFBQVYLJYfvxNZ1rKlqqq0mJspU/aL5eZwZp99Fjo6OqgBH4Ft3+Pf9A2qu6XbMWJCGqZhR2MaMAbBaP6ZZ6ijs2d6u373xCGTJn7IIEn7NBVcR0dH58egKgqBbavwfb8EAr5exyrNtfi+fg//D59hHnMcpiFHIRiMP9NMdXT2D7rA0dHR0TnMkBur8a54C7m+IuZ9Q1Z/BEdCzHtKcx1yrVYiQvV78H37Ef6NX2OdfDbGnIH8HKiKjNrWguJtQ5AkMJgQjCYEgwmMJgRRt4Lr7D26wNHR0dE5jFBaG3F/8DxqwBt+z9h/NOYxU5ESUrtdR66vxL/xS4K7NgAqqrsFzycvYz1uFqb+o/fp/NRQkGDJZkJl21HamlDamlE9rd1XggdAQExIRUrPw5CWh5SehxiX/KPqf+n8stAFjo6Ojs5hghoK4v5sfljciAlpWCedhSEjv9f1pJQsbFPPQx45Bd93iwlV7gJVwbtsIarfh3nY0T9tXqqKXFdOcMdqArs37NFl1mltlOZalOZagtu0fniCxY71uFkYcwb9pHnpHN7oAkdHR0fnIEFVFQj6UQMeTaTIQTCYEUw2BJNVc930YrkIbF6J0qBVTBfjknFMvwbB1PdAfSk5E9spl+H75n0CW78DVHzfvIeUkIoha+9bsqiqSnDHavzrlqO4GrodI1jsiI4EBEcCoi0OVAU16EcNBSEUQPG2oTTWgKpEthv0I6Vkd7u97giFQrS0tCAIAj6fj8zMTN0C9AtAFzg6Ojo6BxBVVVHdTSie5nbLRidXjd+D6m7SngsigtmGGJ+OYOwqXKJjbqzHzupe3Owh01MQRSzHTEcwmfGvXwGAb+1SHHspcJS2ZrxfvkOoYkfsAoMJY8FwTAPHIaVma3E2e0ANBpDrywnVlCLXlCImpCJaujYY7kxTUxOLFi3irbfeoqSkhB07dhAXF8fUqVOZO3cuo0fvW/ebzsGFLnB0dHR0DhBq0IfcVAlR8TK9r6Cg+tqQA16ktP5dMp1Uf2Q7UmJa1/UXLYKbboptDpyToxUqjarVJQgC5vEnESzZgtJSj1xVRKimFEN63p6nqKoEt/+A99uPIOiPzCezANOAMRjzRyCYzOGxgYZGPMXFeIqL8VdXI5otGJwODE4nBocDQ1wcjsGDMGQWYsjsm8iqrq7mqaee4t///jc1NTVYLBbGjh3LCSecgNfrZeHChaxcuZJXXnmFY489tk/b1Dn00AWOjo6Ozs+MqqoorlrU1vrYBQYTgskafiCZUIM+CHhRA17UgAcUGRQZuaEMKTUfQYzUaw0HFgsidK5ns2gRzJ7dNZi3okJ7v1NBUkEUMY+agneFVuXcv3YZhlMu7fW4FJ8b77KFMVYbwRaHdfIMjLmDtTnKMpWL3qb2o8V4iooItbbu8XwJRiPxo0eReuIJpJ58EgZ7z9abDz/8kLvvvps1a9aQk5PDXXfdxcyZM8ONhR0OBx988AFz5szhrrvuYvny5Xvcv86hiS5wdHR0dH5G1FAAubE81mpjMCElZiGYu164BckBFoe2rhxCrt2txeYEvcj1xUjJeQiS9lOuBtotJqKoxawI7a4nWdYsN91lKqmq1lLm5pthxowYd5VxwBh8qz9HdbcQKt+O4vcgmm3dHpfiduH+5L8ojZEWLMaB47AefRqC2QqAe+cutj/8CK0bN/X1dGlTDAZp/v4Hmr//gd1P/B+pJ59E7qUXY+1UOf6NN97gd7/7HQaDgXvvvZerrrqKrKysLts79dRT+eGHH/jzn//M9u3bGTQoKlhZL9Z62KALnF8IgiDw1ltvcfbZZx/oqegcwqhyENXj0qwKqNrFUVXbL6YigtmOYHHsMRj2l4oa8CLXl2hWGAAEBGcKYlwKgrDnzjmCZEBKzkWuK9bOecCLXLsbKaUfgtGMlJypBfPKIUKlWzHmD9dWXLEi1i3VZWIqlJVp46IKlQqihBSfQqi9ArJA95+p3FSL++P/hCslC1YH1ikzMeZqwkH2+Sl98UXKX34VVZbD65nT07Hl98OWn481vx/WnByUQIBQWxtyaxuhtlZ8VdU0fbcKf5UWPC17PFS//Q51Sz5l8J/uJuW4Y0GWaXj/fe6bMwefx8MzzzzDrHPPxeFwdHOoKpIkkZSUBEBpaWlE4PTRhadzaKALnH3MgRD/1dXVPPTQQ3zwwQdUVFSQlpbGmDFjuPnmmznhhBP27873QDAY5K677uLDDz9k9+7dxMfHc+KJJ/LII490e2elc/ChKgqqz4XqbkH1t/U+1tcKLYBkRLA4ECxO7a8udlB8bSgNZZFsIMmIlJyruaK6IRCS8QZlREFAEtsfgoBktCClFiA3lIAcAjmIXFeElJyHcdB4gkUbtfW3fh8ROO3iYI90M07xtLuQJAN0E7QcqirC/emr4dRvwZGA/ZTLwjV3XBs2sPXeB/BFiQZrXh4D77ydhHFj+zQtVVVp27adqrfepu6TJcgeD7Lbzebb7iB3wtHkL1rID+XlbAWeBi675x6Ij+8iSmRZRhRFmpqa+PDDD3E6nQwc2F7McC9deDoHP7rA2YccCPFfXFzMpEmTSEhI4NFHH2XUqFEEg0E+/vhjrrvuOrZu3bp/dtxHPB4Pq1ev5u6772b06NE0NTVx8803c9ZZZ/H9998f0Lnp9I4a8qO01GmiJSpFt0/IQVR3k5b9YzAjOpIQ7Al9slIcjii+NpT6UsIZUiYbUkpetxV6/SGZBrcfT0DusgxAABJtJhJSClAaS7VAXkVGbixHyixEcCaitjYRqtiJ4mpEjEvS7rb6QqdxsqsBpcMqY3N2Eaqh6mLci18KW6TE5CzsJ1+CaHMC4Nq0mfXX3Yji11xngsFA7mWXknfZJYgGAyxb1qe7QUEQcA4cgHPi0RRmprPjq2+oW78BgLKV39Jmc7AekIDBABUVBGfNwvjmm6jnnIOiKEiShNS+/X/9618sXryYSy65hIyMjB/twtM5uPll/trsBzrEf2crcIf4X7Ro/+z32muvRRAEvvvuO2bPns2gQYMYPnw4t956KytXruxxvdtvv51BgwZhs9koLCzk7rvvJhgMhpevW7eOadOm4XQ6iYuLY/z48WFBUlJSwvTp00lMTMRutzN8+HA+/PDDbvcTHx/PkiVLOO+88xg8eDATJkzgySef5IcffqC0tHTfngydfYIqh5Cbq5Crd6J6W2LFjWREcKYgpRUiZQxAyhiElDkYKWsIUlp/LX3ZbIdoV0bIj9JchVy1HbmlRqtv8gtCVRWUpko6xI1gcSKl9usibgIhhWqXl7ImT4/ihvatNHoCNPgVxJR86LAAyUHU1npMg48Ij/Su/ABVDmniISdHu1B3hyBAbq42Ds1q4/3qXdoWPh7OhBI7tXgAtDTydnFjyB6I4/Qrw+IGYPfjT4bFTdzIEYx7+T/kX3MV4gcfQH4+TJsGF16o/c3P7/mHctGi8HjDFVcw5N/zKGxuCguSpoRE4pKSEYBSAFXFKAgoN92E0C5uABYvXsykSZP44x//yMknn8zf/vY3zGbz3rnwdA4ZdAvOPuBAif/GxkYWL17MQw89hL2brIKEhIQe13U6nbz00ktkZWWxYcMGrr76apxOJ7fddhsAF110EWPHjuWZZ55BkiTWrl2L0ailpF533XUEAgGWL1+O3W5n8+bN3fq6e6Kj4FZv89P5+VFVBbW1AaW1PlbUCCKCLQ7RlgAmW8/uJpOk1V1xpmi9hfxulNYGCHi05YqM2lqP3FqPGJeG4Ez5Rbiu1LYmLSgYwGRDTM6NOW5VVWlwB2j2BmLWM4gCTosRSRCQVRVZUQnKCt6gJihavEEUFVITslBqdwMqalsDxv6jNeER8BEq24bnk5exnXghwuOPa3dbghD7Y9Uxl8ceQ/G48G9eqRX5ixKigsWOZczULsfWUXdHMFmxnXxxjGhrWbsO17p1gOaSGvX0/yGaTHvvCupmvADkbN+KPS6ODYOHgSAwpP9AMpoaeUxVGQkMVVVM5eW4Fi9mYU0N//3vf8MZU9OnT+ePf/wjqampqKqK8BNceDoHL7rA2Qf8yPi9n8zOnTtRVZUhQ4bs9bp33XVX+Hl+fj5z5szhjTfeCAuc0tJS/vCHP4S3HfZTty+bNWsWI0eOBKCwsO8FwHw+H3fccQcXXnhhn1ve6+x/1IAPuaE0ciEGEAQERwqiM3mvmx0KooRgjUO0xqEGvChtDageFx1WDMVVi+B3IyblhDOADkdURUZprQu/lhIyuoib2jY/rb7IeRcFgSSbCYfFgDeo4DBJMeu4fEFqW7V4l1ZfEFU1kOpMQW3fj+ppxDbtXDyfvg5ykFDlLtwfvYjt9EsQFy7s4kdXc3KQ//Yg/ng/oQX/iBUeRhPmEZMxj5gUrl3TgeJpRfVqMVliSmaX70jZf18OP8+9/FJN3Ozt3WBv44FEl4t+FeWU5OSCIPCb7FyeLS/lCGAgkAF8M2MGIUUT7KNHj+aOO+7gV7/6VXgbgiD8aBeezsHN4fvL8jNyoMS/2v5P/2PughcuXMhjjz3Gzp07aWtrIxQKxQiOW2+9lauuuoqXX36ZE088kXPPPZf+/fsDcOONN/K73/2OTz75hBNPPJFZs2YxatSoPe4zGAxy/vnnoygKTz/99F7PWWf/oAa8kaycdgR7ImJcKoJk7DpeVQkpqlbLRW2vxIt2bbIYpC7fR8FkRUrKQY0PorQ2oLZpJftVvzsmA+hwRGltCLtwBFt8l4DienesuEmymUiwmgjICvPXVFDh8mExiOQmWMmJt5KbYCXdYSYjzkK1SxM5bf4QBmscCYYWCAUg4EVKzMZ+2uW4P3kZAj7kunLcHzyP/fQrEWcUoy7/ArliF0ErhJQ2FNd2iC5HIxkwDTkS8+ipiNZ263CnDAo5Pz0yPDk2YcBTVEzjV18DWqZU2sknaQv29m5wT+OBvMpyGhISaXM4mJyZxZHNTSxoa2Up0AYcOWQIIydP5qKLLmJKuwsOQFEUxI4aQh0uvIqK7sWUIGjLo9bXOfjRY3D2AQdK/A8cOBBBENiyZcterbdy5UrOP/98TjvtNN5//33WrFnD3LlzCQQiJvJ7772XTZs2ccYZZ/D5558zbNgw3npLK/h11VVXsXv3bi655BI2bNjAEUccwZNPPtnrPoPBIOeddx5FRUUsWbJEt94cJKihAHJ9aUTcGC1I6f21miydxI2sqDS6/RQ3uClpdFPa5KG82UNFi5fKFi8VzV6KGtqoafXi9ofCArwDQTIiJWQgpvQDsf3eSg4i1xWjRlW8PVxQVQXV0xR+LcbFVhb2h2RavBFxkxFnIcluxhdSeG2tJm4AfCGFHfVulu6q578/lPHEl7spa/GRGRcRS+6AjBifEdl3wIMhvR+OM65CaI+JUZprcX/4PO5PX8VVugK3v4RAc0lMjyjB6sA87gScv/oD1glnRMRNVAxMR8yMesPvIsdmi9MChufPh2XLcK1fH16WOfMcxHb39l7fDfZhvAD0qyzTngsC+SmpPA1sBj7Lzub9Zct49tlnw+ImFAppc44qkIgkadkg2kY67SDiwtMDjA8tdIGzD9jL+L19RlJSEqeccgpPPfUUbre7y/Lm5uZu1/vqq6/o168fc+fO5YgjjmDgwIGUlJR0GTdo0CBuueUWPvnkE2bOnMmLL74YXpabm8tvf/tbFi1axJw5c3juued6nGeHuNmxYweffvopycnJe3+wOvscVZE1caNoP/iYbEhpBV16HAVlhbo2H8UNbTR6Asg9uAsAFBVafSGqXO1ix+UlKMdmYIkWB1J6YaTSrhJCriuKVOE9TFBc9VoaN2ip8lE9l1RVpa4tIuqS7CYcZiPuQIjX1pZT277MbBCxGGJ/pv2ywrJdDdhMEgZR+9FRFDXWCtbukpGSMnCccTWCTbuhUJprCZVvj4mvAQEpswDz5Fmsyfw9b22dxvJv7YTL1fSQQSGURvpeqY/8OUb8BO+4I7zMmpsTWWlv7wb7OD7B5QpbXlodDiTAJAgkPPEEiSkpWjfz9gMyGHpwXMycqcX/dCoeSE6OniJ+iKILnH3AgRT/Tz/9NLIsc9RRR/Hmm2+yY8cOtmzZwhNPPMHEiRO7XWfAgAGUlpby+uuvs2vXLp544omwdQbA6/Vy/fXXs2zZMkpKSvjqq69YtWoVQ4cOBeDmm2/m448/pqioiNWrV/P555+Hl3UmFAoxe/Zsvv/+e1599VVkWaa6uprq6uoYi5HOz4uqqlpNllD7RdZg0mqyRKVxK6pKbauPkkY3Ld5gTAtIm0nCaTEQbzGSYDWSaDPhMBtivv+KCq3+EKVNbly+2MwpQTIipeRDh5hSZOS6IhRPy/454J8ZNeANx8QAiHHpMcvb/CF87cHCRlEg0Wqi1R/i1TXl1Lu1/wuHSeLScbnk1hUy/8483n4slfoyzRLS4gvyxjtBpHaBI7e7CSMTiIhKMS4J++m/RrBGEgEEmxPjwLFYp56H86I7+MR3JQNPHMvU4w2xSU0Le46BEVyRmypVDcUsC7o94efGH37QrDuyvPd3g30Zn5yMlJWF3aPt0221oeTlwcKFeE87rX2YEM6k6pWZM6G4GJYuhdde0/4WFeni5hBFj8HZR3SI/+7q4Dz22P77/ygoKGD16tU89NBDzJkzh6qqKlJTUxk/fjzPPPNMt+vMmDGDW265heuvvx6/388ZZ5zB3Xffzb333guAJEk0NDRw6aWXUlNTQ0pKCjNnzuS+++4DtGJZ1113HeXl5cTFxXHqqafyz3/+s9t9lZeX8+677wIwZsyYmGVLly5l6r6MutbpM6q7EdXffoESpZhy/6AJoGqXNyZdWQDiLEYsJokGdxCvP4QvqOAPyfhCCgZRYFi6E7Mk0uYP4g6E2mN0oLbVR1BWSLJFKhwLkgEpNV+r7BvwgqqiNJaj+t2ICRmHbM0cVVWRmyOuFcGZGtPVW8uailhvUhwWVOCtjVU0ejQhGGc2cMHYbJZ+ZOLc2aCqZsCM2apw2m80l9LjL3kYPtpIh25RAlFuvk51i6T4FBxnX0eoYidSciZiYnr4c+g1qelckYUcyUy6xsGIrRERo8TFZnEGo6wkxnv/BF5vpChYH7K5wneDHXePvY2fNw9mzCDu1jm4V34HgkDT/Nf5ePcufvjjH7npppvIz8/vMv+eaHK5SDjuuF9Eht/hjqB2dpQfxrhcLuLj42lpaekSA+Lz+SgqKqKgoACLpWu1zr6itzH56eyrz0KnZ0I1uyCoxXhIqfldeiDVtfpoabe6CAIkWk3EWQxsqmnjs531BOTuC/8JAgxLczKxXyJJVhN1bT5a/ZG7+3iLkRSHuVMmkYLSVIXqaY5syGhGSso9JIOPtaJ+7S5fgxkpvTBGrLn9mgsPwGqUyIq3sqayhU+2axafOIuBi8bm4DAayc+PvWG68E9VjDxOy1za8UkKd96qCQmDANm+MoSOWjv2JASDBcFs65L9FI0s02Uf0Qio5FBGEQVIxH7mclYKbff/BgDjNxswP/8+K5hCFZlk5ZchpGnjx25cj9PjjgiShQu1v53vBlNT4amn4Nxzu06kuyqqubkxd4/V773P9gf/DEDBDddz1cv/YcmSJTz77LNcc801PZ6DaObPn8/nn3/O5ZdfzqRJk/q0js7PQ2/X757QLTj7GEnat6ngOjr7GtXvDosbjNYu4qbZEwiLG4CsOCtBRWXRxmp2N3roDVWFTTWtbKppZVCqnWP6JZHiMFPfHlPS4gsiqyrpTkvEkiOIiIlZqCYrSnM1oELQj1y7C9GZiuBIjumYfTDT0SW8AzEutYslqsUXcc0mWE2EFJXluyOBvmcOSSfeYmTZstjr+diTXGFx426WuObyyJ1TUqAuLG4w2/Gu/Ai5crf22mhGtMUh2J2IjkTMIyYhxSXDihWs+EymvLzndi4qAmXksYIpTOWLmGVKamL4+ba6DM6imHJyAbgi+G8u5wUAAh0BxtFp4EVFWpzQtddCXbsrr64Obr1V+xHtbPKeOVNLHe/l7tERVS6jbetWbrvtNgoLCxk9enSPx9eZQYMGce2111JUVMSnn37a5/V0Dk50gaOj8wtCVRXkpsrwa9GeELPc7Q9RH+U+SXNaKGvx8cHWGvyhyB380DQHGU4LFoOIxShhMYiUt3hZVdaMr33c9jo32+vcTMpPYkxmHDWtkbRmRfWSEWdFDIscAcGRhGC2ITeUa7FBHWKhrRHBnoBocYLJelC7DlS/O9Il3GBGsMbeaQZCctjtZxAFbCaJzTWt4XM2NM1BXqLWrTs6gShroI9z5kSEU1IwjuQk7TzYQm5sSvs+TXZ83y+NiBuAoNZyg5Y6ZCC0Yy3W15ZgWLqSKuVXwJ771VWhpYGrBgk1wYGS4CRwwhHh5X+rv4ZyIsG59YGU8POVpqM5g4/bT1B7GvhDD8G993b1i5WXw6xZcN99MHdurPl7D3ePtoJ8BJMJNRCgbdt2Tnjwfo499thwgdK+MH78eO644w4efvhhnn/+ea688so+r6tz8KELHB2dXxCqu1mrlQKaWLAnxiyPjg1JtGnWhXc3VxNStAuRwyRx6uB0BqTYu3HH2jgyJ5E1lS2sKmuirf1C/lVxI06zgYHJdqpdXlTAE5CpaPaQFW9FirLOCEYLUnohSksNaluj9qYSCldARpA0t4vZrlmejOaDRvCoqorSUhN+rVlvYov61UdlTsVbtayqHyoigdXjshPCzzsSiARBZebvazGatM+gfK2Dmy/W3E5KUCFZ0VLR1WAI79JXI9loRjNSShaq24XicYUzp1RVxnPB8QinH8X4lc2M/raSxjYrkqRgEBUMkoLdHKAgtZH+6Q0UpjdwXFoqrtRbUJ22bo+9uC6R6JyVhmAkU3KJ6SROZUmsi+vxx3ss3gfAn/6kxdY88USfAxhFgwHHwAG0btqMt7SUQEMjpuSkXtdRVZWSkhJSUlLC1dhnz57NJ598wt/+9jdd4Bzi6AJHR+cXRHSWkpSQGXMBDshKOLbGbBBJtBp5dU1FWNwMSrFz+pB0LEapl8ayIjNnJjI+O57vyppZXqS5Xj7ZXkvymByy4q1UubwoKvhDCuXNHrLibRilKJEjiEgJmaj2JBRXDao3qgKdKqP6WrUGoABiu+Cxxmudyw+gK0t1N0W5/ixdrDfuQAhPMGK9ibcaKWnyUtle7ybVbiInPhJz1pFAlDyojeyBmjCq3m3i3KmRbKg0GpFQUNpceL/5KJwerrVOuARDep42N1VFaajG8+LDKEnavNR4B+mnOPjilL4U3XTSkxxZV5LJt7v6xbxX7C0IP8+0V3Z1cTU27nmXP6KLd8JRR9K6aTMA9Z8vJevcWfj9fv7yl7/Qr18/LrvsMq01Q/v3fsOGDdx2221cc801zGzfR//+/TnppJN4+OGH+fbbbzn66KP7tG+dg49Dw7H9M/ILirk+aNE/g/2DGgpG+kIZTJEU7XY8UcHADrOB3Y2ecLG5RKuRM4dlhMXN7NlQUSVjTW8hcWQZWSduRh2+kQuu8LJoERgkkWPykzgiJwHQUsbf2lhFUFHJTrCF05uDskp5swd/sGtzScFoRkrOQ8ochJiYpQmGzi0jFBnV24rSWI5ctQ25sQLF1/azf4cUX2uM9aZzSwZFia17k+IwIwoCX5dELvTH9EuKWUeS4LHHVU66IhKfE+eJJ7u9rIxF9mJXPKiyjH/9V2FxYzQl4ph9U1jcQHua9MZtOO58CtsTb2D4fgsEY1O794Rgi0OyxGOobsW05DssCz5j9TPVnPqXq5GV2M+l0p9NczAegKH2zVSR0TER2Ns6WDffTKQoT++knXhi+HltewzN5s2beeyxx1i1ahUQ+/syatQoli1bxiuvvEJtbcQFmJubi9/vZ8eOHXs3V52DCt2C006Hn9bj8WC1WvcwWmd/0lEfp091K3T6jOqNWG9EW3wX1447ELng2UwGPtlRH359/IAUTJIYbg3kHFhN3pnrkMyxF574gTX85oYpxMebmDoVju+fQp3bT0mTF09Q5s0NVVw4NoecBBuVLVoRQFlRKW/xEGcx4jAbsRjEmLkJklFzpdkTtYtT0I/qd2uPgCfcCgFVQfU0a9lYooRgdiBYHdrf/dTvSus1VY/aGjlXgjW+S+B2g8eP3G4Js5kk7CYDO+rbKG3W3ElJViOD07o2rO0/wcWObZprqXaXhd/9qv2eVFFJDjUhAIFdG1FamxFrGrG+uhjDlmKwF8RYPcpffQ1rcTEJKhjX78S4fieK3UrwqGHIA3MpJ4dVynhcspOQIuIPGmj0JXDyrGSmnJaM6ExCMBg1oZGWFrbACJjxYqIrAlvdw5iQ8A0JxhbizB4ItH+mN96ouaD6dIL3rpGfrX8htvx8PMXFuNatx19TS25uLllZWZSXl+P3+7Xu4Wg1ugwGAzfddBOPP/44b7zxBjfccAMAVVVVBAKBmB58OoceusBpR5IkEhISwireZuula7LOfkNRFOrq6rDZbD1XHNX5UUS7pwRrfJfl/pAmFCRRwCSJlLVffJ1mA/2TtQt2R2uggnPLuogbAKPTj5BTzoknFra7rATOnp7Jf74vo9kXpKbNz6KNlcwemUVOgo2qFg++kIKqat2xW7xBDKKAw2zEaTFgkjqJHUEAk6W9a3my1gfL79GEjdcVqf+iyKjeloioM1kRLU4Ee+I+ETuqqqK6m7QgaCWqVpDViZgY25ep1RcMt2QQgFSHhU01rXy4NWLxmdAvKRxwHU3HZwBw+pHO8LmIV1wYCSE31xMs2oLgasP+yH+02jRRDStDqoqvqYlP//o3nIJA2rgjMQcCmAN+LH4/5u01mDaWY5Ukkoxb2WE4GovBj8PQRoYqsGWXSOgdifRMCcliITktldSmprDpfworyKGMCrJROzkENraNYELCNwCk5Nag+nMQHntMy4Z67rme+z51R3TEdS+1OARBIPWkEyh57nlQVSoXvknBdb9j2LBhrF+/nk2bNjFu3LiYPlRTpkzhn//8J3PmzGHLli1UVlby7rvvMnz4cPr169fdbHQOEfQrSBQZGZoZNdpUqfPzI4oieXl5v1iB6fP59nn9HzXgi00N76a+TLuBAYMoIAhC+NpjNUrhi2/HdUYORH46ar4uRAkayDxuOwD23EbqviuMCqGQmH1KJq+sLscXUihp8vLctyUcW5jM0DQHDe6A1hW7fXshRaXZG6DZG2jPNDJgNxmwmqQuIkAQBASLHSx2VDUT1duK6mnRspmii90FvCgBL7Q1IMalaULnR3y/VFVF9TSjtNZHgrW1mSDGpWhF/aK26wvK4c7fAMkOM6srWli6K2LxGZRiZ0SGs9v92UwRK2ZKJDEJu6K5GuW6CkDFsGFXpPCeqqKUlfHpP/7B61u2sPi996iur8cgCExNSub01DTGxcXTuWa0HTfH83nXSRRDU7H2tB4oHjWW7JoqMmprMCgKj3MTs1mIgBIjcj6sm8656f8j3thCY1IS1Xf+hcyzZ2gLO4r39ZWOiOueg7/CFquMs86i9KX/ogaDVC58k9xLL+a6665j2rRpvP/++4wbNy6mD9WaNWs45ZRTKCws5F//+heBQIDMzEzuvPPO8DVB59BEFzhRCIJAZmYmaWlpBIPBPa+gs18wmUyxjfB+Qdx5550IgsDcuXOx2+17XqGPKNFNHzulhkNsXIKA0PEETXVElnVcZ4KtEQHWujsVd1kSqUcVYbAGsec2ASqqKkSVPTFz7qgsXl9bQVBRcflDvL+lhu/KmplamEy/JDueoEyrL4g3Kh4npKi4fEFcviACYDFKYbHT1bojItjiwRavHU/Ag+JtRfW1RVpSKDJKcxW4GxHjMxEtfTvHqiK3C5sGkDu1nbDGIcanx/SaAs0iVtXiDZ+9OIuR0iZvjLgZmxXPSYNSu7XegOa66sDlD+Ewxbpt1aAmssT6iFypAm4G3v7jHxENBsaOGcMxY8YQamhk2cYNrHa18OzwkaT1UgCwN/xmM7vz8inJyiGrtobp1e+xMDSbm3g8XAcHwJaRgnDeHfDWnQDs+sc/cQwaiHPYsJ5Lv3cmuot3ryWXI8HI5rRU0s84neq330H2eKhc+CbHXaEV7ps3bx4jR47knHPOwefz8d133zFv3jwmTpzI448/zs0338yOHTs4/vjjdQvyYYBeyVhH5yBh+fLlTJ06lSuuuIJ//OMfxMd3dSPtDaqiQMiP6vegtFS3vysgZQ1CEGN/vGVFpahBKyJnM0pkJdj42xc7CSkqqXYTVx6lmeo7Kt/6s3aTdcJWABrXZ1P+8QjyZ64mrr9WtG3rvCn4GyJWiaVLtRCKerefpTvr2dWpYGC8xUBBko28BBvZ8WZUVavJ4+0m+LgDATAZRCwGCatJwmY0IIrdCwU1FEBx1aJ26nUlWBwI1jgEk1WrW9MuNLRYHy+qz43ia4sEZ0djsiHFp3WJtwFt7jWt3rBVzGIQaQ0ovLelOvze5PwkJuUnoarw/toK1hQ3kZdi44iCZAamx7Hya5HdDR5qk7SmloVJNsZmam6q1GAddsWLb/3XyFUlWF98D+NX63kfuA4oB6aNHcvvH3qI4cOHk5enBRy/8847zPntbxna3Mx8QcRvNrPBMJw/yffTEkqgJRRPSzCBNlmLBxIFBQkZUZB5b94uUja9RuOXX8Ucq93jZtzG9SiIWiVjxyAy336GKVMlJAl2/OWvVC3Set0ZExMY+vBDJIwdq60sy1pNnO5icqIrH8+YsYeSy+1CqKgIJAlveTmrzj0fFAVjYgJHv/8uX6xYwW9+8xuKi4uZPn06oiiybt06du7cyYIFC5g1a1bMJmVZRhTFX6wl+WBDr2Sso3OIEgqFuOCCCzjmmGO44YYbfpK4UVUVuWZXxGoRhWCL6yJuQGusGR7T/oNukkRCihwjMjpaA13++4h4SRpVgT23CaMz4orpTIdrK8Vu5tzR2RQ3eVi6s56acIXjEGsrXaytdAGQbDOSm2Al3WEmwaIFHvtDcjhlHTS7kj+k4A8p4crLVqOEzSRhMxkwGyLWDsFgQkrKQbUnaX2i2t11qq9Ns/BoB66570QJ1e8BtXtxJVgcWoVlc9eaMKqq0uQJ0OiJuK/MBpEGb4jF22rD1pzx2fFMyk9iW5WLB9/eyMbyWOGlBiXayhLxVSdx9W0qFqvA7kYPNqPIkFQHzVI8NsWLIT0PuaqEUP8cXv1qPX8APMAj8fFc/8UX2Jyxrq/TTz+dRSefzJIlSyh/5BGGGI1UbR7N8geHdf/BRd3+VlnHMvXvY3HPm0f5I49Sk5IKgoDbZqc+MYnUpkamCsvhPzfCCZFz3//Wm3Hv2o1r3TqCTc1suO5G+t9yM5mzZyJIEtxzD4wY0Xsjv85lnbue+JhgZGtODinHHUv90mUEm5pxrVvPtGnTeP3117nxxhvDzYVHjRrFvHnzuogb0JMcDgd0gaOjcxBw0UUX4Xa7ufHGG2Oakq5atYqdO3dSXV3NoEGDOPXUU/f4w9vTHadgcSImZHa7TI4SDh0p3Ek2I54WmbaAjD+kYDZobsOZM0FVU7jjX0OxjtmGaFAwJ0YsHHXf94ux3kDEtdVBfqKNy4/IZVtdG2srXZQ2e4iaAg2eIA2eiCtIFCDFZiLVYSLJaiLBasBmFOncEssb1ARZgzuAzSSRbDfHCh2zDSmtUHM3tdSCEpUq3e7W6takLZkQrA5EW2JM48xoFEWlptUXk41mNxmobPWzZEeks/jIjDiOzk3g0fe38MbK4pjjDs/TKOMsrMdZWM87i+M47+w0BEFgY00bZkmkIMmGS3ISn55DMCGFbzKSmSsIxKsqLwFnvvACOLvG9RiNRpKSkvB4PJinTIGCAjKXAQ92e0gxZKbJgIT9mmsYnJJCym23syk5FYDK9ExSHfZuOwuLRiPD//YXtsy9m+bvVqHKMjv/9ndat2xh4MSjEevrtS/Irl3w9dfdt2KIDjLujahxKdOmUr90GQANX35JwvhxjBs3jg8//JDNmzdjs9mw2WzhQOLo+jg6hwe6i0pH5wDz2muvcfHFF/Pggw9y6623YrFYqKqqYuHChdx6663IUTVATj75ZP785z8zbty4Xn+Q5cYK1KAPwWhBMJoRTLZe2xy4fMFwMGyK3UyCzcSHW2tYX6VZVC4bn0tmXOyFXZbhjQ/aeOCd9ZjTmwFwlyew69UJqIomhjp5DnokICtUtHgpbfJS2uylqtXX7YU/GoMokO4wk+YwkxdvwWIQkLtZJ85iJMluwtAprktVFVS/V3NFBbRHOL5GkBAsWrVkweLoEl8Tcx4UlVZ/kCZPIEYoJtmMbK1zs7woUutmXHY8ppDCQ+9soq41YmErTHXwuxMH8bubAgSdDTjyGjE6I8utDQXMOD9yP3pMXgLZThPZgSpCteWcc9lv+Gzjbp5LTOTiZ57B/Ktfxc5RlpEkiaKiIo4//niMRiPLly8nIyMj7HbsKalJQCGHcoqypyA98c+wgFFDIb4/exbe9l5S4156AcfQIV03QHuQu2ig6OlnKX/1tfD7drebIbt3YI/uNt5dUb9ly2DatG63HcN992kWISDY4uKb084AWcaam8uRC9/Y8/o6By26i0pH5xCjqamJiy++GIfDQV5eHhaLhVAoxAsvvMB9993H5MmTmT59OomJicyfP59PPvmEdevW8fbbb/daYVVKyu5xWWe8QZnGqBYNxnZLjcMU+Xlo9gW7CBxJggvPcmAKTuS3D5RjSvBQ+11BjLgB7aa+Q9yoqoovqGhuNFVFVTX3mEEUyE+0UZCkxbMEQgrVrT6q2/zUtPqpbfNT7w7EWFdCikqFy0eFy8eayhZS7SaGpzvpl2CNcWe5fEHa/EFSHBacZkNMk8+ODKwOVDmkpX0bTL3ezauq2u4aC9DmC8XMSxTAbjayZEcDRU0Ry9aEvEQ8rX7uWLAuEptjFLl62gAunVzIV1+K7FoKkAeoxA2opeDcHwDwJBbz4btZnDZdE6nflbcwrTAJoymVoFzFl9vLOXPcYC557VVMg8fHzFNV1bDV7/HHH6ekpIRHHnkknCHU4XacPVttjyuPCtxub6/wGDcjVZbFBPMKBgPZV1zGzkf/BkDNR4u7CBzF3YJ3+SJClbswZPUn/4qLcLha2P72uyiShNtuZ82wkQwq2kVab5WLO8o67ym1/N57NXfXzJkY4+OIHzWSljVr8ZaV4SkuxpafH3NudIvN4Y0ucHR0DiBut5vLL7+c+fPn8+ijj+LxeEhNTeXxxx9n+vTp/Oc//wn3yJk+fTp/+ctf+Pvf/86f//xn3nnnnZ+0b1VVafQEaIqKFzGIAlajREhR2FjtCr+faO25YeHsWQKikMtNN4EcKdtCTq7CHx50IeS18pf3WtlR7WJHdWu4JkxnrCaJ9DgLaXEW0uItpMdbyEywkpVgZUR+MilxZlz+EJUuH1UuH5UuP81RXc/r3AGW7W5AEgQGpNgZnenUUqZVLQW+ttWH228g3WnpMRhZkAzQS50cVVVxB0I0eQIxzUc7sJsMNHqDvLulItxAE+DYgmRaXT7uihI3UwancsdZw8nuprkmCLh2ptOwJpfksWUIoorLWsH7nzk5aUoKFrOBL4ubmNY/GZcaj8lsIq//ALxfvYNssWPtNyRstREEgfLycq677jree+89TjrpJC666KKYec9kEQuTPuKmhntiMqFyKOcxbmYmb2nxOFE1dpAkEsaPC48NtrbGbDO4awPe5W+itrsBQ5W7aF30JPGrlzN6y0a2FQ7EY7OhSBJbBwyiraqSgvJShKjth+lQYd3EynQhav3kY6fQsmYtAHWfL6Xfr6+InGFd3Bz26C4qHZ0DTEtLC++88w733HMPjY2NJCcnoygKW7duDVfV7rhYARxxxBHs2LGDzZs3k5WVtVc/1Gr7BT8oK9S1+WIu0haDJjCMksgP5c3huJHCJBvnjd6zRaij/lplpUqVUMXHJVupdfUcePxjSHaYyE91UJBqpyDVQVaiFQwiZa3+cE+nDiRRYFJ+IvkJ1nAHbwCjJJAZZ8Vk6HsQqaqqtPk1YRPoFPgjCuA0GzEbJZYXNbCxOnKhd5gkTh+Szs5KF3e+sTbswpp9VC5/PGsEqiqEa9YtWQIvvthpx4JK+qQdpE/eGbaIWU0GJo3OICXBSrzZwJgEOP/04xk+oB+P/eY80r0NOM64CilZC3x67bXXuOeee9i9ezcTJkzgX//6FyNHjozsIyr9Wu7IhCKTTKqYworYJpkdtKfFtW3fwepLLgMg4+wZDLrzdtSgH+/rTxMMNHRdrx1pWwnm/3zIbnsiNalp4ffjXS0M3bkd05Il3Vcuvv/+vlVBbp+fr6qK787WRJF9wADGv/rfPa+rc1Ciu6h0dA5B4uPjueiiixgxYgQPPfQQb731Fg899BBWqzUsbCRJIhAIYDKZyMjIoKqqCqfTGSNuZEXFGwwRCCntVgs18lBAVhVCstptEG2SzUSiTXPLNLgDMT2Sji3cc+8gVVUprm+jzFDHJy3VbChr7nZcapwZq2Jn0wYBVAFVFUAFwaBgdPgwxvmQTD2nhje0BWhoa+SHqLgWgDH9Ern02AIwSmysbsUTlJEVleW7G9kZZ+GUQam4/cF2cadS1uQhI86K3dz7T2CHxabB7SfYKcDHZBBJsJpQgTUVLaytbImx2gxOdXDq4DS+3FobI27OOUITN2+/LeyxDAyqQM2Xg3CXJZF31jqMDj/eQIjlayuZNl5rSrW5zcSFv76Gv95/N39JT+O2MydRO+9hfCNP5K9P/B+ftvdkuuaaa3jyySfDbWkAwr032u9zJZTYppg90W5uUgIR659oMqG0NuFe+H8oSpTLc9VmLAs+w3/GJALHaRYfeXA/PH+6koIX3sNZvJtdefmookhLXDyrR4xmxLr1OLoTOH1tndA+P0tmJs7hw2ndtAn3zp14Skqw6dWJfzHoAkdH5yBAkiTGjRvHU089xVlnnRXO7Oiw2qiqislkoq6ujl27dpGTk0NbWxtGqx1PIIQnEOrWZbInjJJAutOKxSjhDoT4qriRNZUt4TCHwakOMpxds4ZkRWVHdStrihtZU9LEmuLGmKDZDiYOTGHSoFQGZTgZkO4k3mreQzkTldyCEJ9+6aO21UdVk5fKZi9V7Y+yBjcNbYEu660taWLty02cMSaL608exOY6N6vKmwGodPl4a1M1547MxO0PEZAVVKDa5SU7wYbF2L0lJyQr1LX5Y7KiQKtpk2gz4w6EWFHUwKaa1piAaJMkctKgVIanOfjft6U8+v7m8PKzx+dw99mauOmuZl1PtJWksP35yZw+dw0lbY0EggqfrSrj6OHpkO4k98TzOX3jBl5b9BYvzl/Yfi7/D1XVLH4PPPAAp5xyChBrDQz33thb2tPiOgsc/+aVEXHjC2B97WOMX69HAKwvf4Rx1RY8l52BmpoAZhPeS08nc87j2D0etgwYRMBkImAysfG9Dxh7/vmY09Ni95vW6XVPRI1LPWEarZs2AbD78ScZ/ve/drF6Bos3o7gaMI+aslenQefgRhc4OjoHERkZGfzqV78KNwQEYvrmLFq0iF27dnHbHXcSMMdR0dxNAboeENC6fBtEAYMkYhIFfLLK6ooWdjW4qXT5Yqw7cRYD0/pH+gN4/CGWb63lkw1VfLergTZ/qMs+OihMdXDzaUM4dkjsBWnP5UwESncbqdhmZOrU7tsXuLxBiuraKKptY3ddG19urWN3nVbL5oO1lSzbUstvTxjIBWOy+GBrLS5fiGZvkDfWV3L+6GwCIZk2vxYYXOXykuawYDFK4fR4VVVp9Yeob4vN5LIaJRJtJvwhhY931LK9zh0zL0kQGJruYHJ+Mv6AzA3//YGvtkfSw2eMz+Gec0aiqkK00aTP3H27mTlzjuA3z3/LxvIWQrLKV+urGV4QYET/JI7/7VzOOfc81i/9kOqinQyPkxh42q8451cXYfr2W5g/HzktDSnaMhIV+KMAe6wfHl1ZGJA9ke+fZLVCWeTDtc17C+P6nTGrG7YW47x3Hu5bL0Lunw12K6ER/Ylfu52xm9azeeBgWh1OAq2tbPrDbYz+1zPadn8C6WecTtnLrxBsaqbxq6+peP1/5FwQyTLzrvyQwKavtcOzx2PqP+on7U/n4EEXODo6Bxmd+1B1iJu33nqLJ554goLC/lx+w+9jit6BFgtS5w7i8ocIygpBWSWgKARCCiFFRRSEcByHIAg0eQK0Bbq6g4ySwIS8RI7KTSQkq3y8voolG6pYsa22RyuRzSQxul8ixwxMYdKgNApS7d3GBv2IciZhIj0WjWRmJjJ9SiKSBDedMoQ3vyvjsY+24QkGcftD/P3DLawtaWTu2SNYsKGKJm8Qly/EG+squGhsDiFZwRfSOplXubTIaJNBxGqUCISU2OKGgkCKw4zJIPJtaTPfljXFpINbDCJjs+MZn52A3STx4dpKHnlvE62+iAC8dEoBN58yBFEU9ijyemLgQLCbDfz76gk88NYGPlhbCcCmokaa2/xMGJHBoBETuOXYKWQGq/Gt/ATB3Yhx0CDk8nIkQILYdOx2S4xKH8UNxKTFyd5IVLlksyK4I338lslTMJLdJY5H8Acxv7cCz83nAxCYMALj2u2YQyGG79jG2qkn4GtpoW3bdrbd+wBDH34QoSPFv699AqPGGRMSGHzP3Wy8ZQ4ARf/3FPFjRuNsz/gSbREh7V3+JqI9DkNGft/2o3NQowscHZ2DHL/fz4IFC/jjH/9ISFZ47rWF4WWW9kDZLXVtbKxuxd+58t1ekGIzMSDFzhE5CUgCvPJlES8t391t1lOi3cT4giTG9ktkXH4SAzOcGKQ99w/rXPCvr+N667EIAnNuyqOqIYPM47aRNKYMQYDPNtVgNxu4bfpw5q8tp8ETpMUXYsH6Si4al02NyxcTVxMIaWIwGofZQKrDTJM3yCtrymnxRRfxk5jYL4lRGXGYDCLegMydb6xl8fqoYnNOM/ecMzLGktVXkdfTObEYJR48dzSDM+N4bPFWFBUq6tys3FiNxSSRYU/EYXRgHzwW36rPWDXoZI4qfyGyoeh0bEUBSUKQZcqBOcAZwAygSy3t6MrCALKM3J6hBCBZLGyszmJgvGa1qph8Mn9vOY7m8hCPc5OWidWO4bxLAO17FRo9ENViQkhNx/TYYwwfM5a1V12D7HZTv2wZpS+8SL+rrow9CX09We0kHTORnIsupPzV11BDIbbcdQ/j/vMiBocd08jJyC31BLf/AIqM59NXcZxzPaL9p7VK0Tnw6FlUOjo/Ex11Nzr/3RM1NTWcffY5tLS1cdNtcznh1NMR0FwKG2ta2V7n7r767h4wiAL9Eqz0T7FTmGQnwWrEF5RZ8G0pL36xi0Z3bKxLot3EiSMyOHlkJuPyk8Iunb1hj0XluikM2FOPRUHofhvOwjryZ/2AaNDEynNXHs2QnHheWV0WFihTC5M5Ki8RT0Drd+UNyDHZUZIokOawYDcbKG/2snBDZTiAWBTgqNxEjs5J4ttvRCorVRoNtXxavpXi+ojb6rTRWdwxfRjxttgigX2tWdfTOYlYsqDZWMcrm9bgbncXnjkpn/wUG8flJ6KWbaBpw0pyPB7W3r2RM5oWxG40KQkaG8Mn8QPgbmArcCXwZMfYjrTr6MrC7Yqz2udne+EAANwN6byU8ifmXR0R4ACri7J487uRXGV7hYE5jch5majJsb+/zrxjEI8/Jbz9xm9WsvHW34OiIBgMjJ//Cra8vB/3BWpHCQZZd81vad28BYDUE09gyIP3a/+Liozn4/8SqtwFgLFwJLZpv+qyeZ0Dx4+5fusCR0fnABIT8NkDQVlhV3UjxcXFDBg0GKMkUu8J8unOupjfeEkUGJbmYFCqA4fJgMkgYpZEjKKA1P5QVc0doaK5rERBoM7lY0uli80VLby5qpQ6VyRYWBDg1FFZnD0+h/EFSX2y0uyJDsECsdeo6N6KUUaCXoOSeyJpVBm5Z2wAYEC6k9evn0SjN8iLq0pR0QKBfzOhH/aoYoayouILyqioWI0GJFGgrNnL/9ZpHdAB0h1mZgzPYNliEzfdBA1yM1nTtuLoF8nqspkk7p01ipNHdm9tCATAZtOObU90PifdWbIGnLIL+7htAIzsn8zwwiRyTX5ee/Ru1nz7NS/fcjH1RcOZ/a+ru0/5jkIFLgfeA24QBO574w0499zYQVGK02Ox8P0orXHmxpYRzNn9JA/96mMumLgGm7n7ekedD9A8ZhqWccd3WVT0zLOUvaSldSccdSSjnnw8dv+w5y9QJ7wVFay+5HJktyZEB9z+B7JmnqNtyu+ldeE/UX1aXJH99CsxZBbs+Rh0fhb0NHEdnYOQxsZGPvnkE5YsWUJFRQVHHXUU/fv3Z9asWeEiftGBxNHIikplixeDyayJG1GgpNnL1yVN4TF2k8TYrHiEoMzb35czr7oVX1DGF5DxBbU+UoKgtSxItJtItJtIsJsIhBS2VrZ0m5UEcPLITH57wkD6JTtYsQIWrOraIqjLfKOsCz2NnTlTuwb11luxgx+b5NO4PofkcaXYMlvYWdPKm6vK+NWEfozOimNtpYuArLB8dwOnDUkPryOJQkzaeEWLlwXrI+ImP9HGOSMy+eBdkQt+7SVj6lYGDYv1N7nLE/ntKSM4eWT3AdKgtVvqi7iB2HPSkyWr5JsshrULnJKqVoYVJFIeMOOXVRqbW6izp3HqMRWsXX4m47e82+O+tE5T8DBgBR5UVc5qbGR8zKDYtHKrz4fZ78NvtjDIuQ1khTmvnMX9b57ErKPXc+mUHxjTrzJ2R74AUn0L0vGnYhxyFIbU7mss5V1+GbUffUzI7Sbv8ssiC2bOhDfegGuvhfr67k9WD1izsxl01x/ZcudcAIqfeZa0k0/G4LAjmK1Yxp+E9yutgKZ35Qc4Zlwbif/ROeTQLTg6OvuZM844g08++QSj0RiuZxMMBhkzZgwPPfQQp512GqB1FDcYIhfY77//nvXbdnHUlGlYrFaMkkitO8BnOyM/6kfkJHBMv0SeWLyN11eW7JP5Th2azu9OHMjgzDgWLYIbb9Q8Ah1kZ8MTT3S9jvQWJ9PdNaezGDrmmK69Fv/3P7jwwh93HLbsJgZe+g0A8VYji2+fhgLMW1kSjlW6+qh+JNu79pnyBELM+7Yk7JYqTLIxc2QmgipSMDCI88zlGB0RS5e/0UbV0iG4dqSTkyP02ntr/vy+HdPFF8Pzz4PJtGdLVv+LvsGRp4neI4em0T8nHjHkI6GpiOkTRpBYt52GxZ+Tde8/EEJ7VlfLgbOBs449lpe+iKqL041/bUe/AqrStbYP79dO54nSW/ArkUD50XmVHNW/lKrmOK4qf5iz6l9BUAkX4+sN18ZNSBYL9gH9I29290VLTYWnn45YdvbAzr//k6bvvmPYQw/GbFtVFNrefRalQRNl1mNnYho4rqfN6PyM6BYcHZ2DjDvuuINly5Zx++23M3fuXMrLy2lqauKhhx7ivffe44wzzuD666/niSeewGAwhONyampquPLKq9iwYT3Pv76IqcefSJzFyJsbq8PbPr5/CjlOC1f/+1s2lbeE3zcIIv42EyG/iBKUUEISZrNKSlaAoBAMx2uAduEfkhXH4Kw4hmbFMyInntxkrTfTokXdV8avqNDef/PNiHDpybrQW3shSYpc3xYtgv79u4qjq6/e61MexlORSNOmTBKHV9HiDfLx+irOPiKXif0SWbZbq7K7prKFEwemdln3y+LGsLix+K2kNGriZsUKCPXbFRY3Ia+R6hUDaViTB+09uMrKNOHW07W7r3Gyr7yi6YnHH9fCZXqzZDWuyw0LnFVbagmEZIb0S0TJHYnbnEhiej7xI/rhP3Uilve/3GNK+LFAKlDl8SDLMqIoavFi3URIpzY2hAXOmWnvMcK5ge9ajkZRRVREZEVE3m7AJtuJV4tpiE/EIIcwbdqE9dhje7WQxI0YHnkhy/DQQ91XMq6vh/PO69U9FU3hjdejBoNINlvM+4IoYj36NNwfPg9AsGiTLnAOYXQLjo7OfqK4uJhJkyYxadIknn76aVJSUmICi+fPn8/cuXMpLi7mhBNOYN68eRQUaD7/8vJybvnD7ezetYvX3/+EVIeFL4oa2FKr1XsZnRmHQxCY+791uNqznEwGkROyhvCX6/NQ5diLRnR4whnTZZo92jppceZuA51lGdLToaHnavskJ0NNjfa89+J9vXcU7y2IGLrEwvYZQYB+o5uJP02rcTIyN4GXf3cM3qDM/31dhKyoWAwi1x9TEBNb1OAJ8NzKEhDA7xX428X5tDUZyMmBM8/18pXhC0SDghIS2fbcsQSabV32/dprcMEF3c9LlqFfv1irWG/HAJoV7fHHexupknXCFlKPKg6/M7hfAmMGpnB0bgKJ7gpWvPUao4L1jHrubZz1zfTcXUwLNJ4oCGQPG8b69esj7tMeIqQrUtPZnDcEs9S9u7MnLDnZZJ59NhnTz8CYkNDzwO6sNp3p6YvWF79pFKqq0vr6o6ieVpAMOC+4DdHc9TPW+Xn5Mddv3bmoo7OfUBSF+vp6hgwZQkpKCsFgEEEQkNsDMC644AI+/fRTjj/+eD777DNuv/126ttjCjKysrjt3od49uXXkUSBJm8wLG6sRpE0i5GbX/4hLG5ykmy8cPVEXn4wv4u4gYg4uPlmMIgS6e3NLHvK4lq2rHdxA9ryZcv2HCejqhGrRmc6hXR0O+eO552nGv26p2V/uyuewZnaj+GGsma2VbmwGiWGpGqxT76QwuLttTF1bV5bUU9HQ+3l8xNpa9IM3RUV8OHObeHsrPpV+d2KG+jdSiNJcM01PS+PpuMc/Oc/exopUPnZUKqWDg6/s62kmbU76llX0cwpp57OzQ/8neMf+Q95rjZGGCUuBJ4FvgJ2Ah6gBNiBli7eAlx++eWxsWEdXb07nfDsuho8m+LY4e5jK4V2fOUVFP3fU6ycfjZb772fQHdfug4FvIdgLLW7L9qiRZr6njZN8wtOm6a9XrSox+0IgoCxoL1XlxwiuHPdXh2TzsGDLnB0dPYTBoMBURTZuHEjwWAw3ANIkiSt6aWiUFhYyKeffsoFF1zAwoULWbx4MQCtvhCJSckkJiUTZzHxVVRvqKmFKbzyZVH4onzckDReu24Si16M79Uq0JvQ6MyyZX07xmXLflrxvr6Io4YGuO8+LfYnmpwczU325pvdL1u4EGbNEph1ZKQ79kfrtNiK0ZmRO8CN1a38a2UxP5Q3U9rkxW3SMmxa6iVWLEwMjzMltZE4Qls/5DFS801UXEg7ggC5ueFCv90iyxDsQ4JRB6oKzc19GSkQ2Nqfsg9HorYnS20raWb17iZmXK8F1Y4eMYyzph1DWv9cFpuNXAtMAQYBecAxwGhB4CNBYNKkSZx66qmxu+jo6t1xsFHM8L9P4qZW7tj4V67d/C+u2/wsN2x5ioe23E3L1kSG7tzOgKLd5JeVknPkeBKOPCJyjIEAtR8tZveTT8XurxsF/D3wa2AqMBN4BmhE06QqRL5oPQmjDr9pNyKnw6FhGhwJrQ4WbewyTufQQI/B0dHZT6SlpTFx4kTefvttXn75ZS666KJwCwZBEMLWHEmSuO2223jvvfd49tlnOeuss/BHORAMkkB5i9YpO8lmZGSGk3XtWVQOi4G/XTSO994R+9RkGX58obme+LHF+/ZmLv37w0svRYTX1Knao8PTMGNGxAvR0YaotlYbf8zINEDrRVTWoImX3AQrg1Lt4XYLLn8o3D29g6WvJBH0Re4BU44oDj+v/aY/aqCrk0dVtfikFSu694T0xdPyUzAY4IaZuTQkBPiiWsus2rCzgQE5Yxl57MmUbFzN7bffzowJw6lZs5KSdT+wbdkP7PphKzu9fsqBKccdx7Bp07jpppuIj++m2F1PaXBJScwMfsYM1zt77kg+oD/ccAPeikqq3nqbigULUYNBmld9H1sfKkoBK8B/gWsAM3AkmivtPWAV8GcgA7QvWg+mQS+wVFVZAqy58EIaBw3C4XAwceJELrjgAo444ghUVUVKTAfJAHIINdi1x5rOoYFuwdHR2U9YLBb+9re/kZOTwwMPPMAnn3zSZUxHDZzRo0dzwgknUFlZSSiktVrooDqqieWAZDv1bYFwEb4ROQmIiNx0U9/n1RdBsofklphxPXgtwvRm1eirOPrd7+DEE+HBB7XH5Rf4eOf3KzQFI8vhgGWzGS6/XBvb4ZGYOD6SJdXsDrbPSWDmiCwuHZdL//ag6mj8XoE1SyJWHtEcJHGEZh6TAxLnHZPbxWrUIWYee6x7T0gfPS090p3W6ExDA9x7LxybXcgJ2RF31c7yFjJOvRmv388j//ccK6qCJBxzOhMuuZoLb/8df3zsVl6+8iy+zkrhtauu4p577iE+Ph5F0b6HXUI1Z86E4mItE+q11zQTW0MDuFzhjuQX8DpT+aL72ju33AL5+Vi/X0XhTTeQMF4L5A3U1+OLNkNGKeBv0AoQDgKWAa8DHwI3AS8BjwPk5qJOntzFNNgxg++BM4F/A4rfz+CkJEKhEE899RQXXHABr7zyCoIgaMcta8H4gth7nSqdgxdd4Ojo7EfGjh3L/fffj8vl4rzzzuPvf/87Pp8vvLzjwuHxeHC73djtdjweT7hlgDHKegOQl2BlS0UkY2poVtxe1YrZk/ukg6lTtSDi3khOjlhRevBadNe+KIZjjuk13jNMa2vs64o6E7Mfm8SiaU+ElUSPHolSCSWo/dQ1e2KDYLPiLZw7KovLj8hlUGpE6PzwURwBb+TnMWlUOZJJi51q2pDN7BnG8PX95pu1MZ1r25SXa9ac++/Xivv9mAab0bS07HlMJNZK4I2H+1Py7mhUWfsQalwKA2bcwrpvv+SfL7zK6maRmvgBqKOOxTLpNJSzTqHt/t/QRi1yYzWqqobjb3Y//gRrfn01jV99HdlZh6o87zx47rm9P6COdLz77ycu6ovTsjYq5qVdAcvAAjQX1BxgPJAG5AO/B4YD7wDld9+NYDB0MQ12fJKJwP8BpWiVm5+7+GI+/vhj5s2bR2VlJQ8//DDbtm1DiK4NLumOjkMVXeDo6OxHBEHg8ssv5+WXXyYlJYU//OEPzJw5k08//ZSKioqwKf7dd99l7dq1HH300WRkZoV/Xo2SSG1bxIKTHW+NaQcwMMO5Vy6nnoRGZyQJ5s3rfcy8eZFtdXgteoqF6Slzd2+K3kWjtv903cxjyOVVyLPO46ZrPD0GK8s+zZ0U3QAzmgynhZkjsvj1Ef348PFMFs9Lid4CyWNKw68MFflh99OUKdrx9caf/gRZWfvPLdUZVdX2VVEBzZuy2f2/I5H92gcl5R1F4tBjWbrwZV5Y+D6ravxUmrJwJRVgGjcVy4STER0m2t76P7wrFqF4te9a0zff0rppExtv/T07//YPZF+U2+bHVmNs/7B8Dz1E/Wefhd/2LlkSGdNuHmwBlgLj0FLYQRM9Cppb6kynk62CwMpELWZKSY8UcIxmBHAtmtBxAgkDBpCYmMill17KNddcw5YtW/j0009RmuvCNx+CsWudJJ1DA13g6Oj8DJx++ul8++23nH322Xz66aecc845XHjhhVx11VVccMEF/O53vyMuLo4nn3wSJeoqLQlCTNdwi0Ek2khikMQ+u3nuu69PJULCzJypBfDm5MS+3xHc23lbnb0WS5dqGbu97fOnxAOpiJSRxwoms4IplDf0nMqrKtpZ8/l7N6GkOU3cfKmDUFAMW5+smS1YUrQLfVtpEv+43xEWdn29tu8pI21/0lacws5XJhJo0Yrv5ZxxC0G/n7ef+TtLvt3IZ7sb2eW3Um7Koi0pH9P4aViOPgm5roy2hY/hW/SfGBNa5YKFrPn1lbh3aX2bfsqH6LI7WDNsJG6bZj0zhEKk/fu5iG+v3TzYhJbdNRjoCBmXaA8qvu8+Uu6+GyCchahOntxnv2mwPeI7u12dh0Ih/Bu/CotlQ86gH318OgeWQ0rgLF++nOnTp5OVlYUgCLz99tsHeko6On0mKyuLBQsWsGDBAs4++2zWrVvHCy+8wNtvv83ZZ5/NwoULsVqtROkZLRC5/Q1R0F6bDJF/22BI2WMMDGjL587d+znPmKEF9951l/b49FNNxPQkWjq8FhdcEBsE3BN9FWe9UUUmVVp46R5R+mAt6myNShoZiQm54NjsmGPf1wHb+wtfbRzbnp9C/epcRIORvLN+T/zw4/m2WGb5hiqW7KhnRamLopCDClMmnqR8LEefhHHgKPxtJQwZncRAbxNie3qWZ9duVl9+JRVv/A+1B2tJb6hAbVIy64cOJ2jSLCQWn48xmzdg93o1v1+HaW/mTFx//Ss+IBMI21NycjTz2MCBCEVFgCZOgD77TVVRxGg00tDQwIoVKzCbzRTm5RLcvUEbZrJgGjQenUOTQ8q56Ha7GT16NFdccQWzuiuxqqNzkCNJEjNmzODMM8/E5XJRVVVFXFwcOVFmEjXK/y8KhC06YvsPszFK4ARkJfxbPnt21w7bHb/ljz/eN9dUNN1l/Lz0Us+tF0C7Ji1b1nO2U2c6xFlPzaH7QiZ9Vxl9jRedOVMTd8u+ULhraSVeGcwGkdsvjQgpWY4UOjzYyMnRzmdlZeS8Kn4jFR+PpGljNrmnOYgf2IaqwtaSZirq3Jx0VC5VrX7GZcVRmJSEW7GSngNScjpe4WNs+VmMeXIh2wQbbpsdNRBg1z8eo8hkImnkaOLraolzuXB43KiCQNBgJGAyEjAYCZhM+MwWvGYzPosFr9mCHNWWJN7lYtjObRjbBYpaVYn89D+Q4y3IFonGFBsIAvGXXw4nnQQ7dmhxP/fdB2jZUQD2rVsjJ6EvTc/aT868efN47733uPrqq5mc5YTGkHYzMWCM7qI6hDmkBM5pp50W7tujo3MoI0kSiYmJJCYmdlkWI1CiHFId7zuiGkI2t2dT7U0Dy77QW+uFWbO0ZUOGxAqYRYu0AnbR7pgHH9SCkefN6zqHjgKzs2drc+wszvaMQi7lTEEr7JNDGRXkoNLVlKUENWXjk4MEZQVjH7qiSxKMPjKI91PNhTG+IAmHRYvl2d/p3j+VCy6Ao47q2ggcwFuZxPYXJpM6YTfpk3YiGhRaPUE2FzUyZlAqqytdWAwiWXFW2kQ7TisY8wYT3LkeZkxmzKMvUzx4KBV2raGoEghQb7VRn5cPgKAoqHvRoDLD1URBoBX1qGF4c1KR++cg98sEQwsEWyAI7i3FgErIV4NXrsP4+otIUdlWHd3Zkp56Co4/HuHss7U3OpRqN5WMO9LRn3zySebOncvEiRO55cJzEDctDy/T2zQc2hxSAmdv8fv9+P2RYDiXy3UAZ6Oj8+OwGLWLs6yqBGUl3CsKoCQq4LiX3/K9oi/VhTsCazsEzK9/DX/9a/fba2jovndVZ4GwdwJHkzGPcbOWhiwIPJ70ALMb/6UVfOtkxfLVxmFJcRMIKeyqaWNIVt9KvVuMkQu13L7RnsTfwcQLL2hxUN0hijB7tsgbbwygtSiFARevRDQobC1uoX9OPE6biW/LWjiuUEKwJGALeJFyBhIs2oRcmE3g7OPov/BzUl58ibJ162lZtx7ZHfke9iRuRJMBS6IdS6IdR7wFa4INU6IdMc6OZw/HYzEaMBsMNDU24mqrxDLnIsSSKswvvIdUUUd7NBCZoLm3pk/XDhRim551zLFdwDz44IPcc889HH/88Tx53x/J3LEcOsTNiElIKVl7mJnOwcxhLXAefvhh7ms3YeroHIqogDXKJeULKfRLiQic6Iwq6Pa3fK/Z26SYhoaexU00v/kNeL2wa1f3/RKVbsql9EQy9czjN8zkrbAfbua8U1mI0K0V69yT41lSrrmyNlU091ng2EyGdjchtHpDvYq/PdHRKumvf4Xrr9f6Q+4vegtqlmWtS3tyMjRWJ1D3bQHpk3aBoLL8qxbOOCkVWVX5trSZkwem0GSIJwUFIaM/asV2AqdOxLC7gnizifh//h1VlnHPe46W++6jxeHEY7VhkEMYFRlnYTqWAZkY+6UjJjj6PH+xqh5pVwWGXeWIVfUU5qSRbDGxtrgKty+AxWggkJOOcveVNCxaxo5PVjKY9kJ/ZWUIX37Zbc+sDgRB4KabbuLJJ59k1qxZPHrXbSStWxyufWMcMAbLUaf0eb46ByeHtcC58847ufXWW8OvXS4Xubm5vayho3Pw0WHBAfAGZdIcZtLiLNS6fBTXtbW3fRD2KvalN/ZX4Gx9PVx8cdf3rektOAvqkWwBDNYAkjWIwRpE9huoXVmIuzRSkMdpCfB70+PMdd0RKSAX5YebSfdWrHWlCSxpL9WypriJWUfm9WnOoijgsBhxeYPUunws/UKhvHzvczOi6wHNnKlZgFasgHfe0awtP7dxWVUjj9qV/UkaXYbREaCVZrasTWboGBF3UKaoycOAJAfxuHAljcFZsR0Az2VnEJeehoDWIsHx54dw1FSTXVONnJZI4LhxBCeNQnV0ymyTDIg2J4LNgWCxIbg9UNeIWF2LUFGDWFSB+PX3SK2xNp0BO8sZDCzdVkL5vLdIPf8kDNlayepnHRa2CXBfcjyZ9VqxIKG6usdjDwaDXHLJJfzvf//j15ddyl0XnUXiusUofq3elClvMNYp5yAIh1QOjk43HNYCx2w2h0vj6+gcTKgBL6gKmGxdGl52jiCxmyIqpc0fIs1hZmCGk1qXj2ZPkH/P93DnjfY+x77siX2R2dQXBINM5tRtpBxR3GMGmLOwjtpvCqleMQgUkWf+beKi82+FFUf26Ifrzoo1NDsem0nCE5D5eH0VN54ymLQ4S5/mOSQzju92N1Df6uet9TvRaun2TkcH9A46NNiMGZH+XTt2aO8dKDrmpwQM1H1bSNYJWoDut0slho7RTFQtvhAIAkHRSEKSwgfrBnPq6G1gtyIPG6BdQNpNfqrFhPeCkwlOGh3ZiSBgyOmPkF4A1nhMtt56mGuoJWUob7yFsOBthOra8PsPA8cB520p5pb7/k2/I4bxbaKTJz75lkEZKcy85QL8jS3IG3axs62RNS//lxEjRjJmzJjw/5jL5eKcc85h6dKlzL10FjdNzMZYshrQxKyUlovthPP16sWHCYe1wNHROVhRWutRvS6QDEgp+QjGnoS4SrwlclFoaS9UNy4/ka+2a72T/vhYDQ0NhV3W7C72pYNPN1ZRXO/m0skFmAyxP+Z1sS2Z9gu27CbyzlyHOan36AtBgPRjduPMb6Dk3TFkZ9t/lB/OapI4b0I/Xlq+m6Cs8MqXRdx6+tA+rXvr6UO5+OmvCCkq39TtwpKWga+2dxfX//6nTTNag73zjlZ0+WAMTPbWRI7Hr/rpSMYOtLcMURGQJJUvtvTXBA6guOohox9UVREqyMJ79dkoaZGgeTEpC3/BBOwpke+2CiiIhAQJBRGTGuzayqFfLsptN8Kc6xA+X4Hw+psIny3nKFnmA+A2g4G7QiGE7zYhCXDGwDzmnHciOUlxyElxyANyeeWdN3j0/S+54KQpPHPnDRhQUeUgU383l7U7SxiVm44z4OL91VswSCJWkwlHwVDszkFMFCR0eXN4cEgJnLa2Nnbu3Bl+XVRUxNq1a0lKSiIvr28mZx2dA42qyKi+9sJpqgqG3tNQ4y2Rf9Mmr5Y1NXFgKk9+orkLMqdux1uVgLs8qdv1b7pJsxx0GDm+3FbLXQvWE5QVXN4gt54WudDLMkR5dfc9gkrGsdtJm7CLDg+AEhLxb8+nYVsybfUWZK8J2Wcg5YgSMo/bhiCp2LJaGPTrL/E6RwDZve6iJy6ZVMD8r4vxhxQWfFfKr6f2J8G25xTgIVlx/Pq4/sxbuhNFVel/zjq2/HsSitzVhdERZ9PZPXiwByb7GyNxXaLNQ0TgaBNWETEaYVtVpMKz3KwpYb8thO+Oy6AjM00WsPYfizh4MFbAI1pwSXH4ZAOqJCGIseY6o6BgE2QSBB/i0k9RjxyrBQhLEupJU1FPmgpbtyPdcAfHu7189Kc/UXbVVTQBZhUGby8l/ovV+E1Ho2Rp8xuRm8ZxQ/IZm2JF2bmGYLsFZ0dpJaCyvqya9a9X02EvFUURs9mMLMs0NTVhtVr36fnVOTAcUgLn+++/Z1pU4FhHfM1ll13GSy+9dIBmpaOzdyitDeErnWCN6+Ki6kxy1EW4rk0TOMOy45mYncs3FWWIBoWCc79n56sTurUslJdrXoQOo8f8b0rwBbUiav9bWRojcH5s1f0+ISrknbGexBGV4bdCTU5OOjaZ9NMkoJmWOgPVu01U7TZTtiWNXa8kkTt9LeYkD5JJ5k9vrueUsVmIYu/nrDuSnWbOPiKXN1aW4A3ILFhZytXHD4gZs7vBTVGjh7xEK/2T7OH9XD1tAEu31LCjuhVDUitpk3dQs3xwtzWHOrfD+CmByT8XwVYLSkhENCgY4rzIcgKSRLjpq9IuBHZUp4bXUZrrUFqb8DXsCIsbaVc5lgHjYLDW6FNGpEZM1USN2NX9ChBURVpUEcVsJU1wwKRTUc47G/W8cyC73V86ZBDy808g1XpIPfscUhMTY9PwVqzFVNyA8ve/EBxRyDl5Q5h+1CgIxfYeK3/qD/iDMt74dLyZg/E6U2l1e2hqaqK5uZnGxkZd3BxGHFICZ+rUqV272uroHEKofg9qa8QHJNq7Wl3kqO+4JAo4zQbsJgl3QKay1RdOcZ2UOJyPV3iJK6xHsoQoPP87dr48kUBT1+7Y0YHDEwemhN1b08dm9zhuXyIYZPLPWU3cAG2/qiKglvXjvEuMmKK8c/GpIeJTQww+WnNdffqfJJa+MJnBVy/HFO8jpKq9VmzuDVmGQcYCoASAhd+VcsVxhRjaL87N3iALN1SiqLCqvBmHSWJERhyjMuNIspm4f9YoLnnma0KKSvoxu7C0plK8OvL5da451FHn57PPDk63VDSiSUY0tIsZnzEs0KR2gdfRfLKi0RleR/W6CdWVa7FkgPGbDVjeXIay6kYAGpsl/vdlFqfOinxg27eIVFUKVFWIeD1w7vkyjgRt/VZ/COtpZxIHSDfdhPrYs6jHTUK57w4o6Ad5OSjDUzT3UQ81EURJwgyYhx6NKodQmmo15WkwIhhMCAaj9lxvoPmLQP+UdXR+JlRFRm6MXOnEuDQEU9dA16AcETgGUUQQBLLjLGyvd+MPKdR7AqTazeRkiZQsGkfh+d9hz2nGaA/Q/4Jv2fnyRIKtsXeh0YHDF08qYOKAFJrcAcblJ/U4bl8hWQIUnPsD9pwmQHNJ2esGMuOqSOxFa7WJxjqRtPwAVmfk/QkzWlj2WiKo2kXS1n4+9pZI3R0b+bPTiB9YS43Lx6Mv1fLHK7XqxN+VNcW0yWgLyKwsbWJlaRO5CVZOGpjKtScN4omPtRiUQeet5amHjqHlq91kUsWUqRLS1CmAdNAXAuyMOTFSbiDUFsl8MrbXkhFQ8fthgXoeeArBZkHdugmlLRgZe+IMUOPAoinWN96NZ/xZIqCiKDDtCDuVnTLQTpoC/XOD1LRqGUx1bX7MZ83APGMGwooVCFVVCKKTjg4bamsDqi1Bi1nbQyyWIBn0Oja/cHSBo6PzM6CqCkpjBcjtFwSTDcGZ0u3YkBy5wBva76Cz461sb695U9HiI9VuZsoUyEwzULTgSPpftBJrWiumeB+F53/Hjv9MQglo/945OVqQazT90510x55aJwgCpKTA1VdrYRJTpsCVV/Z8IRcMMv0v/BZruhZzJPsNJLQM5IxLI1298xOsjBseR3OjyNUXWiguU5lxcx3DjnHjSJAZNsmNatVcDSnJmnDrsI70VNAwevmOHbF1dxpW9yN+oJad8/ynpQxJzODUM2XWV2m52kZRID/Jxq4Gd1jwlDV7eW1NOeePzear7XX8UNRITYuPxa89xmMv/xERFR7UTvaiCxYw+28TDmqXVGdMiZFgb4chIrqNUnuMCirmoJuZvIXLcx2qzYJqNiKv+gpGam4+6YxzUE84AfxtADQHraSlaydhyYeGGHHTEaukfW5GvEEZly+ICtS4fOQm2hDaxYsACC01qK31gIrSXIWUmr+/ToXOYYSe6K+js59R5RByXUkksFgQkZKyu7VEBEIKrf7Ixb+jpUCGM+LHqW9vzyBJ8MQTIPuM7H79KPxN2p23JcVNyvji8Pi96UPVlx6Fzz4LDz0EDzwAJ58cGd8dtqzmsLgJuk3sevVojjkhakCdk/HZcYiCQFKyylmzQrjqjRRviFigsoe3IJm1e/gEu4n774e0NK2O24UXan/z8yMNqBct0l53LO9cVLB1d0r4XDny67nltiDb69rCXdtHZcUza2QW1x1TwLT+KcS1B3n7QgoLN1Ryx1nDcUra2OU5o3hhYqQfglxexU1/zT7kXOlGpy/8fMYpEYHTEeqkIIDZhJoQj9isfZ6q00aoXdwIJiuCIwE12N4VSpQ48azIdt5fFLmX7i5WKcUR+X4HZCX8WYTn4UwFScsmVP1u1L50TdX5xaMLHB2d/Yga9CHX7oZAxx2ygJiUg9BN5pQvKFPR7Ak317SbDOH4ECkqqDZad8ycqaWBx1vMFC04Imw1SDmihORUudsU8T3RuZt2Bzk52vudt9cxh+RkuhBoirg7fLVxeGvi2bk68t72DZE7+7Y2WPCakbR+fo79VVN4zK5ib/j58kXJ/OlPsTVmQLM4zZ4Nt92m/e3dNSTg2qEViRMEaBIaWb09coEf3F4p2m4ycHReIlcckRcWmG0BmaVFDTyw/N8I7bEnTx97Md/njgBgBZMpJ5fuw2kPLlKiDIiyLyJAhg2LiIu2gCYkfKIFjEbUc87E/NE30EmAGK1pKIpAKKS97/EZGDo4oqqHjYpYJbv7HrV4I8HAJkkMWy47EESx2/8ZHZ3e0AWOjs5+QvG2ItcWRdxSogEprQDR2tU95AmEqGjxhAOMTZJIapTVRom6oHTOIJo5U+tq/cEbDjKFdACMDj/Pf1gVcxHp6PQ9f772V5Z7XjZjBhQXw9KlWk+jpUuhqKhnsdQxh/vuA0dURf5gq4Vgm3Zhsma0ACpfLkgIL08d3spJE2wMy3Yw+yQzbaEA1zxWjiNBm9z6ZQ7kuIiaqVoT6eYdTUdV3n/8o2/ZSu6ySOyRI7eRpqAmcAQBMuMsIMt4v11F7ZqNVDW0cmR2fDhdv9EbZMP5v2Piys0AKKLEnTNuo8GWQBU/U5XEfcBjj0U+3wfujoiHVm+QuPaGri5fCFVV8YhWrYbNrddiEK1Y//NBeLzg9rLkm/7k54Pfp5383bvh2IlS+LO46rehHr9HgZBMozsicFKdlh7irLpJWfuRqKqCGvKjeF3ILTXI9aXao6EMubEcubECuakSxd2MKof2vEGdgxI9BkdHZz+geF0oDWWRN4wWpJQ8BCm2kmtQVmjyBHD5IsGaFqNEZpw1bLVRVTUcfwMgdfPbLklwwgmQNKCAK/5VA8CC70qYcUQO0H1zy5yciHupp2V7Y/2RJBgxQrPERBDwVCUQP7AWgzVI4ohKajdms/UbG0MmekhMD3H3TZ9gf+t7lk+/hvGnujCatAtZ+TYzH/zbSf4lmsDxN9nw1XUfO9SB3EfPhbsiIfzcltFK0Kj9FNpUM+Kit2lat56Gm+aEx5iBY/IS+WxHIwFVxj2kAO+pw2hbV4Ejv4E6ZzIPnHY9Z7+5ok/7v+su7fN6/334+9/7Nud9TXZ2JEZ3Q5mZ59Zoz0sb3IwanIrLHyKoqDR5QyTZjHhEK/YEkP/3AtIzL2J98QOCRw1l1ccSNzRP5pqLa7BatM9OUWDjBpHVq0TGH6VgtCicNTOEzSTFiBdPIERtqy8sXRKsRqzG7v2psW6/vRM4qqpqri2vC9XXFrnp2NN67nZLosGEYLYjmG0IFqde6fgQQRc4Ojr7GDXg1QKK2xGscYiJ2Qiduiy3+oIxP+6guUXS4yyIgoA3KLOh2sXaihYavcGYMT0xPDsek0EkEFKoa/UDPReZq6jQKh13R3m5tmzBAm3dvtBR76UzbcUp4aDevOnrsOc28sWb+QyZqLntjKf3J3B6fybQEl5n9zor/52bQfb01QiiNvHmLRnsK9dPyBOxjpkdEevB8sUm/vHPI/lo7dFdfhztJonj+ifw0cYmDBaFwUd5qavMoaitFaMjwLKBE7jN/i8kdwgZqce5CoIWXFtVBXFxmsUrVhTufzoHnucm2zBKIkFZ4YO1lcQ7zdBusVpR0siUfolgSUKhGYcFuOV3iJXnYJ73X7y/n8v243fExHlVVBtRVVjyoZHxR2nfwyqXFwEwGURMBglFUXEHYuPNkuzdV/RWFQWC2nYQpD5l0qmqiupr00SNtxXUnxC3EwqghgKa4BENiElZiJbexbbOgUd3Ueno7EPUgBe5viRcG0SwxmkxN1HiRlVV6lp91ESJG1GAJJuJjDgLtW1+PthSzVNfF/H5zvoYcTMkzcHoXjphf7OjnkBI2/eRhcm9Fpnriyvn/PM1kdMXeioSWL86j6aNkXTd5DFl2CZ/y4IXZbyxzdDxewSWzU/kxduzcAyuwFmotdwOtpqp+7Z/3ybSF1QBJdj+mRgi8SGKLDD01HQMqdo5tn38EYX5GSTd/UcA4i1GjhuQgBzULrCTz/ZAXbvbTBC4d/DtyBjoTYipKpxySiQAel+Km+RkmDNnz+M6B547zSZO7x/psfX6l0X42t1GQVlleXETdV6ZBmMy1YZUAoIBsjLg3ts46aRgeFvBIPzvvTiuu1Nz1b32kpGyksi5UAF/SKHVF4wRNxaDRFa8FbEH4aJ6XZH/KVvvbTJUVUVxNyFX70BpKEX1NHcSNwIYLVqRTWcKYnIuUsZApMxB2t+MAUjp/RFT8rVMR1Onwn9KCKW+FLmpUg92PsjRLTg6OvsINeDTxE3Hj57JhpiYFXO3GZIVqlxe/KHIRdVpNpDisNDoDfDmxip21rs7b5q8BCvjsuMZnOro9e51ycZIF+WTR2b85MrEsgznnQfXXw/9+0Nqquba6Lj7j07VrqjoYSOKSOl7o2ktTiH75E1IJhmjPYBs382iDy1MT2kkcUAOHy0ZxDdvx+NrkzAlusk8fmt4E2UfjEL27blR496gBCVEo4IgxV6kfhWVwh437xkYNZwmex5vvyFx9q9kMhNMDE1ysr1VSyufdqrIl1ppHD4fMglW79Np9pm4OLjiCnj99Z7HdNeANeK+LCDrBB+pRxWjqLD42wouP3UAzQGZkKLyZXEjR+UmkB1npVIwEa+0kSC3IADlVQaeezWRF+YnUl0b+Zy8HoFLZ9l49X9BBg5RCIRkgtHxZIJAisOMzWDgyxVCj2n/qqc5so4todtjU1UV1dOC4qoDObaCMYKouZascQgWRxdrancIRsCiBZyriowa8KK21qP6tf9P1d2E7GvTMiLNXYtr6hx4dIGjo7MPUBUZuSFa3Fi1mJsoX72sqFS0eMPl7wW09FiTQeKznXWsqWiJcVeZDSIjM+IYmxVPsn3PGSTljR6WbtbibxxmAxMGpPDmmn1zfP/3f7GvOzKmojuYp6bSCwJNG3JwlyaRMXUbicO0kslSgo8PQzZqn0ynYZcZydmKIyVI4a++D7um6lfn0VrU68b3SIcmvPdeGDhQC4iet0MCggiGiMAxW1UmHae9LikSqN5oZ8J3f6efxcwxa6v5ZkUGE6fIjMyzUrPVT4vsJzvXgGGLmZDox57XiMHuI+TuW6fyfYnLBX/7W8/L//QnuPvu3npkCVR+NhRjnJeEITX45BCvLSlmxuQcgpKArMI3pc0MSLYxKt1JiyEel+rgoTtE/vuaE1nuXniXl4ooXjOZ8dprRVUJhBRkVcVikHjnbaHXGDBVkcOiAkCVg9BezVtVFQh4Uf1uFE9Ll9YMgtmO4EhGsNgRhJ5FTUd18J4QRAnB4kA121HdTSgtNZpFSQ4i1xUjJuUg2uJ7XF/nwKALHB2dfYDSXAUd2RYmK1JKvxhxo6oqNa0RcWMQBdKdFrbWtfHF7gZ8URYdh0nimPwkRmTEYZL65kVubPNz7Yvf4W6voXPKqExMBmm/VCaGWGHTQX39ntcLtNgofWcs9avyyTp+K/ZcLYgz7aQtpJ20pcv4YJuZqs+H7PX8UlJi59O5jcL8+aBsaf98xChrWmJE7BTtFCkYNjJcmfeoMV7e/LOXtAwL/QcqTB4Qx4db60GAAfl2tpb6EQRIGFZF/aqCvZ7z/kQQ4IUXNIHTQffuS4Hyj0Zhy/gSU4IXV8DHa0t3cUT/DPLz7QiCwM4GD42eIBNyE7CZJG55EAR7kP88Z+wicqIL+nUgCgKW9kDi3uLDZs/W0snPOVsA0QCK9t1WGsvB1F5qIOAlJruqY79mu1Yp3Gzrsgw0S6o3KOMLyniDMgFZQRTAIIkYRRGDJGCUROwmQ7gWlXY8AoIjCcHiQG6sCJd/UNorlOsi5+BCj8HR0fmJKF4Xqqc9QFYQkZJyu2RZNHoCeNpriogCZCfYWFPp4uPtdWFxY5QEji1I5jcT8hmXndBnceMNhLjxv99T2qD92BamOrjpVE0UdFQm/olZtX1ib2rbeSoT2fnKBGq/7V0INKzJRQlG7sO6q7UTjSBAbq52gewpxV2WNQuO2n4xFkQFv0d77kwNhmNiCgcqZOfHfo533VjLbb/V4kqsRon+KVp8RkF2JOA0fdJOTPEeDiZUFcrKNJdiBz25L2WfkR0vT6StVEull1WVb3dWseDtetratO9wozfIp7vqqWr1Y7XCnff7WfCRh6EjIgKxp+aj4f30IT7s5ptBUUWktAIES1T9gYCnXVx0WtlkQ0rNR0rN7yJuVFWl2ROgpLGN4kY3Na0+WnxBAh0NRVWt0KY7EKLFG6S+zU9Jo5v6Nh9yp7o/gsGk7SPKXaY0lmtWJJ2DBl3g6Oj8CFQ5GK6hEZ0xJSZkaA39onD7QzR5IqbzjDgrm2paWV4UMYOMyHByzdH5HJOfFHPHuCdCssLtr69lY7n2w5oaZ+apK44kzqrNobfKxAcegarPh1Dx6VBadqTRtDGL+h/6UfNVfyo/G0LxorHUfB3p9p2bC5WVWq2dbrcWdUE1mbQU6Asu0P52XGA7qhzfcguocnufJUmlvkJzASZmBtm1Q9tQTp5CYf/Y+BynQ+GhP1Tx6P3a+IHJ2kU03mEiI0ETOQZrkH4zV3eJ7TkYiG6m2ltj1VCbhV2vHUX1lwPCYkNxNPPO5+UU7dKC3gOyylclTWyobkVVVUaOUVj0iYdrb9GynXoqDNnBnuLDokWZYDAhJuchJueGKxoDWvq2LQExMRspYyCGtIJu42HcgRClTW7q3f6YXm8ddFdcsINmb5CSRndMKQfQrDliYlZXkeN19XxQOj8ruotKR6cXVFUFOYga9EHQhxr0oQZ83dbRECzOmB+7jvXr3ZEqucl2M5Io8NmOSEfxaf1TODovMfza65d5Z4mHsuoglrggOQUBWn1BRFGgMM3BwHQnKU4zqgr3LtrA8q1aCrbDbOCpy44kMyE266OjMvHB2fxRoH5VQa8uHUHQLnazZsHXX8PcuVq9ne7iNqLdUJ3p7A5R1Q4Ljkp9mYHsgX4kCYqKVUaP1QoqyoMHdbkLPHVaGzlZpWzdkM6QkQay48xUuPwcMyaVRYuDYPFhy3CRNLqMhtX5P/rM7A/S0rRCjlVVmhWrV1SRmhWDcJcmkXfWOowOP5I1wJfrq2itGsioyZqQ2VbvJslqJDvegsEAt9wZ4OILJI4aZ+i1RUhfO9d3jBMEIRwkTNAPkqFLXanOyIpKfZsvpv0JaFlbVpOExShhNUiEVBVju8AJKSohWcEdkGnxBrQCh6pKbasPX1Am1WEOx+t0iByFSCC00lSpBTL3EvOj8/OgCxwdnU6Ei4K1NaD6PeH01N4QzPYuGVMA3qAcvmO0GCUSrEbWV7nCmSQjMpwx4ubF+V7+sfIbJHtEFLG26/4SbEZS4yzsqNb6AhklkX9ePJ5Bmd2n0M6cqVUnXrFCc9/cfHPfYmYOFJLVjznRg6cyoT2YVBMvjz0WCUAtLu694WY03blDjA7tAh10mwhEFZcLRmlXZfSY8HMxIROluRpQGTHYTz+llgbS6J9ko8Llx2SUGJycyzb3DkBrl9Gwuh8HQ9sGQYCkJLjssthsN0nac3HEtpIUtj8/mfxZP2hd6x1+Vm6soXJbIadeqRVhXFPloiDZTiCkbSyr0AeCnd6Ova/xYZ3HCYLYNXW7E6qq0uYPUd/mD1cHB+1/MMVuxmwQqXcHWFPRwra6NurcAcwGkQynmUynhQynmQynhbwkOw1uP23tAsnV7tKKTmkPixwlpBURVGRUr6vLzY7Oz48ucHR02lFVRUszbW2AkL/ngYKo1dEw27QmgyZrj3eS0WbtBKsRQRDYUB0xYY/PTgg/X/imwp8/XIs918eeaPYEafZo2xYF+Mv5Yziyf+8BKpKkuWuWLTu4xY09r4GCWT8gWULUfF1I9ReDib5QRgeg9rXScmd3iGgOhhtMBpptqFF9maK3qWRkgldzJQpmO6tLCxFaKhg70odD9FEvK6TaTVgkEZ+sMPpoWPtyItbsJizJbpyFdbTuTuPUU2Hx4h97Rn46qtp9YHhfKz+HPGaKF41j0JVfYrQHiOtfx+bPkpnpdeCxtuELKXxf3syROfH4QwpBRQuqz4yz9pid1JfO9Z0DlPuCLyhT1+aLKcUgCpBiN4Mg8G1ZE9tq22LqS4FWn6ekyUtJU6T3Wf8kG2cMzcBuMoSLcvqCMrWtPtKjWkoIgoDoTEH2aQFcSltjj+nsOj8fug1N5xePGvRpvWeqtqM0VcaKG9GgmZudKYhJOUjpA5CyhmBIK0CKT0e0xvUobkKKEr7zkwQBu8lAoydAeYt2YU2xm8JNHGUZ/vjv3eGsooDLQu23BVR9MYjyxcMpeXssbV+P4qJjCpgwIIWU9vUMksB9s0Zx/PCM8HZ66jfVQV9dAz8XojGEs38tGcduo/+FK+l//ndIFu28pR+zG0e/2CtzdABqXy/QkWNWsec0kn/O6nDMjrcmLsZIF93rS42xQKjsKrFwzFmFbNhiRgAceBEEgfwkzaIgSpAopoXXSDmiBIDbb9cakubk9G2+sO9iplJT9xyc3RdCbgul740Ov86cug1HwIyt3fq1vd5Na0AOWzY8AZlGT6DbbUHfOtf3FKAcjaqqyIqKJxCissVDebMnRtzYTQbyEu3UuAM8v6qUb0qauoibNIcJu6nrjnY1enjph1IEQSA7wRb+NrR1iqsDtMwuY3sl5oAXNeBF58CiW3B0frGoAS9Ka71WJbUzJiuiIwXB6uxTWfjucEX9iDotmvWmqjVinRkSVbRvxQpQU2vDy8o+GElbcWztl+YtcOS98IcztddN7ZVmE9tr5PTWbyraKrG/Usf3GlEhZWwp6ZN3YLD13Bsoc+o2dvwnmWgrTnQAakc/pd5ISZNJGF5N6pFF2DIjn7cSFGlZl8+QX0dionr8tFXt3IVCAku/sjNyqB+r4sUt2cmNt7C1TqvV0r+fhTV1FkzxPhz59aSmqlRUCGRnw65dWhxRh1utvh5++9vurSsdQu6ntHKIi4NXXtEqJ+8L2opSqfm6P+nH7EKQVFbW7+CWY4bxziatwOTaShdnD8+gskW7uDd5AjjMBsyG7lVKT/FhOTnwxJMqp56u0OpTUFQVWVVRFBVF1WJrZEUlpCjIitpNorgWOJzs0ATYd2XNLN0VMVsKQE6ClcGpDgYk2Vn7nZHKKpXBGTL9hvuoafOztrIFT1DG5Qvxxe4GzhiaTkaclSqXdmzN3gCJNlOsFceepJWMAFRfG8IeXGk6+xdd4Oj84lD9HpTWOs1fHo0g8v/snXeYG+XVxX8zoy6tpNX2vi7r3m1cAIMNmGJa6D2Bj5aQhADplUAoCSGEFkKAAEkooRlM7zYY2xT3bq+3eHsvWnVpZr4/RnWbd10gJHueZ5/dlUYzo9Hofc9777nnCiYboi1jQP+MIR9DVelOSk85olVN+iQHVSkpUtDYCL4GJ9bCLkCrxOkPydGX9CTzv6H4icRIzsKFX03/owRU7GXN5B+3C6Orbzl1qMtM5/YC7GVNmLM9WPK7SRvdRk9lX7O//qJR/pDM9roudjW62dXgZnejm8oWDyVnpF6ccI+Runcmc8q3PDiytIhRidOsaa4EAQEwKUnpQkmnpVUyfNisWuhIiE6tyQ0ijWYIdlgxOAKIkkp7d4RLL9U+/xjhvOgibVtZ1iJRA0EQwGgEb9Tnbjil+KC1beiPPB0Mmj8pwzmxEWO6j88r2/F5Q6Sb9XT6w9R2+QGBdIshHuFo7Qlo0Y8BFgpnnw2nn6GyfnOECDJOl4LVrhGXuq7hn58kasd3mPQoKry1u4UtjQlSOz7LxpKyLGxGHcuWwVlxciUAOgoLbdx3n43Ll9r5+xc1BCMKW5vczCl0kpNmxGbU4QlGUFQtSmU1JqbRZEKjDrGh5wgOH0YIzgj+Z6AqMkp3C6q3I/UJUYeYloFgTd9vl2BV1VaQgqANhwMN2p5gJO6dkWwWZtQlCE6yuV9eHrgrssmaWw1A2phWunYm+jclb9cb+/MTEQRtEj3zTC3cv3z58MlNrJLpYGEr6CJn0S5sxamfQeeOPLp35+KtSyfi0VyA/c1plJ6tWTHnHF1OT2UmveMreXnaZ9LYFeCzijZW7mjm071tKSmK3vA12Wn9fBTdO/OYeKSPWSdpLElURY4udRGMCmXTTHrEaJQMQQBRQnp5GRe17yU3+wIgkcLSJd0HepOS0lZCMkVQQtr/McL53HNa6uiDDwZpcUFCO3P55fDqq9DRMfC2vZGRoVWcJXvfHAoU5oucP3Msy6u3APD4R5Wct3AUn9Z0ogLl7R6m5trxBMOEZZVARMEdCOMwJwi5qqqEFRV/KII3FMEfksksThxD3r+uH9C0NTpRRBIFdKKAxaDDFiUcbb4Q7+5upbY7kSo6qtTF0aUuBEEYwqJAz5FzXPHIz4cVrVw4vYA0oz6eevYEwykEJ6WEPTJCcL5qjBCcEfxPQAl4NH1N8qpK0ieITT8lnaGITCCsEJI1p9OwrPTroREjOxC1HVNT7cec5sSgZ0pa6ceM/0CLqjgVF3JQQjLKuKbWE2iz0b0rl1CXdVDB5XD8RBYu7L/j9/5wMOTG4PTinNiIc1Ij5uyelOc8NS4aPpyAv9HZ53Xdu3Pxt9owZ3mwFnRhKejEV++KP5+VK/PM+nJu/qie7mAQVZH7JahStLx+Qr6dNH8Ob+01M2p+mNyLm5i4INEC4OjRzji5AbCL4cQkJRkQFAX5+ht5lk84L1vTaUUU7ZMXRUEL/IhgsYTRK0mROlOYsFtb2ceu40UXDV0/BPDkk0PfNoZHHtEI7VDEvC5Xgjwlb9O7xUWsWk0ln8/u3kNTd4DPK9r50emT+LRG04/tafEwPc9Bls0UT1W1eoJ0+8OaXia6SBgMoqClmBS0SsRYWkpRib9eLwkYJBFVAFSIyCpVzT3UuQPUdwfwhxMXWCcKLJ2Qw6QczatoqIuC8goHG+q76A5E2Nfpp647QIHDFCf83lAktc2DKMVXA2rvflgj+NIxQnBG8F8NLWrTjOrtTDwoCIj2HARbX2ITKy/t9ocJRIY2A6m9CE0yjDoxhdQ4jLr44LinzcPx4UxMekkTXP5Z5MYnc0if0gBA/uLd5C/ejb/ZTvfuXH51XS6SZOtzjOH4iRxs882hQyV9ah2Zs2qw5Pd1dw20W2lcMQF3eTYDqV70Dn+8lBuIC48BTFlu0k/bxMqtG+javZpITzu20um4pi1B9uoItWRhxorLbmD0GB0Tx3iJmAUUoYeTvtPT51j5aUayLAkiahYi6Dob4v8LeiOsWsWq+lHYx2UxbVIFAO0+E5JRI6uxW8nn0xOUBeJ2c/3cHMMhNweCW25JpCRjYt5zz+0biYvNy488ov3urYVJT9ce++Uve4t9RWaWunhrc4PWfkRWSDPq6AlGqO70E4zIWAy6+GNA3DG4P4gCBGWVDl+Y7qiAt90Xxhc++AtlNUicMyWffEeiP9hQFwVrV4vMGu2MR3E6fCGKnGZMOilOvBQVpOh1FIRoWwk5DMoQw1AjOGwYITgj+K+FGvIht9elRm0MFiRXPoLOmLJtWNbC6G5/OMU3IxkCYNCJSIKmvFBVNfobtL+ElKlaL4lkJpmCgRbBmZprZ0ujm2BE4Yu6LhaO0kpczj4bvOGJ/P7tIPq8hHDCnOPGnOPmoW172PKPLP7vmDHMGpWIZAzHT+TLqKDSWYIUnboF+9jWPs+JvjRsigvP+hJ6Kgb2SRF0MqVnb4hrkbrLs+mpyAIUMufsw1b6DHVv/gtfwx70aZkIkhNfs54s0w856ZwQ6bmxiTEMhAkPUDCqFwVK081MzkkIvm2Kj/RwIhckGK2Izjxo/IQmKZ/H/tSAJGm8xYeZNKC+O0HEKjZYMDgTs2eo++D0XMNFYaFGSJIxmJg32RzxzDPh9ts1QtTRof3cfDM8+mhfsXqGLZFy6vKHGZdpZX19N7KqUtHuY1JOGllpJlQC8ZSOKGgRNUkQUFRo9YZo7AlS3enDEzp0rM+kEylwmCh0mJmWZ8dqSJ3qhrMoyC9KnFdaNB2lJI0RfQyQY8/951mH/89hhOCM4L8SircTpbOR+PJZEBEdOdF0VGLgUVSVTl+ob8knWojcZtQRkhXafGFaPUFaPCHCirZatRv1OMw6nCY9WTZTfPDbH44scbGtyY2iwhe1XcwpdMZFqpddYOTic+fx8js+PtnbRKWvkRp3IgLyye5WPtndyoySdH64dCJTi5zD8hM51HqM3kgb00LRqVvQWxPXM9SRRpbFwbyj9Lhc0Wt0ViPdrRLr37Gz8hkX4UAqASk8aRuWXE0YGuywUPPaVDIzPWScvAvBtoWqFx8BRSbziG/jrT6NsillnHC5nqyi/ktzw0GBrho42rcW/Ykn4DDpsBt1WPSS5pCLSpriwR7uRkdsQhMQnTkIVk2zQV4es6+exKgZ2jF27zOTNk477z01YYhy5vL1FmyLNfF02GtACX25w+x99yWiLbKcaobYu4qrtzni8uVaOmooYnVnksi90xtiXJ6d9fXavfpZTSd5aUbSLQZy7WYUVVMrhWSFHc0edrd6qOnyDZqqsuhFRAW8vhB6UUSvEzBIEgad1qyzKNOKXidpqeNoVVWm1UChw0RGUnVTfxjOoqA7yQXZYdKifDF9nSgIfY8T8xwYIThfOUYIzgj+66B4uzS9TQwGC5KrAEFnSNnOG4rQ2hMg0muUtRokPCGZ9Q3d1HSl5vIHQ6HDxOQcOxOybSlVNb3hNOuZlmdnU4ObkKywdl8Hx41NVAhJEpy71MK5jAZG09Dp571tjfx7bTWNXVpVz6Z9nVzz98949aZjybKb9puCiPmJ7I8MHQycExsoPmNTPFWjBvWMyyxg9pL+fYIcWTLHXdqJIMC7f0847RlcHlzTNOWtHJKofmkmBluIsqvX4cNP5fN/I+LtIn3ygyxYsIQjbvWgN6pAGEVREEWRPZ9bqN1mQhcUsYsq+U6ZkvwIc4uM+LMSaT6dGsEW8ZAm9yAl55L0JiRXoZaaIkoU5KM54rvaaxUFnnsvm2+N017TowbRAQGPiLqnCf0ZWkQn1PXlRm/sdjgtaiMwmG1ArIorGcMVq6f3IjhFDjNWg4Q3JNPsCfK3z/aRbTMwPstGXpqJXa0edjb3xF28k6ETBUqcZuwGicY2H7sb3HxY1UGLe2DTS50kcMToDBZPzOHYiTnkJKWg9oehLgrmLpB5ckOCNNtNOq1kPfoeereNU1U1aYcjBOerxgjBGcF/FdSgL4XcCDYXoiM3ZZUVkRVaPUG8odT+NGlGHY09QT6u7qTFM4iT8QCo6w5Q1x3gvfIWxmRYmZJrpyzDmmIcF8ORJS62NvUgKyrr6rqYke/AZTH0s1fITzfzrYWjufjIUt7Z0sjjKyuobPXgD8k8/lEFPz198pBTEIPpMQ4GjnFNFJ+xOU5udN50TlvqxJSUGggFBDa8a2ffNhPTF/cwYYEW5cguSY2e2QoTeqnWz0aTPzpA5omb8ckR/M2V9FSux5xxKRdefSz5Y930dLTS0x7Alp5Jxz4zU40GvnlSD9mXdPU5T5XJeMNt6NUIejWMmERqVFVF9biJdLaCLCPZ6xEdmXz4RQbX/cRFdY2Ort0WQKWyRo83pAPCKKqKzqSt2pv36blD91Pu5jIAZP/gvZIONdxuKCiAK66Au+8eWiQmhuGI1RctIqUprKKoiKLAKeOzeXl7U5wAtHhCtHj6L/2ym3SMzbBiFgS213TxwkeV7Gnqq48aCBFZZW15G2vL27jj1e0U2x1cc2Ipp87s2zKlN4aiS/rzvSpv7G7GHdDGiUKHCb0k0uENxu8aU29/HzlMLGq8vz5ZIzj8GCE4I/ivgRoJIbfXEB9grOl9yI0nGKalJ5ASGtdLIjVdft7c3dpH1GjSieTbTeSmmciza/1pzDqRnmCE7kCErkCYTl+Yve1e2qNpLkWF8jYv5W1eHCYdcwqdTMtzpJSI20165hY6WVvTiaLC8u1NXDarEN0gncT1kshpMws4siyTpX9cSSAs8+LntXxr4WhyneaUflOD9WcaiAyJ4oHpIl0zaig8aTuCqF3UI8ZmMrrUGb/ubXV6Pn/dzvq37fjc2pCz7SMbv3tHE+pOHO3G5cqPV/FYkgjO8Wd3s6OtnGD0Y/HWh0FVOfGSqRSUBdmy8i3Wv/UKzZWV+NxNnLz4aE678lKyM+f2e64CmsZGDYdQg37kgB+5pwOlux25o1Vr4hhFTLk1H1j9Ex3ff/IstuxIY94sP2NLwzid2sVKrqyb1LMpxf041tDzy0RbG/zxj/0/118kJobhNr+Uk75EUtTfaWymjWvmlbCrxcOulh4ae1IXCgZJZHJOGvk2A5/ubuWxd3azp7F/UmMQJWaNTmd2qQuzUdKM/WTNH6fdE+LtDS10J7kF17i7+dWLm3n0nVr+fOUURmf3FeQnY3+LgswZHazZp1XZmXQip03MRVZUuvwJQp5uSdXyqeGkiJNh6BGlERwejBCcEfxXQFVk5LYaUKIGbFFhaDK56fAGU6zjJUFAEeCd8lY6fKmeFXlpRuYUOSlMM7G5ppPPdzazcV8n1a1eQmEFu97C6AwHp87PYO6YDBaNyaDZE2R7cw87m3vigsnuQIQP9rbxSXUHM/IdzMx3YNFL6CWB+SUudrZ66PKHafYEeX9vKyePz9nve3XZjFy0oIQnPq4kLCv8Y1UVPz19kvaepKE5+/ZHhpqb4cIL9//aZOQcvYfchXvj/5fluFLIzeRsGxMmW/n2yQLgJxKBthaBG6410VbXirerloKsYlpatNYSK1fCCrmLmE3P9taEu/Nxk3L48NO9NAig4uGLN55n2T2/ZlTxaC5cuoCa+gZef28F6zdv4883/5gzj5qN4veg+jzab78XNeDXusErwxO0mg0R/nDRG7y/8zLmzdIm1YJ8bR/hpOog1RPmx+IfyWVz9IGhEZxDGU3bHwZygR5u80tZTiY4iffpMOmZV5zOvOJ0ugNh9rR6aPOGyLebyEszcv/bu3lrc0MKQYoh3OqkbWcW3n0Z+BqcNOaLnBgVN8f1RM3QVA4f3zIJU3YPjnFNOMY1Y87RiNK+ng7OvW8VNy2dwKVHjRr0vfT3PZg5N8Lmpm7W7NMYtwB8Y3IeTrOeNk9icWQ36THoUhckaihBcEZcjL96jBCcEXztoaoqSkd9ooeUzoCYUZRCbrp8oRRyYzPqaPaEeK+8Na7BEQTN5XROgYPaVi/3vbaT9dUd/Q7E7UEP7Q0evlimaUVKMq1ctKCEM2cXsXhMJtUdPtbVdVHZoaVhghGFz2o6+awmEZ0wSAKhpElic4ObE8qy0IkDR3FiyHMmBs815X2rlYaC/sjQ+vUDr/5Bc0A2GjXzOaPLk0Ju5uQVMmay1oBQkWFeiUNzBk45psr2LW/S1nIv9165AUmnx5GZw+aXLmHt2p9TX29i2k899LYlCuwYRekxaUxdrGPLSlj/1ksYJBPf+dbF3P+7nwPg8/t59Il/8et7HubeP9/PTP/ZuIwGwt4gYW+QiD+MEpGjPwqqrKCEZSKBEGFfCDmiorOnYyosJPv4hdQFi3n20XZ+cvpKdJJCRpqPcYZdgGbAOG1SEDCmVN19blhAm5JBbvR/g8MH0Qq7wVBYqJHLu+/W/v8yyE7viM1wm18mEztdP2lY0MjOEUXpAHy6t41L/rmOVndqVGdqkZNcNZf7f5FHqDv1foml1H70I62/WmoKTSDQYifQYqf5k3GkjWmhYMkOjOk+FFXl7jd2Mn9MJmNz0wa9DpIEC49RqWj3srnRzcOfe1Pe/6IxmZS6LLgDYbqi7VcEIL2flHKyO7qgH4ngfNUYITgj+NpDcbegBqJhbkFEyihOMXzzBMO0eRODqtOsZ11dN5uS7Ntz04wsHZ/NngY3v3h2E1tqu/ocR1WjnadlAYPTj6hLDPD72rz8/rUd/O2DvVy4oIQLFpRw/vQCWr1BvqjtYntTT5/y81Av08BJOWlDIjdfVLbzxzd2xv+f6Mjn2WcHTkkNB3fdBXPnwpVXanqO3vB6NRfkW26BfdYa1ker2a88dgyfvO9CmNKlbbfbRcl0bQKIRW1y81U2fP4Zv/rh9chKJgu+cS1GC+xY9QEvvngb0AX8nM6thaRPq0EQROSgRGDLRM6+MoLP6Gf09LkIokRD+XZMRiPX/+1WAHwN+xDqK7ggT887Rbm8X17DMw+/zeJ0V6oR2/7Q2o2nopq2jz4hnF7Mjg0XccTa77Hy5r/hsASZaVuL2302drtAWW4PbRixGXQIiogqKkyY78Ooz8bXZMeS68ac04O9rAV3+cCRuVtuSfjMzJ8P118/uLvxoULviM1QdCnJzS+7khYMdsvAepNQROaBd/fwr0+qEtub9Vy4oIRvzC4ix26mtBRCfe2S4ucwGOmOoacim937Msg/fieZs2oAeGTFXu66aOaArwlGFNbXdbGurqtfz52Z+Q6OKHTQ6gnQndRbLt1iSNEggab/IxxNmelNqa7GI/hKMEJwRvC1huLtQu1JNNETM4rilS8AgbBMc1Ilhkkv8caulhRtwMx8ByZV5YZ/rmdHfeooW+iycFRZFo/9IYN9G13IAW3SFiQZS143tpJ20ka1xbuAd/pC/PWDcp74uJKz5hRy6VGjWDohh2NGZbChvpsWb5CwrBCKaK7IIVnBapBYUOKiLNPK/lDe5ObGp9bHV8+B8kL+cOfYxPn201xzuDjrLE2j0R/BiWk4HnsiQvEV2nLaqBMZLY5iT1lTfDtLxARo5/itc83c+scgoVCQu2//LV5vgHDoIU64PA+jWWD01It49EcPAw8CTmrfvAVPnRNLrptROemc/FMfkgSKomCQHBw5dw6rP/2MWVMnIQkqgc2rkeur6apopnl9FTNlPe8D+zCiFsxk1PYV2BvrsAQCyKJIRNIR0UlEJB1hvQG/yUhg4TH4FYVAfQNKQLtf9J01/HjUH2gPZfDS6yfxf+c3QThEZPsaWHAUVsVHuxxBknSMzTJR3u5DZ1CZc4qbTavKGHXeegByF+4Z0NBQEOCxxxK+NWefDQ4HnHDCgX9++8NgrtgD6VIyM+Ghh1Lvq1izVwBnP9GMYFjmlfV1PPlxRbz6D2D+2Ex+e9Y0dm028dHbWmr0UJlPqhGJhg8m4hjXhN4W4r1tjextHsvYnNQoTiiisKG+i89qO/GHU4VnaUYdU3PtTMuzYzNKNLgDBJLIj8Ok7zd6o3gS3lWizXXATXpHcOgwQnBG8LWFGgqkVEyJzlxEU0JYGJEVGrv9iYoHvcS7e1ppiQ7MOlHgpHHZrNnRzF8/KE/Z99gcG1cvLuOEKbms+ljgF2t7HVuW8Na58Na5aF5dhinbzcW/rGRTSyOyohIIyzy7dh/PfbqPE6fm8X/HjuGY0RkH9j5VlfKmHl7fWM8r6+vwRKs63HuzqHppCskT52BVMkPFqlX774/kdTTGz+OU6fl0tkvkjdFIY0O5kQlOkRjBuefhADm5Kju27mL9Z2uRdBehqMdgNGvprfSCXLJLb6V130sI4l8YO+ebuHIymXY8jJqaaMapcwtcMzPE+CsuZPWnn9Hc0kLDx29hVWHfRzsIdfkwyTLOsaOhupJ5P7yOY6+9FkIhbUbviYlZ+6mQ+3glVFWhiiKdn35G7T1/prumFoAMQzsZXc+we+tCxk/VY3TX0LZ3DJljc0lX3HRILka7LJS3a+e6+NJOqrfm4Wt0YMnrxpzTQ/aRFbSsGUNvktOfHqalhcOG/iIxvRHTu1x3nSZYBmhthRtv1ITosfuqK0m3tmpXC4UuS7x0fF1lO79+cXMKsdFLIj84eTzmplKOmCYcNkdtNSJp1XfH70JV4bEVe/n9hYkojicY4R/ra+MOy6B9KuOybEzPs1PqsiAKAqGIQl2nL8VGIstmxG7S9yEvqhxG9UdXBKKEYHEcnjc3gmFhhOCM4GsLbcWUVDFlSxAIVVVp8QTiaSGzXmJPmzdObtKMOs6Zmsene1pTyM24vDSuXVzGMeNzWL1a4PnnYMeO/Z9LoMXOCVkzuPWycTz1SRUvr6slEFZQVHh7SyNvb2lk4fgsLj6ylEkFDhwDlITHICsqW2o6WbGjmQ93NFPXkdp1O9LmYN8rM0Ht3Wpi4CqZoWIo1TTm3ESk69QZBbTWRIjNV91tOoTCxLY5udpn0N0loqoQCWciiArhkIDeoOLIDHLj4xKvP3Qea176B6OnP8KxF16NLEcAHaoKR9hljhkrIwpw2pJFLJg2kbVbdvD6m6tZEtRT0NVNXmsz6e5uHohehPz8aLPSNWu0GXowRFmGsGgRrsYGXC+9gNti5bO8ORhdUU+bL9bgLjgKu0uHpfEL1NGnYsNLl+ogzaij1GmmusuPyapw1T0NvPBwIeRp1ynv2D2Yc7upe3NaSiPO/q75UMW+AyE29/anW+ltG9Afli2DCy7Yf4m5TkpM8o+s2MsTH1dy3KQcsh0mnl5dlVKpuHB8FtefNJ6ta+yce97h1RgJAphainFaKunyhVi5sxlZUeNC6B3NPXFyI6Clho8qdaXYNAQjMg1d/vj4IYkCuXbzgP5WajDx/Ryot90IvnyMEJwRfC2hKjKqPzrJCiKiMzfleU8wEm9mKQkCCAKfR3U1kiBw/vR8Khp7uO3lbfHX/OCk8Vx+zGhefllg9CnDD5tnZ0NBuoWfnj6Za48v44VPa3hmbXU8lL9qdyurdmsTbYbNwKgsG6OybWTYjPhCEXzBCJ6Adt7barvo8PZ1V9ZLItOycvnHvZNQwv1/fQeqkhkqhjLByoHEsRVVZclCA5++IWGxy4yZ6eODpzNYvAR00c0q9wrc/BMD4AD8qEqYZX/M5rTvtWJO0yI9M084gzUv/YMN77zMsRdejSTpcEgqJ+VGKLZoE40cCiKXb+Y788fTUFnDn1ZvJM1o4spggO3AK8CzwOUWC6eefLKmvxmqoKW+PsXtzu71sGTvSl6zL0U/NoAFPw3vr0N38mwseAjt3oBx4hwyI+206LKYkZeGNyzT6g2hN6hc9P0gy5flE7BrUUbn+GYsuavYt3wmvvr0Aa/5UMwYXS54/nmtlcJNNw1MYu68c/+2AckYjtnft48vo6U7wLoqrdooLCu8szWVHc8e5eJHSycyscCBLMNxA+z7UCEeofqTjjV+F+9vbyIQVqhp8zIqWjZe2ZFosHrJzEIKewnhewKalUTsNA06kXy7eVALh+SqvN6GoiP46jBCcEbwtYTqd8dHSsHiSFkxRRSFVk8iNO6yGnhxa2N8wDqq1EV3T4gfPb0hHn4+prgUU8MYfve7/q3qhwunxcDVx43l0qNH8cq6Wv75SWVKuL7dE6Ld0xGfHAaDJArMLnVx3OQcTp6ez1vLDTzef0eCFBxo36mhVNPYDAmdU4c3hF4nkGOw0oMbg0llb0OIOWU2ps2SmX+0zF/uMRAKlqARnHWAzKYP7OxYY2P+GV0UjAuihBeQXTiNlpotlHSUM2dKKUUmGZ0koioykZpyghXb8dY0M2ZPJ7+yO/ldj4ffBgP8HrAC7cCpwE1eL+Lq1RrD21/0JobW1n7d7k53v4l7p5WN46cR7PJR9cqnjDpxKhbKQRCxTJhFdqSNFl0mC0vS2dDoprrTjyAIfOMcKx+/O5qaQC06cxiDI8DYSz+l6aNxtHw6GkEQ+uhhhiL2ffRROP547e+zzx6ExAgqo6Z4cdu7+McXtfxmlYepRU7Om1fMool9hc/DM/uz8tjV86lu9bBsXS2vra+PtzyRRIHvnFDGFceMiUdODqTR63DL55PJXduHaby/XdOF7WlyMyrbRiiiUBv9HjpMOgqS3I9VVaXdG4xXSoGmL8t3WFLK4PuFmqTjGYne/MdghOCM4GsJJak7uGhNXQ23e4Lx8LjNqGNXq5fWaDQky2pgYpaNSx5aHW8AGKrN5oHfT+SBgyQ1LS0Jr476em2+zMqSKCgo5eUbivlwRyMbqzupbPVQ1eLpN0ITg0kvsqAsi+Mm5bBwQnaKiHO4fiXDxf4mWFWFYxcYWBedB9qigu0Ck41daDqEcfO8bF6RxtpVOtauig0zGcBE4B3ycrYydtRUSgrCjCuVmTbGz5KFAW7tOYK7HtpCT9U6Rs0rQZYVwnXVhCu24mtspXXzPrp31TN63z7mtzazGFgNfIZGcM4FjoidbIzhZSXaYAyKrKwBWaHd72XejvVsGzcRHxYq3trMmFNnYGE3AJYJs8gJt9Cqz2R2vp00g8TWZq1k+JgTJcq3jmH1xiZMeV0Iokre4t1YizuofX06995rGLIZY38ppt7l/vvaPLy2oZ415a1UtngI9BLRrt7Tyuo9rfzs9ElcuKA05bnhmv0BlGbZuOmUiXxvyThW7Ghha20nJ0/PZ0qh84D2nYzCQpg5E159deBtLrhAiyj1Jndlufb4Nrsb3Zw0LZ/a7kTaaZTLkqKl6faHU8hNmlFHVpoJcQhiYTUpgqOGg8Or3BvBYcMIwRnB1w5q0AsxB1O9USvJjCIiK/H8uigIOMx6Pt2SGFmXTsjmlmVbaejUXu9rtFPx3IwhG7INhvJyKC3tf5VaWChy330F/PLsgvhj3b4Qla0eevwRrEZd9EfCatSRZtb3KUONYbh+JQeCM8/UIlmxrtIxuKJNzJ/7h5ExF2t/3/bHANYW8IfMEA0KOLJS22AAZKTLXHHhGfzp4Xe47NzHuP1nN8afi/WQOnrebO566O90tHUQrqsgXL2L5opqnnptLc0NnVxsNDC7rg5LwI8A0W5dRBsj9EKM4RUU9PdsX+xnO1MoxIyd29heNp5uu4PKtzYz5rRZmKMkxzxhFnmhJlr0WYzPsmEz6Pi8rgtZhbKpMjmFObzynAP96H0IAtjHtLLgR58wavYMwNXneEN1pgbtXnpnSyOvb6zv1+IgBotBiqduH/lwL2fNKcKYpCs5GPJs0EmcNC2Pk6b1v5Oh7vvPf9a4Zmur1hz0oYcG337NGnj66b7XZXJhQui7ek8b15+kdY+PocEdQFHVOIFJ7hAuiQJZtqGRG9DaMsRerfa0oshhxPS8ES3OV4yRqz+CrxW0dgy18f9Fa2o5pjuQWIE5zHq2NPbEm2VOyraxs66blTubAVCCeqpfmj2glmWoEATIyICbbx44BF9Xp0VEli1LPOawGJhZ4uKYCdnMHuViQr6dogwrLpsxhdzIsubw++yz2m/QiEfs2L3PBQavktkfli3TiNrNNyfIjculrZQ7OjSTv2BnoqQ9pPdx7rlQUZGYIBRZgKQ+TxPLAqx+rZKffncGJYX5vPDaO+ws11o1xFa7it9LS7nm75PRUY33i1XUvv0F7z3zCX/4fDOP1O6DveVYo+RmQAgCFBUlGF6MEQ6G2PaxbQeY2HSKwtTWFvJampCDESpe34inoZPIvt0Et36KTgmRF27GLPsocJhYNDoDS5RA2NMVLr3GQFrLRMJeLSLnDge46tHP+MeqSq1RYy/EGqTm5WkkZ9Uq7X6Iob7Tx80vbeGEOz/kjle3p5AbQYDiDAvHT87hmuPG8sQ181n1mxM5YbKmV+vwhnhjU0PK8fbz9uOX9sgjU+/J5HMaCEPdd0EB/OxnWtXWgw/uv31ILGXWG9l2E5MKNJKzu9FNfaePIqeZ3DQtvdriCbG1KeGF4DAnvG1kRaW5J9DvZ9LvuVvTEZKLHHxdyC1VqJHwIK8aweHGCMEZwdcGqhJBbtuXEPQZLAhWZ+J5VaU7ieCY9VKKc/CRpRm89HmCHNW8MYVwz8HZqQ8nCq2qmkBzKJNBDDGysXgxXHyx9ru0VHvuxRf7Bh0KCw+uRHzZMo2I9SZqHR3w3HOJiFG4xxQlMWBwahUk/3oqsb0cgZ/dEuSHvwxy4uIeVr1SxZiSME6HnW+dczr76hr4y4N/pfGzjwhu+gT/mrfxf/wqaz/4CFQV/6Zadjy9mo7ttRzZ0cpPVJUHgP47TCUh9oFcdZWmwk1mhIPNrDFGGMvPJe+r177FR/7G2Lv/SHFPN3IwTOWbm2jbUU+koYrAFx8ihPxkR9qwR9ykm/WcMCaDXJs2qYoinHZphHmFY5lZoqVWZUXlz2/t4gf/Wt+ne/ZAn/9fn3Jz2ytbOfOej1i+vi7FVXhcbho3nTKBN360mFd/uIg/XTKb604Yx8xSF5IocMWxo+Pb/uuTSpTknlL7f/tceCGMGdP3nJLJe38Y6r4vuGD4Wp2B0l/HTU7ojFbsaEYQBI4bk+hc/3FlO4FItBhBFMizm4kFebyhCM09AXyhSMo16g+CICA5cxFdSQwuHEBuqUypsBrBlwtBHSpF/S+A2+3G4XDQ3d2N3W7f/wtG8B8DVZGRW/clnEJ1BqTsUQhiIvrS7Q/RGu0CbjXoqOz0sSoq4p2UbWN2voPT7l6JqoJdb2bVrYvYn4V+b/RuSFlUpM2lN9889H2sWDG06qYY2ej9DY2Nny++OPQUxlAgywOn2PpAUJn0/Q/QW0NE/Dq233siGQUhfvSvfQA07bLwvXO175g+EiRfbkYAZHcn9Svf5Nt/fZ73t1fwvcVHcOmEMdTVtLJyZzUPV1ZylDOd2/PyyfT5KGxqwBIIDHIivZARXUW3J0zXKCyEiy6Cxx9PfTy2/SOP9GWEL76oGcEkC5SLilIFMLJM/e//QMWrr2u7mlRA/pHjkaxWTDOPQUxz0iNaade5UIGdrV52NHvit1xJ0E0LVp74uDJ+CJ0kcOyEbE6bWUjbjiwuOF8rrRcNESx5XVgKO3GUtWDJSzWkTDPpWDw5l0klTkS9RL07QDCiML84nUVJE3oMl/9tLZv2aeT/pR8sZEwvI7xly/rqf4qKEu0kBrsne1/KeA+p6D3a2tq38quoCO65R4vaHIg/zkDfqYrmHs65TwvvZNgM/P3q+ZRm2Vi2tYE9bVo1VYbFwFlTcsm0aiTUG4rQ2N1XxW/UiZj0ElaDDrNeGlBjo4YDWl88ObbYEhBdBYgj3jgHhQOZv0cIzgj+4xFvpBmKroREnUZuksoxw7JCbac3Li5OM+n514Y6IoqKIMCVR5Twz48r+OcqzS5+6dhx3Hnl2N6HGhZi7q6RiLaSHSpuuEHTGQyG/ZGNmM6mqurgWjMkY+VKbTU+FLim1VJ06lYAPPtcVDwznyVXtHPcZRqhLLXamDNKK8t1RTqwyx4iLXUEt6wh0Obmi1Xb+dVHn7OluxuTpCPT4aCxu4sZko6HQsGEUHh/iKmeb7gB0tOHxzRjeOml1Fm5v9k99mGfd16fGbvZ42X3Hb8HWcZWkE7hCTMxWnQYpx+FLiufoKCnQchG0Es09QRZW9GKHK2fHxN2k1NcyK9e2ILbn5rOUAIGuiszMGV4MGX19FucYzZIzCnLJDvLin4Aj5YTyrKYkyT4lRWV0+5eSWOXH0GAt39yHDmOvn2TehOTI4/UIjfDuScHupSXXKLd3xkZGueMaW5uvLHfXQ+KwkKoru7/e6CqKlc/9lm8WjHLbuTvV88nzaLnyXW1BCPaakUvCswvSWdyjh2nWZ+yWOoPOlEgzagnrZ+GmwCqHEHpqNP0glGI9myEtMwR8fEBYoTg7AcjBOfrB1WRUdprEwOFKCFllaY0spMVlfouH6FomD7NqGN1TSd7WrXXHFHo5MiSdE78/Yf0BCIYdCJv/mgxMycbB/UaGQoEQRPjDndeveUWKCtLjbokTyjNzUMb7IcaDRoKnn12aERN1EeYcO1H6NO0CWDvv+YTaErnp/+uIs0lgwqnTsjCrJcwy36yI60o7c0E1q8k2O1l7yvriATCqIWFVM6bw8a6OjweD6ePGsVZv/nN8E46FlU588xhhJ+S0HtW3l/YbAD3vLbvfo+dr72BGolgL81i1IlTQZIwz12CaE8nIBho0mWDKNLa42fNnmbCRu0eHuOysKDIyfOf1vDGpnraPQNX18UQbk+jOMfBgqMNGPohNma9FNeeCcA5U/MYm6kRzk92t/C9f6wD4KhxWfzl8qHRyaES4Ng9OdClTEbsvj8Y3HILDHbbdPtCXPP3z9kd7T2X4zDx2FXzsJr1vLytMV5hGUO+3cSknDTKMjRHY39YJhCW4+NLbxh1IukWA1aDLoW8qKqC0tmI6uuKPyZYnIjp+SMk5wBwIPP3SBXVCP5joUZCWuQm1iVcEJEyS/qQm8Zuf3zw0YsCXYFInNxYDRJHjXLx8ue19ERbC5w8LY9Mu3G/pdC33KKtLu+/f5BzVDVPkoICaGgYOllKJkSxDErfbsn7x0Dag96r76GkrgarchF0MqZMLZKQPrk+Tm669+TgrXMx/bgejdwABQ4jZr2EqMpkRtpRPd0ENq1CDgSpenszkUCY4hOOo/i3N3OsXs/lsZLap58e+hvPyNBEQYsWaW9s5coDy20kG7ssXDi4yx303/Wxvp7MX/ycUT/5KZUrPsJd3UrnhkrSZ40msOkTzPNPxGQAp+KmS3SSlWZmYbrIqg4/YZOZig4foihw4ykTuP6k8Xy6t40HX6lnR3szok5BVSDQmoa3Pp1gs5MjlyjMOz9Icl9Wi15itMtCSbqFIqcZh0nHx1XtrN3XiQos39HE5bOLybAaeDFJh3bu3GJgaPfLcErIBzMMTMbBkhvQFgr97Tfxfgz85Vtz+c6Tn1He1ENzd4BvPryWn50+mctmFfJhRRubGhJi4wZ3gAZ3gA/KISfNSF6aiVy7kRyrEYtBTDERBa1hZ5M7gMUgkW0zxQ0BBUFETM9H1RlQ3Fr/DdXXhaJEtJ55IxVWhx0jBGcE/5FQQ36N3CjRcuMYuTEkRMHhaK+pGLmRBIGgAq9Fq6QAFo3JZF1FO/e8tSv+2MVHlgL79xo580zITTVI7hd1dXD55fDkkwf2XuvqhtYtuT80N9Onk3h/aYGhNOGMVbk0dQYxZ7sx57gx5bgxZ7sxurx9UiSqItC4YjyTjvZwxvWJBkpjXBYAMsPtiJEg/o0fowaDVL+3jWCXj9LaGoqnTNYETStXIjQ2ajbQq1cP/Y23tydEwXDgroYxvPQSbNp04CRJECh4/O+0ZOXisdmoWb8Ph8uMWJpHcOtajDOPxYEbn2ogpLfgGl3Mycvv5a2FZxIxmSlv87K71cOE7DSOHp9NZF42x58cxpjuI9hhQQnpyRsb4JKfN5M7KhFxMIgik3KsjErXzOgERcEfDCMJcHSpiy5/mJ0tHsKyyvZmN1lGPR/t0r4fWXYjC8dnDXi/3HNPwhooL0/7iIaCvDy4/fZD10BzKMdLRv/vx8Add8/lFeUzKlo8dHpD/PTfG5k3JoOfnj6Z+cUudrb0sKO5Jx7RUYGmniBNPUGIFpvpRYFRLgsTc9LIthjwhiLx8ccXkqnp9JJhTfSrEgQBwZ4FOgNKRz2gogY8KF1NSOn5h//i/I9jJEU1gv84KAEPSnttwh1UZ0DKKO7TJbwxybRLFAQCEYXXdzXHdThlmVbyTXp+8K/1hKK59jNnF3LLOdNSjjfQ6nU4mhToX996ONE7vB+LBA1XBAra9bz/nd289kUjPeGBtQcxqIpA6+pxLFxoZeaSnvjj2VYDC0vTccg9uOQuAlvWIDfuo2bFDjrLm8htaaasuhLhllu00NfBzILPPKO9YRj+h3WY0OFwsG38JAByIz6yrj0VURKIjFuIY1QhIUFPg0GbkfV7ywn8/Be8fPMDgCZ2vXJuMaIgxDVY9fUgCCrHXtzJ8d9sR4ouSYM+gSw1jeOPMA/osisAERVejrr55qoOXlpbSa1X06P8cOkErC2j95tGiqGgAAIBraJuMP+lP/0Jzj9/6NfsYFBU1FfzM1iG8Z//DrEuuIWVOxOEXCcKXHLUKM6fX0y+00ybN8SOlh7K27y0DWLGadaLTMxKY0yGBQE1pfeWWS+RnWZKsXtQAl6Utn3E7BNEZx6ira//0Qj6x4gGZz8YITj/+VC8XSidSb2DDBakzKKUail3IExrUq8YvSTS6g3xQUVbfGAbn2WjyGLge//4Iu7keuLUPO44f/rgPWWSMFRNSgyxQfTyy+GJJ4b+ukOF/dnaDyRM7vAEueFf6wc0iFMiIorbxuyyNE6Yl8borDSqq/XsUzsQjQmGlZ9mZE6hgzQhRG64hci+PYR2radu1S7adzaQ3dbC+KpKBJfr0LDAZAFSMiP4Coc0RRD4bMZswno9qgytR13NkmkViK4czEccB8B2Tx7WDK3hZsbdd/LG+d+mzq2Ryhn5dqblOchLM/LyywLfviHEeT9romhignS21Rg4erSDKROiH6Isk37v3SgOJ54zvoGcnSiNbveFWFGpEZo3nwO3S2ssK7st/PKYhfz4h9IhibTE7v3nnz/wSqgDOWYyYR+qML+yUuWT8hbuen1H3PAzBptJx7jcNMbl2inLS6Mg3YLVrMMnqzS6A9R0+fGF++bVihwm5hU5Ux4TBYF8hxlTkkYqdXwTND2h0XKAV+B/CyMEZz8YITj/uVBVFcXdgtrTFn9MMKUhZhTGc9WqqtLmDdKdVG2il0R2tHjY3JjIoU/OScMhCPzihc14orqb4ybl8IeLZg7oDtwfDiQoIAjaStfnS3UAPpQYbn+e3kjmBR2eIN/621pq27UKNaNOZGqRk/F5dnA7MAbtTC6xcuwxAt2hMFUdPirbvVQmdTfXCQIz8+0UO03oUMgLNSJ0NhP44kPattVQv3oP2W2tjK+q0CqkDwXB6b10h8TyHb46kpOZSbnNTmOUZLxvvY4fXrILEFDnn4PNoccrG2i1aLlPMRxGNJn49+bUhqBOk55smyFeygya/dO615z89DtGMjI0RtHTpSA/8gIzx3kR//IYbN2Bf/6RtNz/VyLFJdR3B1gbJa7Pv1GHYtAm9Opls+jePYT86xAR03q7XIcukHb55XDiibB7N/zlL9CWGBr6VOzD8EXQgbDM4x9V8OTHlfEI70DINNqYlJPJGUe7KMi0UN7uZU+bN97LDsAgiZw6IRujJMQfF4BCpyXFLVruakT1RAcHUYeUOxZBPESlkP/FGBEZj+BrCVWRUTrqUQOJVIdgdSE6c+PVBoqq0uT2p4j7BEFgRUU7TUnlnLMLHOyu7uSxlRXxxxaUZfKHC4dHbiChSRnOalRVD8/q9fLLtX48A6UHhoOYXCUYlrnxqQ1xcpNlN/LAN49gQr42eKiqSlWHj12trTzyuQ93sG/7hRybgdkFDixRUXFWuBUx6CWweTXexk4a1paT1d7G+Mq9CAdiGjQQ7rmnrwp2IFFVzMDlQFTcQ0VSfibrqqvjBEfXsAdPwIDNFCLStA8cY7FKIT78XGHqXBFFr8dhlFhQnM7aJFPKrkCYriTTStWn5+Ef53DZRQIZGdrj5btErr/SxM4PF6AC8rgx6BadjmXNJxi3byNSXEIkerNEZCVObvzNaXTv7tto80Bx+eXwt7+BwXBgZd4D4f334bHHtL8XLkx4Ni5alNCWJ2O4fbRMeonrThjH6TMLeHVDPbsautkTFSH3RlvQw8c1Hj5+phqLzsCVx5VyxZxC6nqCrK7uwB3QtDiv7mzmjIm5WA0SgbAc1fEEKEq3xNs+iI5clHBQqwxVIqi+boSRVNVhwQjBGcFXinjrhXBiUBEduQi2RAuGiKzQ6PbHPSsAfGGZDyvaCUQf04kC84vS+dfKvXy2NxEdWDQxhzvOn46+l1fFUKpGkptOftVxzgMVMPeHvDyNvNz68lY2RyfVLLuRf377SPKcZlRVpbzNy+rqDpoH8AIx6UQmZ9soTTcjCAJW2Ysr0okYDhDY+DH+pjaq39uKNT+f8d/9DkKsOdbzzx+aN5HZ17wOGLyB0513ao+/9JLWA+BQIbk/xtlnYweUP9yHKCnMtH7OJ7sv5uTpezC27wU07yVnpJtwOB29XjOoPHpUBrMLnVS0e9nZ4qG6MxEhM6p6HrihCG+nwHmXahGdgB8uPctMcW7S5zNmFCpa1KAlfSxWiH9nAslVPx1WhmtwORiefBKWL4frr4enntrv5kNGXZ0mVu4t1Xryyf4F8wfaR6sow8p3l4yL/9/tC/H353u48y89GNO9WAq6MOd0x0X2vkiIB97dwxMfV3Lh/BLOnVfMqn2dVLR7kRWV5TsaWTohB4dRIhhRCMsKbZ4g2Wla9acgCIiOHOQWzeBR8XVprR5GSscPOUYIzgi+MqhBn0ZukiqlxIxCRFPCVTUY0cTEiZCvSq07yKc1XfFtTOiZlmXh1uc3x63uJVHg+yeO51sLR/UZOIZTZXT22dqcfOGFh6ak9atGrOXSYysq4n2ITHqJ+y+bQ57TzJ5WD59Ut9PSy4tFEiDTaiDbZiTHZsBh1Dw/DEoQV7gTkxpC8fZokZuqGirf3IQqGZh4/72Iyf0kDrTFeW988EH/7HQw5prcdvtgCE7MlS6GmAlg9OYRzzsP4e2NsG0lDp2b9RVWTp4OSk8nrc0yWTkSR87o4cGXMznzfBlFhS5/iAyrken5Dio+dfDkbyPkTHaTWRDmo3+n014v8cvfBbBpVja8+KyejnaRs07s5bibn4vc0EJ7pkZwPD6N4ASTCE7Eb+BQo7NTs1U41Ogv2Fdfry06eutvZFlLkQ2UGh5qE1qb0cCff51BW12it5RkCmMt6sA5qQHnhEYEETyBCI+trOCp1VVctWgsE7Os7GzVzEZf39nMkrIsnEYJFU03aDFI2Iya9gq9SWsUHA5CyI/q7RyJ4hwGjBCcEXzpUFUVtacNxd1KvCGjZEDKTK2U8ocjNHT749ETSYCdrT62NCVSWds+tvLuaxHyjt+AIGmDeYbNwB8unMmc0YkBKoaBqiz6GzRjyMz87yA3oBG1Tyta+cv7ewBt0L/j/OlMyLfzXnkr6+u6UrbPshoYl2Eh22ZMqdbRqRGc4S6sig8iEUKV2wlX78LX0k3lmxuRgxEm3v4bzL2bZe2vFfpQcdttib9j7BSGxlwP5hwyMuCBB+B730uIQmIWvKIYP07ZGUeyd9tKAMINiYaW3dVNZOUUaJyrywdo93uXP4TdpOe15WL0/tSxe2tiwsvNV7j4ci015fPCQ/doJGXGlNR0ilo2mlUN47FnaKTO3R/B8R16gjMcnHMOTJyY+hEOB9GqfG64QQvWLV/e92PvjeE0oV21qu++5IAed3kO7vIcmj4eR/b8SrJn1iGrKoGwwoPv7eHWc6cxI98e99R5v7yVC6blxRdnLT1BLAYdYrR8XLTnoLTXAKB0NyGY7QjSyJR8KDHiNDSCLxVqSGtApxlfRaMyRovWeiGJ3PhCERq6EuTGIAnsaPXGyY2iwFuPuvhghZv8E7fFyU2xLZ1/f+/ofsnNYOZjscf6a4Z5sBYrXxaGEuF+bnmAX72wOf7/908cz3GTc1lV1ZFCbnLTjJwyPotjStPJs5vi5MasBsgOt1AQasCm+JAb9+Ff/Qahqh2072qg4vUNyMEIeWd9g6wTju97AoN1XDxQ1Ndrs+Y55/SdmWLMNbkT5MGcQ3u7xhKTFa/9HCfz6AWo0ZzGNHUlVS1aY8388Kb4S757aSt2Q0xAD03dfm76odrv/XnN90MYol+PfzxqoLVFe10gmDqEC8EQTaYSnNrh8PfXxkv9alMhRx8NEyZogbADvQVi/oy3395/c9jeGE4T2v1930OdVuremso14xdx3rzi+OMPf1DOcaMzmR3tYK4C21s8WAwao1JUFX8S0RTNaYlmwaqK6kvtLzaCg8cIwRnBlwJVVZC7W5BbKlL0NoItAzGzNGXlEmt2FxvnjTqRzU0etkbJjSzD87/PpCZYTc6RiUaFbRuKWfPneWz5wtRvxKW/lVnqOWqDZkzMGMNQsypO59C2O1wYSJYSh6Cim72Zzqi3x8LxWVxxzGi2NLpZsy8R1z95XBanjMvCmtRQ0CgoFIQayQm1YFECEAkT2LiK4JY1KKEQlW9vpe6jHShhmcyxYxhzxOyBw15nn625EKel9f/8cDFYFEZVtZ+rr9bSWrFzigmS93vRhnkOUYZsyMjANU9rgZBnbOSttYXaZl43O9Zr97FOUnAGW9BFyWNQVvjtXX4mT0u9btk5CudfokVvvF544mF9/Lk3P7ClnsemrRROSzR1DLQb2fheGtWbEttJpoR4OSPj0PHMoeLGG+HSS7XA18Fq2/74x8H34XJpYuW9e7W/n31W+34PFpEd6vd9fLGZX545hSPLtHuoodPP8g11HDs6E2NU87ejxYM+qULKE0ztNybaEgsxJamlwwgODUYIzggOO9SwFrVRe5J0CzojUvZopGillCxrA8+b70Vo6EyQG5NeZGODm+3NUXITgX//3oW/aDuO8Zojq6pA3buTqH9nCu1tIiecoPlhJC/aYeiRmPPPT31t78V6f3A4tIX9V4GMDG0Q318Dz+wFFaSVagLsLLuRW8+dTk2Xn7d3J5yfF4/JJNNioCepYspOgNxAHXpVG5zlrjb8a99GbqlDlQV2v7AWT4322WZ2tDPh2acRB/oQQHvsO98Bt7vvc4cLHR30uTHOPnv/F204SG77AGSfcnL8qYWjK5GjioASz4cgaSRFDPnIFhM6mqMXybzyvo+H/uFn4hQZUVT5/o9DRNtW8fTjBjo7EsP2J59bqW3Q9iu0dyO89TbTP7w7/nzjFjvP35nLu48kSsIlUxhJghde0BqpQ1+SIwjaz6HioIcLHs/gz3d0aAbZY8ZoJeQXX6z9HujWhEQGczDil5WlBe1WroRvH5cQKD+6ogJVVZmep1UiyorKrjYPseyuJxRBSSotF/QmTY8DEA6ghvsLuY3gQDFCcEZwWKH4upFbqjQxXRRCWiZSzuh424Vly2DUKJWnXggxarI/Xq3Q2SqxsaGHHS3RUUyFZ27NxZtRiTlXmxzlgI7K54+gfX1pynGTMwYx8rRjx9DOuaNDe+2tt2rtka68cv+v6e6Ghx8e2v4djv1vM1QIgjZJHX+85r8zEERDmJyj9mqvAe48fwZOi5539rTEHVhnFTgYnW4iEEk0aczSh3EFW+I1N+GK7QQ+/wDV50GoaWLXv1YSbNfIZ3ZbKxMqyhFjS+r+0kMxEdSXZffcG73PqaJi8O0PBFEmnXnsMYhGLa+kr16HaVzUQTvgQ+7uim9u8LaQY1SIhBIz6pJTIrz6oY8V67xc+E2NWPp98Pe/JqI3ALm5AuVtpYiOXMQpc2HRIpSkUb21WdunHEi8Tm8L8Oyz2mWIBbJ63zsul9ZE9u9///IjPIcaN988tMwlJDTq+6ucbG3VolCLF8OpRzsZ79TK7lvcAVbtbmF2oTN+3XY098TFxaqqkZxkiLE0FaD6exjBocMIwRnBYYGqqsjdzSgddYmWC/po1MaREzfvW7YMvnOdwu/u8fOz3wbRR8fhV5dJ3P1UInIjCjBeyKPJ04l9jBYtCHsMlP9zAZ6qrH6Or/2+5hooKdEGouGIGlVVGxgvvfTQBhqKiqClRTMbe+YZLfIyGDEZDJKkZXpiuoLBVp62knZEnfY5fGNOEXNGZ1DfHaDDp02e+XYTx4/JjEduBCDfYcbqTxAR+b77CO3dAqqCtKua9n99SCiqKYh53YjJs0JvYdNQOzAeTiSf0wsvaLP4oUY0xyGZzVjHjAYg2NSEfvzc+CbBTR8htCUmM7O7njFOuPdOI82NiQ8wvzBxre67y4jFJPL++9q9s2KF5nV44lIjYlpGPM2b7Pc0c472mUe8BsIejWyljWpn4oKu+DZnnw3V1VoVlCuqa25v1+7/m27SGqgXFh78ZflPQn+au2XLtMjO4sWaGBn2L0gGjSy9969EhKzVHcRh0uMya2JubygS1+GAZnuRjOT+eqrc12tqBAeOEcn2CA45VDmC0lGnGVlFIVgciM58hKQWyLIMz74Y4bUVAVyZ2oijKPDog3qqQ13MPjlBbs6akodeEShcsjP++to3phFsHziGrqpfXaCgN5KrOAyGRLUyJPpHDReyrIXKY0j27entdmwfncizLZ6kdU1Mdn+eVeDAF5bj0RyrUYdJDSHL0XLxT78glJ4YLgzPvEdDbon23hSFUbX7+ndW6ZW2+dI6MA6G2Dldd92hJVv91CEb8/Lo2bETVJVIWEBXOI5I3R5Ubzfyd69BOus81AvPBlSE1krOOXksx8+zcsFlYa69PkR2jsrmDSJ33Wrki7U6XnxRi9YNhuTWAOdeIDNrIjQ2CpSHx7BslxbGvPu1rdx70VQqKirIyMhg3bpifvtbsd/qwrvv1qwSMjO14FR5uXafHS6n7i8LybdmLGrb+/0rUS5y/fWafifZHSB5P3Ig8d3oiRo0GiTtGxGSVZLbhcm9DyIksSj1v6Rc8z8EIwRnBIcUaiSE3FoNckJMJzpzEawJ4z7QKgq27g1y532J7ZqbBH7yfSN589qZc3JCczNOl0dpuoUrH/kUQacNAO0bi+ipHGJ74/8AxDqU967ieOGFg5OB9NYV9W/mq+Is00ZmnSQwZ1QGoYjCrlbtGhslkfFZNlp6Evl/u0mP0pPYufLJapSpWhi+Z2cXXaoeWacNH9ntbZhCAzcl7PdE/xMwFHHVQOjNIAeoQzZmJxhooLGRtG6IrdED5xyH7bd/gKxM1OOPAZ3EwklV7PrUwVnfzGHxHCsFhSrVlQKFhcKQq4BEQUAvCYRllbCscOyxKoIgEJaLWffnSrZ+uoLlyz/hlV9uwd2lGT2aTOegqtcAS1L2FSvJvumm1M4Yv/ylVsF0772aB87XGbW18ItfDFxdKQgDk5sYklOAsfYwhiRzUSUpaJOswQE0e4H+NhzBQWMkRTWCQwY1Ek4lN6KElFWKaMtIITehiExdpw+bK0Fu3n9Lx3mnGSk7qSVBbmR49ne5BJts/PH1HfFmkOlGC+ya+GW9rYPCdddpBObOO7Xwf3L1xrJlmqD5YDx2+qv4iKUcYmmwN96NIKVpQtaphU4sRh0NPQHCsjbQjs+2oRMF/NEmgjpRwKQTUQOa9inY7uXx9hPi+//uiu/wYfoi7R9VpagxtY/SgCdaXj68N1dQMLQyn8LCw+MyNxDsdrBaUx+zWOBb34JTTtEEX9FyHV9VdXwTxetD98NfIu2oAkB12IiML0b8zg9h3ab4doUZ3Xzx5l42renk9t8JfPihQFVVKrlRVRU1EkLxdiJ31BFpqUIJJBS3sTSVCnG376aGejre+iPVy+7AXf4pxsxSrrjiCk477Rr8/o+BS4HE+SaO1be6UJLgN7/RCPpXjbS0A0/zgmZptL/qysHIDYA+LbE4iPneJPe3iiSxp0Fjhl+1Zfp/Gb52BOehhx5i1KhRmEwmZs+ezapY6HsEXylUOYLcVp0gNzojUvYYBGPqRNATCFPb6SMUzUP7fXDzT4zc97DM5X+qZcJ8zaI+EhJ49tZctq9Ko0qu4fnPNEMsg07k/v+bQfVeHe+++59f5RHrrhwTJMaqN2RZC3sfDGKuxP0hZtp70UVw1FGJQdNi1KIuyYNvulmPnLSqNOokBFWJa6dWbMnG4UiQ0Q1VeRSYNVITCeiwhFNLX1MgCNqJHnlkomRnKLjlFti3b+Ayn+Ttqqu1kML+Sl+SkdVXtzVkuN19y3e8Xq2HgMUSL9fpPv0MOj/9DAC904nD54O6OgyfbY+/TLVZEPx+pPMuR/zl76CzK/qEQll2I+cdX8XC6Q3gbkDubNR+2uuQm/YgN5WjdDZo/ikhH0pbDWpQ+/7EHXOBLl+Ql156ifnz57NmxTvkTZxL6bm/JvOkG7juV3/kooseBp4AHMB1A77t3tWFoOnJvmoYDFBZqRH6731v+K8/eI2dSta8qvh/C8dn4Q6EaezRCiuyrAZkOfH9Sk4hguYNFkeSF9gIDh5fK4Lz3HPPccMNN/DLX/6SjRs3snDhQk455RRqamq+6lP7n4aqyMht+yASTVNIeqSsEgRdYpBVVJWWngDNPYH4CkYviXz/aiOhnA6+dXsj9gwtguDvEXn8p/ns+CSNUXNbWbY7MSH8+htTmFrkZPly+L//g57/8KKDgfzgLrlE+/tg0NuVNVYt1tvrQxRTU4MA4SSho14SU3QBkihA0qC7r05PgUszIYvIIoLfg0nSBu8N/tnIN/wwUVecjOS0zZo1w3vDkyZp4ohgUBMC916iFxVpPaV+8xvtIkiS1oBzqCtgRdl/dGgoCtMBoAIVJaXx/0vmzkEXTQcJ3kRZuGrWJjQhEkH853NIi05HaE7K+URt/LWfDu3H363lbvs5qtxegxoOYjNqjrmqqvL43//OVVddRU9PD/feey9/feJp0kpnoE/L4KnVVdEo4EnAAmAzsLvf9xTTqRxIZeLhRHu7dnstWqR5PR4uDGRMaC3uwJKnfT/G59qZPzYzpQv8+Cwb3qTKKZsxVRmihhL3Q7LgeAQHj68Vwbnnnnu48sorueqqq5g4cSL33nsvRUVF/PWvf/2qT+1/GkpHfcK8T9IhZZUiSKnkpqHLhzupO3KaSetldNpP65l9UoKl7PrUwr1XllC9xYJzYgOuE9YTia5+Lj2qlNNnFcYrjQcLKxcWDj5/CcLBLeIPFDHfueeeO/B9ZGRoc3tyyiK5AqS314co9ENwkiI2eklIieBIooC6fWv8/+paQ5zgNHalUWJKrFZ3+Kew6rQ/9F9rnGwfO1wNzoUXJt7MzTdrF+2WW1LLh3pfgJtuGvr+Ozq0mbE/QhS7XgeYO1QEgb0lo/BYNXM9q89L3j+eRM3KJDx5NIGzF8W3VS2pK3ahoxOpJ4KYWQK6QVoqCAKC0Ypoz9a+b7FIqSJrnlPdTdgNAhu/+Jx7f38broxMnn/+ea6//nqWzioly64dd+XOZkoneSkslAA74ANMg76/a65J3GsH2m7hUCN2e8UqCQ8lYkHIhx5K/B9/TpLJW5QghJcfq1XNbWtKhIXGZljjEWujTkQn9pp2Q4nGqiME59Dia0NwQqEQ69ev58QTT0x5/MQTT2TNmjVf0VmNQJXDqIEoQRElpMxShKSBWY1GbmI6AAHITjPR4Y/w7OZ6IqK2sgn6RF68K5t//CIfd7tE2em7KD5zE5FomuT4yTnccPKEIVUaZ2Vp9iaDmZiBFkX5OiK5NBwYkPDFokVvvpb4mgfD2vVUki6gKAhx3QBECU4wsapsbBTITNMG4dp2J4WmxIFq/MU0frBDi7Q8+SR9aphjJzrcJpu9yUVDgxbJMRq1pXpydGUojLc3Yu+/92QDmljqhhuGd75R+I1GNk2cQmOOVjYs6iTGZBjwL52Du+YTfDdehJKfYNaCL6kbeGwmXbgQ0WRDyhmLlDNGs1ZI+RmDlD9B07fZszSik1GUMIxTFVRPB8b2Sh64+07a21q55gc/ZP7RxwCg14mcP0+rgFNkmY92N/Ozn+0FlgN5wMCTbKwycSiX+sv0z4ndXrFKwkN17OQgZKxXXYzH20raKLtiNdaCLgDy080smaK1PWlKSk9JSedi1qdGb5SAJ1FtKuniBpAjODT42lRRtbW1IcsyOTk5KY/n5OTQ1NTU72uCwSDBYGIAcX+Zzqn/I1D9iWsqWF0p/aQAugNhPDFvFQEKHRaqu/y8uqMpPscUOc0snZvDsVY9lReHeKdlE7vbE7mdM2YV8OtvTEUniaxcuf/BtbVVC1n3X1GUqGhyuRJ+F18nNDVp6YHGRsjO1rQ8g1WA/PAmkfwrJAJhGbdfi6IlryIjskokqXpDJwqQ6Yz/39OS0Jvsasgmx5gwx2sK5ZF323XAR9oDseaWyXXwcPBNNnt3WEzuHn4w3jr9Va10dEB6+rB31eZMZ/fosfHqMmtWGqMXT0Rw2ghDaglwOIJh1SaM73+u/d9PFZYgCAnSMgBUVUUNeCEcQswoQu1pQ/V1garS3tbCF2tXc/Lp3+CcCy+hM6iQZlGRRIETJufw4Lu7EESJVbtaEDc/jiDUYTbfh893aKoTCwo08+gnnzwkuxsQkqRJvGI4+2zNu+dPfxp+UVLvRvG9qx/PPhtmHe3lV0/vYltrwgHcbJD4/QUzWFfXldL2ZNHoDLqjkWsBcFoSBEZVFZSuRGRTtGenFGOM4ODxtSE4MfS+AVRVHfCmuPPOO7nly6yu+B+E4ksQHNFiT3nOH5Zp8yQIZk6aidruAMu3N8bno+l5dk4an40oCOSUdXP3uo3UdWjRAkkU+PGpE7lgfkn8Mx5qpiO23dlna/PhqlXaY3l52lwrSdrceCgaW3/Z+O53NefkoSBWATNa1BNApidKcPRJy8qwoqSIICVBQE2zgCLT2SngEBMj/u7GbIoNqxPbBmUWkiT0r6vThBDPPw/nnZd4PNmk50CRXM4TM4PZX4OxA8WDD2okZwg10CpQm5dPdVHUF0gSyZuYS9a8cZBkuieIenSjJ6Nv7Eb3y9sQKqsTO+nHR0AN+lF6OrVVvt+LGvCi+D2ofg+KtxvV0619/6J6HMHmxDB6KrqCsQhmM5FIPWaTkbxsrVdSMBSmucdPttVAaZaN4gwbeysqeOG539BTtYGlS5dy2WXnHHTLkUsvhbFj4dFHDz+5Ae17HNPggBbQu/vu4X2nYxZGe/dq++o9VgB0+UI8/lEFz6ypjqfNASYXOvjVmVNwyworKxPGW8eNycSsF/FFzTDTLYaUhYXq6UjoFg1mBIvzQN7+CAbB14bgZGZmIklSn2hNS0tLn6hODD//+c+5KSkv73a7KSoqOqzn+b8EVY4k8sc6A+gS0ZtYaioGp1kPCCzf0Rg3k5uSm8bJ47VVy7aabq589DOC0cE63WrgjxfN7NMVfKiZjuTtYhVFvTGYMd5/MoZKbpKhF/RAgO4YwUkSHodlNSVFpUMBRRuUw1vLGZ+fWHXuashijkErnZFViZvDtyDRzzL5oou0i5pMaGIhteuvPziF9fnnw9/+pjnPvfTSge9nIAylLjiKsCSxZ9RY2l0uJKOOzMmFZEwsQm/VrplgtaPrCmB4ajnSirUI+mj69rzLU1i3PH0S4ertKCtfQHG3o7g74hVRQz5tTxfBLasIblkFooR13BFkOB00VO6msaaKvOJR+EIyXVKETJsOU+1qKp65j7C7jWlz5vOnP/2JsWPzKSg4uI/nqacO/LUHitiC5kACeoOZcEZkhVW721i+vo6VO5tTiE1WmpHrTxrPqTMK2NjQzYqKRNT52NEZTMy20RwdAyVRwGlJTd0rnkSkR3LmjURvDgO+NgTHYDAwe/Zs3nvvPc4666z44++99x5nnnlmv68xGo0YjSNld4cNkYS5m2C0pnxBw7ISr9Qx6kQyrEZWVLQlvFeybCydkIMgCDzytIcH13+BaNLIja/BQefaWdRMNDNndOoh95fp6MdMVjufiMKuRjebazrZVttFICxT6LJQlG/lD3+38OBdVmp2m0H97xxkTAYBAgmb+OSVpKwoKVVUohKKV7rljHNxXu5yYsLT6lYXvkwLAJIgc5ru9YRzXTJkWYvg9FZDx0Jqt9+uiYcPBB0dqdGhrwg9Vis7xo4jaDRhK0inePFk9BYDgtGMlFeCLq8UyR5NdS04PjVFlcS6laAP74v3aamm4cBgQrQ6EK0OUGQijVWJtiiKjGXver7/zfP54R33kvnH33HFTb+iq6uTzo52nnzoflZ/sgpVhcwjzuCIq39GWdk4XnpJpafn6/cdaG7Wqgebm4cf0OvPhLO9J8jTa6p5bWMdre5gyvYGncg3jx7F/x07BotRx9ZGN++VJwjxwlEZzCpwUNeZIKiZVmOK2F8NeuOWGoLROiIuPkz42hAcgJtuuonLLruMOXPmsGDBAh555BFqamr49re//VWf2v8kkvumCL3Ecd5QYjBPM+rxh2U21muhB0kUWFKWhSgIPPFvP/eu+RyDQyNLnpp0Kp+bC7IUF/UlDzyDRV2SV2KCoLKlpouPdrWwoaqDHfXdBCMDJ+TTz4IivYEFWWMxNBVzx+1fG/39flFUpNmz4AYpSmykpAhORFFRky9k8uc6fiL5J+xEbtGsGP5wzLvkesLQpT3vsdpwJTWO7IPemhlIuMRNmdJXIPWfDJMJQiFURaE+J5eqohJUSSJ3dinZM0vRZRWgHzUBMb0fLUWGE7mlCtGRjdDL+DK47r0+5Eaw2hHtGYhpLkRLGoLZhmCyIpqtCCYbotWOioi/rh5/TQ2h9nYMxQsx6MOIkR7UjnoUdwdXjEtn8zdO5vnX3uDp51OjXfMXLCA8+RzCmRNYV9XB+T8t56W7x/F1gyRpXlPDwa9+pTkR9E5DAby3tZHbl2+jy5fq7+SyGjhtVgEXzC+hIN2Cqqpsaujmnd0JM6AFJenML3ZS2+mLLxLSTPq+peHeROpTsA5f7zWCoeFrRXAuuOAC2tvbufXWW2lsbGTKlCm8+eablJSUfNWn9r+JSNLKpg/BSUySFoOONfs64qXJM/Ls2Iw6Oj1h/rTqcwwOLYzrb06j6sU5qJGYyLL/+XFg8bDKj27rYrtQx4N3NtPh3U/7gF7whEO817CDXGsNpqwZBFrt+3/RASIjQwtEfBlpsXvvhWfrtAPFJCHJBEdW1HhVlSiAIIcTbquSHrUtmhb2+rlg2UW0ZGSya0wZAG7bfghOrNlPfznCQxHN+TIRCBDS6dhVNp4uhxPJqKd0yRRs+S4ME2aiLxnf5yVfbDJhNYSYNEkBVJTuZoSABzGzGEEQiTRWEtq1TttYb8B68hVIrpyUSkSASE8P7u076Nm+jp7t2/FVVxNobBpQRSuaDBQtnoazCO48aToXLFnIRzuqqGzpZty0WeTmFXDa6adR0xbge//4AlVVKZf2Yi3KwFub0e8+/1NxINX8xx+v3ZKx7uGNjZCTq/JR+1aWb0gMKjpR4Ojx2XxjTiFHjcuKO0SHZIVXtjVS2ZGI0swudLKw1EWDOxBP+Rp1Ilk2YwqhVeVIomu4KCGY/8PdSr/G+FoRHIDrrruO664b2G1zBF8e1AH8G1RVJRC1/deLAgadGPeFkASBecXaiuWOZyvRObSVa7DDQuW/56IEk6sMBp4fe4uH/aYO3q7dzuO7+nf+K3RZmF7sZFpxOtOL03Fa9NR2+Kht91Hb7mVPUw+r92hh5iavh7JLP6f8n/MJtNsO7iKlQEXQKYj6CH94QGbt5xGefUG7ThG/HtlniPa0OXQpgltu0a7Vv/6sTYSxCI4uWYOjqElES0BVkmaM1WtR3V1gsyCoKqokYvMmqqpq8gsRVJXihvqBz3p/yvBHHx3WexoyCgvB7z9kTNJrNrO9bAIBkwm91cjoU2dgSk/DOP1IdDkJbd+eSgPPvuzg36842FttRK9X2L2+laJ0TaOhBr0o3i7CezYR3LSSmHm/aebx6LK1/QSamuhat57ujZvo2aYRmuFACYTY99Y6vFOLKFhQxnyxh2MuOgWxcBz1hjwUQSKkwORCO1ctKuOxlXtBgKJTt7L70YWocn8mhyqH8t48WMQKBYaD5BT2smWpiyTHhCZKz0qQmxMm5/LT0yeRZe9bybazuSeF3MwqcHDC2Ex8ITk+9kmCQJ7dnJKagljlqfaZCxYngvDfEy3+T8PXjuCM4D8IyXOGlLiVkucSnSQiKyqeaMoqJ82I3aS1BlhbqykZVUWg8rkjiPj610sNND9KEhyxIMID7+zmuc/2pRzXpJdYUJbJsROzOXpcNplpffed6zRzRJKIeUtNJ7ct38aexh5EU4hRF35OxVMLCHUfaH5cxZTVg7W4A1tRB9aiDvQ2Lar0wFbADGXf7PUKBSJ+A2G3CU9NBp4aF94aF0po+P4YhYVaBwMgXqpvMWoTlz0pZN7hC6GX0ogoMoqqIov6hEHWO28ghVqITC9DtVkIHzkN86pNZHa00+bSnBT3FRbTaXcyobK8/6abg/WgOhxVUN/7nlbJtXAhLF9+SFTkHQ4nO8eUIet0WLLtlJ4wBb3DhmnmQqQMzfPGF9bx5KvZ/PpmB+7uxKQVDotsq3BSNKed2JcmsPo1InWJ6yJlF+Nu8tH5+h/oWreOQN3gKl/JYsFcXIy5uAhLSTGGzExCHR0Em1sINjfjq6oi2NRM29Zawp4AxYsn49/+BTZnFpl2Ay36bFRVpdMfZoppDJ6aDmzFHRjTfWTNr6RldVnS0fxo3jiHluAc6EeSnq7d1z/60fCPB1pEM3ZbJB8/fVJD/O9Tiidxx8UlAwp/Q0lO4ItGZzC/xBV9PMG40q0GdFJf8hL3vQFEi2N4b2IEw8IIwRnBgSP5y68qgDZ5qr02SU5XWQ3aNl9UtuOJaKkpd0UWoa5ezQuTMFDl1Ce7W7jtlW00dSeqtSbk27n4yFKWTMnFbBje7T2tOJ3HrprP1Y99xu5GNwZ7gHGXfsbe52fF01W9fTJ6Q9DJ2Me04JzYiK20HZ15kD5N/b1eBL01hN4awpLnJnteFaoi4Gt04KnOpGNLwaDXChIfy333JVJ7sfJwu0kjSia9RJpRR08wQqs3hFEnxpttBkQzlui+1PFjMf5+GZHp2oQXWHok+jVbmLh3D/sKCqnJ1/o/ue121k+ZzqjaGvJam1OnwUcf1Wak/lofHI4u4+eckwj5DZTP7A92e5/GRCpQl5tHVVEJCAKuCfkUHjUOwWTCNHsRkkMjyF1BA036XJZeDDOO8fGDa8xsWhdLtaocObmB2DcjXLs3QW4EESVzLDsfeYlQS/83liBJ2MaPI23KZOxTppA2eRKmgoL9Vt20fvAh5Xf+ge6qVvZ0fE7JkikEt31Ge/FU9naUM3b2USgqmNND1L8zmXFXfoIgquQsqKBzWwHhbgvwCfAMsBVIA74HnMKhIDoHyjdPPBHy84f/OpdLM/8880zNiTn5+KIhTNoY7fqHPQaevrOE264WBuzWkXzt05IWC+HknlO6vi9WVTVRHSeI+/U5GsHBYYTgjODAkRxaTe6Wm/S3gIAvnFjVxAjO9rqu+GPdO/tnMANVRAE8/EE5D3+QWAGb9BLfXTKOi48sTdGXDBd2s56/XnEEVzzyKfvavEh2HxOvXs2phdM5fmI+Rx6pEa6Ojt6vVMmYWUPusbvRmfsrKwI5oMPfYmfebB1FeRIWo476fRKvvwE6cwjJEkJnDqGzhNDbA3GiIogq1oIurAVd5By1F39LGt66dO2nxoXsNadIMXpXhYQiMoGog7HdnIgEZVoN9AQjBCMKSVXiBFQBiyBqjrjzZiNVNaLbVkFkyhjUrHRCS+ZifPtTSuvrSO/uZteYsQSNJmSdjr2jRtOQk0txYz1Z7W3aNFhXN7AOZ7gOx/uDw6G1uN60SWvlbjBoF+K00zTnud7NwWIQBNDrE3+rKhFRYvu4CXTbNXKbe8RocmaWgiBiPuJ4xDQnAJ1dAtW6XNKjkq38QpVnlvu49lIzaz6W+POtLTjM2qSmBPyEdm3QDmN10lrppf5vD6eeik6HfeoUnHNm45w9G9vEiUimYVaDyjJZkkja+eewe8VHdFdWUf3eVrJPhp/89UVWbtzB/X//F3OPPpaM3AiLFpjY/EUx2fP2IeoV8o/fyb5lJcDNwAqgBGgD1gPfBr46f7H33oOrrx7+68zmRFq7N9e1l7Ug6rTvSPeeXOprhAFvWdC0ajEkcZpUw0ypn3EoEgIl6ltksIyUhh9mjBCcERw4km3u5XC/vXNUVMLJ1bHRL3RyhUK4xzRoRVTvVVRjl59HV+yN/z9vbAa//sZUCl0WDgVcNiMP/99cTrlrBaC1NfDbm1i0SFs2fv/7mrYlBoPDR+HSraSVtqfsJ+LX461x4anV0kyRbhuOTJkTz1A5Zam230gEVv8bOqoEAjLIYQE5IhBRIojpXaSVtGMvacWQlWidYM7uwZzdQ+YsrbLJU+MiXFXAGXPzOOdMXZ+qEE8gqdGfKfGVz7YaqYrqCBp7glj12ufpDcq4DFYI9oArHfXIuRhfW0VkyhgAAucej5Jux/je59jbupi9bQt7S0ppydQccH0WC7vGlFFVWERRYwN5Lc0IA0VqYnX/hypN1d2daBr0ox9p/anuuktzbxuI3ECiB8Ett8CjjyI3NKSQG3NWGtkztGIGKSsvTm7o6mbnzR+R/3BqG2u9Hi6+LMAdP2hh/uzEZxfculbrF6WI7HrsbSI9CR2Hc+4RFF5yMY4Z05FMvVb2yWrY/kp/kpEkLjEB04DaCRPZJ4oEdzZhCfvx+f0EmqpQlIUIgsDv7/dyyTljCXua0NuCOMc30+BYTbh7BXAT8B0ggEZs7gCOAE4b+HoeRsQWF/uLpvZGXR088ED/PeiMrkTaKH1SA56qTBobcwfclzkpOrN2XwdjM6xYDNJ+nZPVJGsNDCPRm8ONEYIzggOGoDfF01FqJBRv+CeJAgJaQD4iq2TaEsSnxaN9wdOticd+8usQj9zSfzuF5BLxGF78vCbeHPKsOUX85qwpg66EVFVl+/btfPTRR3zxxRdMmjSJyy67jLxBogdvb07k40UBLligTW7LlsFjj8UugErm7H3kHrsbyZBgcZ078ujYUoinOgNUgeLJAY79pptpi1oxmFUagce/SBzr0jv7P4dISKCn00pPu4P2NoWGjm66Qh2IDl9K8MxW3AHFHXwU2kH3+lwsRQXMHpURj2QlN9JM1gSMy7LyWa1WrrqtqYfjx2bgCUaQVRWv0YE1qAm21csvRnf1DzC+uZrg0qMACB1/BKHjj0Bw+5DKaxi1t5a82mYqRQs9Nq0qJGg0sbd0NJ12BxOzslIb3yVP2EuWwBNPDPhZxPGzn2nkZagtV2QZ/vhH7e+ZM4f2mrIylPJydlz9bbr37AFAZ7dTdsZ8BEEjivp9HRDdnXjXfSx85UU2/uib2Mcmqu7cHQqXHNeEWdJeo6oK4T1bUDqaCXlDVL21KU5udPY0Sq+9hryzz0LorzdWbzUsJFpi9P6CxPpypURRoXj3LgyZWexRVX57/GwuWXIUC5csoFv14xWtWK0St95ZxZXXvIllrBtzXhmuqe00fyIBFwFjonv7JVok5xfAEuCr8RlradF6yQ231cqNN2r+kL3RvrGY9EkNGF0+JFOE0nM2sLa7lHMjE9Dr+n4mYzOtZFoNtHlDdPrDvLK9kYtmFGA2SAQi2ljgDUZwmFMXfSl2Gv12hB/BocQIwRnBgSNJWJzinSII6CWRUNTsz6yXsBt1uIMRmj1BVFWlNDOhIzHkdFJdnTekBWpYVli+XhvodaLAdSeU7TfM+4c//IE77rgDj8dDTk4Or732Grfeeiu/+tWvuOGGGzD1Xi0DS6bm8dD75YRlhRtOnsARozNYtkyTdwAYMzwULd2CtbAr/ppQt4naN6fiqc7EZFU4+pxu5ix1k1M6vHL1GHQGlfScCOk5EYqBmZiBAuorJTashvrGIKaiFkyZ2upTNMhsaa/nmr/Xk241sGhiDkum5DIqO1EJlpy+y7ebyLAYaPeFqO32p0TQ3LIOaygCBh3qkkWoBXmYlq1EbO3Cf8nJEF3BqnYLkdkTiMyeAEDmjk5aXm/HRC2CU9thuyuDne+8x8Rjj0XU6/ufsIeCv/wFevqvkhsU99wDb745pE3V7Gx2/fZWOqPkRrJYmPLL76JWfAaA2NKJLs0eJ/ZCTQPCCy8wc34+Hd4QbR0hHGoXJVZ3XKWieN0Et32G0tVGZ3kTdZ/sRgnLCAYDBRecR9E3L0NvH8CSoB/CAiQ6qSYbRQ1m46uq5LS10lRQSP2KnUw+aRry1jW4Zh5DxFzICy8v55c3fZ9gIIC7wYQg6TBlFoOgcMb5Fbz63GxAAaYDRwN/QktdnTyk63qokZenpZsOpJdcf4G8iMfEnieOpvCUraRP0qKNH9dUc9qfmjhjViHfmF1IQVKEWC+JnDctn3+ur8Ubkqnp8lPR7qPIaaLTF/X06ofgoEuqEo0c2LgwgqFjhOCM4IAhJFdOyali2hjBUdGM5HLSjLiDEUKyQrsvzBFjtAiDrKh8vKuZm06ZwKJF2kppsGj8yh3NtEU79R47MbvfEs5k/Pvf/+YXv/gFxcXFvPHGG2Rl5fLaa1U8+eSd3HbbncyYMZuTT17Sp6dZocvCdUvKyLGbWDqjAFmGa67RntNZgoy9bE2K1qZtQzGNKybgDHcjiCrfebCW7JLUa+L3iOz53MLCIwVKSwWefkrA3S0gCCqiDiSdik6vIkoqZptCWkaENJeMLT21FrZgtEzBaAAddbtLefMpIyFbG+mTGpCibtCd3hAvr6vl5XW1LJ6UaGWSXB4uCALT8uxxi/k97V5GOc2EZIVgRCH00hsYLtJMiJTvXoX0i99hWLUJqaKO8NGzkJceR0QIQTjhh2SflM7MskzOvfcO0nZv5ydldyKI0P7xKqoe+AtjSov7n7CHggMhN6DdUFu3DskCe1/5XtpWrARANBqZ8sffw+734puZL7sRNZgok1ee/6dGdloqcMgRHGLis1IVmXDldsKVO5FDYeo+3kVXhdag0XXUkYz96Y8xDdBmJn7egxCWPkZR+6lIE1SVsXt2sWHqDKrf3cIonYRD0tOTPoo/3vprgoEAV1z7XU694EZuf/YDWmq24K3bwdpdj5BZPI22mgnRPWWjVVYNDosFfMPrNjEkuFwJXd7BZjeTU+NKSEftqzPw1rooOXknsqrQ3B3g0RV7eWzFXkanZbOkrIQrz8xErxdwmPScOC6bl7dphGh1dTvfnF2EXhQIKyr+sExEVlKipoIogShprVBGCM5hxwjBGcGBIznc2uvLatCJxHz2/GGZAruJ8jYt0lDe5mFBiYvZpS4+r2ynvtPP3z+q4NvHlw0ajT/rLJUnPq6MP37O3OJBT6++vp4777yToqIi7r//flpbF3LxxVBXVwZMBUo566y/8uSTx3LBBX31Q1ccMyb+98qVmkQDQGcJoTNHCHsN6K0h9j41H2+tVib6NBdyQ+lTKeSmcrOZdW/a2faxjUgQVmeFeOJZE8/c0d9ZqyxeIjP3qAjFepXiDIWCYhnJoNDsCbKzIYCPxLUuHB/iyt+G+OAf+Xz04ATsY1s48ZtN7PO3xv04VuxIdD12WY3xTuR5eTD9CFuc4DT1BJmWm0Z79IOL+MLEr0pmopxeamhDWrMDnnkTVRB496UWlv21iu+euJqSrC6Mepknvv1vFv/u2/xoz73cPeUnCJEQjctfpaRqL7qvoulXdfV+LbDDd95JwxP/1B4TRSbd9XvSxo+hZ/Mr8U2lvFLk9tp4D7ZkL6hkyO1NBHesQ/X14K7toG7VLsKeAJLFwugbf0Du6aftX2C6vxL63kZRQ6hIs/l85E+bSsPmLVS9tYnRosCKtvU0NzXxzSuv4ee3ajflv2+5gN8um8QHPe00r/439rKbQHoA5I3AI9G9DZ5imTIFPv98v6c0bPzgB4kFT3L/1gO5rXrfCoWFAvf+soRJ89N56P09rNrVioqKClT0tFCxoYX7XsrjtnNncM45AuMyrWRZDbR6QzT2BOnyh7GZ9PEoji8kYzf3SnHpDBDygxJBVZURH5zDiJErO4IDh6jTViOAGk7t12IxJPJLvlCECdkJt85tTT2oqsoNp0yIp0weW7GXh55yc+65fcf0WDT+7ifb2BFt9zA+z86Csf0k05Pwz3/+k61bt3Lttdfi9S6J7js2KBuBKQQC27nwQh3Llg3+VleuTPwdaLOx58kj2fngcWz540lxcgPwNJeiL06Esr94w86jNxay8T074aCIikhtqyllfzFMmiLzzHI/jzzt56rrwpy4NMKESQppNgGLQWKUy8LSKS5OHpfJ5GwbVp22PpEkOPH/2rnyD00orVl8Z/4sVv7qBL59fFmfY/z11/ksXgwXXwyLF8O0CTqEaP8trRFn0qSblkgjCqs/Td1RtDJKUUWuuimXR1YsYO6vf8B7W7VjZqb5+Od1/2Z7YCYfdJ8KgBII0BpIvU++NIwZkygZLyhIfa6wEF58keraeiLRKFHO0lNwzZ+HYLUjWBNeJf6PlyGm54E+VXuiqipqOITs7iC49VMC61YQ7upk3wfbqHprE2FPAPu0qcx++l/knXH60KpnhlpCH9tuiBVpo847h4yFR6OEZarf3YLaounN5k8aDapKMBAAEb41bTppoy/E4MzBvfcdBHEmcD5QDvyU/YmMd+0a2ukPBxkZCW8nGPgjHSpiouAbboAVK6CqStvnhHw7i2xz2P7AYhpXjiPUnYgUG0c1cuUvmlm2TIuCTs5JjG172rxYkuwpfOG+JHBEh/PlYYTgjOCAIQgCQmygVyIpvalMOileSukLRXCYdBQ6tEGi3Rei2RNkUoGDK47RumlGFJWHPtmCKvQtQ9BWWCpPflIRf+zKRWMGnSSCwSBPPvkkTqeTCy64mJ/8xBzdjw5NS+BCIzk6oIYbbhiOK6qAv9GJqojxthIxeLDhzEpch4qN+zcJNJlVbvtTgJff93HEgtSTiIShq00h0BFC9oZRVRWbQcfEbBsnjc9gYlaChIye4efGx2vInejBpJe4evFYJhUktB0ht4nqzanGYvX1Ak3V2oDbHYygqEnXvyAxYQpbdvQ98cbGlCBDMKLnqkfPp6pFc6qeWdrA3Ze8yr+rTk+8JGuQlEx/cLn2v83+IElayThos1d1tTabPfNMfFbrmTCBxleWa5tbLJReq+UjBVHCsviCuN4sXLmFwOpXkdvbCTc34F+/Cu97z+F799/4PnyJwNp3iDRU0VXRzK5/r6WrogUkieIr/4/pf/0LpvwkEiLLGnN+9lntd+8bcKgl9LHtYhVpA30vBAGKipCOO45Jv7+D3DPPIOIL4a9qQxIFKrZuJCPSgTGqSRs9XuVIVzGi3kL+CVdTcsYPsBR+D3gF+EF0p/2HTURx6Fpwkym1IHMgCILmY9Nbm9f7I33/fbj77v73obMEcUxoRGdJEG1B0HrCJqfCY9nBsMdEy9qx7PzrIurenRR/Td5xu7jhJhlZhnFZCY3bnlYPJp2YMvapvUNL0ogO58vCCMEZwcEhyahKDScM9wRBiK9kFFUz+5uck5hs10d9cK45bixjc7QBQudyU3r2BgRd6kAv6iMUnb4ZfY5WH1qaaeX4yQOXcAJs3LiR8vJyTj/9dGprS3tFhUSgB1gHGIDseKQ/GckDU2npduBRNA+QW4Hd/R73aPs27JkJguNu6z8LvGiRthoF+PktQS64LBwf5KsqBH5+vZ4XHwzg6qhjRlodE6xNjNE1UhyqJzPcjiXiBVllck4ax45yYY5WephsCq/saGRjfTeSKHDrudOxRo3Imj4aR2+DNlUFb7c2qsuKijeYdO0nRXsryTLqrj3IhdkEl8zF+/3z8fzicnxiO6aaVZwwZQ8F6V2Mym7n/479HEVNDCuXHLURU4aDULa2L4/NRsDQNx04IH7xi6FvOxBuuknzw4kh1sn7oou035JE7VPPxHMVJVdfiTE7UUusyynWSE4slVWxhcDa1wht+gilrS6lH1TAHaD63a3s+2A7ciCMfdo0Zv/zSUqvuQohGnFDluHWWyE7m5RwWmkpKaHEhQv3H5ooKkoIUmKdaKEvyenluyDodJT9/KfknXMW88I6xmY4eeeDlWz+6B2cka74y044po5AaxVKKIBj3ALKLjuZ0Rc60dtjRLl/MrV06eCnnYxAYMCWWin47W/7r6qE1I/0+OO1iEwy19M7fBScuI2J162g9KyNjL96FdYibTxJzvTF0Cc7qIq0ry/Bs08j3MZ0Hz22JlatApfFQFa0KrTeHSAQUVLGvt5NfgVdcgRneEagIxgeRjQ4IzgoCAYLKtGBwt8DpsRqJs2oj7cIaPcEmZBlZWWFSFBW2NrUw4x8BwUOM7eeO53LHlqLrCo4ylqYcM1HyEE9gqQgSAo6UzgungX47pJx+zXze/XVVwE444wzekX6ZTTH5bfQNARTAQug0NjYP9+/9957ueuuu4AmwIkWAfotcAHwezQTNI2wfP8vU1n9ZkKEanWmkjUBlcIiIT4nuTIUzrlQG+T8Prj3DwYyJTd//VUbTkffUV9CwaZ4seElA4F2IYMsq4UlZZmsr++m3q2tTN/Z04KiqswudPLDuQu59P+C+JucffaXWRSidKomGNXJKoGIrPXOUWQM6TZARqmuwnf7t1HtqQ7KsqeJiTTx4g39XjYAun1G2nqsWpVQS+waDMPcLDd3cHEwaMYml12mTe7JURBRhPPOgzsHqMNPQrirK3HI0/umXvQlEzEfeQb+Na9FXbsTUMMSndWtNK/fTcgdvZYOB6O//11yTl2aWvq9bJmmVo8JupJRV6eV6d1yi5aHWb5cm/0Hw4UXDrUTbR/fBUEQGH399+lat57rA16+v20rP/7N7dx5i4Apu4gdTT3cefMvSHelU5Rzdlz5ZSttY/xVH9Pw4UQ6NxehqonPU5I0Prl0Kbz++uCnDv2aRw+IcFj7eAey/0lGjOtd8u0esuZVkj65AUFMskuwhBhz0WfUvzeJ9o3a9zd5nOg/OyjQ8ukYbCXaeGfOdtPYqBHQ3DQjrVHtmjckx5tyAvFGtond9G+QOoJDjxGCM4KDgmCyQdT1RvW7UZ258dSRxSBh1kv4wzJhRSUQUTh6lIsP9mqi1vfKW/nm7CImFTi4dtYcHlizAckYiXYX7zuwy0EdVx4xlSVTBw/dRyIRampqcLlcjB49mu7u5Gdjg8uL0d/nR3+rfTICgiCwffsubrrpJsxmO9/4xuO88sp8oB54GfgbkAP8GRB4+GFYk3k249ITDsujZ/jZ9rGWoxfQKl/uvVfznWtvh+/cEMYYDYI98w894zM6uenaxOQXiUDznhbSOz5HDfiRXNlIrhxkRz6mdDtZkTa6VAfdOgfzi5xsbfawJyrmfq+8FVlRWfVuOv6m/lNlx13aEZNRUeL1xhsD2hUPOmTUcIhA3Y4+5GZ/2FBVwCvrp/Dip1PRd3Wgq9qADBhsVgzhYYTlCwr2Kw7m4Ye1ifvOO+HaazUnY69XCws89xysXt2/Z0wS9I5E6i7i8aBL69vh2TDhCHSFZcjtjSirVuJ5aRl1DV10k9onzD51ChN/fwfG3oYrA5V898bNN8P99/dPgnrj7rth/vzU99a7E+0gvguSycTUB+5Dve77fN/v518NdZx82XcwGg34A0EMRiNXXPtdrvreXL59g4q3ZCcGRwDJKFN0yjaOu7iJaeIUWqotjBmTMI+W5f3z0szMwb0Xe+O22+DJJ/f7UQJa9LXWspvxV1WmPC6HJIIdViy5bgRJpfDk7Ziye2h4bxJ5eQniMVB2MNieZG+R7otvl0xoIoqCIWkB1uf993cPj+CwYITgjOCgIIgSgsmGGujRLMhDPoga/gmCQKbNSG2nVmnS4QsyPd/OlkY3rd4QTT1BNtZ3M7vQyZXfyOSOW+dhnL0VU7YbVRZRIiKqrP0EWu0o2yby/bv2P9HqdDpaW1sxGo1IkhSN9KvU14NGxqqAV4Es4LRohbCU0hJCURReeUXkW996GFUFn+9HvPLK5WRkgKpOoKNjEdAN3E9Oznf55jfLuPFGbdFstIzml6dUoTeoTF7o4bUHs1AVgcIiIb6IfvZZ0OtVLr4iHD0emAIebrquPf7/rg3dFHtX4Qz0xJUOcmsDcmsDsBGf1Y5p1rGkW7TITocunak5NkQBdrVqJOfDijb2+mOao9TBNGdUkOnHa6LasF9kyjxNDyUpYZyytqwO7vgCNeAFf5BQTSudrR66GrsJuv2YnBaspfkIWQU0+5z4SePtbZNYvm4K+9pcCGiRjldslyJHIytps2YhfPc7cP31RD+QAW6spD4dkjS0qMTrr8M//jE0z5heSCY44W43pn5mOFWW6dy0naY77qSrpQVZ0kESuTEFAhQ2NZD7oxsRe5ObwUq++8NQyE0MyaXiMcRyNgMhyYvBlJfH9L88gHD9Dygxm/jUEKEhEKFsylTmHH0cS87XOsL+7V644BtH0lO8h4wZ2udQ3tnGPt3HnHdyMWfPL8FgsMYPvz9eeumlw/exGcJHCcBf3tuTUnGpBPS0fFFK2/oS5ICOvMW7yZ5XBUDmrBoc+R7mLTiCWD+9mJypN0ELuU2osoAgqdiyfPExQ58UpQvJagrBGRwjBOdwYoTgjOCgIZjtGsEB5O5mpKxR8SiOUSdhN+lxB8KoKrS4gywpy+KZTdrk9l55K82eIAtHZXDPzQ7OPfdooP8B8cUXE2O4PyzT3BOk1RukzRuizRui3RfCpJOYkW/HaDLT2NhIeno6kgR//rPC+eeL0cH2PiAEXBP9X+bee6WU+eGVV8ToYjsWZ9dqUdvbw4DKLbcYqKtbxGOPPc0pp7zJ3Xf/IH7OQZ9EU4WBoolB7BkyN/4kwukn6VMW0Xl5cPm1IXLztBd9sVrgp9ckyrkbPttOiXtL/P/aFitr1qVhdejJzw0xNq8LO278n72HadYx2B0gqjJtugym5KQhCgI7WrRU2VEXdODIC1K1xUzAI+H3aIPxN25siet+phdZ4qtQl9yNiEq4rgK5qQZvi5e9yz/vMzn72z342/cAexABK3Bc2ElpxhgqLWPo8jm51PYvzFk+YgN52uTJiQjD7bdr0Yo+N1Q/fTr2F5UYrmdML+gzEmLmirv/RNYJxyMajYgmE6LRiL+mhsZXlhNsbNI2SvKAsnk9FDU2kNnRrr3LH/5QO9/k4xyOrumx95ZcKj4U9OPFYCwsZPrtdyD4A8z1dFF2yWzEUWOxzz6GFtmPTzRhdwg8uyzMD66exNbncik8ZRsGe4BQROHp1dU8vbqaI8syuWB+CQvHZ3P22cKgvHTbtgN7u/v5KNnV4OaxlYmChO+eMI609lIuvjf6manQ+OFEAq1pFJ68DVGnoM/t4PPKNo6dqIngByRoqkiox4TR6ceWFYgfPzllLisKyfJWtbcQO2VwG/41GMHQMUJwRnDQEMx2cLdogrmQH8XdguRIVMtkWA14QxFkRdN4pBl1zMp3sKFByx1taXSzo7mHI2Y4ee7FdG76gdTvgHjqGTJbm7zsaO6hutPX71wWiCisrGzHPv1YeO1V7v/LQ/zq5z/jvPMcvPQSXHXVU3R2Po7mtX8lhYVwzz1CymowMVc2o+l0ikhY1Wsj2mOPwaWXaivAV16JVWhpA1t2aZCiiZoWprtN4qV/67jrttTBeP6RClljtFSNokCxpZOYBtVXvpP0KLnxBPR88UmEjOrXmaCoUAds12JQkexRTD+1lMAXH2CcugBbThFiRKVVl8GkbBuSILC1WSOeU47xMuWYRL+dZNiNOsZla6XtVtmLRfEhuzsI7VqPv8ND5Zsb4oOyoNPhmDUTc1ERvspKPLv3ICe5uaXru5jtWM9sx/qkI0TJrsFA9sknRi+jBL/5jWaWMgS9SPw1A03iw/WM6YWcU5dS/+xzyF4v7q3bcG8dfPbVh0M43W5yW5txut2p81R/xzkcXdOT8cEHQ+9TNYAzsuHybzHtiSfZeO/9VL65mTFnSITT7GSNm0mTPoeQYMCZDo8/5+f3Nzt594O5zLlqG3WdnfF2IGvK21hT3sb04nRuP386Z59t6ZeXgtbT7UCwP0734HuJAoDvnTiOqxaNBcAgpd5qnVsLyR7XiWlcLZDaiBYGljPFmmgm9z8NygldlkESiSS3R+lVIqYqSaXh4sgUfDgxcnVHcNAQRBHJVYjcqk34ak8bitGGaIr1phLJs5up7/KhAj3BCLMLHTjMetbs6yAYUYgoKmv3dWLOdvOPD+001Il4esCRJjBmjFad8MBqb8rA0Rs2g4QnpKVCxh95AlMXncJ9997Llopa5k4ZT2PtPrq6HiczM59rrrmLJUtKo3NB6gCUmCtzgLloep3XgbOIrcxqa9t59dUPkSQdXV3O6Cu15xZd1JnY1/Pp7KtO7Uysqipt3kBce7P2fZVLjo2mhHp8qNUauWn2uNj90me4grX9FuPqWqrY9UwtJSdMA/kTlNIJmMumk6tGaNFnMT7Lis0o8XltN/IAqZF0s46jS1zoRBGb7CEj0oHS3U5g3Qr8Ld1UvLERJRTBUlpK7jfOIOeUk9E7nfHXq5EI7pt/S+c//onbZsNrthDuVSUlyTJFDfUU3HQDUm6v6rdh6EUGxXA9Y3rBnJ/P9L8+yNYf3ES4s7PfbRAEXJ0d5LU04+rqHHzx3fs4h7prem/cdlvi74H6VA0hymX49a8Y/4Mb2PL8i+x7fxujDXoEk4XcYmjVZeKXzEgS/PBmH+O3doAhg2DISXeXnx3VnTR1a9q5zTWdnH//Kn5y2iTOnF3IokWpV2vlSmho6Hsaw8FAH/nVi8cSCMnUd/r45tGj4o/3d6s9W+Hns2iwpySzb/q7v9fc+alCs5uUHlX+pI7CZod3EwABAABJREFUFr2EP8n/pk9X8aTKqWQ3+BEceoxc3REcEghGC6I9G8WtlcooHXUIOWPiX2CTXiI7zURzjzYAdvpCTMiyMjU3jbX7Ollf34WiagPF53XRCSYNuoB9FX2PZzfpKMu0kW01kBn9MeokOnwhvqjtYmuTwMlX/xiLPZ2P336ND5e/gDUtjeNPXsovfvxDFi9ePOB7SR04bwI+Bn4EtKIRHgF4iB07PiUraxQtLVrlmDMnzPwzu5l2nBY18XaLfP66o88+3YFwvCFfyC9w3OTEk2r5F6Ao9AStND//Mo7odj0RG081fAujGGB8Ri1zR9dj6tmH7PFQ+cYGsmeUkKuC3N2BacZR5KkyrfosCuwmTh1voN0fJiwrBCMqXp9CSFaxWQXKMrTUlD3iJl3uQmlvJrDxY3wN7VS+vRk5GCH3jNMp+8XPUn2HohoOobERx7HH4Pj9nfFa35BOh9diwWvWokLZ7e0YVEXT3fSH/elFhoLhesb0A9v48cxd9gJd6zcg+3zIwSBKMIgSCCDo9GR0d2L+zncO7DgDiToOBwYSqgwxyuWcPYv0L9bRWVVN3apdFEsiRoOJ7Fw1LmgXRYHsdIkWr4zRIJGdbSMr24pFgXc31FPf6ccXkvntsq18UdnOb8+ZliLEHUx+NVQM9FFOL07n0avm0eEJYdClEuXkW01VVX7/mZbGtZl0KQ2AB3oNwC2faPe5Ien9+EMJgmPWS/QENBIjAFJvIXGyuZ+UGjUawaHFsAjOQw89xLJly3C5XHz729/muOOOiz/X1tbG3LlzqaysHGQPI/hvhpCWiRD0oga9oERQOhsQM4riE2OaSU9YVuiI2pi3eoJYDTqOLE1nVoGDVVXtcd1If7DoJSZk25ick0a+3dTH6E9VVdLNek4an83CUS421KeTX3QrZ/7gZmp2bsZotpJVPJo9ZiPG6g7GZlrZt3ML7e3tLFiwAEdUaJo6cM4Fbgd+B3wHrSS8Fk20C4VFo7DmFXPUOQ1MXOCNVyQBrFnmJBwQ++zTG0wMcCXpKkZRGwwjLZqAWBUk6l5+HzVKbuzTp6Oc8hvOCOSlBDhkn489t99J6/sf0LJpH97mbkpPnI669h2MMxeSa1do17nw6GzkpQ3c9Tk90olD7iHSuI/g1k/xNnZQ+dZmlLCMc+4RjP3pjxPXWpY17cx990FHR2InNht4tM/OEIlgcLtJT67//fGPU71oDjX2RyCSRcuDQLJYyFh4dP9P9mc/3R+ysvoeZzDV7aHGQEKVYUS5Rt16C53fvJzOPY0YHBZyAb2vh/TRkzGoYdp0LhaWplPTHWBbcw/+sIKAgF+E4+cWUl/fwwfbNL3SG5sayE+38N0l4+KHaG09uLfY3yVOhiAIZAxyzwO8t60pHnEam5M2NHdptCopSNXd+KPfVQEwSALhaMpKJ4l9x6lYBEcQUkvGR3DIMWSCc//99/Pzn/+cK664gu7ubpYuXcrNN9/Mz3/+cwBkWWbfvn2H7URH8J8PQRAQXQXIzRWgyKiBHlRfF4I1Pb5NusWAoqp0+bUvuTcUwRuKoBMFjip1Mb/YhScUQVFVZFXzkJAVlTSjjjy7kbCsEorINPcEiCgqiqKiqLEfbbVkMUhYDDoWlLg4osjJp/s6Metn4g9rg06HL8THVe28v6OW5X+6hY0r3uRnt97Jr35yExaDvtdcKQJXRn9eBqpIz83CkvYaTVUv45wwjvOuNAIJfUskJLDubTsfPevqd14NRQc/QQCjnNCvRBq0xUH79pq4n0rBBecz+vrvaSZxscqX57VYubRwIRNuuxX7jOlU3v8g3sYu9rz4KUWLJpEWeh/DhJm4CsdiUgL4RROyIKEgaj+CiKgqOOVurJEeQpU7CFdso2NPI/Wf7EGJyDhmzWLSnbcjxsRBg3m4eKPvXxRTXdtixih33TXs+2lYGErZTrJo+UAQuzH2Jxb+y1/6P85Aoo7+EHsPGRnDq6iKoT+hyjCiXLZxZRRfcTk1jz9B8xcV6GxWMtmC4nFjmTKPXDVMsz6bEqeZAruJPW1edrd6kFUIKZCdl8a1+XYe/6CcsTlpfHPhqJRDZGUNcOwh4pJLtN/JfdWGk9n0BML88fWEO3dyKmt/MERTU2E5cY/FzPyMOhGVhL+zXuy7CCMSJTiSYcikagQHhiETnL/97W88+uijXHzxxQBcd911fOMb38Dv93PrrbcethMcwdcLgqRHTM9HadeEe0pXE4LRiqDTVu9a6bgJk16ipSdATFITUdR4gzqdKCCJYEAABAQBIrJCTcf+WxPLqkpPMEJPNEpi1ktMz7MzrySdqnYfmxq72depkQdR0pE7fipZFbvZFzZz/+pqLHoJl0XPtbc38+G7pciySHpOhPTcMM6cWaTnTmX3Z2/x1iNrySwaTdm8YwBt4HK36/jsVSdfvGHH06nrd15VVTWuIzJIYrxpI4DSqS1ru6u037lnns7oG3+gDYIDdCEV7ruPgvPOxTlrJrt+cwvevXupfGMjjlFZ5Lu9mOqrMI+fgdWZ2W9Tv0hzHf7dG/HWNFK/Zg++Zk34nXfO2Yy56YZUcjOYh0ssapCXBzfeqHnnJxujfBkYhsndASGZRA10HX78Y+04A828/Yk6Wls1EtjfOf8/e+cdHUd1tvHflO276r1YsuXeMcXYxsamd4MxnSSQEEIgBBNKCgktoYQApiaEEkoAAzYQCL3ZYDDVuPei3ru2l5n5/hhtk1bSSrb5EqLnHB1tmblzZ3bm3ue+5XkXLtTbWrVKJ44ZGbr44fbt8XE3fSHWajN37sCEKTMzwsZLLr0Eb00Nze+9T93Hm8B4MFlU4A8FMU2bQ77WQKshA59oYWKOnRFpZtbVdtHoDqABnZrGxceM4ewZxTjM8a6YodaOCiM9XRd+TlSUN5mf+aH3dtHs1BMBJmXlMG9c8iVEogQnSuTDixajLMYRH7lHfB+qQpj+xCkaD+OAQNB6FcpIDKvVytatWyktLY18tmXLFo4++mguvvhilixZQkFBQUTv4j8RXV1dpKam0tnZSUpKysA7DGPIUNpq0Twd+hvZiJQ5Ilq3qhuqpuHyh3D6gnFBeoOBAIiCgCjq/wOK2ufcYzPKpJgNBBSVPW0e9ra6qerwElI1QoEAstGIpmmoqsJbf/szO776mFOvuIHiCdOwpqQB0FS5m6d/9zPaG2pY/Os/c8SJpzEmJ43mHRb+cJmBqhhBvexsfTF/1lnRPgRCKlXt7u7+SGQ7KwAN1ePCu/rfqCGVzU99jIbA4a+9gnHrVl3Rtj/BkJdfhkWLUP1+qp58mtrlK1BcLkSDRO6MUrKnFOtqupKsD6qy/l8LBgm2tlL/1R7adtaDBn7ViOHMq5h/3WnRSTgnB370o+QDJ1au3PeYmn1BjMZLwnTyfQ1ovv56uPfe3qrJv/oVHHaYTupiFeySmXn76lcfxJaf/jRxin1PLF0Kubl6m7NnQ0HBwASnsTFyTVS/n42/uIqujRsRJJGRp8/CkWlCzMzDfNA8kCTcoo02OQ1VkNA0ja1NLrY1Ry2aU/JSOHlCPIFQlN4EJRkIAqSna7S3C72e81g5iUWL+r6k9zzVyjPbv0QQQQ2KbH9sHrkOa6+fqK/9T7tnFVWtHlIsBj75w7EA3P3xbkKqRrbNyNlTCyKxhpk2I+nW6Lin+T2RZAzBlo6UXjC4C/A/jKHM30lbcLKysqiuro4jOJMmTeKjjz7iqKOOonZ/RI0N43sDMS0Pxe/WMwZCAZSmvYgZRYiWqEKsKAikmA2kmA2EFBWnP4TLHySkaGhh9QhNX++IgoBJFrv/JIyyiDGRf1vT8AYV3fXlD8VlXYXdYZIgUJJmZny2DUkQqHP62dPqptkVoN0boMurkls6mo0r3+Tlu2+gdMrB2FLTcXe0s3vtp6ihED/8+S+59w9XkW4xILz6Klx1FdaGmVzOX2kmB4guzCUpOnDGxemqUWO25tMnBG+bC03VsGVnYTz44ORmgEsvhYULEU0mSi+7lOIfXkjjrX+k9u13qP9yDy1barFmOxBlCdHQ/SdLKP4g7TsbUAIhmgNZvN50Ot8aFrK+7OOhzT5hHOiU6IHQV9ByX2Qh2WV/uI277+5twVHVvis81tQMrE6XqM/9pHRz8806GWlr69uaJEm6NS2M7OyB3V2trXFuLdFkYtJf7mTdTy7FV1NDxb+/ZPyFCzC0NuD7+gNMU+dgt4Il4KVVSscj25iU68BhkvmishNRgk0NXYzOsjEupiilJOl1o/7yl9iDa/QnDBP22gmCgKZ1AY+gl1ypAjLQtEXApSxZkomqEhHeDKOoCBae52ZV8Fvk7nVIw6djCHZaqe2K/4n6u1XCVzvc06CiRsYZkyzGWXYMPSw4mhJV8Q5btYdx4JC0Bef8888nJyeH+xKsJLds2cKCBQtobW0dtuAMIwItFEBpqYJQTOVesx0xJRvBaD3wx9c0AoqK0xfC6Q9GtDp6QhYFzAYJkywiCgKKquH0K2zavoNl/3iUT95/m4DfR0pKKqUjR/HzK69k3tx5pNgsSK/9C+mss3hFO53FrOge/KKDWrg8w4srFEYd5iSoqOTYjJH6NMWBeiQtiOp14/3kdbytLna+/BUpLifTtw5CCe2DD/Qqg2GsWoW2YAGtaelUFRbhstkT7lblHcHTdRezqm0Biiaz4rRnWPTvH+9bEOxgLDj7w6KSDPoiCz2X/QP1dV+IX3ExlJcnd34DHUsQdHdVa+v+D1p+/nmdfcTAtWs36358CVoggGQ2MOmSExECLhBE1LFzcJQWAeAUbbQaMgHY2+rl23rd5WmRRX5yWAl2k0woFOKhh3Zw9dU70DMT84B5QDr9kZzsbLjiCrj55kb0mLj3gbHAeGAvsA69ttybQFGv/UVjiNE/WIMlRw+G79qdTfmKQ6C7llY4Xu6ee+Ccc/q+VWb/4UNcQT95qWbe+fVRdPqC/O3zCgDGZduZNSItUoOvON2KKSaTS+lsQnPqLmgxsxjRMjwPJYsDasH5zW9+w9q1axN+N2nSJFauXMny5cuTbW4Y/wMQZCNSzkjU9jo0r55Ro/lcKD5XN9HJRTCaB2hlH44vCJhkCZNdItNmxBNQ6PIFcQdCcduFVN1V5vLH7z9+zGhuueMuuOMuKvbuxm5PIStHt850BDQ6Ah6YtQDDyjXs+KQM210aLlfs5KWROyrAjOO72GbuYs8ufWU31p7G1FL9vL2yDXuwA8FsAUFANOj7C8mUV47FqlXxBGfuXISiIrJqa8nqaEcVBIKygaAss1Kez1PyRVSrxXzbeQgKMsVUcR9LWPTGa/s2WcZWtx4IiZbJWVm6hv/ChfuP7OyjynEE+6pGPBjF4WRSultb9cKcjz0Wv60kxbvPBosEwcj2MaMpW/JLdt91N4ovyI4XP2XCDxagudoRd6zGGTgEx9gxOFQ3wZCBLjmFkRlmqtt9NPv8eEMqb2xrZOG4TB599DGuueZW9MB8GV1V/ETgRnQBzihiY9abm+H++xXgYeAt4HLgd0AqOjH6J3AFcBm6blUsWdIoOmFzhNz4WmxUvj49Qm7Cl7S6Wvcw9nerON0KghEsRv1e8cSkiNuMUpwFx9gzBid2sSf3n+U1jH1H0gRnxYoV3HjjjX1+73A4+Oyzz/ZLp4bx/YEgSogZRWieDtSu5ojIVYToWFL0P5PtgIpeCYKAzSRjM8m6onJQwRdS9P9BJaGQXixKR42OvFZVFTGsTmowEJw4ibMmwq46hScfkQCNw07tZPYZneSW9i4s+c1eN1NKTAiCgEc0YwcEQUQwW5G8+vbivq7Ie2QViZqGKRjAFAxwKm9xEu+wmrnUk08+9cxlNRIqDJJX9UKymUp9WVRaWvQ27rtv8O6jvrCPKscR7A/XW7iNnpar2bP1Cqzh98m6/MeM0QO6w201Nsa7pQaDAVLp8xedQcdX39CyahX+5jbK39tC+uwZpNKAWP4NbR4/GdMnk650EBQMeCULh5em8ubWFlRRpaLdw+0Pv8pt11+Fph2JTlDSgdXoMgy16JpT0UVPT57f3h5AJzjZwB1AbFHUS9EL4L4NbAEmRb6x5HWSPklXFlT8MhUvH4zqTxzk228BUEEFuTuBoZvgdMXIPlgMEqHuIGNJFHq70IMxq6jhIOMDjqRnlKeeeoo33niDp59+milTpsR99+ijj3LttdcyZ86c/d7BYfz3QxAEBFs6gjUVzd2B6myJEh1vV8S6g8GMYLYjGMzd9mChewEmgKZCKKhrSChBtFBQ/6z76/ALQZTAaEUwWRGMFv19D0hilOyA7srS1ZRVVA1UVUPpTj0HPfVcFAUkQUASBVRNIxBSCewtx5UXTQepr9E7MvuMTk69Ml7oIxgQMBj19lJyguyqCTG22IBbkVEQkVAR7anIXjeCKKAONn000cTcT1qyhMp8Ph7cMfpDZiY8+mhybp5Vq/Qg2YFIXE0NnHmmbqW44YahW3P2UeU4gv2hRpyfn9hy1dPqkmwedX5+fPzOsmVD61cSqfSCIDDmht/g3LEDf309nZu309Wq0TlmDnPHV2Bq3MS376Ux47giskMt7AkUYLJIzC1L4+PyNoJ+H0tvu4m09ALa214nSk6OATzAPcAT6FaYvhACuoBxxJMb0MuoTAY2oltwJhB2F1tyo5pMjZ+Oxt+W2GU7EGzFbRHpmlE5+vErYrI7M63GiGp4T+uNFgpELTgGS8KsxmHsXyR9hTdv3szkyZM59NBDueOOO1BVlaqqKo455hiuv/567r33Xt5+++0D2ddh/JdDEEREewZS3mjE1FwQegykQZ9e5qGtBrW1GrW1CrWlCrWlUn/f2YDmatUJUdCrDxYhPwTDfz40vxvN2YzaUolSt51Q4x6U9jpUdzta0EeikDNB0GNw7CY94DnNaiTTZiLbbibbbibDZiLNYsRhNmA1ythNBjJsJlK9UVHCvbsF3ntLpnCsjxMvi5Kbik1mXrsvmz+fU8qLt0czSb7eoaeqC5KIW9TjkUS7LjQoW02o4iAGv5SUvt1CixbpK/wPPgBHzwlhPyAzUycgjY0Dk5tXXtHjSo45Jl4kcCDcdBOUlOj7DwX7QeUY0K0s++IyE0W94vnixb0tSj1dSv2aEdAJSSJ34FBJWEZG4jikMCFdtgxWrcJgszFl6T3I3feSVr+D9W/W0dyllzkYx6c0NmiIaOTJevxNts3I+Gw7tTs343V2MeWIo9FLs2roJkMNXTHcCjwP9PAVx8EIpKDH7ni7P1MBpZujhZ+9LTHfgyk9SkK8zX0/BwOFdjjKos/2nLHZaJrGnlY9QUASBXLt0cBhsyH+Xoks5CAu2WIYBw5Jj6IpKSk888wzvPjii9x///3MmDGDKVOmIMsymzZt4pJLLjmQ/RzG9wiCICI6spAKxiJmlSA4ssAwhFgcQez+67b2JApODPrQ3O2o7XUojXtQ6rahNFegOlvRAh40beg+mbYxEyKvX1jqwmhWOO/Ghojl+ZOX0vj7VcV88Xoa7k6ZzZ/YI5W8xXRvxFfvlvTJIUxwjDYTymAm0q4uXXOmLwIgSXp8zj/+MXBbkhSf6hWLsAvjgw/0QNSVK3Vic+ONA0/8YZfUUGNYwuUHhkJywgJ9/Z1XMrFDa9bsW2yLqsI99xASBAIGQ/9u0f6sW/1ZWwY6175gsegxSLEIE9IFC+D88/X/paVY133LxLvuROg+9jzHOyxdPjXSb1P9twDYVBdBn36Pj8u04GyoRlMVMspGc+QiZ7f7RkQnOPnASOAroK9MLxUwoVtpmrHZwveCCEjk5KwjI2NL92fNQHQBYkyLEpxAe98JDprW3+XTSCnTS9GIAswak0Wjyx9xUZWkWVBjEhl6EhzV64y8FoaDi78TDDroYebMmUyZMoUPP/wQm83G9ddfT3Fx8YHo2zC+59DjTuxgtkNqLpoS0ss8hKXMNT2NWtM03ZwrGXQdF8mga7r0MPFqmqa7rwIeNL8HLeCFoI8eG+lWHn9Yp0MAo1l3ZxltCGZbQrdWT/iCCt5u9VLD3j2c8sptdP3qQTIL9L5XbTXz7mNZcfuEAiIbV9qZeWoXBpPG9ooQU8qM+AWjHg5p6S5OajYQCBO3ZGNxetYfSpSdtHixPom99lrf7ZxyCrz+emI1YE3T25AkOPvs5K0Z/QX5DhbJBAOHjxl7/vfeq6fG9HVel1wCL73UfybXIGNwNKA5M4uWtHQCRiMBg5GAwYDa3bYcCmHzuLG73dg9bhxuNxaftzdNz8rqravTl3DhUEtC1NTExyD1l6K+eDFpK1Yw6ojZ7Pl4NQDzmv/Bss/O5bw5GzA076arfRop6TI5ciftpGOQJUbU7ALAZLVx0hUtbPnKQmutMcaqOh7d8tIIFNA7oyr8+jfARvz+iznppJXk5BxEIFDL228/QlFRET6fE4/HBUSffckSW+Cy72vidMK11+pZ+D0vX+b0asxZ+rgxuSiNVIuB9zZGK4aOyrTiiklgMMdkT2l+d1TUUzb10gQbxoHBoJyAy5YtY9KkSaiqyrZt2/j5z3/OiSeeyFVXXYXX6x24gWEMox8IkoxoTUV0ZOl/KdmIKTlIqbn6a1taRBU5kf9aEAQE2YhoTUNKL0DOLUMqGI+YVYqYmquvmnoVt9Mg4EVztaG2VaPU7UBpqUJ1d6Cpfa/Ww6rLAOkhH1NOFDjkRH2F5nOLLPtjHqrSexnodUYHvfR0ffSUURAAzasPgCFPAEMoqLs0YlFY2LcNPTwSL1kCy5f3XnmXlOij9urVfZ4TAN9+q0/0PaVmw325777ISj5pa8q+Zh+FERsM3B8SWR5+9St95up5XhkZupvtppvirBQJz20Q7h+XxcqGCZPYXjaGlswsuhwp+MzmCLkBCMkynSmp1OYXsKNsDN9Mnc63k6dSm5tHMJZg3Xefbi0LW83Ky/t3B4ZjrwYrFxwbAN1f1hnAkiUUmE0YgjpxSEvt4KaXjsYXlEFTCdXsBsCuuCM11XJmTNPPO+BHFVR++2Q9JSNjLaj69f3d7xopKgJ62bjC6dwnkpX1DHPnzuWjj57j6aev5NV/3c/p517A72+6hcLCXCZNksjLi15DV3l0sZEzK0H13hiMGdPj8gka+fO3U3RiVLbhB0eM5MuqdvZ2x99YDRJFKeaIFIXNKMfVqlK7oq4t0Z7R7/GHsf+QtAVn8eLFvPvuu9x+++1ceeWVANx1112cccYZXHTRRbz99ts8/fTTzJo164B1dhjDGCwEUUIw28CsW0f0WjD+bgtPt5UnFJvppOk1tHxOaBcQLA7E9II4q04gpEZSzSVRQDrkEN759Z3QrcY8sc3LX++RyMmDiy6C2hoNrXtwNlmjA7rNopMGk6rHHKguPWbB3+UlLRCMukPCVgtF0eNX+kKYAJx9du/vamv1WJmBUF2tWwzCmTlhFeWerpm+KlYnwv4W/uuvvf4sD3ffrZO3rCy9jV27dNLXh5Wi17klURE8JEpUFhVRm5sPgoAtP43U0iwkowHZICIbJKRu10VXVSvNu5sIuqKWBrfVxp6SkZQXl5DV1kJ+UxMpBQUIg1WGDpeEuO02/bydzoH3CRO4JLPOhD/+kfSRZTRl52CRvIySt/HJ9lEcN2Un5s49wHgkVHzNPiz5NjKKRgCgVuoEIyAF+N3z1dicdtSWNN59M4XnnoMpUzq4/34480ytlxVFNqqk54a4669HYs0cxVEVNXSFBIy2FOzpmbz31cfsLS/n/PNn8/hj+ZHEtJSMEn7z7l4kc5D0SbU0fjaaQLst4enl5Oge3YUL4Z0PAzz2zXoqXFEL2lkzRzC+OI3n10ev0ckTcuMqimfEVCbXVBXN3229kQxxtfmGcWCRNMGpr69n3bp1jB49Ou7zWbNmsWHDBn79619z5JFHEgj0TosdxjD+UyAIgp6tZTATrgiuqSE0v1cnNl4nqGEzs4bm7UKVjUip0QDhTm/0Hk+zGPi4vBV3N7kpy7SxeP7oSHpoxFugqWiImG1RgiN3r/CiBKcDJRBC8QcxBgPhDuulGMKT83eB+nrd1TF3LvzgB4m3GYx+TLd20H5DX5aUZPRufvUr3QICuqVmMNo4AxT09JhMbBo3Eb9Jdz8UzBpD9pS+3ffZeelkHzwS5cm3cVU205aWTpdDt9CpokhTVg5NWTkUbdrMqAULBrgoCfDaa4kJXE/0TA8fBCFtaC9AzNafl7npn7Bq6zyOm7ITzd1FQ51CXoHEmLQ2KpsDZOfkIssy8u6NSCE/imyi1ROgmWbE3DZau4OSv6pqZd7J9dz/lsrGjSL+gEZqVojUnBCODP05q1NVRJ8ROW8UsfaQuvLdqIpCV0ohX9S0M2OmgxSLCZB57tORbFV2Iogw5odraF1fTNeuXDx1acS6wlRNo7bNy+odTTyxdg/N3QJZkihwzUkTOHpKHi9trItc1tklGaSapEgBYbtJjhP3093h+saC2T5cYPM7RNIEZ/Xq1VHtjx4wm83cf//9nHnmmfutY8MYxncFQZQRLA6wONDSNAh4UD2daO52ADRXO5ojK2LFCVcOBkgxG6hs11dnsihw4rgcBEGIhID4/XDzzQKP3u+lts3CiEnR1bqpu2ifWfPrNbC62vG16YGRFn/3drFumUFmyCiIibVuBsLWrXrmjKLsH/2Y/YUBdFoGpXcDQzu3PlLvvSWlbCgaQTAQQBAFRhw5nrQx+WAwIqVkoHS0gNJNnBVVPxdRAEnCeMRkRnzxHCPq63BbLNRn51KXF/2tO75dN8CFSYBk454SBSwP4j6r78wnR63HIIYYb9/GvdsujnzXvLeFvIJcZFHD/+E6Mo+ZSmnZaHZoChf6G+nIGEN1uzvibu7o6EA2mWnXzGxvcqGZNEYflpgMxM5F6RYD+Q4TFds38+nyf1A4bgrjZh/Dmsp2vqru4LDidI4ozeC4cSVsXFuObAkiW4Pkzt5L7uy9BF1GnOXZiMYQpgw3v1npQfko/jnJsBn5y/kzSHWYeG5dDYFurZsRaRYm5Ngi5AYgwxpjvdFU1M7G6OU2DS09fRhDQ9IEpy9yE4t58+btU2eGMYz/bwiCACYbksmGoqlonk7QFDR3B4JDl6CP9a2Ha+MAGCQBu0nuo46NlVt/sBVDgR4DlGMzYpREzKoXoxYkWLUT1eeh/uu9CKpKdmuPNOH6et31NICLJIxXOIOruJ8aohaEIqq5n6tYxKv9X4Q//Un/y0gyVuD99+HDD/XX8+frf7EWnaam5NoZCJrWv5Dg/tK7icVrr/Umbz0qgvttNjb+42mC9fWIssSIo6aQWpqBmJqJ6aB5iCYzXq9G27sbKH7pIfj0S0i1477+QtScDJSxJagZqYhtndi8XkZXVSDZbVTbdWtO1oL5PXs1MJKNe3I44PHH9d962bKo6GCS91m+1kh9IIdCcx25xga21ebg9huwmYKYW3YAuuWz9KBU/H97kDlz5/P044+wftsmfjBrJv6iNHa1etixp5wtn75PdvEoSqccDOiunfbGWpxtLWQWlpCdnY3FIGESNfau/Yz1q99n5uGH0w58vnkjr7y4DJPJxIU/u5L0/CJCikoIkTWVbVR3eikozKPidweTf+QOrEXtEW5nsAfImBIVVlR6nPJRE3O5+sQJlHd6+WBTXSQyKN9hYm5pBp0x5CbLbsIYY71Ru5pjtG/M+kJqGN8ZDpx07DCG8V8O0ZGF4tHN5qqrFcGegdAt9heGompI3SOlovYfArLBmMkh6FoYpekW0DTSQx2o7i4COzfQ+G0F7voOcltbMAWD8Q2EBd2SyJB5hTNi6mLF9IFCFrOCFSwemORA8lo1t98eff2nP/UW/dsfAnmgxxD1F++zv/RuYvHcc7p7sCep6hbXC3Z1semyy/HX1yOZZEpPPgR7lhUpuxDTtNkRdW6LRaDw9Omopz8ONXWIL7yM4evt+E+eDaJA8JjDMV16HTQ1oeXl0fzQ33SCIgjknnhi8v0NI1kS19UFP/95fBHOoqJElTATYi6r+bv/JxSa63DILqyim03V+Rw+uooCSy0ff2bmyDk+rKXZ2Dd+ySWdXpYZjTxy/z0UjyihuLQUf3s7q/7xMM7WJn7ys8s5fVopAtDZ3s6Nf7mHj99/h9vve5iT514YOW7rt34+eO1lPnz9FTweN1arjdnz5vPzq69l2kEzkASBXW0e1tZ26sa4Di+thiqmTMzly+dmIVn8OMqaSB3bhGNkM6JBt9hoikhZnpWSbBulWTaOmpSHJgu8tr0BV0yMzbhsO4cVpeINRj/L6tbLCkMLeNGc0YWKlF447J76jjFMcIYxjD4gdCsraz4XKEGUpnKktFwkMfrYuAOhCOEJKCrLPmvBaEnH74lOiPb0ELPO6OCQE3VyI4sCBSlmbKoHkxbEu/kr3NXNNH5bAUBRfV1MJwQ9nUNRoivsl16Cyy7rXRk6MxNFFbiq/f5eRT8BNEQEVJZwHwt5LTl31VDQ2qorEL/8sk5IkgjOTQpjxvT//UDH6eniys7WCxz1h+ZmePBBuPLKhJaj8of+imfPXgCKj56GPcsKRjOmaXMiOjGbt5soyg+Sltp9vYsKUK+9Eund9whrvgTL8jCoKo02O7WPPYGv2/qSdvAMTDlJqhrHYjAkrud9FA7IPuUUXZiwH0iojA5Es5JyjQ1sqCzg8NFVAKxeGeDIboF79dlHmfnPF/ln6HQuXXo/l154NqVlo+lsb8ftcnHZkmu4+Kc/i5RAwG6hID+f4hElZKbaMak+RE1DROW0o49g3Irl+EMqjrR0MrLzcKRlIEqSrkauaYzJtFKabuXdnc04/SE8QYWFv66jbJaNte+kUL83j4pNRQiyiiXHieIx8uzjFk47XaPNE6CizcMn1e04Y0oxiAIcWpTGyHRLPLmxx5Mb1edCbYtahQRH9gGtuzeMxEi6mvj3AcPVxIcxWGh+D0pzedxnPnM6DVrU1LyxwcnOFnfkvbNN4v0nM6neZmbOmR1MP9qJbIw+ZmUZVmbk2ykM1KNVbcO74Ut2LP+SoNtPZlsbk3bviO9EZmb8JNTzfRiCwCptHgtYNeB5rWT+/i3VkAhFRXo2liRFTVuQWIcmGYSrlPdXgby/40B8ZtTVV+sur2TPJUFdrC2//i2tq/TrOOLSH5JODRiMmOctQpL1Y/7gykLyc0Pc9fvGuH2DFTsI7NBF8UI7qtm5poJgD8vdpL/8mcx5SRYvjcW+Vj4XBD3bbCACCJQXjaC6QM+pXrLtQUrGGXnqshcBqP2sk7E/Px3y8+L22V1Zw5Mvv0ldXT1ZGenMOWwGc2fPxG6zI6Iiaioiah91xRMjJBrpMGXhUuPX7ZIg8EVNB5XtvaVMvE6R+r0mfO0GDpkTQnYE6PKFem0HMDrTyrT8lF7KPLkpZuwm3fWsqSpqVxOaK+b5NJiQckYNl2bYRwxl/h4mOMMYxgBQfS7Uzga9HAR6PkSHMZNOQU8zDakqe9t8bKzvSiimHIaArnY6Ld9BrtqB3dWAd83b1H22neaNVdjcbqZu34pBSTzAJoNlnMv5DFyP6HnO4zxeSL7hjIzBlVcII0xKIHENpuJiuOcePbtpIMtLebkeE9M7wCmefPR1nJ7ieKtW6bo3ySARQQLK//YI1U89A4C2+FqmZ+iEpdoyi/HzSvtsLrB3C8G9W0BR8JU3snv1ThRflNyk+P0UX3Aumdddl1z/EuGVV3RL2r4gK0sn031NE5JEZW4eleEU8IZMGs/8LcdO/gBJC4Ao40idhDaxDM1upre2Td/QNA3N7+vObNTQBBAQdPVy2YBocyAkiA0NSFbaTFn4YpQNNFWh4a0PWZ9agDs9q9c+/aE03cK4LBsOUzxxMskiOQ5zJGNK9blR2+tAiWZZCiYbYkbRAS0k/L+CYYIzAIYJzjB6Qgv4UDpbUDtb0NxdqH6vrpHj96L53AhmG3JhGVLeSASDjNbVBN0CgJ2GdNrFqCWnvVPlpVUuMss8cccQNYEx2VZGZ1qxGCTMqpfcYDO+rz/CW17OjuVfYnG5mbp9C8bQ0MkNwCqOPDAWnN/9TnfpZGfD9u16rE0yeP55PZ4jjL6sL8lYXiBxgFMi8tGflSe2L4O1cmRn69sbdXdE4zvvsuMmXV9IbU3BdtEJjM3X4y6+tZzH3B55F/7aSpTd69F8HjRVpfXL3dRuronM+9mtLRQ21JPicfc+p6Hg1lt1EcOhYskSnTxC4t/lpZeo2bqdvW++BcCEP95C9nHH4vnkZYK79Owv61HnYhg5WdeD8Tn1Ars+Z1x7qseF0tqA2tWG6naieV1ofu8A1j0BMSUNuXgccsFIYg0kGuA2Z9KOPVL8EkBxOul8/S2cdU20ZubSXDoGZ07UnWeURDKtBtIsBhwmiSyLEasx/r4RBMi0mUg1GwBNHy88HXpCQlzfshEcWcNxN/sJwwRnAAwTnP9NaJqG5nWhdjShdDSjtjehdDbrpMaThABaNwRrCnLJeAzFoyKDllO00yqnxxWvaegK8NUeF4KsMqHAQmmaBVkKi/r5yAm2oFbtILDtG8rf2YCroplDNq7DvB80pBRESqmglkK0BELlAipF1FDOyKHF4BQV6ZXAk500Yy04A6E/y8vChf2TkbCVZ/duIupu/ZVdiD1mImLVH7Ky4O9/h0WLcO3cybc/uAgAk9/Hm1m/45rz1uvbSTJMOw5bdirB1mZCu75F7dStYKEONxWrd+Cu7wBADgUZv2c3GZ0dvc+pvDx6DskQt1gsXw7nnqvXwRoKVq7ULXf9WMTqXvkXu/98FwBjfnM9+WecTqh2N+53ntJPw5aC/aRLEG2pkb5rBfkoY0vwb/6MUNX2vo8vSoj2NL1mXDCAFgxES7n0gHHSLAwjx8eVZ1FlE642hXazHc3RY8xXVeTaGtSKCjwdToxHzkVMcdCXGVYWBRxmAykmCSno0Yma1wk969kZrUjpBcPlGPYzhgnOABgmOP/ZUF0dhBorIRjQa0JZ9NpQosUOBlPSKyEtFERpriZUX0GooRy1tUFXLN5PEKx2zIcchdhdO8ormGiT0wmKxj73MahB0pQOrKoXzePEu+YdOvfWU/HuJorrahlZU7Xf+hfOogLiSI7QTWiSzqLqDykpegZOf0hP1yfYnmnjseg5Yc+enZigJOtO6hk43EfsTBwSEauBIAiwYgXaqaey7tCZuEx6AKkaMmA7+STGljgj2wkmC5pPt+qpikrzhioa11WgdRdbdbicTNi9s2+CGyaJifUH+j6/vlL6kj2/WHLVD7HqWPstGy//BQB5p53K2Bt+i6aquP/9CEqLHjAviEYMn61HNYpomamoGSloKfFKwoLJqltLswoRUjIJeoJ0rNuK67PPMMsy9vHjcJy5CENWFqqrnWD5ZoJ7NqK2R2ObxIw8rPPPRPN2RiytqCrq86/SaU6n8+wLwNCzXEv/sBklUgxgVnwQcPeyPkVPQNRLwtjSh602BwDDBGcADBOc/yyoznZCtbsJNVQQaqxEc3X0vbHRjJSZj5RZEPkvGM2oni40dyeqqxPV3YnSUovSXBMVVesHgtmGmJqFmJqFlJaNaE9HMFsQTFYEkwXBaEHpaEap30uobg+hhsro6lGUkItHYxg5AdFkQQP8ghGPaMUjWQgJ+iBqVP2kKl1YVb2Ioup24lv3Cd66Bna9+g14/Ry24dt9dk1FT0qfmF455wWuWlpKjVIQ+aqYKu5jSd/kZqhxNgOhr0l4MBP2smV6rajBoo/YmV4IBHTF5c7Ovrfp2W5RETz+OP5TTmHDhEn4zBb9K1EgpTSbjPGFpBTpsvxKIETr1lpattQQdOuxXEaTkdJtW8ltae4/mPb558FkSt49B8m53/oL8O4mcMm4xxSfnzVHH4sWCmEZMYJDl+uxXarXhfutf6B29K+DJIhGzEecglg4gY4vv6Tt8y9o/+or/PUNCbc32m3YDzqIgsWLSJ85k+CudXg/fyNSckVwpGM99gLwOePiYfD5CL3+PuudpWzIO5bskWZKRqqkpkU3kUUBoyRgFFSMWhBz0I0YdEfJUu/OI1gcCOaUpAv1DmNoGCY4A2CY4PxnQFNC+L5+j8DWz/dPhekBIFhTkNJzEdOykdJzENNyENOyEE3WBH1T8JRX0LV1K84tW1FcLuSUFOTUFAwOO0aLiFFwEWquwSApOtEpKkPOK0ZMy0YQBDQgKOhBhQYthAAorQ0EK3eiNNeharDjpTUEOr0UNdQxqqpyP51o/GSnvLic1ec+nLyS8U03wcMPx1euPgD9Avq2LvQ1YQ8mIDjR8Xu6enpiqO1feCE8+yw+o5GN4ydGSE4YxhQLRrsZT3MXandaccr0aeQcdyy5dhvSCScMfIwPPtCLmg3knos9v325XgDXXQd33ZX05usvuZSuTXoxyplvvIYpW09tV50duP/6B9Ss1OjGqorQ7kRs6cDw9VaM5a20PP4Ee5beT2CQ917JTy+h5JIfo3Q04X73mcgiSTDbsB7/QwRZQnO3xd1nfr/Azr1GupwSLo+Ixy8hGgWKsv1Mm+BD7qfaOACipJMaS4pe/Hc4O+o7wTDBGQDDBOf/H0pnC56VL6K29hAikwxIOUXIeaWI1hRUnxut+0/1ulA7WtA8A7hEuiE60pHyRyLnjdTbc/QubqdpGoGmZjzVVXirqvFWVuHauRPntu2o3oHdWXWU4S6dxZxDu8hP18srYDAiZxUg5RSipRciyaDUlROs2onWXUgTSWbvW+twVjYjmowcdslPMP7q6sRp330hvPLumS4emyk0mABaQdCtN4PpQyxSUuDyy3Vy1FdRx9hJGJKLp+kZe1Jaum9aOv3FAw3VQnT66fCvfwGgCgLtqWm4LRbcFiseqxWP2YImithTHORc9COyjjkac253XbNkzkmS4A9/0GtKDYTY8xvq+UByhLAHyh/+G9XP/BOAlGnTmPLAfUhmE6xahXbi8YQmjERweRBbOxE6XQjd7rmgLLO7ZCTNmfGZTYKmkdrVSXpnJ6muLnxGEy6bDZfVhtNmR5GjWUmTl95DxuxZqO4u3O8+HXFZCSYrtpN+jJiahfrt52jpVjAPQYtGlBCMVt2ya7SC0TLsgvp/wFDm7+HctWF8ZwjsXo93zesQLiQpyZgmz0EuHoeUVTBgKqXqdaG01qG01KG01oOqIFpTEOypiNYURFsqYmqmHszYDU3T8Dc3496zB/fuvbj37MGzZw+eyipUn6+fo/WPAvZAxR727nXwUcbRpJTmsmBqJfZgBaH6il7bC/Y0jBNmsufpf+Gs1GNECs85B6PdNnhiUVQUDbztK+A0Wan+/YGuLp0g9Vexel/rQCWp4twv+lP3Hara8hFHRAiOqGlkdrST2dEe+VoVBFRRRH73Xb1EdSxiz6kvKEpy5Abiz29f1KOHUGOsYPGZNPz7DYLt7XRt2MA355xLzoknkKsoWH0BDOt29NqnJS2dXaWjCBqjsWsZR8yhYMxoUq+4HCkmMDoFFzlt+nOiAZWFRVQV6mVIdvzxNg5+7hmMGRnYT74E93vPoDRVo/k9uN9+EuvR5yEdcgRr7v6UL90FLD61i4xUBbM5/h5SVdhTaSQj20RWngFCCpqiogX9aM4utJZGtFBAd2nnFCe0AA/jPwvDFpxhHHBooSDeNa9H0kYBxNQsrAvOQcrcTzL+MVBDITq++YbWT1bTuvozAoOohWTKy8UxcSKOSZNwTJqAKTeXUGcXoa4ugp2dBFd9zO5XVyKk9H5stvunoo6ayqxDPZTmdAB6PIB5+nwMo6fT9tnnbLn2egCMOTkcuuxZpAkTBkdEli7tU1U3Dsmu4DMy9DiYfUklBn1lnAxhfP55/X8yfeuZYg5DCwgOoz8LzlAsRNnZUFmpW876I6mZmVBX13d21/Ll+nkqfcR5JIuVK/V2V6/Wz2PJkn1zNya6/v3AuW07G35+RS8LqMPlJLutFVUQ8ZrNeE1mfGYzgRhiI1ssjP7tr8k+7liEF14Y8P7QgC1nn0tbhe7ezZg9i0n33q27iAM+3O88pcfidUN0pLPLO4Uf3zSJTVX6mJOT5qYk10lRtof8DDcZcgsjs5o55rAmHEJr33E34TZTs5Fyi5FzRiCXTEA02/rdfhj7hmELzjD+I+H97DWCu9dH3hvGzMAy6xQEQ3zWkRoI0LJyFYG2dizFRVhLSjDn5yHIyd2misdDw+v/pmbZC/gbGvveUBSxFBZiKRmBZUQxluIRWEcUYx1ZijEzs/f2Bd2BuooCP/kxeTV1HGldyRG5n3B05geYRN0iNd60EWo3Uldl5fm0q7j10TKMpeMQRIlgV1eE3ACUXXUl0jffJD9Rh90GyZAbSH4Ff9ll0LPu1VCQrDVsMJaFRNvGFrr88MPk9XiKi/uuQg5DsxA1N8PYsfDjH/dft+nHP4ayssTB1AsX6mRkX8hN+N5obt439eKeGKQVyDFhPNP+9hCVjz5O25dfRc7JaXfgtPddZDLD52XM669iCrvukjiuAIz9wQWsfehvBNvbaVvzOZWPPk7JpZcgGM3Yjv+RTnJa9HIJqrOdMj7h4z98gi8oY5AUJLGP31gjKT1CtbMZtbOZ4M5vEb56F8uRZ2IYMX7gHYfxnWHYgjOMA4pQYxXuNx7V38gGLHMWYhw9PW4bNRCg4d9vUPXUM72sLYIsYy4swDZqFClTJuOYMgXHuLGIJl1jwtfQQNuaL2hfs4b2b9b2Wj0KRiOp06ZiHzMG6+gybGVlWEtL9fiAwSImcDOcip0qt3NS9pscm/keo6x7I5tqokTJRT8k0NpK1+bNkXpFoAeZTnvkr0mtVKMnknxWC7B/Ylb2JxLF4CSjXNwfmRtMnEm4LtZASGQhCqdJJ0I4FuPaa3WLR220/lCkaOXddycOpk4URzUUCIJ+/ETHSYSiIujoAJer7/YGGYPTE4HWVprefY/Gt97BvWtXr+8NwQAWn4/85iZyHvkbQqzi8kD3bkz/2r78is1XXxP5Kvfkkyi9/DJMWVlooSDBPRsI7t1EqH5v8s+BKCFKZkRVQrSlIJSOQjBb9VpSooTSWo/SVKWnwPfQwDFNOxLTjKMTKiwPY98wHGQ8AIYJzncLTVNx//vRiKnYPPs0TBMOi3yvBoM0vvEmVU893b/FpQcEgwH72LEoXg+eveUJt8mYM5vcU04m4/CZSNYh+sp7an/U1upZM914hTO4ivupQY8FGGnZwy+K7ueQ9G/6bXb8rTeTc/xxg9N2eeSRwSvaDkXE7kCgvywqGLhmVF9I9vrdcgvceCONjY0YDAbS0tIQ+5uAev7uM2dCSUnfdZn6EhmcPbu35WZ/Q5J0YnXNNf0HbWdl6e7NwkLdbXXWWf2325MQDlZgMGZ7lwZdW7ZgeOklLHW1mH0+ZFVNXD4jjEHcH7UvvMiepfdHNhHNZorOP4+iC89HtuluI9XrIlixlaqv99BR1YjLb6Kp006L00az00aby8bepgx+cVw5R7z0S/xtbQSMRkRVxZCWhnzVVcinnYohNRVDWprerVAQpaUW/+Y1hCq3Ro4v5Y/CuuBsXb9rGPsNwwRnAAwTnO8WgT0b8K5aDoCYloP9jCsiOhHeujo2//JqvNXVcftkTBhPZmEBPkXFC3ira/BWV6P6/QMez5CeTua8Iyg852xsZWX71vlEK/msrF4xDQoiq5kbScU+gtWUX/VL6j//MrqRJGEfXYZj8mQyDp9Jxtwj9CyMZKwsPUoD7JfzONDoqafT10SWbM2ovpDM9SsqYsU99/DYE09QWVmJqqoUFxdz3XXXcUIyKdqQPJHqGeOzr6nayWLpUr1w6EAIx+gM5MbKzITGxvgCpoMVGEy0/dKl+jM0GJXpJO+PhjfeZM+996G4o0VvDelpFP/oh9jHjEGy2ZBsViSrlbfekXn4pgpSXLspMNeSb6yn2FHPCEsNYmjgDErb2DEUnnUWuaechCCKekbm5jX4vn43YtERrA6sR52LnFsyYHvDSA7DBGcADBOc7xbOlx+IiHxZj/8RhqIxke+2XPdrWj9ZHXmfMWokJV9+gaM86soJD6LawoV4Kiro2rSZro2b6Nq8GW9lFQgCKZMnkT5rFhmzZ2EfNzaxaXiwq8+hKsB2r+S1vXtp+fgT/A0N2CdMwDFhPJLFknif/WHJGAjh8x9MzMq+4IMP9OtbX6+L5wE0NSW+9oP9bXqij+sXBF4F7iorY1N1NVOnTqW0tBSPx8N7770HwOr77+fw9HSUnByk+fPRRBHF7UZxexCMBkSjEdFoRFi+HOGCCwbuS8+g3H1J1R4MfvELeOihgbd7/nn9Gg+GrA1Wr2iw2w+EQdwfwY4Oqv7xFHUvv4K2v4Qz+0HaIQcz7qYbMeXomj+hhgo8K1+Mln+RZKzHXoihcPQB78v/AoYJzgAYJjjfHTRVoevJmwENMSMPxxm/iH6naXx+/EmEOjsRZJlp555NylW/HNSgGHI6QRCR7QNkLgx29ZmsfkzPQNR9IST7aslIFgd6wu0ZuzHYaz9UJDjO+9nZXCoIpBcWctFFF3HssccyYcIEAB74yU9Y8o9/cK7Nzq+LRxCQZEJmMyGDAS1RzSZBQA4GSXV2kd7RQUZnB+ZAAovif7oF54MP4KOP4PbbB9722Wf1GlaD0Ssa6NnZD7E9ycBbU0PF3/5O8wcfJr2PIIqYPB5MAT+mgB+zP4Ax4EcTRYKyTEiSCckynunTcdVE46zklBTG/v63ZB15JKC7wjwrX0Sp73adSzLWYy6IW9wNY2gYJjgDYJjgfHdQ3V04X9CVUOURE7AdG10BB1pb+eKkUwFIP+xQpvzrlQMzKA5lNTnUmkf7Skj21ZKRDPZVDVjT+k4Hj6kuTVYWvPaafj362i4ZIjiYa9Jj2zc6O/lg5Up+/OMfM3XqVAD8Tc003XMv6/71L07fuZ1FuXn8YkQp4hBE2yxeLxmd7RTV12EKBhO7Er+LQO/iYj32p6ys/+OkpIDDER8E3R+WLoXp0wdn7RmqK29f0cd94tq1m/bPPyfkdBJye1A8HhS3G9Xvx1xUiG30GKylJZgL8jF9+CFCTHxdn/jFL2ifNo0d//o3gZjnv+jCCxj5i8v1NHUlhGflS9G4HEnGevT5GIrH7r9z/h/EcJr4MP5joMZU6Rat8Smi7piMIqvJNHjRt2SgKPqqPtGAr2n6RLtkiZ6mGztp9icGF4twwOb+IiSStH8H/USYO1cni0OZcMNKx32lg2dk6OnQV1898O/Z17WPxWCtP93XT9M0BEHgREXhlIULUUMhWj9ZTf1rr9P22RqcwSD/7K7YPSctA1EQEBUFQyiEIRRCliXk449HDYVQAwHUQACREBZTAGd1C+7GTjRFxWuxUGux0JSRxcTdO0ltbtZJRmz/JEm/TwYK6N0X3HOPTqoGSnHv6hq4OGossrOTfxbC2w12+/2Bfu4T+6JF2Mck6R4qLExuu4ceIh04uHgEO4+cT+tOPUOs5tnnMGZlUnTeuQiSjPWoc3SSU7EFlBCeD57DOv8sDCMnD+78hrFPGCY4wzgg0PzRYD/BGJ+S7YsZ4IzJTrR9DYp9rfIHUvLtizglq/1RWHjgCcm+oK/r0s9E6DcY6UxJwW8w4jcaCRj1/6ooYtTA5EjtNuEHMPv9OFwupHCarKYln6Y8EGnty/JWW6t/vmIF2hlnJJTLD3/m3bOXxjffouGdd1A6OmkPBtng7GJ1exsftrYw02ZjRGM9Obt3ME4SIS8XJT+fHfnzcE2/ENuYQiZOkJBkEaWtEfcHz5EzrRgtqOCrbaW9oRNnTTu+Nhcbx0+krLKcgpj+sWiRfh7JuI72Bd01n1i0SD/u/gooT3bCh+gzk+yzsy8qy7FI4j5J2qI6SPJvqKlm4nP/pO63v2PPBx8BermKrKMWYM7NRRAlrAvOxrtqOcHyzaAqeD56AcHq0NXXTZZIQV9NVSAUQAsG0EIBCAVBNiKardHCv2YrUm6JXml9uExE0hgmOMM4IJBSo7VlQk3xmVL2sVF/dPPecoqg/2rKkHhQ7G+Vn0TWFdCbOA000IVdZv2Jxh0oJOuyGcj6kWAibMjKZnfpKNSYIG3RKJMyIhOz1YSv3YWzxUnIGxUFFFSVFJeTtK5O0ro6cQCDUv9IRFr7sLx1AV9rGgXAqKuuwtSH9cdbVc3OO/5M57ffRj4LqCoftLbwcFUFBuAkSUI1mzirrZ2stFRuv+EaLjrnDADGIKAiIlIH7Rph9RvbgkUoHS2E6isQbFVYSvXg6fbdjdR8sp3dI8tw2eyMrqpAXLJEP49zzjnw6fmx13DRIjjllKjo31ARK4o4mGdh9uz+NYMARFHfbl8xVAttXxis0GO3pbDwn8/gW3I1tS8uRwsGqXnmWUZfp+vyCKKEZf5ZIMkRoVPN40SJsW73e4oJPhNTszCMno5x9HREe1pS7fwvY5jgDOOAQHRkIKZkona1ojRWoQV8ulAW4JgwAcfECTi3bsNVV0fr6DFk7dk9OEIx0Oot2fo9PYlTfwNdeOV0330HNEgyIZJ12SS7qu1WA1bee4/dL7xEY7Y+YctmAyml2aSOzMZRmI5gMCIYjGg+DwBBtx9PixNPUxed5U10iiKdKalUohMeWel29XS7fERVRZEkQpJESJJRJAlRVclpbSHf4aBX8nsPy9tu4BbgFfQB3wTMrqnhpr//ncMvvxzQg9ZRVWpffImKR/6O6g9E9hcMBgrmzeUwh52pHW2cgx/rOYtQszNZuXYrl/ziV9x479/ImXgYkw8+DFWI/q6CpiKiImkKkqZgy7BhTcvGOG4GSms9obpK0gUBS6adivc30UAuHouFibt2Yrz88u9Ge6jn/btmzdDJTaL7ezDPwpo1AysyqyrceSfceGP0s6HEWn344f5zbYfb9Pv1kiUPPZRciYvuY4wYN5Z6iwXV66X+tdcp/tEPI5lVgihhmXcmcv5I/Fs+R3N3oQW8Q7431M4W/Gs/wL/2A6T8kZgmzxlWT+4Hw0HGwzhg8K75N4Ftuh6M9ejzMJROinzX+PY77Lj5VgBEWWbMzu3ktLYgJJOZlEy2RmGhPojU1Q1NLfe7ymxKBskGSyeTAVZcHDlnNRRiw6U/x7llCwCFc8aSOaFAT7WXJAxlUzCUjEUQJbSAH6WrDbWzFbWzFaWzDQI+/J0eXPUduOs7cDd1EvIGUQPJpegKBgNphxxMxpzZZM6Zg7kgPy7TqxNYDHwD/BgoBrYDjwKjcnNZ+thjnHrqqfhbWtj2uz/QtWFDpG1zQQFFF55P9jHHINvMqO11aH43KgIdWHAb01EEiQf/cgcP3n0nN/55KUctOp82TwCfomEzSDhMMnaThMMoI4n6tRY0FZvqJkVxYdSCKJ2tBLZ8TbCtmYZvymnZXIPZ62XC7p04PG4OKIqKoKIi/v7dl0y5zEx49NGh6xUle+xYnZ1EbYfro91wQ/y5DUXT6dln+4+V2x86Uc8/T3lbO9XPPAtA/plnMOb66/rcXNNUCPhR/R4I+ECUEQwGkE36f0mGYBDN70Hze1H9HtSuNoJ7N6I0VPRqzzj+MMyzT0EQvt/qycNZVANgmOB8twhW7cDz/j8Bveik4/QrIlYcTVXZfuPNNL//QWT7DJ+XMdu2YgpXG++LUAxGwTZsyUl0m7/0Uv8BoN9FZtNAGEzq7erVg8piaV39aaQ+VvaEAgrm6itBKSsf48RDECx23KKVgGBE1kIYtBCyFkRGQUDPlFPam1Hbm1DamtF8+oSuqRpKIIjiCxEKBFH8IRR/CNUXQPMH8LmDtO9tiidCgkD+ojMYNX0q0vHHA/AwcCXwZ+AKIKxH/QJwsdHImHHjWHbvvTjvexBfU5OeDSUIFJy1mJGXX4ZoNqO52lC7GlE0gU7RjktOQRVEPF4fjT6V55/6B0/feQPzzrmEEy7te1KyGSTyU0wUp5rJsBgQBAGT6ict1IFZ9RGq3k1g90a8jW1Uf7wNX2Mno6orKWhsGNj9Ogh4jSba0tLxG434jzmGgMmEv6kJNRgi94TjKB5dhpysgGEixCoYx97/YT2jhgbdQpSdrROH2GdiMFl6K1fqYpD96U3FEq6halP1FOeMtXoOtc2eWLmSwLRpfHXGYr1UjCAw9eEHSTt4xr61mwCqs53A7vUE92xA7Yyel2HsDCxHnP69Jjnfa4Jz22238eabb7J+/XqMRiMdHR2DbmOY4Hy30FQF9xuPozTrMTiGkZOxLDgnEiSnBgLsvO12mt55L7KPZDYz6ojZ5B19FMKRRyYmFMmuFJ9/HkymvldoB0KTJRF6EqXZs/uuLN0Tg0m9ra9P7rosWQJLl7L77nupW74Co8PM+HNmI5rNGCccjJw3giASrYZMfKK59/6ahkELYtL8mFX9T0ZB9XvRvG40vxfN50Xze1B9HjS3E9XdBUqU0KiaQMu2ehq/3IEajLo1zIWFjNvwLda9ezkN+BSoAVLRXVQCIBYXc9Wpp/Lg3/7GOXkF/Lx4BIqmYcnNZcKtN5N60HS0oB+lvRYCXtyilVY5nZAm4Aoo7G33UtnuJahq/Ou+m/nq38v44W1/Z/zh8we+doDVIFKUaqY41UKaWcameskItSMF3AR2biBQtZv6r/bQsqmazLY2xpbvxrCPlcIDBgOVBUU0ZOeg9VNmwpiVxahdO8jevSveGpoMYsnya6/1fm7ChWhja2fFPkOKohOhWBXrvvDss/Cb3ySnN/XSSwNn5yWLsNXzxRfhV7/atzZ7WIGrn32e8gd1wUVTbi4znn0aQ/c8o6kqaiCIr64Ob3UV3qpq/a+2FsXlRvF6UbxeVJ8PNeDHlJeHfcwYbGNGYxs9GvvYsRG3F+hu2eCub/F++lpEPfn7TnK+1wTnpptuIi0tjZqaGp544olhgvNfAtXZjvNfD+umWMA880RMk+fEbdOy6mN2/fkvBGMGRsuIERRf9ENyTzyhtzrxYPU2VqxIbKnZn2rBYfQkMy0tvQfnnoGY/RGtwZC5nBw45piBt+1OAf763AvwVlUx6uSDSBmZj3n2CQgmCy7RTpucRmdAZUN9F62eIAZJwCSJGLv/LAaRDKuBPLsJgyQiaSFMqr/byqNbe4xaADGmLLPq86B5nKjODoLVu9DcTpBNeAJW9i57C8Udlckvqq/jgupK6oB3gfEAgoCqabgeeJBP3nuf895+k5Cm8fqMQ8iefhAT7rgNY2YGmqsVtbMJBYFWOR2PZKPR5WdHs5smdwBN02gs38mn/1rG2jeXMeKQ4xh53K/x+GX8ShBNCCGoMrIoYTKKWK0iGVkCmWlm8jOtGA1RMpprNzI9P4UUo0iq0kWK4kRrqcO36Qs6d1VTvWobcpebseV7SOvqTGzNCRfcTBDcGpRlanPzqcnLR5UkBFHAlGpFNMpIRhmD1Ygx3YEWCtG+q55Ap34NU7u6KKuqwD4UN1nY8pnM1NDzGbr1Vj2OZSAkK04IvTWnkulTf30P1+bal0DsBGOHpqps/MUv6VzbHeAeXrTsI7kNI2XKZEZecTmpB02PfBYs34xn5Uv/EyTne01wwnjqqadYsmTJMMH5L0KwYgueD5fpbwQBy/yzMY6aEr9NZxd7lt5H09vvxH1edP55jLrqyvgGB1FtGPju1FX31Z9/yy294w4GQ+YUJTmCA/heeZWv7ryL1JHZlB47BeOkmRiKRtEpOWiX0/GFFN7Z2UJI7X94kEWB0nQLozOs2E09chY0LUJ0TKofoxbAqAUR0dBUlVDtHoK7N6MFfGC00ripnoaP1wIQ0jTu3L2TtV2dPJmTyzRBwOBw0Hn4LJq2bkMQBO4p38u/mxv50wUX8psn/4EqCAhdjWieDoKCTIMhB0WQ2dniZtW3W/A4O2ms2EVjxW42f/4J7TV7SCk7lPwFF2HOLE7quhllkZGFKUwoScfcfb6CAGMybUzItmERVTJC7Vh8HQS2f4t3zw5qP9tJZ3kzZp+XnNZWctpasP7mNzBmTNSC120xUWtr6bI76EhJoSMllS67A0GWcBRlkDoym5SSLGSTIWHfNEWl5rOdtG2v0/ulaYyu2Et+s14upVeNsL6Q7HZh9HzecnP7rpAe3vaOO+IK1+5XOBzgTC5Tacjow33ua2xk7fk/QOmrUnsiSBKSxYJkMSOZLQiyjLe2Fi0QSLj56GuvoeCsaPX1niTHdPAxmKfPH+wZ/cdjmOD0gN/vxx+TLtzV1UVxcfEwwfl/gO+b9/Fv+Fh/IwhY5p2JcfT0Xtt1fLOWyieejKT5CgYDh7/xWqSCbwTJ1nD6rtRV95c/v6c1Jxk13HDg8EsvJR1g2vmXv7Bh+SvkH1ZGzvQSLLNPRHSk0SRn4ZGs1HX5WFPVEdne2SZhTVGQ+sm7zLObSLPI2IwSNoOE3ShjMYjxuh2ahlELkKp0YVW9EAoSrNhGsGI7KAoBMYVd/3yXkNvL4zVVPFtXy+/LxnBMZlR2QNE0JEFgd2Y6l7zzNvPnz+fDDz9EbatB83bhFww0GnJQBYmdLW42Njj5+s2XePXeP2C2pxAIBDFll5J92OmkjYu3Joa8MorPiGQKIZkDJFoIS6LAXT84mHXVLkJSjHtNFpma56A41YxV9ZIZakdoqcW/9Wuce6qp/3oPnkZdbM86ciSG9HREgx5gKsoGQlVVdO3erafqC+AoyiRjbB6OEZlIRgNSZh5S7gik1Az9UqqKfk+oKlrQT7BqJ2pbE207Gqn9bDtqSO/byLlzKD7l5EER4CFhMPWrMjK+mzIW+xNLl+rkbQC3cvtXX1P52ON6gWBBRJDESOC+KTcX64hiLMXFWEaMwFJchGSz9dK20UIhPFVVuHfsxLVyFW2bNuOJIZ1jf/878k49JfI+WL4Zz0cvACCYrTjOuQ5BTkyE/1sxrGTcA3fccQe33HLL/3c3hgGYDj4a1ecmuOMb0DS8H78MmopxTHwgXtohB5N2yMHsufc+al98CS0YpOHfb1L8gx7FDvsSNisqil9ZfRfqqv1pcgwWNTXx6dyxaet94dxz9e0GI6AWQxhioXbP6J6YuJhXl2bz1b/TEFAwWTWeSvkZh4zYza7ZR7P5tHMIdavfNLj8NLji9YdEAVLNMmlmA2lmA6kWmTSTgYAhG7PqI0Noxzh6KnLRaALb1mJsqmHKXTez94kXmOd282xdLSvbWuIIjjk9nRE/uZi5i87glrIydu3axa5vPmVUfgY+wUi1lsbOTZto8Gq02/RrMv3YhRgUlXVVTsSccZjS89FUaFxThqsqg2CXhaDTjBqIDonFxRp33h1i3jFBqlrcfLilgTfW1XL05Dw6t+Rww4VZHHluO3PPacdg1PCFVL6s7uCbbS3IzeVMHj+Wg0eNwZGehVy4HfuIHDr31NPw9R485eVRi0cMJLOR7HH5ZE4qwpRqRcrIRcobgZxTBEYTXtGMWzAiaSoSCpIW0tPYUZFyigju2UwGAmpGHi0rP8ff4aF89Wco48ZRcvFFCAPp2oQVq4eC8DOUzLOpKPr7ZK2dPYOF9weys/U2B3puw1anK69MysqbftihpB926D51TZBlbOvXY7vqKnJqahgJVBSNoLpAF2DcedsdiCYTOccdC+jxjYZRUwnu3Yjm8xDY9S2mCTP3qQ/fB/y/WnBuvvnmAQnI119/zSGHHBJ5P2zB+e+Fpqn41rxBYPtX3Z8IWOaejnHswb229VZV8/VZ5wBgLizg0BUvDa1S+HdhwdnfRRUTuc2uvx7+8pe+91m+HM44A0pKBq43VFRE5+uvs+GyK3pZcOoMeQREIxsbutjZomvfPHZNIXvXWSmmivtYwiJeJShKGFQF77IXWXf4MXxb24ErkFysgShAWYaVybkOJAHsqouMUAciGqrLiWHsoQiiRMDtZtbs2WzYsoX3HnmECdk5CLJM1pFHIlotiKLIRRf9iPfeeYf3X3yCEaPHUy9msOKF5/n9Nb9k2tGncsav/ojBZKaxwcWqzQ2RPgQ6zVS+Nh1PbUbCPi5dmng+a3cH8PgVDp9miczNGQUBTr68heLxFXz28tN89caLBLweNE2loLCIn11+BT9bfBJpop/g3q0EKnbQVdGEr82Nqqhoqoqmalgy7KSNykG0WjAUjUYeMQbRbMUvGHBKDjyiJU6nJxaCppKiOElTOlFbG/Ft+Ay/O0D9xxvpLNdjTYrOP4+RRQUI4Xi0RJbPm29OLoYmEXo+QwM9m6+8Amee2auZ+BPrfhbuuQfOPnto/eqrzXvvjbbZ1zR4IOL0kkECK5gG7B1RSm1e90JGkph4x21kHTkPAKWlDtdrf9W77UjHsXgJgvgdZ30eQPzXWXB+8YtfcO655/a7TWlp6ZDbN5lMmEymgTccxncCQRAxzz4VRJHA1i8ADe+nryHllSKlZMZtaxlRTNphh9Lx1df4autw7diJY0ICQauBajh9F8rE+7O2DkRFyh58UDeJ5+Towcb94dxz9W0eeGDgSeP+++nL16R1h8F6YsjKTc3XM4LNzGU1CHDZOX/ki5EzsPvcZO62k9W1k0yHCYdFxmY2YDbJGAwiza0CzR2QXRQilpuqGuxq9eD0K8wekYZLchAQjOQFmxDtDpTmSqTsERhtNq7/3e8477zzeP6LL7jrrrvIyMggGAwiiiJaKEBjTRXtHZ1Y0rJpMmQjCiKdiojZkUpKVi5oGgFXII7cdGzLo+adKSi+vk34ubmJF+vpNiMbvo43PLTVGfnn7/OxpV2Ku/Mtxs+cz5hD5iBIEhvee5U7b/sjKZk5nHXK8aSNNWMtLEPKXIva0dIdgKrfl4LZiqFkHHJRGcgG/IKJTikFr2QBIKSoVHV66PKHyLIayXeYIvo8miDSKaciaQopmWCeNhu+WUnRUVPwv/olvjY3Nc8vI+XO28nqz7qycCE89tjg65VJUu+g3YGezUWL9LT0Sy9NbDWKFRNctEjP/ktUwHUw6NnmQOUtelqEvwv0YREWgFFVFSiiSENOLigKO275IxnvvoVoMCBlFSAXjiFUuwvN2U6obg+Gov/tAp//rwQnKyuLrKzEpvJhfD8hCALmw09GCwUJ7lwLmkqochvSlCN6b6yqkZeiqZfmrY6BVonfhTLx/qqt0xODqWOkKPpq9OWX+540YnRF5D17AAg49ew21eNEdKQhayGCGDDJ0esxp3AzZTV6/NT6ggl8MVJ3K7rMNlwejcqKvgNS1aBI14oChOZ8cnMNFI/3Me0oF7JRo8Hl56uaTmYWpxIQTTQYcsgNNiEFvTrJySrhlFNO4dhjj2XZsmVMmzaNK6+8ElmWUZ0ttFTuZN3GTRxy0DQ6LHk4EKjr9GGbOIfrn/0Qs91BqcPMPa9tifSn49PJVK4uZqDiIP39pIn57KO4O95GEE4lt/RBDj05gMGkMWXeCTx02Rncd/ddzDhsFvl5haTLNhyHpER6oCmKnkIvG9BECZdoxSk5CIj6Pd/pC7K3TU9tD3Xfv7tbPagBETqs5JotzJulxzq1SWlYVS9yZh5ywUioK6d44QJ2PfUmaBpVTz5N5tP/QOhWsk743AymZEEY4dIUkjQ4MhBW1b7tNv24scHNPcnFwoX7TnB6thmj6h2n99PUdGC1r/obt/qpoycAYyr24rZacdodKG43/voGLCP0IHlD6URCtXoBUM09iOKq31P818TgVFVV0dbWRlVVFYqisH79egBGjx6N3W7//+3cMAYFQRAwTZmjExwgVLcXUw+CE+zspGPdekB3UVlHjuzdULLlC5KN1+mJgchT+Pva2gMTIzAULFmiu7YWLtRdZ6tW6Z/Pn6//dfff2L2wcNW3AxBqqEbOLcaqevBKFrJtRva06S6qyumHU/alTnAqM6JFGNNljYBkwO3vW7lYNKikTakBamiuyGTbq6WsfSefi/9cj8GkUdPlw1AnMKMghYBootGQ201yfCjN5Vhs6dz0u+s574fbWbJkCUrAx8xJZdTX1vLYc8txub2c+9MrSEnPoM0b5KuaTowWXRLwkIJUHnlnO0FFJ8oLSkZy3x0jBryE2dn9G/R6kx8X8BhQgKZdz8cvjGDLZx5+/3QzpGcy7vD5fP3Gi7yzqYLjbOmojnScoh2TFkDUVERJRdRUQoKMS7KhChKKqlHT4WVvm4dWT7B3JwDRqEKOi2ZcrPjGwNGTUsiwGmiT08kJtSCOmQHNdVjxUHTEJGpWb8a1Ywftn39BxuxZfVtXws/LL38Z7+7MyICOjriFRy8Mpv5TGJKkl2644Yb+n7dBFsSMw+9/D0cfnZiwDGRp2t8YaNwawCIsAGldXTjtDkAvXhwmOLGWWW0/paf/N+O/huDceOONPP3005H3Bx10EAArV65k/nd5cw5jv0BMzUaw2NG8LkKNlWiqEucvbvssWtcm68gje1fQHWwl4Z4rtZ4DaDL6NT1VUIeaEj5QQcJ9QWz9naOP1v+Hz2v16sg5yykpCAYD/g4PoYACzbVooSBW2UurppFti1rMqg47Av7+ZwAqMwoin996waHMHZeDNxCiZcW/ab7nAZq8Ko2OLD5Jmc2Hjvk4SluRzDoBcpS24ihtxd9u5fm/TODC33QhyVDe7sUoiUzJ060W9XIOBaFGxFAAtbORw8ry+Ouffsdv77iXX133a2RZwiAbEESRny25hrlHHYcnoPB5ZTtK9/0wKdfBNzub2d2op+uOzXMww56cuf6CC/qfn3vPs+uBb4FLgEMRBLBg5dLZRXxc0cLno8YBsHfHVtYUjCTHZuSw4lSCcu+FmapqvLX6Cz788EPaGuvJLxvP+MMXYE/PJOAVWP+hg11rrUyc7WbyPBcGU7d7yxLk04p2ThynZ8F5VDNWM2jjZhDc/DkZozNo3mjF3+lhz5W/xHHN1RgGyrjr+cyJYv/kZjD1nxJhIKIhSXDeef3HovVE2AV9883fvQp5IiQzbiVhETb5fZHXvrq66Bexrmc1uZIp32f81xCcp556iqeeeur/uxvD2E8QBAEpM59QzS4I+lFdHZE4HE1VqX3hpci2mfOPjN95qJWE+xpAkyUrNTV6fMs55+gp2cmsIouL9QDJbnG9iJLxnXcOPZhzIIRXgP2sFIVFizBmZuBvaMRd30FqSSahxmoMhaMwaz6QLaSaZDr9IRpHjsP70SosDXU0tWRAvU7O8lJ1lWPLG69TfOFiimOuRwmt/INrEI0h0qfUkH1IBaYM3SJkSvfQZm3ghdtGccFNemzMjhY3aZqV4nyJkGSkwpnJKEvUInb8giM47KApfL52Pes2b8eSN5ITF5+PxWrFF1L4pLwdb0iffItSzdQ1OHnh80pA1665/ezp1GxPboJbuLD3Z4oSbxC75JLYeq6vhPdEEHRieN99YJThhHG5vBzQzyNM4BucPj7ao1CWYcVhknAYJewmmZrmNpbe/yBvPL4UBAFrShpf/fsF/i3fQXbxz2ipvRO/W29jyycOXnsgxKwjG5l1sZ+UTIWAqlLb5aM03UqbmoIVH8bCUkK71gNe8ifmU/H5HryygT1/uInxZnNi62Vfk3CyFsoPPzwwrp1XXoG7705++4Fc0N91KZZkx63duweMG4yVzQi0xrj1AlHi850Ue/0Px38NwRnG9wuaEiLUWKW/MZgQbamR7xrfeAvXjh0A2EaPJmXK5Pid+/FR640nsZIMD26vvTZ4v/6LL/b9XVgl9ac/1Ve8PVxDkWPPm6cPZs8+u/9dW/n5Sa0UZUcK/oZGnJVNpJZkonQTHKvixSdayLEb6fSH0IDyiTOYuOBIct7dAfV6/E59h5cx2baEg/ZcVlNENbWBQlrXltK6toTUcQ2ULloHgDHFS+cGB+meAO1WfYB+8FGF3y+RSEkFMc3KdXeWcH7VvcwY0wFlI0mfdwTHHH8C006+kKCmT15BReXzqg66ut1k6RYDvq4Af/tgV6Qv158ykdF5DkZmDy3e/JVX+o6D1QtIfIxeTOJgBAGuuSbMG/TffMfmjQCcf8wsKkSZLl8QT1BhU6MuRqeqKrIksuZfz/HG40spGj+V+ef/jEMmjsXiaeahu//Bxg33ACMBvYJ6VhY8/LCM9HU1v793Ej+8TSeKzR0qpengDBkjo7voyEDx15I6IhN59Q5CskxTVjbZv/4NmT0XAftD8uBPf4Knntq/ZVCG0q/Cwt6aUmFCs2uXHo8W64I70KVbkh231qwZMG6w85hjoVsx2Toq6r4P7N0YeS3lJCde+X3G90/PeRj/FQhbbgAMJRMQuk2rIZeb8r/+LbJd2a+W9E4P31dtm1de0cXzFizY96DFntA0PZvk9tv1gf6YY/RjvfJK4mO3tEBqKpx4Ilxxxb4dWxB0i9Hs2f2vFAGWLEGy6tk5zroOAJT2ZjRVxaLpq8A8RzQD8avqdlRNY0yeI/LZzgYnrF6NUlPHKo5kGeeyiiNREJFQuZ+r9G6hAgKdO/JQQ/pvKVmC3HsvPHBjtL2Sg13ceF30mD+6QuPLuZfTkjmStpYg7e9+Rb2UHSE3AUXlq5rOSIyKwyQzPs3Cw+/viLRx9YnjWXyYHncTjjcPX6qelw56L/bDmcx9S8O0dv9NAVJQVd2D8vLL+nVubGxk8+bNZGVlcfIRh/DDg4vJT4mv7yWKIqoG37zzMgDnL/k9d111Mb9cdDQ/vfBc3n/vWYqKJiBJjwB64Ghzs15KicMP5w/XRdepze26dc1iE1G6h3cD3cdLtVOmRks37LKnEHz33fjTGWgSThZhIh2+7/cVQ+nXU09FyUrsc3f++br1tKecwv7uc9jst2yZ/n8g+YYw6uujcVCFhfHfFRWhLV9OS5Ve30+QpIjmjupsR6nXtZXElEyknIHjzb7vGCY4w/jOoakq/o2fRN4bYso2VPz97wTb9cDXrKMWJK7Im2zWUqLtwpaN/TGIJ4vwwHn99YmP3dkJb78NDz/cv4k8LMTW13egz9Br1iS1UpS703oDXV6Elg4IBVG72pC1EJIWIttmJMXYLeLn9LOhrovx+VH9iQ82N7DiNZFSKljAKs5nGQtYRSkVvMIZLOJVVrCYQsIDuxBJzc4pDJKVBVu/NVK+UZ+Ac0sDbNyu8upL+oRtd8AJ55lw/vAiOn/8UzrPOT9S2SqoqKzc20a9UyfJFoPIwol53PXvrREOd/yo0RhrRrFqlT7XKIp++a66SreAxKKoqHfYlqLocbb9wwUEgAIgGvNw1VUqigIff/wxHR0dHHusLshmlQV+dHAxPzq4mFMm5DKrJJ2x2TbMQTd1O7cwauwEbvzBKeQ5zCiKQjAY5NNPc6ipmYyibAa2hn/EyG1l0KKkULZEY7uC3RYkMTM38lnKyBwyOvTnK2A0snvZi8RJoe0vyYMYIr3P8WaKoru9Boum7hIVyT7z+7PPPQnVggV6u8kgPG4tWgQVFbq+0PPP6//Ly+koKSHQ/eymzzocuTvJJrBrXaQJw5iDesct/g9i2EU1jO8cvm/eQ2nSVyBiahZy0RgAWj7+hLqXVgAgGI2MuvIXiRsYqrbN/lQcHgzCx7v33oGPnWBgDcgGKopH4LTZEMtGIxkNiDt2ILndmP1+ChobMOXlRjPCBtLM6UYwRrdErGlCyUpD7WxFSsvCovpQJDsHFabxcbnuQlpb08GPDy1mbJ6DnQ1Ottd18ct1o6gnfpVZSyGLWcEKFrOIV1nIa6xeupbPbUW8WN5NSCzRuXTjRw5GTtWtRkXjfdz6m1QmTvYwbmLigFaDJPDRnnac3W4pm1HirKkFvPplFbXterHJYGM6d905Jiwxk7AQdlaWXg5p4cLE4RfhBLn+kQl0AC1AODBbpbZWZPVqePbZZwE455xzInsIgkB+ipn8FDOapiEIAl959Mm3pDAPq8VMIBBAlmVE0cAvfxkCwpaX8DGESNjGr5YIXPmMoJewMEbvL6WxDXIdCAX5sE0DUSA0bQxj3ljDN1OmocgyzTt34Xh+GUUXdAcc70/Jg30NOoZ9C+bPzx/8Mx/u86pVepD+ULAvMUyZmfHjVo+4QS0UovyBhyLvw0rGWtDfrS0GCALGMQcNre/fMwxbcIbxnSKwax2BTZ/qbwQRy5zTEASRkNPJ7ruiAYRlv7wSc0Efg+1QfA2w/8zvQ8VgVoXdfW9NS2ftlGk0ZOfgttpw1tfTUVlFm9lCc2YW1QWFfH3Y4VT+/kaUk07S901ikvIbDDiD3QTB40bLTgOIFPgzaLrbJ9tmJK+73lKLJ0CTO8B1p0yMCMzlzN5L6vjGuLa17mFlCfehICEVFyIcm8ur1d9EtjlpekGkmz53/DDkcgksPNrK0YfZOGGOlVPmW/nZMU52nXALBTs28XF5O21evX/pFgMXHTICTdF46pO9+vEVgT3/mgJa9N5obe3tZmptjUqvJDKcJWfMEIHJwGYibAoREPjww1W88847zJo1i6OOOgoAqceBwqvskSNHMm7cOHbv3s3u3bsxGo2IothNstYD64AcRHM9qePqsZe0INt9aBq0u5RI9pjNGG1f9HdXZzcakfbqTE0tysGQamVcxZ7Idnsf+istq7rrxIUXD/tz9T9Uq9C+WFvDC5yhPvNnnz00V9W+LqJaW/W4wD5Q+9JyXDt2AmAtG0XW0fp9Fdj2FZpfD+I3jJoSF9P4v4xhgjOM7wyhpmq8n/4r8t4862Tk/FGAPsgGulc4GbNnkb94gEC/fnzUfcqq72/F4QMIRdPYee75bBk7nqChW3E3UakKQA0GqXziH3y9+Bwa33obbc6cASep1rSoqyuzqwM1VzdxCE7dUhAmOACji6Iq01sanBw6KpPTxkyIfFZ88kbMOfGiYhoi1YxgNXN558YHuGbZegLdWU7zJ+Tw86PHROfSmNPqLoiMoghUVYjs2SWxY6vEyo15nLr+zzzhLqGuS7f22IwS50wrxGGSue/t7fi7229ZW4K/bWBtrIE8EskZMxzAz4Em4ALgM6AO+JBHHrkAh8PB9ddfj91up7+qONnZ2Vx88cVUV1fzk5/8hFdeeYXNmzezcuWnwE1APZCHJVujdNE6Rp33JemTddKSnhf9rcRQlOBImdHfWN60O/I6eNBYstrbGTF7lv6BqrL1179l9933ovj9fS8ehorYC9kzLiX2wsd+9+GH+0YUfvpTnbUO9ZlvaxtaPM6+LqLCmVQJbkhffT0Vf38sst3Y3/4GUZbRggH84UUjAqbp84d+/O8Zhl1Uw/hOEGqswvP+s6DqD65x/GGRYnAtK1fR8C991SJZrYz+9XW9/ceJUjoH0rbpiQOlOLyfoYgiG8dPxLlXDxiULUYKTpxNSpYRyWTUA7IlGQQJb0sn1W99jqepk0BzMztu+SM1z79A1lnnkPrEY6Q4nYjdk4QqCDhtdtrS0qjLjV6LzII01G7lYtHZvQqM0dAoSrUiCZ0omsaWRidHjspkjKGEto2dZEytRTIqjL7gCxo+HU3IZQZRQxA1BFHlgeN/x969QcLWjROm5vPHs6ZhkHRWc//9cNuj0XMfc6iHdR848Hvif0Nbusqpv2jGbdT7Z5REzp5aSJrFwMaqdt7dpE9kIY+Rhk/HJH2t+/OizJ2r8+eB3VQ/BLYD9wNfomdP1SAIWdx1110s7M47Hygm4uqrr6apqYlHHnmECy64gMLCQiorq9DjewAKMDjygCYEQUDx6sQ3Iz/6W7U2SHqyFbB2Vxqzpus6QPLWCvxn6J/7T5mLdPSJlPzqBry3/onm994HoG75CpreeZf8MxZS8OSTmH7/+32frGNdxYncTVlZ8Ne/6s/sUF1RiRBWRN7XZ36wwoX7uojq44ZUAwF23PJHVJ9O7vMXnRHJLvWt/QDN170wGTUZKS1n3/rwPcIwwRnGAYXm9+Lf+An+LZ/rcvSAlFeK+XDdndK5fgPbbrw5sn3p5ZdhzsuLb6SvgbG/AIpECJsM9rebKiMDXngBLr4Y6ur6XnUmIfCnCgJbR4+NqJSqllQyTphDepYPCEF32nYYZiOMOf1gXO0Klf9eQ8gXxL1rF+5du2D8JERVJcXpRFIUOlJSUOT4Rz4j04J6atTnL44cDYAcEzArSwJjsmxsb3bhCSqsr++ioCCNmncmY8pyYSvoRDKHKDxme6/z2RvzetGhxdywcHLEvQU6R/XIBsK/yMQ5bq56vIrlf86lfIOVrOIAc89q56DjnBi640sEYNGUfHK7s7weXxV1tzR8MgbV33eNqb6QaF6SpORKe+mE5s/AOcAKoJxjj53BsceexMiRE1AUva0NGzawefNmZs6cSVlZWRzhUVUVg8HA3XffzbXXXsvrr7/BF1/Uk54+nb/+9SN8vgeA6Ziz7ejWIgi6TQgCjJwULSiclao/B34ftMWEfIiXXYFUU45SlINmM+O2hTBedjbjTjyf1KlT2fvgg6j+ACGnk+pnnqVakrDOmYfVZsVisWD1erC89CKioiJoGqKmIWgakqpgCCUQlOvpKu4vLmV/FdGMxdNP6/pT+6J+PJQYov21iIq5ITVVZccfb6OzW9ndmJ3FyMsvA8C/eQ2BLWv0DQUR0/QF++f43xMME5xhHBBooSCBbV/iX/8xWsAb+VzKH4XtmPMRJBlPeQVbrrs+EveRe/JJFCzuMZv0NzDed5/+l6x+hSTpZaLD1ZT3BwRBL0547LH6bNhfzatf/apfFVZFENk6ZiztaekABC0Z5J90BPmZLhBEXHIOTp8FszFEmkNBEhU0nwfN48SeLjHxogW07myhduW6yPFVUaQjNbE/PsshU3jKwWDUCYHRlocwoju11BBNZdY0jVklGWxv1q0Bn1e2kduSgohE+YuHUXDMVjKm9G3myE+z8LOjRrPw4KKEVowLT7OwoTaXt7Y2Icga6XkhLl1aS+UWM8UTfHGeuVAQ8ty5lKbr5Rh21HfxyXZ9wk83m9mwYWjaH33NS4sW6ZqOP/wh+HyJt4liBnb7DEwmeP99jfff1881zMUbGp7nxRf/wqWXXsp9992H2axf41AohBxDPNesyeOPf7ykm4evAT4CSoBjseQ4I4HJ/mY9oy23LNqxskK9ndoagY7OKOlXH3kca1sTnp+chjJRN/EEZk8itOV9ciceSdqzz1D99D9pevc9tGAQFAVPeTme2NObOIVE8HWYMVYpzPV9ikS3jzG2DMr/R3B/R0eUmIQ1ZYaKwVhl9oVQxaL7htRCIXbfszRiZRNNJib++U5ku51g+WZ8X74d2cU8+xSk9GHrTSyGCc7/CFRXB4Ed3xDYswHNowuMoWmRh1B0pGMonYhcOgkpq3BoKYaKgrbqIwK1O/FrbWih6MoSUcI4aRbmg49BkGS81TVsWnI1oS69L+kzD2PM734Tf9xkB8a+yjMkwr4Ud01N1VO6wygu7l24b6CaV4ceqsvN97DkqILAlrHj6EhN0z+wpzLipFnkpLkQUzKQp88n12Iil94INdXi3/Q5QihI1ug0sqedQaDFT8cTL9GqyfhNuqVDDgZJ7+wgw+0kpSiDwMKToLuIqbx+J4ZfRsml5MhGcOuOJU2DXIeJcdl2djS7cAcUHlreiaKkg2Kg+o1ptH47AltRO5omoKkCqCI//5nA0fOMzBmTjUHuP9xvWmEK9TstvLK+kVHTdUJcMilGjt4t8tUbKXz2chqvLY9aaJ78OGq9+enRo9j9mDiouWWgYvKvvAI//3n/5GbxYhg/XufPt94KLhfEFvPUubgKzMVo/BJVnRYhNwDr1q3jySef5LLLLmPHjkmcc47U3X8X8BdgE/BnEGZjLfxYd0/5ZHJTzFxy0haUUXpbrnaJ9En6da6pEumKITjrU+dySPnT2JY+T+CYw/AtWgAGGbUoB1fbZox1aYxZcjkjL7+MuuUv0/LJJ3irqnWyMwDMaT5CqRI3NP6Rw06fzaIzSLp45AFFmJgkUzW8PwzGKjNQcV9N0zOl2toGzAANudxsv/EmvWwNgCgy4U+3kjJpIqH6cjyrlhN2/Zqmz8c0/rDBn9v3HMME53sMTVMJ1e0lsO1LQlXb+x311a5W/BtX49+4GsGWiqF0IobSSUi5IxCE/icnTQkRWv4Mga8+IjS2CAzxt5Vh9HTMM45GdKSjqSp1K15m74MPR/zJ9nFjmXDHbYg93CdJD4z9lWfoiX3xkT/8sB6U0V+8z0BxQWedpfe1hxWpPjs3Qm40BTomnca0NN0qooyaicVioi/IOYWIhx+P99M3AQ3N3YHBAtm/OJW8PTXw7U60xjbkEbkox4xHGVVIIOY3MjR7MJ56Lli6U5BlI5jt4NEZjtY9iB5RmsGObivOsRe3ggBfv5mCzyXhqUvHU6dbniRJjxMdrKHs2HkGfvqjQkoO6+D4S1oxmDQ6m2U+ezmNr95MIeCR4sjIJ9sbI7E3GTYjiw4rxtzH3JIIAyn5h0X+Bmrjyy/hueegrKy/Y4rAKQSDp/DYYxonnBDlxS0tLTzxxBN8/fXXbN8+H00rBjzAMnRycx5wJcUnb0e26O4gT0MqZ0zZjbprDRb7fABaK80Ic/WTqqkSUbuiJ+XM0C1bggam979C3lqO56cLUYtyQZYIbPmcwNYvMYyaTPHiUyi97FI0RcFXX49nbzneVavwrf0WdeNGKoRSPhHmIQkhJtk3k2dqRBYUTsp7g4deHAViIYuI1j1LWuBufyOWmCxapC9Qjjkm+f0HYr99YaCFDvRv6b3vPtyVlWz99e/wVulq74IsM/b3vyNz3lyUljrcHzwXiWc0jJmBacYQU9q/5xgmON9TKC11eD5ejtrRHP+FICCmZoEodT9QAqgKakdT5GHT3J36gLflcwR7GsZRUzGUTUPK0O0HmqaidrSgNFcTaqwitGs9mqbApNK4Q8kbd2N+ZSXS/X8DRzq+hgZ23nYHHV99HdnGUlrCpHvvRrbZep/EYMhIsv7yffGRFxYm54sfqGjg4sXw8stxA2CsG6l1Vw7B2dEsoLW7czgyV3fjfbHWwhffWlEUWHiayugCXbRNtDmwnXIJvm/eR2moiOyrlBVBWRGgy9HFQTZgyh6FfOz0uAwtMSWboKpFxt5wQHC23UQ2KTTThcGkcdLPWjjmR62s/9DBF/9Ko36PTsIURS+9NVhIEtx/n8DixelsWW0nqzhA+QYrSkjoRUb2Nrn47YsbIn38yfwyzAapz7klkQ5Of8Xkw8bDgRC+7f7618FwcSGOi8+bN4+nnnqKv/zlcVyuB9HJkAkoAm4HfkXu3IqIG1ANijR8PJYH6tI54afpHIl+D3RWWiPHaawXcNVELV1FY4y6p6sbUm0z9j89iff0+fiPPgzRIIKmEtyzkeCejUj5ozAUjUHetpv0W+8kY+tOBEBB5GhWUUMhIGIU/Jyb/zw/KdKze87IXcHV993NwvvKkArz4Ygj4K23Br4w+xsZGb2JSVj4LxmEb7h77okKIjU36zd2YeHAcX8DLXT6IUBNNjs7L74ksgCUHQ4m3nk7aYccjNLRjPvdpyM1p+SisViOWDgs6tcHhgnO9xDB6h14PnoRQtEpTbA6MI47FOO4QxBtKb32Ub0uQpXbCFZuJVS3N7I60FwdepDwxk8Q03MRLDaU5tpImYWeELpcGL7civHzTUhVDSAIqEuW0ChK7H3gIRR3VCo+/8wzGPWLK5Cs1oRtDYmMxJKiRJlXQ/GRD3Ul1x9iBkCtpobO+x4EQA4FyehqZ4szSvg+Xm3gyDn6b1lcGOTXf0phzTdWZi5QGJtmQu3Q6xAJEthPvgSlq1WfqN5ejprbW/lY8irIxjSkw+chpMbcC7IJMS0f0WzD7Y7+vjZjdJiwNGSzcZvK1AW6Jcdo1jjs5C4OO7mL2l0m2htkPF0SW8vbsZRmYzUZSDHL+p/JEBdg3Ndl0cd+A7vXxkzQMWSkyxtkyT+/wd0t9Hfs5DzOn12a6NLG/fSQfMLdYL0qe/YMvE0YPbm4zWbjvPPOA87j/PP9wNruLQ8CLGRMqyJ3zk5ARNOg6vXpEWvZqINiomRcJsJqyq0tAju2Rt1gIydEr6WCyGrm8lroNJ5dcSG8a+WSBV/y0wVfkunQ21Pq96LUd4eI/+oscHuRapup3QklW0M07lEJKiIBzcQzdRcz2b6RmWlfUmiuoyC1ltWdc5lf+3H/dduSgSDohOKpp3SC0tgIV1898H5XXRUN6g//6O+9l/xxi4rg3HP1uLlEN0IycX/9LXQS3KTuoiIqH/8HLStXRTazjR3DxDtvx1JYiOpsx/3Ok5GMKSl3BNajzo0UcR1GbwwTnO8ZAns24P345YigiJiZj3nafOSS8f0+CKLFjnH8oRjHH4rm9xKs2k5w7yZCtbsjbantjXQvFuPhD2JYvwPD55uRt+5FUHXiEJRl6rNzqcvMIXDHnyObG3NyGHvDb8k4fGb/JzMUMhImRf1U0u7TR54IA/kx9gXdA6D3pZcIdbed4nQygW9Z67wyslnFlk4qqi2UFgcpzAvx4fJyVn4kc8Sqf4D5BCjRAws1nwvN70ZKyUQ6aAHm8naUKy8jePB4NLsFyZaO+KMfQWmPGjWCiJiSjWDPRBAEVE3D5Y9mxlhjCE5Bvsiy8/P54KkAhy/sYMbxTsw2/f4oHOOncIxOjLqQWFneRk/YjRKHFqczc0R6n5elL4IiCBpf7G7lrx/spKpVn4jH5jm4dfHUXivYvuaW2M/CsiuJCM9gPZlDiSfteQz91jUBs8OtkjGtiqITtkTcxHUfTKRzp55laLIpkevdUG4kzRG1wrU0C2zbZSAYBIMBmDQegFc4g6u4nxpigrGdcOfrR3P/O3M5b9Y6rjjuM8pye/x2NgvK2BHkjYU3T3kCl8/IA+8cwV1v6EJzrzYtZmbalwAsyl1Bfed+yCYK/6b33x9VFVYU3arS35ggijBhwtBUkG02uOUWfaw477y+j1FTk3zcX1/ovkk1TWPvAw9S+9vfgxpV7849+SRGX38dktmE6nXjfucpNLeuNyVm5GM79gcIBmNfrQ8DELT+1Ke+Z+jq6iI1NZXOzk5SUnpbMf7boXrdOJffG7GuyKWTsB65GEEefNpstE0XwfLNBPdsiJRXEKwOpOxi5OwipE07kX5yOUIgGozoMZupySugKSsbtYc4Xe5JJ1B22KHIHR0DL6Oh7yyqnghbWcrLdSXQ/vZZsgTS03tXE07kx+gZSNwTiaxEgyRCrbfdzpbX3wBADoWYsHsnuw5dzLjzounyb2yeQcm8KRx+aC9HUxwERxZSakwocvcgrxXlofzzEYgJbgXAnMJX2/KorDGQnw8zZyk0u30EFH2gNUoiIzKi1iRF0UvshOcXo0XloGO7mHlqJ/ll/fctDFkU+NXcMsR+rDmKqlHR7GJ7fRfb6/S/HfVddHmj91ma1cBzV8yhML0PC2A/6E+S5ayzdOKzYJAZt0moAMRh5crehCt8bWW7l+JTNuIojd6LTV+OpP6jqMDiYad2cMbVugt6zaupHDXZwSln6MR08QlWjhtfzm13+nTy53Tx+sS7WMzy7oiqvuPqRCHEUUUb+XfhxWhFWaiFOSiF2WgZ8WNmSBEpuOJGAiEZEYVXDzqFNEMnHsWCea3CfD5O/mIkQmam/oz2fPaSHRP2Bcn+mJmZulVpHxY/Nc8vY+/9D0beGzIyGHn5ZeSecjKCIKCpCu53no5Y1MTULGwnX4JoGVjM8vuEoczfwwTnewTv529E6pEYRk3BMv+sfgOENUVB8flQfX4MaakIAzykqscJmhbv4oqZCTxmM5WFxTRnZMaroGoamZMmUjh1Cmn33pPYqtIfgbjttqimfiKEj7Vihb70Ly1NbtVWVKQrno4Z078fI9FnYW2PvqxEg1jVhW68kfWvvo4n7KrTNMram+CHZ2AviBISVRPpyj6EvAmFSFZz74ZkE1JmEYIh/jvN60JpLo/G2RjMiNZUPvzUzo8vNXd3X+P8i4L87lY/pu7dBSAv1RLnooLo/NLd1QgMxhDWFJW/pl7N7JR1eFPScGdms3v+CVRMPiSy3dT8FE4a3zsfTFU11lfqon0fbK6n1dU3YcpJMXPXeQcxvaRvS1BfGGh+vPZauPPOeCK3PxHLxXs+cq+8Aj/5fR2FJ2xGNscI+K0rpuadyYSzsyRZ45p/VpCeq2/z5DVFvPJaCJMZgi1OhLv+zug/xujL1LYycs5oapQ8khWwX8n8OJLiO2M+/pPnRN7/c/UMrnxav88zDC28etBpAFS4S7lgy4vRlPGhoqhILzYZvkixi4mdO/WUNXUfj7E/cMstcOONQ9q1Y906Nl7xywiZKr3sUgrPPQfJYols4/vmPfwb9OLEgsWO/bTLEO1p+9zt/zYMZf4edlF9T6B0thDY9pX+RjZgnnlSL3Lj3LaNPffeh6e8HMXjRYtZocipqYz40Q8oWHwmoilxxo5odfT+cO5cvCUlVEkGGjOz4oiNpITIa26mQATLydfogl49Z4v+UrwTEQi7XZ+ou2JKA8QGaKxalbxJurYWbr5ZP3bsUjr2dV8k5rzz4O67B3c+iaAoyE8+yfS6enaUjaY1PQMEgT0Zucivr6a4JB3L3KkY7EZEQSWt5St8q0Gw2hFTMxG7PIhfbUA+cTHysSf1al4LBVHaayLkRjDbETOLefVVMTLJp2eo3L7UzzEnRidUr1tkbJEZkxwzA3dPMIv89ay4eQpXPTaJmpro750XqOO+liUsank18lnjqPGs/lHU3VaWaeX4sfFaHbsbnLy6tpr3NzXQ1NV3PnbQaULrSOX8Iwv59cW5GCRx0Aa0ZJQHwj/rYDyZsehv8d+fx9PpC7I2tIWS0+sinwW6zNS8NQVneXzU9ozjuyLkZvvnVg6ZJmHqJkRZb7xI+g2nRY9pz2S1dyI1yuACUevJR5NE1LxM/MfNJDhnWuS7J1cdwrXPnRJ5f3jq55HXOR3N+05uQH/mVq+G2bPhsstg+fJwDv5/Fu6/H264YdBWHH9LC9tvuDFysxT/6IeMuPiiuG1CdXvxb1itvxFErEed+z9JboaKYQvO9wSej14gWL4ZANNBR2GecVTc9+695ay76GJUf/9uBEtpCVMeuA9zbiLFlXhoikLNshepePivaDErKUMwSFF9HfktTciKogca9hWsB4mXtH0ts8OzzS23xFtewvstWwbnnz9g3+OQna33zdjDnz1QH/pCf0v0noixgGlAZWExVYVFcZuIBoncyYWkj8hEzk5D6MO1I+WWYJo6D7l4bCQmRWmrQfN0a/cYzEg5I1FVMWLkslg1Xv/ITemo6Pk887iB554wsXOHEO1+AqKnFI5g9eHXUf/yZ+RTz1xWRyY2DdhyzGl8cMUN+FJ1K0tJmoXFUwsiWVnt7gB/+2AnK76qQu1xOSVBpH1nFu7aNLyNqXgbUgh5THHGOhi8AW0wrqeXXhp6BYGlS6GyUk8fjyna3qfH88vdLdz48kYaO6MEb0ZePtbyybz4rCGuEHXROB8/+GMdKVn6xPj8DYU89LBKQZF+EQv+9U+MJx4JgGBNQ0wv4IUXBG76VTOHjKrGJIcwSCqypCJLCiY5RKrVR5rVS5pN/59h9zA2tQaDo7d72/T6at56PTMSy2MU/NwzfglTHRsAmL5lEynu/URETjlFz8L6T7DU9Iff/16PExqEi3rLtdfTulqvIZV26CFMuX9pnBVdCwVxrlgaibsxH3o8pqn7MdHhvwzDLqoB8H0mOF3/vA0t4EUwWXCccy2CId4Ks/uee6l7SZ8VBFXF5vUgqiqiqoIg0OFIiSwvTfl5TH34QSw9C1nGwLVjB7v+/BecW7ZGPpNVhaLaGgobG5BUNeoCCgbhT38a+CTCQQnhYITBEKIwhhI8ATrJeeSR6MwzUB+SQc8gi0RIQMg6HCnUZ+fQmp6B2uP8JKNMSradrAwLlsIMhBG9iaiYkYd17iKkrAJCjXsh2K0kLRuRckbx8SdS5BLlF6p8si6a2bZlo8iFZ1hxOYVo9wdJ9Nzpmbxz9a3sOuLYyGeFmp9z5k3EKIsEFZWXvqjkkQ934fTFl4SYNTqLYycX8IuzcqguTxw7Jgh6FnAirbRYApSI5AyG/2ZnRwOBw1airVuTu5Wffz6q5ziQhamu3cui+z7GF9QncbtZ5nenTeKk6frzt3w5nH22RuFYPzOO1+OdpG7be/MeC2fPTiMzW78QxpCPAiUmHdpgJ1S9g86t2zAHe0hGDBZeP+YVH2H6RFfKVhBZLR1BcIyIKUWP+zMGAsxcv5bB2Yq+RxiEi/rL0xfhr9czIMuuvorCc8+J+151tuN86R5Aj7uxn/nLATXJvs8YdlH9j0ILBiLlEMT03F7kRtM02j79LPyGw9ev7VU/xmW1sXX8BHyyAX99Axt+9nOmPvQA1tLSuO1CbjeVjz1O7YvL41ZVReefx4iLfoi8bp0+mu/apQcI3nRT8icSO5v0RyzCebYPPghXXhk/YwxVKr25Od61tD/UV5NJxUmQCp/m7CLN2YUiirSlptGcmUVbahqqJKEEQrTXdtBe2wGb6rHJWygekYFl0TGoXfoyX21rwPXGo1hmn4qhZAJKa5We9h8KoDRX0NpcQvjRr68V+csfjVxzQwBRhElTVV59380vfmyhvl5K6NMJpxnXa/m9LDfb5x7Hu1ffijemWvn4VW9xwrwpGOXJrNnVzF/e2Ep5c5RUWYwSl8wv46yZJaRYDKxaBdXlfV8yTYuPA+/5XX+aj4NRHmhujqZyz52rv966dcDdAP32h4ElkQDufnNrhNxMG5HOnedOJz9Nj8Focwd5bqWTq5/sIqckXlVYcRm56CQ7FoP+20iaQpaqpzlqmkawYjvBnesBSBCx1S+UoIrc2YXY6ULodCHvqsb46QYErx+Ki1HOOgvXY4/hKOnAbdWD0EVFYdze3QOTm8xM+Nvf+rfq/reipkZXh3z55QFJTunPLmXHzbcCUPHo42TMm4uloCDyvWBPQ7A60DxOVHeXPt5K/7sEZygYtuB8D6B0tuBacR8AhlFTsS6IL17n3lvO2vMuACCts4OpO7YlbMdvMLDp+JPwhAWxJAlLUSHWkhKspSUEO7toW/M5gRibu7W0lNG/uY60gw6KNjTULIewyWAwy+xEK6a+omAHQqxl6KWXBu/q6omlS3sTsJ7omZaUqE8ZGSht7bSnp9GUnklbekav7DRjdhal55+GzeJDbWuIfj7+UEwHH4PaVh3RNnL7TYyfXUJDU9RCcuTRIe75m5dwpQivB7xtZg7r/CzOIpYozbiIau61/wbTL2ew9Zho7Ie1vZXjl97IuE/fp+2ZZdxlGMc7G+NJ32kzCrnyuHFkp0Sn4KF4GRMhkQFNUSAvjziXT394/nkwmQbvpgrHx0L/FpzPdjZzxVO68GWm3ci/fnUkDrMBT0DhvZ1NkfpfcecQFMgz2Jg7yYbYbbIyK16yQ61IqGiKgn/9pygtdTF7CXSoWbz3nozXI6AqGqraLdqpCLi8Ml0eI4LXy3HuN5kWWIsmCGiigCYIqMceh5LiwOP24O7owFcX/zsagkEm7dxOitUCF1ygZyk+9lhvtcVf/jIar7Jixf6tC/efhCSzq7bfdDNN7+j6PClTpzDtbw8jxCi6e1YtJ7hHd/3ZTr4EOa/0gHX5Px3DLqoB8H0lOKG6vbjf/gcAxslzsMw8Me772hdfYs+99wEwsqqC4oa+LQvBJ55g48qPce/c1e8xRZORET++mKILzkc0xLgShuLa6elyGoybqS+fxFA0MMJYuVL/PxRXV08kY7Lui5CFz+2ll3TTQXcmWUgUaUvPoLGwiHazJa6pjHlHMOrUWQR3rY18JuWNZFvKWYwvqMdi0i13nV0iN9+Tw9//mU4wqJOl4hKVBx73Mnla1DKXVrGHzJkzEDSNVziDxazolWZsTw3y06W15JRGLQxjP3mXE5beiLWznW+LJvHrS/5Csz96blNHpHH9KROZXJTW63IM1cvYE88+m7iyhu7ySa6NW27R49CHMkreckvvOT72dlBUjTPv+4SKFt2addtZ0zj5oEJqO738a0sDTn+8lVXtMjEizcyMcSaM3St5TdXIVNpxqC4EQPW48K1dFa03l1aAq8lNw6pvcFdUD/4kkoA5I50pp56CZeKEeAY3kH/u1lsHZ+H9b0MS2VUhl4tvf3ARvjqdjGbNn8+4W25E6pZzCOxci3e1HrRvnHg4llmn9NnW9x3DBGcAfF8JjtJSi+u1vwFgGDkZ61Hnxn3f8OZb7LxVDxzI6Ghn8s7tfTe2ciXBGTOofOwJOtevx1tZheqPVy1On3U4o6+7JnGMzmBnp0QEZSCrRqI2EsXkhAfY116L1oBJBs8/r8+A+yNPeKCgkDASEbLiYl1Nddmy+M8zMvRtb7gB1549VD7xJK2roum8aYcewtifn4//63dA0SfJTVV53PLWmTzyoJOSoigRqa6T+ctfs/jHC+n4/SIms8bv/+Tn3B9Gt7G/9gpZV1zGyMDOiER/GCarwiX31FI0Tr9HTM5OjnvgViZ++G8A/jlzEffPvwilW2Qy1WLg6pPGs3BG78ri4Z+rtlYXq21p2bdLn5UVb6mJJRfXXadnS/WHou5Y7/3pRYm9HRxj6rlu2ToAppek84+fzuTbui4+2t0cCbo2iiJjsqyMSLNgM8ZbAwxqgOxQK0ZN/61CTTX4N30BoSCaJlD3bTUta3fuv853Q7RYsJWNwlZWhn3sGHKOPw7ZkSDDsj+Ck0yhr/92JGnF6dq0iQ0/uzyS1WqfMJ5Jd9+FKSsL1ePE+eLduvVVELGd+jPk7L5jI7/PGCY4A+D7SnA0VaXrudsh4EMwWXFc8Ju4YDTF5+frs84h0O16mrptC2nOrvhG+iAJmqrib2jEU1mJt7oac0EBGXNm9137ZLD+hb7SSobiZuovqPeVV+BnP0vON3HTTXo7r74KDzzQ+/vwuV97bW/ykQjJZlX1nBCam+Gcc5KKpG3/+hu2XPdrVK8ei5VSVERreTojf1gSld9XBZZ/exj2iVM45wx3XJN1jTL3PpLJW6syuPPPIkefGKDZFSW2Xfe+yMF3XhK3j2xQuejOOsoO0o/Z2Sxz5JLrOKH+ZbyyiT+ceg0fjI/qphw6KpM7zplOlqO3DEGyBrdkCjL3ty9EL9u11+qiuH1te/PNB8bAoN8OGkf8eg1ba/UMtwd+eAjtisK2pqhLKstqYGZxGhZDzD2jaRi1AHbFjV11IQJKezPBqp0oDXphxo7yZuq/3E2gyxvdT5JIO3gGWTYrhkcf1eNkNA1Br6aqu6MEAU0UUbtfC4CQlob40EMIRiOiyYRlRDHm/HwEcYBYkL7kFe69VyfoZ5/dt67VdwmLBbzegbcbKpJJNADa1nzOthv+gOLRn1VzUREznnoC2eHAt24l/m8/BEBMy8F++uUI0v9e+OwwwRkA31eCA+D+4DlClXpsjf30K5Ay46MpG/79Bjv/dDsAFq+XyTu3YQlbZpK1MiSDZC04yaRVDtbNtGSJHveSaOUI8OGHOmlyOvtvRxT7T0uNJWWKogc7J1MfJ8nBDhhSJlnXpk1sWnINSrdWyE7nWB7ovIVnfrGCkTnRGhsun5EVWxcwYW4xRx0RT3QQJcT0QkSLA7c/RH33JBnqcHPwQbl43EL34TXOv7meyXP1/T2dIn9fUsR9lRdxqvE1fn7un9hUOD7S7CXzy/j5MWMT1qIaTMhW+NJD3169wWTwL18Ol18ez3vDx/D7908sUCLYS1ooO1/XrRqXn8IJhxfTGCNsODbLyuRcB6IgYFL9mFUfZtWPSfMjoqEpCqGGSkKVO1Gd+m/rbuig7svdeBq7Fy+iSNohB5N99FFkzT8SQ1ra0Px/H3ygX6xkxYa+C6Vh0FW5fX3rJiWFZ5/VC4n1JyTaE9nZEApBe/vA24bT6ZKAe/ceNl9zLf6GRgAy5h7BpL/8GTQV1+uPoLbqoQWmaUdiPuTY/pr6XmKY4AyA7zPB8W/9At/nuty/YeRkLAvOibOyaIrC2h/8CM+evZHPUru6yG1pIttqQVq6dN/JDSTnXiouTk4jJtzeJZfoxfYGQna2rrV/9dUDl5LeFyxfHp1dIXmr1SAGu6Qnoh6kybllK5suuphQt0vow9ajuavqD/z8mM+5+qRPSLFErTK7GzKxzDqJcdMsaN540idmFCFaU2ns8kZiQf7xiIE7btRjA0qnePnZ/fo19nsFnrimiOrtZlYuXf9/7J13nBTl/cffM7P9eu/H0XtHEAQF7A0QO/YaNRq7JuaXWGKJ0dhbNEaNBQti7w1EEQHpvXO99+07M78/Zuvd7t3egSJm3q8XL253Z2efnZl9ns98K1vZxdO12nYJZgP3nTmGw4dEFvcLEE/IVlaWpls7NnFesEATJ+F1ZrKyIh/HIvywxfKk7K9YoGj0mb2a1GHagnXxzAE4JO23ahQFJhSmUJBsQVRlsrwNWFVtEVcVBaWxFl9dBb6qveB14/PINO+opmlbFY5aTdiYMjLInTObvNmzMOd0OO49df9CKCc/QFdxZfujvEI4hxwCq1ZFVk4UBM0CdNllcNRR+7b/b77RTviiRfD119rNSnc3QG+9pcXExWPe68lNDeCqqmLV+Rfh8xcyHXrv3WQdORO5sZr2d5/S+gKKEkln3ay3aogDPefsN4KxZDj408O9uzcEWzYEECSJATfegBBWzK4lOZlt/QawrN8Atu/Yhbe1g9uqN0iSNvl1xVlnxV/187334hM3oK1sp5/eeXJtaNh/4ga09NbwCTfe3OOe5CjH2+2xw3ZJdbWM2rQBxactmEdmfMVxaR/w8CdHMO6263n260n4ZO1nPyC3gYLdL+P6fCFY0xEsoQlTaSxHaW8kzRa6Xi6+wssRM7V4D2d7aOrYucpG+VYLRUUw7Zox/GANfc/nLp0UU9xAfNn4dXVa5hNo8daLFmni5vrrI8VMZmb8+jH8sAVSuQOBx4HPmDJFW8tjeWMh1P0inHjmXluhdvdvNUm4DX6rGHB433QKki2YFDf5nmosihNf5R5ca77D8c1CXD99g3fvNlrLm9j75QY2vbyEiu+24qhtJWXcOIbeezcT33+Hkssv7SxuAl+2u99nRzpaNgLVuhcu7Lzt/iivEE51tSY4Hn4Yrr5a+9/lgtdf105adycoFoKg3WjV1WmC7Kij4N57uxc3N9+sffc//zl049TV/gPW4wCBDq/z52v/dyh5bcnLY9D//Sn4uPQ/L6IqClJ6Lqbhk7UnFRnvjrVxf9X/ZXSB8xtBtCVhOzwUtOf68RN8NaUR26SOH8fEdxbQ9/dXYu0T6igtO5xUvb2Qn+adS+P3S/d9MIEAh1g8+GDnyTHaDz9Qg+XXRlmZNpEHCNTeiTXRxprsIPaE11vRVFVFosMBu0MC8uriRxmSsIn6tkRuee1kDr/rKpZu6xN83eeox77gMbyfvY9gDa3OSnMVBmcTmQmhmJn7H3eTma1Qu8eEo1WbPoqHu1BVlUceAY8ss6G8GYA+mQkMK0jp8qvGq+POOEOzpsybp/0fS8dGC5mKRsfDtnChts6Ff0b//iHB1PHUBh5H82R2t0Yak1yYkjWrTF6aFdlvSOmXbiPNakRSfeR46zAg49m8Evf6H5BrylB8Cg07Gtj06nfsfm8ZzbtqUWWF1ImHMOY//2b000+QdeRMREM38Rlz52oKsYtCnl0SsPxcd13nnhQ9bcPeHWVl8OOP2mc9/rj2f+AmLVysdSVyYp28s87SYtziEWRZWZry/cc/Qp/97LPRPzdWP45oF1lJSae5MOPww0kapjVVte/YQcO32lxjGhzq5+bZvor/IedLr9EFzm8IY8kwTCP9i6iq4Pj6deTWSMuFOTOTovPPY8Ib8xnz73+RO3sWor+xm6eung033MSeZ57dt4HIsraKdUX45Bjrh3/PPb/eQmAdJ/LLLotdxwaiNx/qasLrrWjyr9zTmr7n4+oTATCKPu4Y8BfMoraobqrI5cR/XMwfn5uB0OxfjQ0SblcN9vf+A1JI0CgtNSR6mrH5A10zslQefdZFSirs3ahdN4mpMjklWvzIutImfP4Ve3zfULG/WF91e9fVCILEEx4ROPyS1PV6V1gYedgCISMdL7WKCk2L33RTZy2Qnh7dehM+jmgIAtjym4OPzVatxIJRFBiWnQiqSpa3HgkFz+7N+Mp24K1ponTRJjY8/xXlX6/F59COdfKokYx66glGPf4oycOHxf7QAOEKMz0ddu3SUpl7Q6DYZrjQh55ZKeOlK9HUnVjLyNC+aziFhZpYmT+/65OVnKzF6HzzjTaGjjV75s7V9pMV2SeMwsLoZStiXWQdrGGCIFB8ycXBx3uf/w+qqiKlZiFla/WnlKYalIZKdLpGFzi/MSwTjkLyF4NSHa3Y338G7+6NnbYTBIHkkSMZdNsfmfDGa6QdOin4WumLL+Es3YeaGfFWIl6ypOsf/q+5RkZNjbZgBFbuWGONNtlB9xPee+/FvjvtSjT5hZEkqEwsW8nG9uEA5JmrGJuk1cYRUACBo398kqQ/P4P5k6Xg08SmqnpxfP0WmBJC+2yvI8PbSEO99rkTp8i8/7UdR01ICM29qZYbb1YobXAEn+uTmdDtV73jDm0N6o2XIRay3PW65XRqhzewbazmm4HnXn9di0P95hstjOrOOzVrUTztkTIzIx8XFsL5vw+5gjNStFilodmJmA0iaXIzFtWDr2ov3m1r8GwtY9P7a2jaVo0qax+YMn4cIx56kNHPPkPq+HFdDyAgaq6/XhMfHU1UI0ZoVXcDOfHBgXXhfgmno/joTpj3hu5E09y5musqGo2N2sm6807t5H3zjRb/l5nZ/c1Ta6t24qdPj+5SX7gwup/0n/+M/L3Hc5F1sIalHzaFxCFakL5923ace/cCYBw4NriNtzzOu4P/YXSB8xtDECWt42yq5n9X3U4cX8/HsegtVHf0dEhLTg4jHnmIwvO0aseoKuWvv9F5w278x0HiNVNXVHT/w/+1cv31kJOj1fKINVHeeac2mXYUN/FOeLNnR787jSWaIMJsP5f36FMXEqppRi3uo5ByFnAac3kHwe3B8vY3JP71X0i7KrQhuOw4vlmAYA25lyR3CxmuOtq0rGbyClSuv8KAvVGb+IuHuTh0Xg2u6pCL68cd9XF91Y5/7w8Su4i/bGwM3TTHq8WXLg3F6Tz3XPzjeOSRkDAKrK15YS0XrGbNnZTvT51PlLWsNPcmrbrx9uV7tUGoKsUV5UysrWL04492XaohQLjZ7JFHOkdfB8Q0aGWXwwf6RpTffzQ6io943UbxIkndl3aQ5dhZjIHeHf/+t3byAmIl3jnqsceiz3OxVHtDg+b2Cnc79eSGz48gCGQcETIztvsLrxoyQ0JUafsVpNn/ytEFzm8Q0ZpI4kmXYSgZHnzOu3MtbQsfx1uxI+p7BEGg+ILzg+6qmg8/wtvSEtogTv8xEL+Zuq5u/7igOt4m/1J0FbgcmFSj0ZMJb+7czotPNNEUTpjZfoK8Mvj0lbb/8A3T2U1f5vJOxFuk2iYSHnkdsVxLUVXbm3EsWoiQlA3+mkolWQ4GGSuoLdcm/OxMkemD0vC6tYVs9Mx22lWFvFTNKrF8VwOff+3t9qsG1oT9SXsXzazDNWRFRXz7C1h8ehpDW1Cgralnnx1aW1scoXRwk1E7tjajhKT6kFBQ2prB58XT4sDn1LbtV7qXkooyLHv2dHYLRSPWAhxO+IGAyIF2F8DbVVzZvsb4hCPLmjCJNs8EiPf3tGhR6Ll456iGhs7HuzvVrqpwxRVaK/lFi+K/yDqIrsRBg4J/t2/TCjaKySF3W6A8gE5sdIHzG0UwW7HNPAvrEaeBSVtwVEcrjk9fxLXy86gBaoakJHJP1kqBK243Ve+8q73QA/8xEH/8SEffdZdfKMa+br+9Z1WKfylixShAzzOkAmk+4atkd8ydi7prF/WnhITQyLMHM53FwcaYHREcLhIefh3RoIlcpbkWx9dvICRl0eK0AWCWZCZkVSLbtYW3T66B8XkpqP5dtqY0Mnmgdl59ssrSHfF1sP7ii7g2228ETk88KeWgrVU9CYqG2Ot/syNkwTEbJawGEVEUMPorEsut2p15uz/tO7WlmYKasA/ubhBdLcAdiXWddmWJ6cpFGiBcmF933b7fhEQLaA5YlN9+O759hAuleE88dD7e8ab+nXuudiMYT40s6CS6wgVOoHWOYLIgmPy/T13gdIsucH7DCIKAacAYkk65GkN+/+Dz7rXf4v7py6jvKTjrjOAEVv3Bh6g+X4/9x3FPjv47PBVosyVQnpvHxgGD2NJvAE1JyQQ/8c47Y98NPv+85ifvKUVFWspnx9iD/U20xSjeu8dAnE8vsO/azdqrrqF+/Ybgc4aRI7sVnmJyGgmzL0fw19hQGquxf/hvbPZynv1vMrIMIip9DTUYfFpNnUH5FvpnaAJIRcVgDfUmW1NRG9d4D1RR26ys+HR2oLN4T2JoY63/XjkkMEVRi4hSVRWlw3SseLT6QzanM7JDd3eD6E2qdrTrNJYlpisXaTiSpJ3YRx+Nv7tpNMJFWLSYoieeiG8/Ad/kggVaqYd4iZKp2CO6++4xrGHGlJC71xdmkhQs/t+aZx+LHP4PoAuc/wHExFRsx12AZeLx4J8q3WsX4970Y6dtrQUFpB6ipSO6yito+e/LPfYfA91OjspJJ1FZV8+GUWNYOu4QVo8Yxa7iEhrSM6jNzGL90OH8NHI0lYOH4Lvu+thBhPFGqhYWahVZw908//iHJo5+ThdXtMUo3kDM66+P7QaMgeJ2s+eZZ1l13gW0rlsXfD53zmzyzzg9LuEppmaRcMLFiElpAKjOdjw/fca8nGf5/s3N7NqpIqKSL9diUbS4rlG5icFMK4wiVn/fpN32OhBiWxIEoet4mZ+b3Fw44oj4tq2qiu/USZKWXBNr/U80h9K4fT4Ft0/B4ZXxiGY8ghFDdoHWdygvFYDWxCRN7HflFuo40J4SSzR15SLtLiavJ5akeHj33a5jiuLlqqviF4BdZCrGTXffX1W1FhYd1LCzLDRGa1FRaHOfZj0VjJ1bnvQWVVGQm+tQnF34dg9C/vcaWvwa6a7r7n5AEETMIw8DSQpWPHb98CGiLVErEhhG7kkn0rxcC3Ks/PobUuP5gFh3gLNnd/puDUt/YOdZ83BVVEKHbtjhOKw2dlht7D5pFrkNdZSIIlLH1JVAEGH439E6cj/6qNYaoiNLl+7b3WUsBEETd4GU+fDzGrBwnXZa970FAm7AOO6YPQ0NrPv9H3Ds3h18zlJYyMA/3kKaX7QGhWe0PkFhPcGk1GwSZl2B89uF+Mq2al8JlfHpa2DXOuzGKSQUF5HjraNJSqXVkMz4gmSW7GlCFAWyUi2U1toxWL3Y8ptxVKR1eah6iiBoN+HHH9/7tkbp6XDBBfGHSOTlRZ66WLz+uvZ6rJ91oiU07Xp8ChYzNDi8JJgMtIsJpBu9SBk5WFUFyWKknUQa09LJaG6KNAvF+oCeLMCB3hVdiaaAizScWL2mwisc7++ifz0tUBgNVe2ZMDrzTO37hx/r7Gztu/amEW/HDrABrr9eqzsQ9ht3+DOnAGwlobpVAcvNvggcVZGRG6qRq/fgq9qFr3oPeDWLrGBJQEzLRkrLQUrLwVA4EDExtdefdSDRBc6BJp6JYj9iHnYoqr0V97pvARXHogUkzslGSg3Z6TNnTMeYloa3qYn6nbtwWCzYuuv5EmtSDZscVVWl/OVX2P3k0xGbGG1WUhoaSK2pJqWtFYfFSmVuHi1JmolWdrupSEymadhIhu3Y2nksgUjVO+/UUly6WLg7sb8Lk0FItDidkaXkw89rLKHRkYBoC2RVxRC+qqKw+c9/CYobQZIoPP9cii+8EMnSYSKMITw77lu0JJBwzHnIrQ14t6/Gs2MNanuzlh+9+TvcrmGYB40mXW7W3pCYTN80K7ubnORmJVJaq2UEpQ6tiilwLrwQXngh9tfv6rD8859aqZLeurd68r6MjFDtyUAdy4ce6uyZveGGUFharJ91oiXkwvN4tR1UtLooTrVilxJIk5sx5JUg11eRXJxB07ZqdvTtT9J112AKXMddfcDs2fEtwPHE0kQjVq+pjmL85/ht/dI8/LD2Pd94o3P7l2g3VN0xb170apRRbmTaNm8OvmztowkcVfaBT4vVEkw9FziqquLZtAzXqq8ghotLddmRq3YjV/lvlCQDlnFHYhoxBUHcvzfePzd6L6oDSayJYn82v4yCqqo4Fy/Au3MtAFJmAQknXx5x8e59/gX2Pqvlw6a6XYxctwYhViG7ODplq4rCrsefoOK114PPpYwfR7+rryJx6FAERdEW3PfeCwYN2602KnNyqMnIQvHvW5J9DN65g8zmKAF2r72m3c73xBq2aNH+bziUkRE9wyraed1PzTrL57/Orke0idOUnc3IR/5JQv/+UbftLaqqIFfvxb3+u6BVxyAlYj7sKFSrlUpjLg7VwMdb62hzeXnv292oKiiywLZ/T8Pd2NkXdfXV8YdQRCMxseuMqf1NZqZm9Xnoodg/25tu0goExnr9xid28UX5FgCmjMihOE+biw7rk0pekoUMbwOJchvO7z7CU9vApvlLQVVJHjWKUU8+hvjhh93PGxC9G2k44U1j46UnTWCXLPn5mnntK7EsKfES3ta+J21guvrcsGPntdtZfsppWuNcSWLSO29jzsnGV7kT+yfaHYFxwBhsR3RhSuyA4nLgXLIQX+mWzh9tSUDK6QNeN3JTDWoUV5WYkYdt6ilImflxf+b+RO9FdTDRi+JP+wtBELBOnY2YosWeyPUVuNcsitim8JyzMedpDYCazRbq09J7l00BKF4vW++6O0LclFx5BaOefJykYcO0eh6SpImRwOQMJDgdDNyzm3Eb12FzaAXkZMnApkFD2F1YRKcjF/Ah9CTjKBBUsT+JZe2Kdl4lSaunEw8x7ogdu/ew+6lngo+H3Hn7fhc3oLk5DXl9sR01D9GUBIBPbkf5738RgETFjlESyU40YTEZGFqiWW1ESaXgmI3Q+Yyxr8P8JcUNaGvTP//Z9c82mvgJf33B86nB5xxhOn1VRSuS4sWoehEEEWO/4RgTzZiHF7OmtYXtK1aw7d6/o+5LDaWsLO31eMoNRKMnJQ7iCVhKTobbbtPi4956q2eZlb1FkjSVui8ErDdWqzb2V17Rxt5V5mhWVteiKuzYlb82XxM3QO5JJwT7innLtgU3NxQOirqbaPiqdtP+zhMR4sbQZxiWySeTeMo1JM37IwlHzSPh+ItInvdHkub9iYTjL8Y0ZCKBuE2loYr295/BufwzVJ83xif9utAFzoGiF8Wf9ieCwaSlkPtrnLjXLEZuCC2gksVC/+uvCz7eOWYcvo5CII5sCsXrZf0111L7yafaE6LIwNv+SPGF53cqVKYuXoRzyhCc845FSQnd7dtcLsZuWk9WQ2hyKMsvZMPgociiGH/wZTQkKXYAczQuuKBz6feO2O2xX4t2XvehWacqy2y962+oHi3wsOCsM0kdN7bTdvsTQZSwClnBUr7OPCts2YZN1kRotr931bC+6Qhe7e+kvg2kDg1dX4FTdtVV8RfNjUVS0v4tnLsvqGrX9ySqCnvXpaAq2oB/WO2gdL2WFeP0KWxv9mJB24EjMZN/f7eeM9//lFu2b2HeutUce/993N/WRvSSnXRfQ6mqSrve4y030JGeljiI1cIkQGur1uTywgu1GJSe/BZ7iyxrKnRfiy+pqjaHSxKccw4847/JiHUjeM45ce3Ws3MXlW+8pb3VYKD4oguDr/kCAkcQMBQOiGt/7nVLsH/yH1SHVnZAsNiwHX0eCUfNwzxsElJ6Tqe5WLQmYMjvh/WwWSScfDlimv8mTFXwrF9C+/vPoDi7mOd+JegC50DRy27R+xNDViHmMdO1B6qCa/U3Ea9nHD6N9ClaB1tPezvV99zXs4JzQO1nn9Oyeg2gxYUM+/s95M2eFXVbV9laPMdMwjNzAm1/+x2ew0YH7/klRWHIzu3027snOGE2paRSlu+/Q+1pHEF4Bki8gZCZmXDeefsnpzn8vE6b1v0qn5ERVcC1bdlK2ybNV2/tU0zJlVfs+9jiwFDYD9OSNdoDsxF50waMyBgVD1mJWjNEgyRSZAuZszPGRjZ/feQR+PDDfW/0fswx2v+/FpHTHapPwlWrWcAsmW28cX8Gbqc2+LXVbfhsmoD+64NPcPNLH5Bqs3DdiVO5cvAAEiQDd7e383hOLnXpGXiFGFN4tBpK06Zpwqe7SuRdEa8Y37696xYmHQnEoPSm5ENv+f57yN8P7pbAse4urX727G535ZMkNn7yKbLfYp07+2Qs/mPu2fYTSosWIC1lFyOabd3uz1u2DdeKz4JzppTXl8Q5V2MsHhzvt8OQXUTi7CsxjzsS/GEMSlMNrh8/jnsfBwo9yPhAsQ937fsT8+jD8Wxdgepow7d3M3JzXTDgWBAE+v3hGhqX/gBA1QcfUPDG/O5LxIfhDBMPfa/5PZkxcnLl+ko8vjDhYLPgvOgkvBOHYf3vx4gNLQhAYU0VCQ47GwYPRRVFynPzybvnbsw9MbVHC9CMh3PPhdr46rp0S7xdJrtB9fmCf6dPntw5oPjnYto05K9eCT4Unf4MDMAQdn0kCakoPhHRoCCZQ2O94w5tvi8p2fehXHmlFrvZk1Oang5NTQeuI4izNglrbiuCCA6fhzVfJjHp5FZkRaXUa2H959/w4hvvcOj40Tx/+RzyRS8KIse+uojfLV3Oky3N9Bk8lPx+A0htbSW9pQmD/1oQVBWhtg718y+Q7XZ87e34Vv6E76uv8DmdKKKEIonIFitKcTFqUiKmzEzMublY8nIx5+ZiLSggaegQhI6dyQNup1gBzIKgHdw77ujZwQ24fJ57Tnv/z10YKWB9ufPOfe95Fz5HdxXAL8sxj50KVGdls7e4BI8/PdyYlkbxxRcB4Ksrx/n9+8HtTcMO7f4retw4v38v+Ng8ahrm8UcjROkSK7vctG5YjzEllYQB/TvN8YJkwDJ2BsaS4dg/eg7V7cS7cy2+IYdg8Pc+/DWiC5wDRTwTRXfpm/sBQTJgHj5FU/mouNd/h23aKcHXbX1LSBk7hpbVa/DUN+CqrMTagxLsvuaW4N8pY6O7TlRVwbn0/aiv+Yb3o+3Oy7Es+Arz4tVQUEDaiy+S9+YCKtesRRFFSiurGRjvgGIFdsdDHHdgcXPHHVqjw7lztcmwOzNGoGR8hyBjY1pq8G9vtMDrGPgqdyHXV2h3ZAYDgmQAyYhoS0LK6dOtiPU1VCKXaDFaYlUDQoFWasAnGPAqIcuAs12MaiceOHD/ZBFnZIS8LbNnxx+vfe212inY3wiC5mnpzjjibgi5YC0Z7WxemsKkkzUXwpbqFp5+eQFGo4E/Xn0ZAw6fjP2HzxCd7Uw5ZwYzdu3l1b2lNHm95JktNKWm0pSaGvkBYfFuoCUWCAmJkNAh0LumBmpqsO/obDmxFhVRfOnFZB99FELAOtpViYPwx735fQVExy/JwIFa7M9ZZ/XcohVrjo6WVh94Psqxa0xJYVdRCQ5byCJjSE5m1BOPYc7MRHG24/jyNfD/rkxDJ2HqN7Lb4blWfYlq1+ZfQ35/zBOO6fS7ll1uqt99l7L/voynQROV1sxMMk88gayjj+okdqS0bMzjj8bln6+dP3xI4uwrf7XZVbqL6kCxr6XQ9yOmIYeAv6aCd8caFHtrxOtFF17AoL/8mUM/er9H4gbA29wc/NuYmhJ9m+2rkeu0iU0ULSRf8Xdsj76B0Ogfh8WE69zj8UwYGqxnU/z3e5H8E0LV+x9EWIpi0tvCY+ExPtOm7b9AyECw8T64K01pofTrwATVHc5lH2P/5D+4VnyG68ePcX3/Ps5vF+L85g3sH/2b9rcfxbXyc3x15VFbeqiKjHvVV8HH5p+2w4SxyIgogohHDr3HZQ9NMeF7ysvbP97XZ58N/UQkCa65Jr4uIX/+s5b5uz9/XoHPvOEG7e+uNKKrIdSt3ZxhZ+dqKx6X9oZlazey+LvvmTF1MtOnTEQwmkiYfCxCYioiMjOO1WoatQtukmyGTh8kSCIGmwlzqg1LZiKJ+Wmk9MlELErDV5hKar9skoozSMhLxZqZiDnJEnWMzrIytt5+Jz+dez6tGzeFXujKFRNotX6wkJenCY7583v+XlXt+Rw9dy6+V1+lqW8/SvPyWTNkOBsGD4sQN5nTpzP2xedJGNAfxd6C4/OXg/EzUk4fLJOO7/Zj5IYqPBuXaQ8kI9bDZncSN+66On6adw47H340Yu5w1tdT9tJ/WXXu+Wy47gZklzvifabBExAzNKuV0liNZ+tP8X//XxjdgnMgibPo2s+NYLJgHjoR97oloMh4d63DPHJq8PX0Qyf1eJ8ej4eqqirWVVbgdNixSRItGzZiyc3ttK2vclfwb9P4IxDeeBPjtddi+Ou/aP+/i1FytfgU+ZrLgsfElJZG7pxZWmaWLNO2cRPW7rKhemMyiCY2n3oKTj+9Z/vpSHgDwJqa+N4TxV0pJSYiJSQg2+00/7SKltVrSBk7JuYu5NYGPBuXdvkxSku91s5j7bcICckYckpQFRm8blSvG8XZjurvgyMkpSI+8yx4nbRJmnWgMazXUqvDhZimBSOrXinipjfe+Pkzz9RussNrPIoi3Hhj559IdwYGCJ3KrKz9m6QY/rM99NCuXWayK1QLx2Bz4/OI7FxtY+hkO6uXfIUsy5xyxjxsyWngdSEYTZgmzMS7dgl7678HAYzjBtDviHEoXh+0ORHMRrBZOgmryqZW/v7+EtaX1dDQrsV2zBjWl7Mmj+LQAYUIgoAgWSB/EG41EXdDCw1LvqNl1WoAHLt2s+bSyyk6dx59Lr0E0WyO7Yp58839d0B7SnKyFrQcDx2tL6efrvW06omf8847tWOwaFG3ZSla1q6j+r33ad24EeeevZCZ3WmbpGFD6feHa4K/X2/pVpzfvo3q1s6ZYEvCNvMszdraDd49oaxF89jpEU06A5S9+F+t2KqfjKZGfJKk1R/zX0RNy36k9D8v0PeqUGyfIIpYJ5+M/cNn/Z+1AfPQid2O6UCgC5wDTZxF135ujIPGawIH8O7eECFw4kFVVerr6/n+++958803Wb58ObW1tcheL263G0VVOfbii3noow8ZOjLSvGocMCZYk8e94XtMp10Hxx2D+5PXUZr3BLcTR42LeJ+3MeSSscST6t0bk0E0sXnaadqq+8YbPd9fR+Ipw9uFu1IQBArnnc3e5/4NisLmv9zO+FdewtjRZeHHu2Nt8G9D4UCMA8aC7EOVfageJ76ybcg1pQQmR9XeinfXuqj7EiwJWCcfD14nXsFAs5SCqqrsbHQEt6nzVROoVd28WRNoAYERbzjHm292fl1RtFozhx7aWeTEe9+wv+L3A2MMT0zq+LPOztYS8Corte9iK2gOvt9Zk4wgQGuFBVVtZ+uPizEYjRw2dSpSVh/kulLwOhGNRiwTZrL6gRcBGJTnbzEiSYjpSQgWG4ItCdGWhJigPf70h5+47N4naHc4OXLyIYzNSqe1uYXFG7bwxcY9XDxzIjcdNxFVdkHZOkwIWPP7kn3jZTgaPex64mnat24FRaHsv6/Q+MOPDL33b9iKi6O7Yn7mmMEu6Ym4gc7Wl8BJu+MOuPvu7vfT1NS5JlCHIq0ta9ay99/P07xiZczdWIuL6XPpxWQdfRSCKKLKPlw/fYln/XehISekkHD0uYi2pLi+ouJoC/5tjJJO7mu3U/OxFiQsyjJjNm8g0R/Y7DEaqUvPYFdRHy3O8dXXKDrvHAxJoc825BSD0QReD6o9zuN+ANAFzq+BWD7bX3IIKZmI6bkojdXIdeUo7c09Ks/9wvPP8+Q//sHqHTsAyM/PZ/r06fTv35/KL75ke2kpn5WVcsacObzwxhtMmDAh+F5j4UAMfYbi27sZ1dlO62t/RzAYUd2hZFjBbMPYd0TEZ7Zu0BpJimYTiYNiROGEl1iP11Ly8MNabZquxObs2ftH4MQjbqBLU3jxRRdo1ptVq/DU1bH1zr8x/J8PdAomVFUV7441gR1jnToHMaGD23D0ESjOdnx7N+Pduxlf5c6g7z+IZETKKcQycjKgIisK9WatF1hVqwuHvzrv5p8ErH20rA9vuxmhoog33ggJjP0RzhGrwHM89w37ay1ubAx14Ain48/6scdC3zWxKHTeHeXa3fXcY83ssLfR1lhPycAhZGVlIYgGpKw+KA3liO52HE4na8tqMJvMHHH5dRgNBlAVEMVgHISiKIiiyFdLfuDmh54jLTWVZ/5xF7OP01qVNDa1sGnbDp577S12KwK2mafiqSpF3rsF1d6KXLkLZ+UupLQcRj54F1Uff83e555H9fmwb9/O6gsuZuCfbiX7mKM7H4zuVOuvga4s5JKktXSJR+D4C5JG4M8Ga3nkEfZu29FJ2AgGA4mDBpI0bBhJw4eRNHQo1hIt5k1VVXw1e3H9+EnQZQ9gKB6C9fC5cWVNBQgv1BdonBtOzSefIDu0+TWnoS4obgBMXi8FNdXYrTaqs3NQfT7atm4lLWzOBhBtySgt9SgOXeDoHAQYS4bjbqwGwLt7o9a7qhvcbjeXHXssryxeTF/gZuBkYJSqknTBBXhPPhn7BZtY/7ur+KqhnkfLS3nyySd5oUN9fuuk42kr3w6yL2hRAECUMA2dhHnMdERL6AfuaWzEVa41EUocPATRaKQT0bKlAtkMsZAkbQLsqtkQ/HJ3qnG4KwVJYshdt7PqvAvwNjXTuPQHqj/8iLxZJ0dsJ9eVo7RpC6uU17ezuPEjWhMxDTkE05BDUD1uFEcrgtGEYDBpd22yj9qtq9i6aTN9CguwJxfjFc0oisL2htBE2ehoDv5du6wv9dUSN9ygHeLA1wncMD/6aKTWKyyESy/tOsElvORLrJjOru4b9udaHI81KGhZuk4loVCzPnrtJrJsCTzyLBxzrJl73nPg83pIzspF9l+ngiihphUgtFaz9NullFVWc/ThUzCZzciyjGSIvPZFv7B96F8vsnNvGQuee5RZx86kpdWD2WQgPS2FqZPGM3XSeNxuD4LBiLGgL6bCfng2LA+6jJWmGhwf/4eCWVeQMXWq1gpkzx5kh4Mtf7kdd20dRefOi/ySPemz9kszZ47WWfWqq8Bkir1dPBdGjHlEVVX2FBZT9mpkkLelIJ/iiy8i++ijNBdf+Hu8btw71+LZ9CNKU9hNmChhOeRYTMMn9yhzFcIFjhDsPh5O9XsfBP/OjlF80OYK3WDat27rJHAEWxK01GtWHI8LwRQ9lutAogcZ6wQx9g013fSWdS7nHY3XbryRVxYvZgbwH+AOYCqQXFWFcPrpmD78kNTRo0kY0J/JqWlMTkrms48+6rQfMSkd86jDw54RMA4YQ9Jp12E99IQIcQPQsCRkvk0ZM6rzwALZUh396d0FXciy5jbqroN3d1VaBSFU26a3BVoefrjrWkNhtXzMGzcy+P/+HHyp/quvO28e6C0DcWVhgNbvRkrNQrAkoPrcbFv5PReddzaHHDuXORf+nuEzZvPHP/+FttYWKtvc1Nm1goN1FSJ727SJ0+cw0rimGAiVO1m4UPsXKJUSEDfp6Vpow+7dWoJLPFRUdN3UOhbxxPnH29ElXr07dy58uawdyaIJ+PF909i9W2DuXK3LeG5WBs72Fhobm/Co2vQcsMiIaQW89uGXAJw592QwmML+mREsSZQ1pPH7P+Vx6EktfL54KZDD2VflYiy6nczhD5E86FP6TXKx+bJnER98AsvnX4NPRhRFBEHAPHYaCcdfjJjuj5WTvTiXvoetf1/GvvBvso8/Lvhddj/+BFXvvkcnYgUhZ2R0rveUnAwTf6H4jXff1VLs+vfv+vcdz4UR4yIrzS8M1eYCLIUFDPrLn5nw5uvknnRiUNyoqopcX4lz2Ue0vv4Aru/fjxA3YlIaCSddjnnElB6LGwCColdFae1sJVY8ocDh7SX9cIUJPhWoTc9gb37I7d9RlAGoDr+IEoRgwdhfG7/OUekcEMSULMRkf0Bv9d4IF1E07K2tLHj+efKB94EjIBhvEV42XlAUii++CJskoagqnrY2aqqrO+3PPOYIrIfNxjz6CBLnXIXtiNMQk9I6bQdQ92UoiydzxvTIF3ubLRVOd20y4pkEn31WC1zsONF3Vwk5QE5O7FisgDqYMUMrAjNjBmlnn4UpUTNHt6xeg+KOzH6QG0IBhVJ2UdTdqqqqxeO4HSj2ZuSWGrw1u3CVbuDRf97PlGNnseSHFUweP4YZRx6FwWTmjZdf4NWXXmB1eWgi/Wy+GdGoRQW3bM1F8Rr8+9dev/xyOPXUzvqzqUmz6Lz3Xvyi4frrIw4DJSXd69MAXSUEvf221ng6MzP2+3tTRHtjRXPw7yPHp0Wc4iFZiWT3GUBt6U4212kLiCiKiKLIqlWrWPj+R4wYMYJZ8y7BkDsQU/5gDLkDMeQOQMos5vv12fz3nRRKm7U0XlWtQTTehjVpJybLclT1Vsoqz+fVT9ciLvwIKacvUvEIMPl/uT4PQkICiSf/LjQX1JTi3bYKyWZj8O1/oc/vLguOd/vf/0HtZ59HP7Adqyj7U9L55hvt95WZqcXNLF8e/8HrSEAR94Tycu3ie+ut2Nt0dWFcd13Ut5Tl5rG3MPS76jvtMA55Y74mbAyGoKhxrfyc9rcepv29p/Bs/CGi6aWUXYT18FNJnPsHDFk9y1gNx1g8NPi3d9f6Tq8PueOvwTg9h83GmmEjabfZaLMlsHbocLYMGITsr4FkTEsj6+hId6Tc2hBRdFAwdmERO4DoAkcniCAIGIr8FS5VBW9518XoDMuWsd7lYgpgQ1P+4ZJCVVXksjKUxYvJmDEdR3ERa9pa6W820+IvHhjx+aKEacghWCYcjZQRe3XzNDbSvFJLTbQU5JM4dGjkBvtaYCXeNhndVS4NdA3vONHHm2kSa4WPYZ0SKipI2+13L7jdtKyPnNjker/AkYyIqZGp7qrsRW4oR67cgly1FbluN0pTBWpbPYLXybrN23jk2ZcYPKAvT//jLh587An++tDT3P/Y0+QXFvHZxx/jViVUVWXLMhs1taFMKkdVauRnqbEzicPbKU2Z0n0rI9BESDjhVqJ4mDtXK6D78MNa88+HH4YdO7TnTSb417+ip333tprD+rJQcPyootSI1yb2z2XiiWfiaGnipt//juXLl9PY2MiKFSs4x1/q/+abbybTr7oCd/ceWWF5WRPN+VX837u7sCR8hiBA/oChnPWXP/LX997kzo8/5IJ7/4XJWs5rOaVU//ADzJ2LIIpIafkEeg6pbfWgyFinhFycrhWfobjsCIJA8UUXUniO3zWlqmy5467oIidaXzhJ0sx1jz66b80uA4giDB7cu6SMs8+O6H3XiWi/3d27o9bDqszOYXdxSfBxv717KDrpxGChRO+ejbQveIT2957CvfbboKsYAMmAcdA4EmdfReLJv8M0cCyCIYrLvQcY+4XiFb271ncq95A0bBhjnn8Wq7/MhMdkYtWI0aweMYrWpJDZMq1fX8Y89y+MKZGmTF/p1tBnFQ/Zp7H+nOgxODoRGIsHB9OIvbvWY+ofxf3jx9zQQDpgB6qBPEDx/5PQpksJoKaGzVu3cs/undR5PFxUUETdE0+RmZ/fqxT02s8+D+YMZx15ZGcTbkVFj/cZlXgDK7qLZu0YDNJFRVOg6yKP3TRpTW1toSZLS0FtXbM26DdX7K2h+Jv03GBAqqqqqI5mlOZqLVg1Bvc+9ixlldW88PzzHHbkMZQ3abE2E6dMxWKxIJttyD4vimLigyeysI0KTYCuus5Bjl0R0JdLl/YunCNQFDdWAHJHooVq/fOfoWSYeLOywuPZu4pP31QRCsrMTbVGvJZmNTH79LOo2bONH959hbmnbyTBYmL79u1YrVb+9re/cd555wW3VxSV9TWtfLe7kTa3DwTwOttoqalENBiYfMp5DJ6ouX4VWWbIodMZe/RJLP9gPstWrGBOQYFWBNBoQUjKRG3T1KLS3oChYADG/qPw7lyH6nbi2fgDljYDQlUVfUcOR54zW3NRKQrb7rmPtEMnYUyJHtcVZH9YV8NpaNCK9PUGWQ6lh8dyA0cL5OoQo9OakMiOPn2DL/cpL6PQZAj+fuX6Shxfv94pil7K64ux7wiMJSM6ueD3FTEhBSmnD3LNXpSWOry71mHqPzpiG2thIWNef5WNF11Ka2Vl5Gs+L/3POpP0m2/qtG/V48KzJWR1C94U/wrRLTg6EUi5JcGoe1/pZuSmmsi+TeFBDnl5nAp8AbwEBAytgTl9O1pczkmPPcaIESP4adMmLp5wCNPTM5DtdjbccBMVb7zZqZBUV6iKQtXb7wQf55wQpehVx1v63hKvj6SnHcz3pchjN9YpOSxzSjSHgv7cYbVvDAX9AVB9HpT6vShNlSFxI4gI5gSEhDTElFyE9CLk9D5YUjJIT0/HmJJBTasLFXC7XHz72UdUlJcx7LCjkQxGVn+eRGOlEcUrhY0j1KKhJ1RVxTaSdVdrMV4jXKxQrY5WoFg384F1MYrHMKarrDgjtJjd894GFCVysZ9UksmJV/6JSx98iaKxh5GW34frb/kjX331Fddcc43/+6l8smw1Nz09n7eWbdHEjZ/26jq8HjMG4wDWfHkUb92fw4dPZvLtG5koChQOHoEgiKzdsBEIWYFEa1gKsv96EEwhASbce3/wCwozZzLgqSfIHKylICtuNy1r1nR9sAHuuWf/ViveH0KpO3d0R8J+v4oosq1v/+DvtrCqkuKqiuDvV1VVnMs/DfWCyizAMmUWSWffSuLxF2MeMnG/i5sApiGHBP92LnkHX1gMXgBjaioj579K5vQjtPGZTPQ76QTGf78kurhRFRyLF6C01Pu/T34na/CvCd2CoxOBIEqYR07DtfwTADwfvY71Tw9Gr/UwezYX5Oayprqa24B/AdOAKmAXsBfNmsOPPzJ69GiuuOIKLjnvPLbeficN3y4BWWbnQ4+w+6lnSD/0UDJmHEHGYVMi6i10pHnlTzjLygBInTAeW9+Szhvta6XhX6JNRm+LPHZjVWpJDt1BBwqGaXdcK7QnJQOmoZNQ2htRWqojFgjBlqKJGikQL6PilRUcHpm8Pv1obHyLhx56mDPPuwiPx82qH3/gv/9+Bp/Px571K9nx0/c4228DMnDWhkza1uxW2nf3/JwE9GU0I1lFhdYarDu6OlzdGMM6WYFiZWXF6v4REEkBb2WAW08ezk+7G2m0e/huax2vLt3DeVNDFoBBWYlM6ZMOTKBkpN8CJ8tsFK1Q2Uaq1cjSvU28/Pi/WDz/WQ477UKOueQGhualc0S/DMzjc3jjriSqqwV2rk4HkgEfRUUG5p4C7U0NqKpCs0v2f1dVS1P2hJVlMNm062a7VuwPlwfTJ5FqUaioIPvDD6gfqN3Bt6xZG7PXXPBA9bTv0403wksv7R93Viy6SsWLhf/3u/f//hqsQpxob6cvCkLYCfeVb0Ou0tzGWuDwZXEV6tsfGPuPxlexQysNIfuwf/EyCcddhKFD/J1kMTP07/fi2LULU1YWxhiR9aoi41z6Ab5SLQFFMFmxzjizd0HQvxC6wNHphGnweFyrvgSfF09zOZa6GiIu4bCZu/jJJ/nXqacyDHgd+ApoQrPiDAFmHH88s66/nqlTp2K1aneDw/5+L7ufeIry17Ty6IrLRf2iRdQvWoRgMJA28RDyTz+NtEMnRdRyUbxeyl99Lfg4b+4p0b9AD9tJRNCbwIp4fRMd6U2Rxy6sSirQ7Bc4oslE4hBt4fFsWQFezUpmHDAGBBWlObTyq6IBKS0ft8GGw+XDK3vx+BQ8shLM4Dl21lxKS0t59835fPTeQmSfD1VVOfzwwzn++OP5etFivvnuK1RlHXAPzupZwYUzbVgVdcv7gho6lxkZWihGLA9dQYF2WOfPDx2W8PVn0aJujq2frnqadheq1V0aOvRcJAFkJpm5+/TRXPWiJjof/WwLIwpTGFsSCj6f1jcdWVVZUdaEooIoSdTZPSza1eD/XB8ZBSWkZOdTmJ/PhZP6UxR0d5kZObIPFRWf8uqr2hjy8gxMmwZrd7Ty9xVLMJgtJPcZFBGboXpCKf6YLLjXfgs+LSvOtHQdgrODpVVVSWkPFZRrXRu9IGTEgeopL76oBUWdf37P39sTvvqqxwVW61JSKfPHqwiiyKDb/oRw5hnBfaiKgmv5Z8HtzROO+cXEDWiWOevUOaguB77ybeD1YP/sJRKOnIchv1+nbRP694+5L6W9Gef37+ELxGUKAraZZyIlZ8R8z68B3UWl0wnBZMHY159GbDXjOv2oyA3CIkHVWbPIfPtt7iooYAvwObAEKC0oYMPbb/P4xx9z9NFHY7VatXgPVUWQJPpdew2jnnyc3NknR1TdVX0+Gpf+wIbrb+Sns8+l6p13tU63Gzex6oKLaFr2IwCmzEwyjjicqAR85L0hPEA4Hnrim4hGT91bMdLTPQYD6wcPxeuvB5Q8ejSiwYDcWINr9Tf+rQRMwyahNFaw+IflLF+9nnqHjJDdnzqvgYpmB00OD+1uHx5Zc1EEaqoMHDyEh596lnc+/oy/3n4Ho0eP5sMPP2TRokXceuutfPj+e7z15pvIchnwL1x1XpzVmtiy5rZSeKxWOj4jQwt5ePZZ/4iieOhUFZxOOOqo2Ic03lP83HOxvQ/70AIsSE9EUjhTBmVx/jTNauOTVX73n+V8sCq0I0EQmNE/k6un9OOYQVkUJEfWGJEkAyeePo+fNm7l+XtvCxM3GsNnzAUE/vHQPLKyvmTw4CpcLjuP3Hsne9atYPjUo0nsM5RGh4d6u5u6Nic+V0jgOD5/Bfe6b4OPTd/EqMSrqoj+Axxe9r8TvQ38b2iA2tqev6+n3H13z363QNnLrwT/7nv1VSTOOzvi96s016I0a2MXU7I6FSr9JRAkA7Yjz0bK81sIPS7sn/wHx3fvorQ3d/t+1efFtfob2hY8GhI3ooT1iNMwFAz4+Qa+n9AtODpRMbvMeN1eMBvxTB+HtHUPphWbQxv4Z27hu+9Q5sxBnD0backShodZIlRRRPb5EAQBSZI6mTJTJ4wndcJ41FtvoWXdOhq+WUz9osW4/RWHHXv2sP3v/2D3U0/ja7eHmhH5BZJoiHH5hhcbC4w1Hu68U+vCGO9dXE99E/uDKIXUWhKT2DxgEJ5ALQtBoOCsM1CcduxfvBy8CzcMHs8TTz/FA48/S31TEyBQ3KcPZ51/CedffmWwmmrgPJkkEbNBxGSQsBklzEaJ3BmH8+A9dzFq1CiOO+64oGg1Go3Mnj2b448/nm+/XYbFsp6KL0bR/5wfESWVjLFlHDbBzKt3DAoe3mgeuvR0bU3rmGUVfkgDRq9DD+06CQa0fceywMQbYtXVdvsikq45ejBbK1v5cWcDHp/CXxaso77dzUWHh+6kbSaJcQWpjCtIpcnpYWN1G3V2D0OzExmclRjVPfDyd7v5vDGP3OkXsv671zjn3HPJy82lvLycxsZGSkYdwtEXXUt6aipNDv+1gUKqomW++WrKkGvLgvszffoDUlX0tLedxX1Q/Cc0dcL4nh2AeGls/GWqI/fwd+up09xmpqxMCuad3el1MTkdTBbwuFDaGlFd9qhVhX9uBIORhKPPxf7lq8j+Io7erSvxbl2JmJqNlF2EIacYKatIi8trbUBpaUBpbcBXvSfYkRy09iy2GWdgyI9t7fk1IajR2gX/RmltbSUlJYWWlhaS463g9b/K/Pl4nrof54UnaY+dbhL/9jxSbVPkdq+9plkfIGShEYRe+2VVWaZhyXeUz3+d1jVrO72eOHgQg/7vNhIHde6v0olo6TGxCMTd7N4dn8CR5c59aPZlfz1l4ULUa6+lXFbYXVgcNIWYEhIY8sD9pIwZhf3TF5Gr9wBaMOBjK/Zw5wOPcMLMwznkkAk4DEm88Ny/qKoo5+Irr+Gam26hT24WFknEbOwsSAHWrl3LuHHjeOedd5g1axayHCoSB3DSSSfx1VdfsXz5TzQ0DOPrzZV8ULYmWD7gT7OGc+ahfYL7C/fuZWfDhRd2fUjT08Fi6VmiXNglGkHgFHaXzNbVKbzrrvjCSr75prPIkmVYtFjhlVWbWN1YCoAowLOXTGJCv96Z/j9YVc5fFoRcRY7qnSTuWYvatJMhQ/KYNO1wqtIGk5ZbSL80K+MKNCtbmq+ZFLlVC4pd8gGq046YmoXVWoThxFP9TRiTkCUDiiCgiCJuk4kyfzE4yWJhwttvYo5VNGjRIs0U1xv+7/9g7Nie37D0hh78br87fAaK203CgP6Mf/XlqNs4l38a7CllHjcTy9iZ+33I8aKqCp7Ny3Gt+BzBmhBsmBsXgohp2CQsY2cimK3db/8z0Jv1W3dR6UQnLw/jd2sx/uCvpWI147hiLqrJ2Gk7gLfffptbb72VrVu39lzchGVpCUuWkDltKmP+9TRjX/wP2ccdgyBJiGYTfX9/JWP/829MGZmsu+Za1l55Na3rN8Teb3jqy//9X9djiDftJkBPfBOxstD2Addhh7Fhzlx2F/UJipuUceMY99brpIwbi3PpB0FxI9iSKM8dwX2PPMnk8WO49/Y/c9Yf/syFV/6BZ195kymHT+c/Tz/O5wvmk2o1YZRiC9Tt27ejqirvvvtu8LnAtqWlpaxZs4aMjAySkxOYPh3uujKfm08aFtz27x9sZNmOUMBouIdOkro/pA0NPa8CEMsCsy/JbKCdxoCrrSuixasHPJtHHSny4s0jqP5OM/crKvzpzTV4fD2/RjaUN3PHwsjaR7bc/nhGn8G6TV+wZMmLpBWdQ1quJkpspsAXU0mUtaKCvtJtqE47GIwkHHsBniFj2TFsBD+OHsfGQUPZ0n8g2/oNYEdJv6C4Aeh/w3WxxQ10X/m7K6ZPj51Ot79vHuKcBxSPJ1hIUzAYO9WZCWAeOin4nT2bl6N6488Y3d8Igoh52KEknfoHLGNnImUWxFGBWMBQMIDEOb/HeuiJB0zc9JaDQuDs2bOHSy65hL59+2K1Wunfvz+33347Ho/nQA/tt8u0aQiFhVhf/RSxSluQlOJcHJfPQQ1UPQsr4frmm2/y4IMP8uWXX8a1eyXgbuoihiVp6BCG3HkHkz/7mEM/+oCi888DSWL7/f+gefkKWlatYs3vrmTPc/9G9cVIRQ6soMOGRX+9I/sjMCOc996L/v3eeqtXokdVVao/+pifzj6XpuX+zChBoPjiixj1xKOYMjLwrP8O7zatECKSAdvMM1m75ifsDicXn30aiQMn4PFqQcLDR4zg0UcfRRRF7r77b9TX1yNJUuj8hH0uwLHHHovNZuPFF1/knXfeQVEU6uvreeeddzjrrLNobGzkjjvuoE+fkJVm3pQSLj7Cn5quwus/7N2nQ9oTJKnrBJx4ajXGYsmS+MTWZZdFrsPRUtNrvhtIe6kWZFzX6mZzZc8bGK7a3YjsTzl3VKZgr0gFwGD1YUp1UFlpYsFnoaq5SWbNxZsgO5BQUGUfnl1a6ricUsLG2+5g5dnnUJmYFKxqG420fn3J6dD3rBNdqcmuyMgImb6i5erPn9/7Vihd0d3FqKpakUGgfcsWdj70MGqU37CYlIahjzb3qM52nEs/2O9D7SliYiqmgWNJnH0lyef9HwknXIJ5wtFaTZ5B47Ecciy2I+eReMo1JF/wVxKOuxApPedAD7tXHBQCZ8uWLSiKwr/+9S82btzIww8/zDPPPMNtt912oIf228U/IQluL7ZnFoJDmxh9YwbhOuNIbZuw29sLL7yQCy+8kNGjR8fYYSSiKMLChbii1evvUITEkJQUTB2v/+prGhaHgh+RZUr//R/WXH4lzrIubv/3R8BFb7Z75JHO36+8XOt31cPAZE9DI5tu+SPb7rob2W4HwJSVxYhHHqLkd5chSBLePRtxrQhVlbVOOwUUN/WNmjm62iXgVsBgMGAyiBSkWBk/eiTXXnstbW1t3H///QCd7kgFQUCWZZKSkrjtttsQBIFTTz2V0aNHM3XqVM477zw2btzIPffcE1GILsDVRw8iPUGLEfpxZ31UC8XP0b80ntZi3dW4iUW8lqTw5JSYWVeqgKMqJWK7nnLsqHwS/KLFlt9CQkEzAJ5WC666JFRVZehkzVIjAFn+85Esa5lQni2rwOPG44aN9z9L04+hYm6iwUC2w07/vbsZuHsng3duZ2hTAyPPOI3hL/4nPqttQE3m58f/pZ59tnPRzGnTtIulqkorCfHGG71PKohFNxejaDbT7w/XBB9XvrmATbf+CdnZub2N5ZBjwKj1cvLuWIN3dxdW518YwWjCkNcXy+gjsM08C9u0UzCPmoaxZBhSes4+V1Q+0By0MTgPPPAATz/9NLt27Yr7PXoMTi/wx7H4kgzYrzsbJE0TW2xFmM/+XXAzRdFSig1d3OmF8/knn3DFySdztizzJ6BT6F0UX7i3uZmVZ83D29QMQNZRR1L3zaLgaiBarfS//lpyZ53cecLdHwEXPdkfdN+5vOPnQ0yzQf2ixWy/7368zc3B57KPP44BN16PISkJVZFxr/0W95pFoGifaR53JIb8PuB18uk3Szjl4mu46vqbueK6mzAajRSm2jBJAqIosmvXLsaNGwfAa6+V0dKSRG6uyuGHC8HDEUgZd7lcfPrppzzwwAP4/JazE044gRtvvJHExNhBlH9ZsJYPVmmq4OmLDmHywMjaOPEc0t7wc4VDPfKI1gerOy66CP7zH+3vrkJR8o/aRNYhewC4ftIULpid2uMx3f3fvSzYsjHiuZql/alePJjsEjfX/0eL9clKMHFE33TMios8by2+6lLca79HFUS2vr4Ud4uWUWVJTyN/3jxyZp2MMTGhd+UQwulJXBx0rjIc7f2FhVoqeWamdvHU1WmWnxtv7HnRzx5eLNUffcz2e+4LWm8SBw9m+EMPdHLXeXaswblYi4gXzDYS516DaItd70unM/9TMTgtLS2kd9O00O1209raGvFPp4f4b28NT72A1Rqy47ucFSj20PEURTEucRPQ01MFgSNkmWeAT6Nv2MkXvueZZ4PiJnP6dIbe8zfGPPsMFv/dm+J0sv3ev7P7yac6+8T3NeCiI/vQcTgq4U2YOryvcdmPbLr1T0FxY0xLZdj99zHkjr9iSErSsqU+eQH3qq+C4sY4YAzGPoPAq91RDhjQn/SMTH747lt8Xi9pNhMWo4QoiqiqSr9+/Rg/fi4tLa2ceOJbzJsHM2eqQcNS+G/HbDYzZ84cvv/+ez799FN++OEHbr/99i7FDcDo4rTg30u3d/Yb9daL0R2BSyne2jnxEm89yffeC53SmJ4PUcGSFaop09jQuwPQz1CMvTw14rmm9drvdsS09uBz+UmaRSFFbkVVZDxbtYJ+Vct2BMXNwN07OeTTjym87Y8Yv/qy5yUNOhKrbHQsAkWEAgevq7LTZ5yhZVudc472nvPOg2ee6dmF1It5IPfEExjx6MNICQkAtG/dytrLr8BdGymsjP1HYyjxu6rcDlwrPuu0L539z0EpcHbu3Mnjjz/OFVdc0eV29913HykpKcF/RUXROyjrdIN/YjOd93uMg7S7fFQFuT5kow8IitraWmr9dSuiGQcDqci2piZuAXKB57r6bP+K4G1tpebjj7Xh2Gz0v/kGAJJHDGf8yy+SOyfUAK/85VfZ/UQUkbMvARfR6Gp/J53Us31BVFHnbWlh29/uCT7OOHwa4197JVhaXW6oov39p4MBxQgi5lGHY5l0PKqjOfhcyZjDGDZqNCuXLWXjmp9It4W6/yqKwsKF8PXXF/mfWYlWNlCkvNzOqac+xsyZ5/D99993GnJaWlqwVk537K4LLbBKDBNNV4c0I2PfhE93rqqeEm89ycbG0CkN93xIVg8pg6soOHY9Q363mKQSLRVb8Yn0L7BE2VP35OcLlH0yEsXrjw8pTcfdmMjgSXamnxPKmslPNmNUvFgVF77S7aguB+5mL3Vr9wCQ0dhIXl2tVuCzp91Lo9GbHlQdA/W7qqgInW8OAhdTR/dVVpZm3en4fG/mAVkmzd7OmLmzMaelAlo9oHVXX4OnIdRQUxAErFNma2njgHfvJlS5dy1MdOLngLqo7rjjDu7sptX9ihUrmOBvGAhQWVnJEUccwRFHHMG///3vLt/rdrtxu0NR662trRQVFekuqn3AveF7XD9qbRysM87E1G9k8LV3332XuXPnMnbsWG644YZg9+Pw2ipB/Lb6PwMPAZVAGlHw59eWz3+dXY88BkDBmWfQ/4brOm1aufBddtz/j+Djwnln0/cPV0d3V+2rqT0cjweeekprSd2/P/zud1Bc3Pvy8v68ZlVV2fznv1D/1dcApE2ayIhHHw5+H+/ujTi+XQA+rX6JYEvCduQ8pKxC5Lo94K9MK6bk0Cwm8cmXX3PeKSdywkkns3DBW5j8dXNkGfr0UamoKAMOAU5E6yIGUAucBSziiSee5Pe/v7JXX6nV6eWEf3xDu9uHQRR474YjKEiP7MHTMWUctBpvgVP03nv7niksCPuvPJEsa+NsbIx8XjT5SChuIKmkgcSSeiwZdswGifQkAwlmA1s2GPGpMpbs1k6CTfGK2L8fy7YlOb26JANuvnpXG4kl9TRvymfMdBdzrq8N7q9PqoVDClPJ9DaQ4GvF+e0HqEhs/NdH+DwyoqIwYd0aLJ6wjJ999fPtS5r4a69pF0E874+Vjx/t976v80AHd5nbaGLtyNG4/JbspOHDGPP8cxHzj2PxAq11AmA79gKMhQPj/7z/cXrjojqghf6uvvpqzuqmE2xJSUnw78rKSmbMmMHkyZN5No78TLPZjNls3tdh6oQTXmpc9ka8dMghWnO37du3c9555/HSSy/xyCOPMCxKBpM6dSpCYSFDy8txA0vRltUgYf2gOjbYzDs1eouG/LlzEESB7fdpgbKBVhD9rr0mcsNYTYV6QmByfO89eOWVSDFzzz371jvHf5tf+9nnQXFjSE5i0F/+HJws3eu/D/YLA62Jn+2ocxATklGcrUFxg8GEkJiOvdnJhEmTmXTYND7+8APeffddTjnlFIxGI99846Wiwgh4gTogrGQ/2cBNwPUMH95NpkwXvPLdbtr9DSFnjS/sJG5ihVY8+mhkEs2CBfCHP0QG+BYUgMsVu/VDR7rqMh44rYFQjqwsbf/R1j5J0sYcqIOTOrSSzPF7seU3I0iRA/EoPqpbtO9vzIaOoZuKLOAoT6N68WD++1har/V2qA5kEu6GRGae38hRF4QUWGGyhXH5KRhUHwmKHV/VXlS3k4ZNDfg8mvWjsKoiUtxAfH0rumJfUuQCAcW9/Zyw37unsZHmL7/CVV2Nu6YWV2UlrvIK3LU1SLYETBkZmDLSMaanYynIJ+eE47FGM9VFKfJp9noYtXEda4cMx20207ZxE01LfyD9sCnBbYzFQ4ICx1e6RRc4PzMHVOBkZmaS2VXthDAqKiqYMWMG48eP54UXXojbLK6znwmftzvEihQUFJCXl8eAAQMYOXIkL7/8MiNGjODWW2/l6quvpiBsonD7fFgefRT11FMxoDXoDNLBF966ek2oweYhE7CFpSB3JG/ObBBFtt/7d1BVyl+bT+6sk6M35ewt3QVK9lbchIk6x9697Lj/geBLA2+9BbM/6MO98YcIcWPsPxrr1DnBjAelNeT/F1NyUFQBt09BkiRu+b/bufri87jrrruw2WycdNJJ1NUZ0dqiBvYZaM0ho3UVOwHo/Rr17soynl+8EwCDJHDp9MgS7z0tCB0t5Onii+HBB7sfS1frdFenNSC2Olp+/vxneOwxFePwbeQctrPz5ykgtyTSf6CK3e2j3eXF5dVS8H2NSTRuz6R9Tyb2sjQKcg3897HY1qWKFidL9zZS1eom3WakONVKUaqVghQrJik0H86dC28tUFn4Ux0jjwpVoU3xJTKpKAFBEEj2tiIA3t2bUQ0WKpZqxQHNbjdFVV20XOjtRdCbFLnwprfx1qeK8jm+9nbqFy2m7vMvaFr5U8zYOMXtwdvUhH1H6LmK115n+AP3R1Zp7sJdZnG76V+6h03+BqQ1H38SIXAMBQNBlECR8VV2vl509i8HRauGyspKpk+fTnFxMQ8++CB1YZHxubm5B3Bk/3v4yrYE/xZTO0dZTp06lc2bN3Pttddy1VVXcccdd3D//ffz4osvcsstt3DppZditVqxWCy0HX00r4wejWndOiaFTxYdOmo3/vhj8KWcE47vdox5s07GsWsXFfPfAMBZXr7/BE6s1XhfCRN1ssfDpj/ehuzQLCnZxx1L1lFaar5nxxpcyz4Kvs08dgbmsTODlh1VkcHrr3ViMCNYIjM1xk44hL/+9a9ceeWVXHTRRfz973+nrCwZ2I7mLJwOBOKHIs0IPV2jVFXl34t28uQX24LPzZtcQn5aqFhYT5pVBlxU0YTQgw/CTTfB009De3vnfXXk7be1/wOWme5Oa3l5dLGlqAon/Gk96xpCJiV3o422XVm0782gvTSDN181RrzHKyvIiopRlOLykJQ2Ofh+TyN7m0MpyI4WmfIWF+xtQhQgM8EMqHhkrQO8J0Nh5FGhLzMsLYVhBdpxl1QfiUo7vtpyVHsr7U2g+uvnlJSXInWogRRBwHfYUwKF/nqSIqeqcOml8Oab2ud29f5wMeTHvnMne599noalS1G7qJkmWq1YcnKQnU48jY2o3pBlWnY4WH/dDQy583ayjvRXIe6myKfN6Yj5mmAyIxhMWuf2gzOB+aDioBA4n3/+OTt27GDHjh0UdggMO0iz3A9KFEcbvgrt9kZITEXKKe60zZw5c1iwYAHbtm3jxBNP5MUXX+T888/n7rvv5sYbb+SBBx7glFNOIT09nS+++IIV69dz5JFHMvSPf4wMuAib6ZuXhxr9pU48JK6xmnNChalkl6uLLWMQzT8PPQ+UjEXHFHK/qFNPOYXtd9yJY9duAGx9+zLw1psB8JZuxfltKNDTPPoILOOOjNit6gktgoLZprXNQOsr5ZEVfApcfvnleL1eHn74YS677LIwa+hRwD/RQr9DRFk7ukVRVO7/cBNvLAsV9Zs3pYTrjhsSsV28BaEXLepeCM2fD2ZzfALniSe0f4EM4+uv7/60qmqke6vd5eWm11azrqE++HrlF8Oo/6kE0Opg/ufVzhYZoyRi9F/esbw9iqqyq8HOstImTciEYZIEPLIati3UtkevkCsAk4tTyfc36xRUhUxvA4Kq4tmuWW0qv9IKRhplmaymxqj7CXLBBfBYF2amWHTsD9cdogipqZF9MDIyQic7/GRFyX5qXb+B9ddeH6wXFcCcl0f20UeSOGQI5uwsLPn5GNPTQzcIqoqvrQ13VRW7n3yaph+Xo3q9bP7zX/A2NpF/+qndWrFc5lCAuKWDe0tVFFSPdj4PtqrAByMHhcAJFJHTObB4d60LTiym/qMRopT5PuaYY1BVlfXr13PiiScG3SDTpk3jrbfe4t577+XFF1/E6S+Idcwxx/Dss89iKO4slgB8bW20bdaafNr69e26HHwYYtgko/RU4MQKCLnsst51RA4nMBnPn68FeIQJKMXnY+c/HqT2U61Qn2SzMez+e5FsNnxVu3F8PV/zeQCmIRMxjz+q8/49obtHwRSKczEbxGCHcLdP4eqrr+aMM85gy5YtbN++ndbWidx4oxYw3s3aERd/e3c976wMHavrjhvCBdP6dgr4jtfjsWhR90KoN6emogJOPz3+7QNia/REN1e9uJxtVVpqt9kgcvdpYzAem0tVFeTmqgwc42ZLfRuvrXZjMYokmgwkmgwkmCSSLQYKU6wYpc6/oeo2F+9sqKLFFZllk2Y1MrlPGsNzknF4ZcqanZQ2OyhrdtLg8CKJAiZJwCiJmCQRm1GiX7qVTH/GnIhKjrcWs+rBV7UHtb0FGROuRu075B42GXHVis4CIpzKyt43kg0EUf3ud927cRWlc/R24HGgI2uADhbf9q1bWf+H64IWUGN6OllHzST7mKNJGjGiy6KEgiBgTE7GmJzM8IceZPt991Pz4Uegqux48J+46+ooGTKIrpL5mpNCAbCWwg4Cx+si4OfXBc7Pz0EhcHR+HXj3hrqJGweMibpNRkYGJSUlrFq1iubmZlJTU1EUhZSUFC699FLOOecc1q1bx8aNGxk8eDDjx4/HYrFEz7QC3HV1wS7iktUWc7uO2Ldt63abqHQVEBJPV8WOZGVFFhvrMBkHsO/cyZa/3oF9h98vLwgM+sufsfXpg696D/bPXwZ/Wqmx70gsk0+KehxUb8gUL5hCIs9slGjzB/nWtbvIT7GRnZ1NdnY2hx9+OAB9+sCTr7fS5lCp3WuirtRERooUbbhdsrWqNShuREHg+IJRlCgFKEpnkfRzVC/uCb0xxp19qZ3hF66gya0toClWI4+cP56xfdKpt3sQa9rYVNvGj2u8Xe7HKAr0z0hgcHYi/dMTMBk0sbOmsiVC3GTYjEzpk87Q7CREUTvnSWYDw3KSGJaT5P8ekb8LVVWpbHHi9GpWQoMokO2qxqR6UFUVzw6tZ1V7XUgQZ111JRxxeNfxZR39hj2Nhp47F5xOOPfcnr0v8NmgdVv98suYFt+djzwWFDepE8Yz/J8PIFlCvwVVUbQbheDJV0EQEaTI5VA0GBj0f7dhysig7KX/AlD20n9p6NePnMFDydqzC4s/S1cFGlPTKM/NpyUsw8fWoTSJXB2yaAqWhJ4fA50eoQscnbhQZR9ynX/RSkpDihJ/E2DGjBm8/fbb7Nq+nXF2O0JlpVaefdo0rFYrkyZNYtKkSRHviSVarEVFmPPycFdV0bZxI7WffNptHE7bpk1UvfueNlarlbTJh8b3JeOptREvAb/Ojh2wdGnMQAtVUah88y12Pfl0ME5AMJkY+MdbyJo5A1/NXuyf/xd82muGwkFYjzgVIVaQfdgkrcoygj9dJ8lspNnhwaeouH0KVa1O8lOsiGHHfcZxHnam1yKHfVebUcKTaGZdVSJDspKCi3BXfLwmFKRa8fUg7l1WwL1ED9TtLjQjcBinT4e77+72o392rDktZM1aQZNbOx95qRaeunAi+elWFqyvZEe9vZs9hPAqKlvq2tlS145BFOibbmN0XgoDMxNZE9aL6rRR+aRZTV3siU7ipsHuDoobSRTIE9qQVG3MqsOuNdQEhJSQO7J13QYSTz8VUlLgqCjWwdAH7FtGVbwFhGJRUQHffw9//Wunl1rXr6dllVa00FpcHCFuVEXB/dMXuDctC5ZWCCIIGIoGYz30BMSk9LCnBfpedQWmzAx2PvwoKAqOXbvYnZLK7tHjSG5rJbW1lbr0DJzWSItM0vDhJI8KldFQFQXXT6FefcaS4ft2HHS6RRc4OnEh11cELQhSbkmX255xxhnUrl1L2qxZUF0dMueGrXCB2KnurDGi0cjAm29kww03AbDr0cdJP2wKxpSUqNurssz2fzwYXC1LLrskbrdWtwEhPeWRR8BkirkItG/fwa7HHqc50DQTsPXvx9C77iRhQH98NaXYP3sJvAFxMxDbkWd3utMMRzCYQoluPjeg3SVKokB+ipXyZieKquLyylS3OslNDomcb3c3RIgbAIdXZk+Tgz1NDr7cXsfQ7CRG56WQl2yOeu4UReWdZZrAUWWBhrWhO9hoWVHhoRldhVZMn969ECoo8MfBVP488ZuJfeopOfUnJLMmHHzNiTx/0yEk2gy8sqqcOntkIGufNCvDcpIYkJGAT1Gxu2XaPT7aPT5q2txsq7cHRYhPUdleb2d7vZ1US2QS+adbajlrTEFclkuXV6a2zRV0RwLkWEWkgHtHEPGWh7J3UqfNgBffBKD28y+0GBN/oc5u6W1GVW8Cjjty++0wYkQn02LZSy8H/y4679yQuPG6cSx6C1/pFqKiqvhKt9BWsRPz2OmYR05FEEM3IgVnnI6tpITdTz1N++bQPlqTkmkNc0kBWNPTKbz8UnJOOB4h7GbGu2sdSlMNAFJWIYbiyHg0nf2PLnB04sJXGer5ZciJnaYNcKzdzrGrVnWZ9yv0wOeRftgUMo+cSf1XX+NtbmbjjTfT/6YbSRoyuNO2FW++FZyAbP36kn/mGXF/To8m7K7iFIqKorqhQBNgjT8so3LB2zT9sCzitYKzzqTvVVcgms14d2/AseSdkLgpGIDtyHndN78zhO70VW9k4KnJIJGfYqWixYGqgsMjs7fRTqrVRLLFyHGDcxidl0KDw0ODw0O9Xfvf7q+P4pFV1la1sraqlcIUC7OG5ZLcYTFetbuJVn8WV9vuTGRn2HhieDcCoRnRwp7CD2N3QijQ5iHaNvuCIMlkT95F9pQdiP76NvayNHYvGM/K032Umspo9x8ji0FkSkk6w7KTSDRHTq8pHY7VsYpKaYuTrbXtbKtvDx7nZlekdWFvs5OXV5UzJDuRIVmJnY55ALvbR1VrZLPHzAQzpvZQhpeYnIXc4L/OjWYSho3E1rcvjt27aV23juaVP5Hai4az7ro6WlavoWXNGpx7S8k5+SRyjjs2+vu6UrU9ocOFZN+1m4Yl3wFaE9rs47XPVxUZ+6cvIddqfbgQRKTsIu2z/ReP0lyH6mwH2Yt75Rd4d63HdsRpSOkhC1faxENIm3gIztIyah97jLqvF+Ewhs5FitNJ4Tlnk37zTZ0srIrLHmG9sUw4Or4GpTr7hC5wdOJCaQkFBUqZXZiY/W4eRVURIDIYbx/89/2vv5amZT8i2+20rt/A6gsvJmPqYSQMGIClIB9LQQHe5mZ2Pf5k8D0DbrkJMVp/rFgVTOOd2O+8E557LnI1zsrS+uDMnh0139fX3k71+x9QueBtXBWRdUbMOTkMvO2PpB86CcXRhuO7T/BuDWWOGfL7YzvqnLg6+wrGUGFL1d6IYk1GDPP1W4wSeclWqlqcqICsaO6MRoebFIuRwhQLJWFF+FRVparVzdqqFjbXtgWzd8pbXLy7sZpzxhYiiaGz/P3KkKjytHQOoozl3Zg7Vzt0XaVNxyuEom3TVd/TgBvsn/+EG24IvU80e8kYU0bmhD2YkkOB6i3bsin9YAxHnNHKJrUB/IabVKuRM0blk24z4ZMVWpwe2t0+fIpCRoKZRHPk+RNFgZI0GyVpNgyiwIry5ugDBCpbXVS2uvh6Rz0FyRaOHJBFfkpkOweHJxS3Y5JEspMsmA0CcqM/zkYyISRmRLhnBFUh+/hj2fPUMwCsv/Z6Bt5yE7ndmMvUwkIaRYn6u++lZfUaXB0sn80rf6Lph2UMuOUmDAlRYk26OplOZ2QQcSw6XEg1H4bKJxTOOwvRLz7c678LiRuThYQjz8aQ3z9iV6rHjWvVl3g2LQNVRWmspv29pzGPOQLzyGkRvz3ryhX0eeifFKsqDquNtoQEEhwOkpwO+NNaGDgg4ubGV70Hx6K3UO1aTSIpv1+nz9f5eThou4n3Br2beO9xfv8+ni3LAUicfWVskRNvSfZoJdW7oXn1arbdfS+u8oputy087xz6Xf37zi90VTJ39uz4O45DXGXenZWVVL7xFtXvfxAMfAxgzsuj4MwzyJ87BxQv7rXfasc4rEeNsd9IrNNOQTB0HYMRjtxcjdruXyBECSm7X6f3u30yjXYP9rBFMTgug4jNaMBqkrSGnP47TY9PYXNdG4t3NuDwu1am9Enn8H4Zwff+52UPj6z7GtGo4HMa2PTEkai+zsfF342iV8RTYb/jNvX1Wi8qiG79CbjNZBne/dzBy0v2sNNdFnRHqYqMu7GWmu/bUOoP44xb8+g/NmQt6ZNm5cQhOciKit3txeXrXEsmN9nSSeQA7G1yMH9N52t6Zv9MLEaJ5WVN1Hdwf5kkkXljC8hNComc+nYXzU5NvBSkWrEaDaiyF7lKC7gXLElImcWR7QKOOQ8xq4RNt/6RpmWhelOFEw+h6JmnMMgyQtgBc1itVGdmUztgIB579/FGlsJCht7zt6jWViD6yXzvPTj11G73DYTamsgyP86ei6euDkGSOPTjDzCmpiI319H+7pPab0oQSDjpcgzZsXsSyvWVOL59O+hKAq0khnXS8Rj6DENQFG2OiOXKDpsjVAHcaxbhXrMoeNEJtiQST7osIs5HJz56s37rAkcnLlwrP8e99lsAEo6/KPYdyPz5MG9e9zvs5QqneL1ULlhI6X9ewBejO3z61MMY/o+/R/i/gdgZUuGrHERveNRxJewCVVVpXb+BitfmU7/422AWWIC0SRPJP+N00icfiupx4V73LZ7NyyNbXxiMWA89EeOg8T02ZauqitJQiuryF4QxmpGy+kbEFATw+BSanR7aXF6iTQQCYDVJpNvMWPzFWypbXby8qkwzyAHzxhZSlKpZaxYtgvMfXEv6KG3BLvtoJI3rOi8ovdC3+0w0bRvwJp48W2H1nkbe+rGUrzZWo4QdDHdzLRWffUjb7k8RBA8gk5KZy+FnX8bE409n+uBcRuYmUd/ujnhfNPJTrNhMYYHgqsqrq8uDtW7GFaQwJj+FrARTxHmvt3vYWtfGxpo2Gh3adWI1Spw7rpAMfxp4g91Nk8MT8Tmq14Vco8XcCLZUpPQCvHs34/jyVQCMA8dhO3wuqs/Hzkcfo/LNBRHjFVUFk9uDyetBEUXaEzp3jBeMRpKGDSNl3BhSxozB29jIjgf+GRT0gsFA36t/T8FZZ8R/Ld91V3xZi/4LqXnlT6z7vdaSJX3qYYz45wOoioL9o+eQa7Uq6KaRU7FOPK7bXaqyD/eqr3Gv/y5YlgFAyuuHVcpGOq77liXK5x/hcJUFPxu02EXbEachJqZ2/710OqELnG7QBU7vca9bgmvFZwBYp5+Oqf/oiNdll5vdTz5J8aCBmGbN6n6H+7jCKV4vrooKXBWVOP39ZFwVFZgyM+l7ze87m8UDXQjjuPPivfdir4RdiBvV56N+8beUvzqfto0bI14TzWZyTjiegrPOwFZSgtLaiHvjUjzbforM6JAMWo2bUdMQbUn0FlWRkWt3BbOvMFmR0goiXFjh+BSFFqcXu9sXEaAaTkaCiVSrtvAu3dPIt7s1K1GiSeLccUWkWo3IMgyY0EzK8Uu1/TpMlH82nJatuaAK+9yzcV8JNxgkZ3pwp9SwdHsdy3bUB3tlBTAIItUrcqladC2KdwnDDjuZIYdORFUVVnz0FvVlu/nnk89wyilzaXd7IxZvkySSYDaQYDLQ4vQEU/QlQaAkIyG47Z4mB6/7rTcZNhOXTCyOyGzriFdWeGNtRVAQZdiMXHxIHyRRoMnhpsFv6QlYi1S3XWu8CgiJGUipuag+L62v3afFdxnNJJ3y+6BFoXLB2+x46JHY/jw/giSRPm0quSeeQOrEiUiWyOvKWV7O5v/7a0RAbtFFF9D3it91ud8gPfm9ShJb776Xmg8+BGDIXXeQfewxuLcsx/X9+wCIyRkknvL7nllCm2pwLfu4U0sFaXcl0p4qpNJqpD1ViFX1KFmpyH3ykPvkav8PCqvrJYiYx83EPOrw2NmPOt2iC5xu0AVO7/Hu2YTjq9cAMPYfhW16KHjXXVfHxptvpX3zFpKGD2f0F58ixuPmiWeF21+dv3vqOuvB58oOB9UffEjF62/iqoyMrzFlZJB/+qnknTIHY2oqvtoyPBu+x7tnY+TxkQyYhhyCedTh+yRswlG9bk3khN2FCglpiMlZCFLseB6fouD0yDi8PhweGTnMLGEzSeQkWRAEgfmrKyhr0dw0KRYD544rIsls4O23Vf747jISipqC73M32qhb1p+mjfm89YbUpRFsfzd774jXpzD/hz08+/WOTqIGID3BxJmH9uHQwVn87W+P8fZzdzB65kmc/sf7kSQDqgJSrYd/Xn8Umdk5vPDmu6RnaG66BJOBjARzRDq9qqpUNDuCbqs+6QnBAn/vbaxic61maZs1LDdY16YrXF6ZV1eHsraOHZTN2IIU2lxeato04ZNuM5GeYEb1uJBr/RYcv4sKwLFkId5tqwAQU7NJPPnyYN2klnXrqX7/A9w1NXgaGvDU1QetpQkDB5J70glkHXsMprS0LsepeL3sefpflL+qzRsIAmOef47k4Z2b70YlYHGFLq2p7vp6lp9yGqrHg2SzcejHHyBaLLS/9TBKm5Y9lnDCJRjy+sb3uWGoqopv72acyz9BbWvq/g0dEJPSsE4/o0u3mE58HHTdxHUOHgxFgxDMVlS3E++eTahuZ7ASpyCKeBq0iaRt40a2HXMsg1/4j3aXui9lcbuKl+lpFdWediOOo+O4z26n8q0FlL86v5O7LGFAfwrmnU320UchiALePRtp/245ck1p5E4MRkyDxmvCJmH/im7BaEbKKkFuKAu6v1R7E7KjGSExAzEpM6rbyiCKJFlEkixGVFWl0eEJuj4cHpnSJge5yRZOGZHHa6vLqXd4aHH5eH1NOfPGFnLqqQbsvnHc//lqjDnadWFOd1B4wnqGzN5GW1Zf2lxFJEXJBtqfp7wjqqqyeEst//x4M2UNkfFQqTYjUwZmMbw4lZQUC7saHSxctYPvvniV9LwiDjv1AiTJQLLPyFGjMzBLAstnHsXC11+lrbWFrMxMMhLNJJkNndwwgiDgtLezeu16hgwfAemaddHpldlWp8WxWI0Sg7Mi3T+xhJ7FKHHc4GxeXqUdpO/2NDA8NynoQgRNBAFgNIMggSqjuh3BgoDWiccj15SitNSjNNfStvBxzKOmYRo0npRRI0kJq98CoHg8yE4XxpT4r1HRaKTfH67GkJzMnqefAVVl+333M/bF56MH/3ckzqjy8ldfC9aQyjtlDpLVitxYHRQ3Ul7fXokb8Fc2LhmGoXAgns0/4tmyAqW1+wBoISEF04DRmtXGZOl2e52fB92CoxM3zmUf4dn4AwCWySdhHhYqoNe+dStrLr8y2BahaNJECt99B2NZ2IIeh5snSDzxMj1Z8fZj8LO3pZWqhQspn/8GvpaWiNfSDp1E4byzSZ14CKrbgWf993i2rkR1Ry6ogjUR07BDMQ2ZiGix8XOiKgpqewNKW32ENQdBQrAmIiakI5i7HoPD46Om1RVRJyfbb8l5dVV5MLU52Wzg5GG5FKVa8flUnn+vgY+37GSvPXJRSDAbmDWugFMPKWZArma12N+nPJzqZie3L1zHjztC4xAEmD2ukFMmFGKyGPmxrInqtlAW2LYV3/HiHy9h6mkXcO1tdzE0NwWzJGgZgoLAc088yoN3386///sq5599ZkQ2WUfe/+wr5hx/NADDh4/gpJNOZOjUY6i0ab31DilM5ciBoeKZb7+tcttLWxFyamnenEfN9wMoLBQihN7CDZVBgTStbzpT+qSzp8GOrKqIAvTNSEQQBOSGUlSn1o5Byu6HYNJuTOTWBuzvP4PqDutfZk3EPOIwTEMmIpiiuzMBFJcDuaESpbURwWhCMFkRzP5/1gTEsOtJ8flYfeEl2LdvB6Dv1VdRdF4XlYw7KrspU2IWy/Q0NbF8zqkoLhei2cTEd97GlJGBa9XXuFd/DXSeq/YV9e23kP98s+aOKs5ByclAaGzVXFZ7q5Buvw/xtB6Up9CJC91F1Q26wNk35MZq2t95QntgNGvBxlmh5qd1X33N5tv+L/QGSSK1b18yiwrJmHIo5hNPjN8t1QP/e3yDl+PPkIqyT1VVadu4iZqPP6bmo08i+1uJItnHHkPRueeQMKA/itOOZ9MPuDf+AB1q0Yip2ZhHTMHYf3Rcad/7E1X2obTVobY3QceQYpNNs+hYEmMGgvpkheo2V8g6gNamwCCKvLamIhhnAjAkK5Hp/TNJtWrfcX1ZMy8s3snXm2o67XdUUSpzxhdx7WkFlJdFj1GIdXricWdVNzu55LllVDSFFvLxJencfNJQbDYTi3bWU9ka2a/MIAp89vjtfLVwPq+98wETphwe9pkykiTxlxuv4Y1X/sv777/PSSedhKIoYY1LQ6iqygOPPM6fbrqeGccch6j4WL1qFQ0NDRgtNvoMH8vbr/2XkQM1K8PChXDB9Y30PydUJ2nHy4fiqNDiZAJCr8Hh4d/L96KqWhzU1Yf1o6rFGcyMK07T2j8o7Y0ozZplUkzJQUwKFb6Um2txrfi8cwE8kwUpNRvBkoBgsWkiXDIgN9YgN1SitjdHPU/Bt4+chnViqA5O68ZNrLnkMlDVoBtJsnYuI9BTE96eZ56l9IUXAcg/43QG3Hg9AG3vPIHSWA1A0lk3IyZELwzaa7qKWN9XU6NOVHSB0w26wNl37F+8iq/U35Mqisgpfem/wZoaEQgCKWNGk3fKHDJnzgjWqIjKz5VqHqdPPxxnRQW1n3xG7aef4Swri3gNUST7mKMpvuQibMXFyK0NmsVm+6qIVG9ECWPf4ZiGTETK6XPAC3ypPg9Ka612Vx9u0QEwmBGTMhBsKVGbqaqqSn27m5awYnRmg0ii2cgHm2sihIIkChxSmMrEojRsJk117K5t59Wlu/lwdQUub+Rnt5elsffdsfjaY5v0w095PGthTYuLS59bRpm/FkxuioUbTxjK4UOz+WRrLVtqI1uP5ySaGZ2XTKZF5JgjplBfW8vHS5YHY2wEIMliJNVqZOqUyaxcuZJdu3ZRUlISs0+aw+HgrHPOZcXy5Tz0zPNMP3Q8O7du4cXPl7F25Y/Ymxv59PUXKC4uDurwNls1JaeuCu5j73tjaN6U30noPb98L3V2D0ZR4MYjBlDb5qLVf24KU21YjBKqx6nFYgGCNQUpo7DTGOWGKtxrF+PdvZFO4reXdIx72XrX3dR89DEAg/7yZ3JPOjHyDT004clOJz/OOgVfayuCJDHxnbcx52RrqeFva1UfpaxCEmddsV++Tyd+7mAxnQj0GBydnx3bjNOxf/4yctVu8Lqxf/JChMgpvuB80iYeQt3nX1L3zSLcgZgWVdUqna5eg/GRx8ibPYu8U+Zgzsnu/CE9jZfpgKehgfat22jftg379h2aJWnCeNIPm4q5G5++4nbTsn49zStW0rxiJW0bN3Xav2ixkHvySRScdQbWwkJ8tWXYv5qPb88mIhYHQcQ0eALmMdP3e3zNviAYTEjphaiqgupo0VxXgWwrnxulqRJaahET0xES0yPidARBIDNRC6Kta9esU26fglf2MHdELjsaHHy7S6uTIysqy0qbWFnezMjcZA4pSqVvdiL/N2ckfzh2CB+vqWDhijK2VWvuk8SiJgZd/B173x2LvTSj07ghdMq76okaaAdx+NFuLn/+x6C4Kc6w8e/LDsVmNjB/TUWEOyorwcSUPmkkmyWcXoXy6mqam5oYPHQ4CQkJSIJAitVIssWIQRIpKytj48aNFBYWUlJSEjw20aitrWXt6lUMHjqMfgMGkZScwtSpUylNKKHPlGNxOx3k5mt1pQLdQlKHRoo/0aQJ5u7aQEUdgdHsf0X1d7PujJSRh23mWcjNdbjXLsZXuhXV44y6LUYTUnoeUmY+Umo2quKP73E7UVoa8JVrdXec37+vZS75W4vknTInKHCq3/8wUuB01wcuSoHQmg8/Dsa+ZR17THAu8e5aHxpqv5GddrffiCNOT+fAogscnR4hGEwkHHNeJ5FjGTMd07BDEQxGkoYOJWnoUPr+4Wrs27dT/81i6r76CudeLR7H29hI6QsvUvrfl8maMZ2SK3+HtTDsrrIXpeKd5eXsfuoZWlavwRvouxNG3WefA1pxPevsUzD5fFrFVJsNITcHNmzC+dkXtG3aHAxYjPziAinjxpJz/HFkzpiBlGDDV7GD9g/+FVHrAgCjCdPgQzAPn/yrrnkhCCJCQhqCLRXV1YbS1gAef6yQ4kNprYW2ei3rKjEjuIALgkCK1YTZIFHd6sSnqCiqSnWri7wkMxcfUszK8mZWlDcjKyo+RWV1ZQurK1sYmJnApOI0ClOsnDW5hDMP7cNPuxu56ZW1NLtcGBM89D/7R2q+H0jN0v6gRFqR8vLiWwuvv9nL+Kt/ZK+/+WVhuo3nLp2ExSTx8qqyYLdusyQyc0Am+clmWpxenH6rksPejs/rJS8vjwyrgRx/arfsT5/++uuvcTqdHHus5obx+XwYYgTObtq0ibLSUor79mfpt9/QWjuQcSOGYRAFjGYLRrMlWD8nIOAEKVLgSKbIbK+edRURNZHjdYHPjaooMdOVpdQsbEdoVk5V9qG6naguu/bP60FMyURMyYhq3QMt3sv+4bPIdeUoLXW4132LZexMAJJGDA+1hVi7FseePdj84rDbPnAdlJ3q81E+f37w5cJ5Z/s3U/HuDhM4fUfEe5h0foPoAkenx0QTOa4Vn+HeuBTzmBmYBo9HECUEQSBx0CASBw2iz+WX0rJqNZUL3taK38kyyDJ1X35F/bdLKDznbIovOF/zy8fbYnraNP9E9zp7n/s3ijuKMOmAu6oqZFWKg4QB/ck+9hiyjj0GS06OljZasQP7N193EjaCNRHT8MmYh0wMZpgdDAiCgGBNRrQmo7odKG31qC7NqoKqoLTUgLMNKb0goo6IxShRlJZATZsTh7+PUpvLS5vLy/CcREbmJbG6opW1VS14/S0eAg0lB2QmcOSATNKsJib0y2DBdVM5/A9rMOXXI4iQO207yQNqKPtoFK665PBTHtdaqA7bxK46zf2Ul2rl2UsmkZVsYf6aiqC4STYbOG1kHqqq0uIMudwMokC/gjza21ppbaonPVkTN+ExNq+88goAp/ldnrGsN263m5qaGkpK+rJh7Wq+X/wNWdnZnHXmmYw/7TLAjKqq7GlyMCQ7KajZVSVyf6oa+TgvDxodHhr94w6knfvCUvrDhyQYzSHrjewBsfvMHkEyINiSoAdlCwRRxDp1Du3vPgWqgnvtEkyDJiAmJCMIArmzTmLXo48DUPrCSwy501/Mr4dW24o33gy2PEmdeAiJAwcA4CvfhtJcB4CU02f/x97oHFToMTg6vUb1eXD+8CHebasJd82ISemYx0zHWDIsaoqku7aOqnffo2rhO3ibQrUlTNnZ9Lvm92QdfRTCO+90Gy/TNnQY2+69D/u27cGXJZuNpGFDSRw0iITBmriS7XYaly2jefkK7Dt3IXdRYt6Sn0/K+HGkThhP6oTxwU7kir0F7+6NeHeu1TqrhyGmZmMeeZgWONxFp++DCdXrRmmvR7U3h54URMS0fERb5KKhqirNTi9NjshKvgKQajNhMYqsr2pjZXlzsCklaEXvJhanMrk4HZNBZMHbKlc/soOcw3YgiNqOFFmg7of+1C4dwFtvisyd232x7JQhVZScshqARLOB+VcfRlFGAt/vaWSJvzhhstnAOeMKsbu9QXEGWv2YVJuJ9rY2jjzySMrLy9m5cyc2Wygr6NNPP+Wkk07i8MMP5+OPP8Zi6VoslJeX43a7ITmL1T+t5O35r/LGyy8waNhwTrzln2QV9SXVauTSicUIqkhJCVTVeel7+koSippoL01j77vj8NnNQaG3a5fKm+srKG3W3EgTi1I5ol8muxvag8c2vKCg3FiB6tDOpZQzIGbBx/1FeMalcfAEbFPnAFpPtuWnnBZ0LY377wskDh7co7g71+DBrDzrHC3QXxAY8+9nSR4xHFVVaH/3qWBwsW3mWboF5zeEHmTcDbrA+XmQG2twrfoS397NkS+IEoaCARhLhmPsM7STVcNnt1P24kuUv/Y6qi9kgk8eOYLcWbPIaG3G+Kc/RdyuK0VF2P/4R2qcLioXLAy1QRBFCs48nZLLL0OyxU55VlUVb3MzcrsdVBUV1X/Lr2JITsaUHuoRoySsPwMAABvDSURBVNhb8O7ZhHf3+s71a9CEjWXcTAwlw2Ka7A92VLcdubEioo2EYEtFTM3r5OaQFZUmh4cWpyciTDUQmJtkNrKtoZ0luxoihE6iSWLGgEyGZSfxzjsCN9zVjGXiOiyZoQDgLEsi/7xwJKOK02KuhYIkkzVxNzmH7UA0atfFPaeP5sSxBVS0OHlldXmovcSYAgQhVC9GAHI69Ip64YUXuOSSS5g1axY33ngjxcXFbNiwgQsuuABJknjppZc47rjjYgYXdyRQadjtdvPUA/fw9OOPcuoVNzD+dK2678z+mUwsTguLhVcRTD4Utzam8Fjb/pNb+GRLLaAVWbxkYh+cHl8wLirFaiQrMSS85IYyVKcmKqTcgT2q6NsbFJedtjcf0rIIBYHEudcgpWoxMuXz32DXI1oQcOKQIYx68jEMVmtcWY7e1avZcMsfadugVQrPP+1UBtx8IwCenWtxLnpL+46ZBSTMuuKAB/Tr7D90gdMNusD5efHVluFa+QVy1a7OL0pGrfDV6MM7NZpzlpax8+FHaFz6Q6e3WQoLMZtNCB4PPkXFXlfXKUYmYUB/Bt72p/grpMZAVRTkujJ8ZVvxlm0L3gl2REzPxTz6CIx9h/9mhU04qiKjNFUGF0gADCbE1DxES+feRD5Focnuici0CpBkMZJkMrC8vJkVZc0RdXUKki3MHJBJbqKVrxfJLFy3gx/rd6GEbTO8MIWi9ATemW+mqdqMt92CaPSR3L+OxL71SKaQcDpmRB73nz2GOruHt9ZVBtPYDytJZ0hmQnB8ogB5yVaspkjrmyzL/OUvf+Hhhx8mJSUFURSprq4mPz+f++67j/POO087Pt0InMDrsqKyt7EdRQWH3c6M8cNJTknlgscWYElIxCgJzOifydDsJD75QIqZhTzq8HY+2FSD299S44xR+ZSk2yhvduDxV0sOZFAFv0t9adDtKOUN/kUsja41i3D/9CUAhoIB2I49H0EQUTweVp41L+hishQW0P+6a0mvrUE4/XTtzR2stipQe+dd7P5xOZ66egBMmZlMeOM1DImJqB4Xbe8+Gaw2nHDchRgKBvzs31Hnl0MXON2gC5xfBl/NXry71uPdsxHV0Rb5oiBiHDgGy+jpiMmRQqfhu+/Z9fgTOPfsjetzBJOJPpdcTOG582JWRlWcdlR7M6rPi+rzgs+LKntRPS5Ul0PL/vD/L9eVRxQ9C0dMzcbYbwTGviOCd6L/S6iqiupoRmmujmz9YElETMmN6vLwylojz1aXN3K9QnNdqSp8s6uBnQ2RLsMhWYkc0T+DNKuJLZWt3LlwHZsrozdWjTpWBSbn9eXRqwZR1uLk/U3VePwxQIUpFk4amkO939IhAAUdxEBH1q9fz7vvvsvOnTsZN24cxx13HAMHDgyKmpUrV7JixQqmTZvG8OHDO4mdQN0cgLo2Fy0uLzXVVZx27AwEVJ77ajXrq0PfTxSgf0YCQ7KSqN6cQHm9F1umm/RCN7XtbvY2h67R4TlJnDQ0h+pWV7D+jUkSKUqzBcehqorWUVyRAQGpYMgvIsxVn4e2tx4OzgHmsTOxjNMCjtu372D91X/A29wc3D5t0kT6jRhGwt/+FqHs2vr1Y+eosbSGtUExZWQw/KEHSRoyGFVVcHz5WrCWj5TXj8QTLv7Zv5/OL4sucLpBFzi/LKqqINeW4921Ds/21ZFF7wQR44AxmMccgZScEfYelbZNm6n/6mta16+nbctWzJmZKF4vnvp6rH2KSRw4kJSxY8icPh1TRnrnz6yvwle2FV/Z1k7xMj1ByizAUDRYEzVp/3uiJhqq143cVAEdUoiFxHTE5OyorR9kRaXVpbV7CI/RkQSB9AQTdXYP3+ysp8ERsviIAowrSGVScRpWg8iC5aUsWF7Kjpr2TvsP4LWbUKqzuPrkEq46J5kVZc18vbM++HpukpmTh+bQ7Ai50LISzaRY981dc8stt/Dggw9y8cUX88QTTwRjctauXcvo0ZFNaR1OJw0uha++/JJrLjmf8RMn8dbCd/l6VxO7mxzRdh+TgZkJnDAkh9awZp7RBJviaEFp1ARDrDo4Pxe+ih3YP3vJb5ERsB17HsbCQQA49u5l+33307J6TegNoqi5mD0eVFnWhLUSmVGWPm0qA26+EUtODgCun77EvWYRAILJSuLsKzvdPOkc/OgCpxt0gXPgUN1O3Jt+wL1hKXjCanEIIsZ+IzWhE8Uyong8uOvqUH0+zNnZ0aufohUq82xZjnfvZlRn7EWwS4xmLWaoaDCGokGI1s7uF52ANacFpbWmU0FDISFNq58TpZmnrKg0OtwRGUugZQCl20xsr7fz3Z5GHGGVkkUBBmclMiY/haJUK06PTH2bi9pWN/VtbmpbXGzfqZKmZDC2XwpHHC4gqwpf7qhjXVXIKjIkO5HjBmVR3eoKZholW4xkJ+1bnyBVVfnmm2947LHHOPnkk7nkkksALW188ODBTJkyhSuuuILRo0eTmKhdT61t7Zx9zrl88uH73HH/Q5xz4SXkJlto88hsrG5jc21bRIxSRxJNEpP7pDM8J5HaNndE9/f8FCu2jq62uj2obs1KJmaWIFoS9uk79xTX2sW4V34BgGC2kjj7KsQkrVGnqqrUf/0Nux57HHd15yrX4dhKSuh3/R9IPzTUdsG7ewOOr1/XHggCtmPOx1g48Of5IjoHFF3gdIMucA48qseFe+MPuDd8Hyl0EDCUDMUy+gikzIL49iX78O7ZiGfzj1GDgEGLlzFkF4HRrLVGkAwIBpPWP8eSgGC2af1zLP7//wdiavYXqqJoKeXt9R1iJkSt7UNS9HopXlmhwe7u1MnbYpBIMBtYV93KirLmiJRn0Bb2gZmJpFmNJJoN2j9/heSqVheV/n817ZHZXIeVpDOpKJXqVldQDJgNIoWptp8tCLWhoYFbbrmFhQsXkpmZyZQpUygqKkKWZb766itWrlzJoCFDmf/h5yQlaXNRisVIstWIQRQoa3GxqaaNunY3qVYj2YlmchLNZCeasRhEGh2eYMViiB4kDaDYm1Ga/FZMg0nLoPqFA287upAEWxK2mWdhyOkT3EZ2uSl/7TXqv/oaxedDEEUtTV2SEC1msmbOIO/UuUFXtKqqeDb+gGv5p0GXqWXS8ZhHHPaLfjedXw5d4HSDLnB+PQSEjmfjD50aUUpZRUiZ+YipWUhp2Yip2QiSAaWtCaW9Wfu/tQHv7g2org4p35IRQ0F/DEWDMBYO+lUX2vutoPq8KK01qI5WIio5S0bElBwEa3LURdXplWlod+HyRbogzAYRs0Fic107aytbIyw6PUESBY4fnE1ekplmpyeowSRRoCjVhkH6ecWs0+lk8eLFvPzyy3z66ae0trZisVhITU3l0EMP5YEHH8Scmk272xtxfERBwGqUsBolzAYRWVXxyio+WcGrKDg9ckSWmkkSyU22YDJEugcjxA0gpuYiJkavEP1zo7qdtL//TKgTtyBimXgspuFTeiy4VLcT5/fvRxb0GzgW67S5etbUbxhd4HSDLnB+faheN54tK3Fv+K5zQHIPEFOzMQ2bhKn/mC67IOv8fKiyF6W1DtXeFPmC0YyYlBVV6Kiqit3jo9HuiXC1gLZwW4wSVW1udjTY2dXgiMi6ikWGzUh+spUhWQkIqBHWHKMkkJdiwxQmblRFBkH82RfHrVu3UlZWxvDhw8nzV/RTFIVWl48Gu7vHHaBEAdJsJlKsJsQOY+8oboSEdMTU3AMqABRHG45Fb2rFQf2IqdkY+w7HWDICMS075vhUnxdf2TY8O9fiK9vqD5jWMI+ahnn8UVHjv3R+O+gCpxt0gfPrRfV58WxfjWfD96G7vO4QRIwlwzANnYSUW6Lfvf1KUL0ulObqYNxHEMmImJKNYE3pkdABLSDZIAk4vApun4LTJ2P3yLS7ffhUlZwEM9mJJtKsRlSgzeWLSC8HzQWUnmBGEkOfraoKct1eBFFETC88YIukT1Zoc/tweWWcXh9KF7OyIGjxQ+k2E1KHWkSqqqK21WttNgLbJ6ZrmW6/gt+Hqsi4f/oK97pvO70mpmRi7DMMwZoIqoIq+0BRUNoa8ZZu6eDSBoxmbIefirFk38pD6Bwc6AKnG3SBc3CguBwozbXIzbUoTXUozbWoqoKYmIaYlIaYmKr9n5qNaIld1E/nwKGqKqqrHaWtrnPGldmGmJyDYO587lRVxeGRaXS4cfs6C51wREHAKGmLtsenxLSAJPnFgLGDS0qVfcgNpaHxmWxIWQdeKKuqikfWXFFeWUGSBIyiiEESMUoCkiBEHaPq82hFGT0hl++vSdyE4y3binvNos593OJAsCZqiQnDJ3eqqaXz20UXON2gCxwdnV8WVVW1WkNtdZ0tOiarFoxsSYq6AHtlBYfHh8Mj4/D6oha47YpEs4F0m9b5vCOKsw2luTKUBSaISJl9ooquXzuqqqLam7SeYWE1isTkLISkrF+duAlHsbfi3bMR756NyNV7IZZMNZow9hmGsf9oDPn9dHfU/yC6wOkGXeDo6Bw4NFFRFdH2AdCqIidmaDE6MSrsBiw7Lq+MV1HwKVrQbSDTyiAKmA0SZqMWoGwxiJ3cN+CvytxcHezLBIBkQMooRjD9/A1SZVlrFlpVpTXMnDYN/DUAUVUl7iw+rdO3XfvnsmsNNANIRsS0gl88HXxfUeyt+Kr3AKomYPz/BIMBKbMQwfjztpfQ+XXTm/X7t9EZUEdH51ePaE1CsCSgOlpR2urB5y/86PNowqe5CsGSpFl1OlhSBEEgwWwgwRw5ZamqigqdgmyjoThbUZqqQAmlpwvmBMT0gqh1e/Y3CxfSqf1CYSE8+ijMnQty3V6QPQgmm/bPbAOjWbMyyf5K3LI3KGyiISSkaZlrB6GFQ0xIxtR/1IEehs5vCF3g6Ojo/GIIgoiQkIpgS0F1taO2N0Qs1qqrDdnVprmvEjMQLIldLtaCINCVtFFlH6qzFcXREhGbgiBqWUW21F/EhRNooNnRXl5RoT3/0fsejh6jjU91tkb2/eoWQTteyVlRe4Pp6PyvogscHR2dXxxBEBCsSWBNQvU4URwtqI6WkHXF4wy2F8BoDlk1TBYQDZrrIkomFooMiozqcaI6W1BdnataC5ZExNR8rfDjL4Asa5abaMEAqqplRT37tI+jX7CB1xURRxMTg0mLXbIkasclijtOR+d/HV3g6OjoHFAEkxXJZEVNydbcV+31kX3LvG5Ur7tzfR1BBFEERE3YqN0UBDSYtH5ZMQoP/lwsWRLpluqIqsK7H9v4blNfjjhCBa9LE2geB6rPgyAawGDU3GgGk/a/0fKrDh7W0fk1oAscHR2dXwUR7iu3HdXZhupxaFaNaKgKRKmZE4FkRLAmI9pSDpgoqKqKfztB0NxNWsCzngKto7Mv6AJHR0fnV4UgCAiWRPDHk6iKjOpxgceB6nNrLihFBkXxW27UUMaN5M++kYyIliRNLBxgS4e/aPF+205HRyc+dIGjo6Pzq0YQJQRLAhxkac8Bpk3TsqUqKqLH4QiC9vq0ab/82HR0fsvokWk6Ojo6PyOSpKWCgyZmwgk8fuSRUD0cHR2d/YMucHR0dHR+ZubOhQULoKAg8vnCQu35uXMPzLh0dH7L6C4qHR0dnV+AuXNh9uzYlYx1dHT2L7rA0dHR0fmFkCSYPv1Aj0JH538D3UWlo6Ojo6Oj85tDFzg6Ojo6Ojo6vzl0gaOjo6Ojo6Pzm0MXODo6Ojo6Ojq/OXSBo6Ojo6Ojo/ObQxc4Ojo6Ojo6Or85dIGjo6Ojo6Oj85tDFzg6Ojo6Ojo6vzl0gaOjo6Ojo6Pzm0MXODo6Ojo6Ojq/OXSBo6Ojo6Ojo/ObQxc4Ojo6Ojo6Or85dIGjo6Ojo6Oj85tDFzg6Ojo6Ojo6vzl0gaOjo6Ojo6Pzm0MXODo6Ojo6Ojq/OXSBo6Ojo6Ojo/ObQxc4Ojo6Ojo6Or85dIGjo6Ojo6Oj85vjoBE4s2bNori4GIvFQl5eHueddx6VlZUHelg6Ojo6Ojo6v0IOGoEzY8YM3nzzTbZu3crbb7/Nzp07Oe200w70sHR0dHR0dHR+hQiqqqoHehC94f3332fOnDm43W6MRmNc72ltbSUlJYWWlhaSk5N/5hHq6Ojo6Ojo7A96s34fNBaccBobG3n11VeZMmVK3OJGR0dHR0dH53+Hg0rg3HrrrSQkJJCRkUFpaSnvvfdel9u73W5aW1sj/uno6Ojo6Oj89jmgAueOO+5AEIQu/61cuTK4/c0338zq1av5/PPPkSSJ888/n648bPfddx8pKSnBf0VFRb/E19LR0dHR0dE5wBzQGJz6+nrq6+u73KakpASLxdLp+fLycoqKili6dCmTJ0+O+l63243b7Q4+bm1tpaioSI/B0dHR0dHROYjoTQyO4WceU5dkZmaSmZnZq/cGdFm4gOmI2WzGbDb3av86Ojo6Ojo6By8HVODEy/Lly1m+fDlTp04lLS2NXbt28de//pX+/fvHtN5EIyCK9FgcHR0dHR2dg4fAut0Tp9NBIXCsVisLFy7k9ttvx263k5eXx3HHHcfrr7/eIwtNW1sbgB6Lo6Ojo6OjcxDS1tZGSkpKXNsetHVweoOiKFRWVpKUlIQgCPu8v0BMT1lZmR7T00P0Y9d79GPXe/Rj13v0Y9d79GPXewLHrrS0FEEQyM/PRxTjy486KCw4+wtRFCksLNzv+01OTtYv2l6iH7veox+73qMfu96jH7veox+73pOSktLjY3dQ1cHR0dHR0dHR0YkHXeDo6Ojo6Ojo/ObQBc4+YDabuf322/VU9F6gH7veox+73qMfu96jH7veox+73rMvx+5/KshYR0dHR0dH538D3YKjo6Ojo6Oj85tDFzg6Ojo6Ojo6vzl0gaOjo6Ojo6Pzm0MXODo6Ojo6Ojq/OXSBs5+YNWsWxcXFWCwW8vLyOO+886isrDzQw/rVs2fPHi655BL69u2L1Wqlf//+3H777Xg8ngM9tIOCe+65hylTpmCz2UhNTT3Qw/lV89RTT9G3b18sFgvjx49nyZIlB3pIBwXffvstJ598Mvn5+QiCwLvvvnugh3TQcN9993HIIYeQlJREdnY2c+bMYevWrQd6WAcFTz/9NKNGjQoWR5w8eTKffPJJj/ahC5z9xIwZM3jzzTfZunUrb7/9Njt37uS000470MP61bNlyxYUReFf//oXGzdu5OGHH+aZZ57htttuO9BDOyjweDycfvrpXHnllQd6KL9q3njjDa677jr+/Oc/s3r1aqZNm8bxxx9PaWnpgR7arx673c7o0aN54oknDvRQDjoWL17M73//e5YtW8YXX3yBz+fjmGOOwW63H+ih/eopLCzk73//OytXrmTlypXMnDmT2bNns3Hjxrj3oaeJ/0y8//77zJkzB7fbjdFoPNDDOah44IEHePrpp9m1a9eBHspBw4svvsh1111Hc3PzgR7Kr5JJkyYxbtw4nn766eBzQ4cOZc6cOdx3330HcGQHF4Ig8M477zBnzpwDPZSDkrq6OrKzs1m8eDGHH374gR7OQUd6ejoPPPAAl1xySVzb6xacn4HGxkZeffVVpkyZooubXtDS0kJ6evqBHobObwSPx8NPP/3EMcccE/H8Mcccw9KlSw/QqHT+F2lpaQHQ57ceIssyr7/+Ona7ncmTJ8f9Pl3g7EduvfVWEhISyMjIoLS0lPfee+9AD+mgY+fOnTz++ONcccUVB3ooOr8R6uvrkWWZnJyciOdzcnKorq4+QKPS+V9DVVVuuOEGpk6dyogRIw70cA4K1q9fT2JiImazmSuuuIJ33nmHYcOGxf1+XeB0wR133IEgCF3+W7lyZXD7m2++mdWrV/P5558jSRLnn38+/6sewJ4eO4DKykqOO+44Tj/9dC699NIDNPIDT2+OnU73CIIQ8VhV1U7P6ej8XFx99dWsW7eO+fPnH+ihHDQMHjyYNWvWsGzZMq688kouuOACNm3aFPf7DT/j2A56rr76as4666wutykpKQn+nZmZSWZmJoMGDWLo0KEUFRWxbNmyHpnUfiv09Nj9f3v3D9JGA4dx/LFCpKKIQcFBU4s6OAhiXCwOHgriVMShUw3oYEEOQRBEJKsIiliQYhGDg+ImOkktmLgUkuXQVYwEK6WCIAqiqPcOLw0Ufftq/90fvx/IkDvueDguycPvjtzh4aEMw1BTU5Pev3//h9O520OPHX6spKREubm5t6Y1X79+vTXVAf4E0zS1tramra0tlZeXOx3HMwKBgKqrqyVJjY2NSqVSmp6e1uzs7L22p+D8wLfC8jO+TW4uLi5+ZyTPeMix+/z5swzDUDgcViwW05Mnj3uw+CvnHW4LBAIKh8Pa2NhQZ2dndvnGxoZevnzpYDL4nW3bMk1TKysrisfjev78udORPM227Qf9plJwfoNkMqlkMqnm5mYVFxdrb29P0WhUVVVVj3J68xCHh4dqaWlRKBTSxMSEjo6OsuvKysocTOYNmUxGx8fHymQyur6+lmVZkqTq6moVFBQ4G85FBgcH9fr1azU2NmanhJlMhnu97uHs7Ey7u7vZ9+l0WpZlKRgMKhQKOZjM/fr7+7W0tKTV1VUVFhZmp4hFRUV6+vSpw+ncbWRkRB0dHaqoqNDp6amWl5cVj8e1vr5+/53Y+GXb29u2YRh2MBi08/Ly7MrKSvvNmzf2wcGB09FcLxaL2ZLufOH/RSKRO4/d5uam09FcZ2Zmxn727JkdCATshoYGO5FIOB3JEzY3N+88xyKRiNPRXO+/vttisZjT0Vyvp6cn+3ktLS21W1tb7Q8fPjxoH/wPDgAA8J3HfbMDAADwJQoOAADwHQoOAADwHQoOAADwHQoOAADwHQoOAADwHQoOAADwHQoOAADwHQoOAE+4vr7Wixcv1NXV9d3yk5MTVVRUaHR0VJI0MDCgcDisvLw81dfXO5AUgBtQcAB4Qm5urhYWFrS+vq7FxcXsctM0FQwGFY1GJf37QL6enh69evXKqagAXICHbQLwjJqaGo2Njck0TRmGoVQqpeXlZSWTSQUCAUnS27dvJUlHR0fa3t52Mi4AB1FwAHiKaZpaWVlRd3e3dnZ2FI1GuRQF4BYKDgBPycnJ0bt371RbW6u6ujoNDw87HQmAC3EPDgDPmZ+fV35+vtLptA4ODpyOA8CFKDgAPOXTp0+amprS6uqqmpqa1NvbK9u2nY4FwGUoOAA84/z8XJFIRH19fWpra9Pc3JxSqZRmZ2edjgbAZSg4ADxjeHhYNzc3Gh8flySFQiFNTk5qaGhI+/v7kqTd3V1ZlqUvX77o/PxclmXJsixdXl46mBzA35ZjM9sF4AGJREKtra2Kx+Nqbm7+bl17e7uurq708eNHGYahRCJxa/t0Oq3Kysq/lBaA0yg4AADAd7hEBQAAfIeCAwAAfIeCAwAAfIeCAwAAfIeCAwAAfIeCAwAAfIeCAwAAfIeCAwAAfIeCAwAAfIeCAwAAfIeCAwAAfIeCAwAAfOcfvWDAjkWjNQEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha = 0.0005 # 学习率，试着自己调整一下\n",
    "n_steps = 20000 # epoch迭代次数，试着自己调整一下\n",
    "\n",
    "w, ll_train, ll_test = fit_w(X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha)\n",
    "\n",
    "X_tilde_test_rbf = get_x_tilde(RBF(l, X_test, X_train))\n",
    "y_pred_prob = predict(X_tilde_test_rbf, w)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy:\", np.mean(y_test == y_pred))\n",
    "print(\"Classification Report (scikit-learn):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "plot_loss(-ll_train)\n",
    "plot_loss(-ll_test)\n",
    "plot_predictive_distribution(X, y, w, lambda x : RBF(l, x, X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (scikit-learn):\n",
      "[[96  7]\n",
      " [10 87]]\n",
      "Accuracy (scikit-learn): 0.915\n",
      "Classification Report (scikit-learn):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.93      0.92       103\n",
      "         1.0       0.93      0.90      0.91        97\n",
      "\n",
      "    accuracy                           0.92       200\n",
      "   macro avg       0.92      0.91      0.91       200\n",
      "weighted avg       0.92      0.92      0.91       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 调用逻辑斯谛回归的轮子，看看效果如何\n",
    "model = LogisticRegression() \n",
    "model.fit(X_tilde_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred_sklearn = model.predict(X_tilde_test)\n",
    "\n",
    "# 输出评价指标\n",
    "print(\"Confusion Matrix (scikit-learn):\")\n",
    "print(confusion_matrix(y_test, y_pred_sklearn))\n",
    "print(\"Accuracy (scikit-learn):\", accuracy_score(y_test, y_pred_sklearn))\n",
    "print(\"Classification Report (scikit-learn):\")\n",
    "print(classification_report(y_test, y_pred_sklearn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (scikit-learn):\n",
      "[[93 10]\n",
      " [14 83]]\n",
      "Accuracy (scikit-learn): 0.88\n",
      "Classification Report (scikit-learn):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.90      0.89       103\n",
      "         1.0       0.89      0.86      0.87        97\n",
      "\n",
      "    accuracy                           0.88       200\n",
      "   macro avg       0.88      0.88      0.88       200\n",
      "weighted avg       0.88      0.88      0.88       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 调用强大的分类器SVM试试\n",
    "model = SVC(kernel='rbf', C=50, gamma='scale') # TODO: 研究这里参数的含义，试着调整调整参数\n",
    "model.fit(X_tilde_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred_sklearn = model.predict(X_tilde_test)\n",
    "\n",
    "# 输出评价指标\n",
    "print(\"Confusion Matrix (scikit-learn):\")\n",
    "print(confusion_matrix(y_test, y_pred_sklearn))\n",
    "print(\"Accuracy (scikit-learn):\", accuracy_score(y_test, y_pred_sklearn))\n",
    "print(\"Classification Report (scikit-learn):\")\n",
    "print(classification_report(y_test, y_pred_sklearn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
