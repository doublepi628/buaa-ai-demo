{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 北航软件学院人工智能作业一演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入本次演示必要的库\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "def plot_data_internal(X, y):\n",
    "    x_min, x_max = X[ :, 0 ].min() - .5, X[ :, 0 ].max() + .5\n",
    "    y_min, y_max = X[ :, 1 ].min() - .5, X[ :, 1 ].max() + .5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    plt.figure()\n",
    "    plt.xlim(xx.min(None), xx.max(None))\n",
    "    plt.ylim(yy.min(None), yy.max(None))\n",
    "    ax = plt.gca()\n",
    "    ax.plot(X[y == 0, 0], X[y == 0, 1], 'ro', label = 'Class 1')\n",
    "    ax.plot(X[y == 1, 0], X[y == 1, 1], 'bo', label = 'Class 2')\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.title('Plot data')\n",
    "    plt.legend(loc = 'upper left', scatterpoints = 1, numpoints = 1)\n",
    "    return xx, yy\n",
    "\n",
    "# 预览数据\n",
    "def plot_data(X, y):\n",
    "    plot_data_internal(X, y)\n",
    "    plt.show()\n",
    "\n",
    "# 预览损失\n",
    "def plot_loss(ll):\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    plt.xlim(0, len(ll) + 2)\n",
    "    plt.ylim(min(ll) - 0.1, max(ll) + 0.1)\n",
    "    ax.plot(np.arange(1, len(ll) + 1), ll, 'r-')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average Cross Entropy Loss')\n",
    "    plt.title('Plot Cross Entropy Loss Curve')\n",
    "    plt.show()\n",
    "\n",
    "# 混淆矩阵\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    return np.array([[tn, fp],\n",
    "                     [fn, tp]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76069, 24128, 24490)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "txt_path = './data/txt_data.txt' # txt格式: 存储文本，可以直接预览\n",
    "npy_path = './data/npy_data.npy' # npy格式: 用于存储单个numpy数组的二进制格式\n",
    "npz_path = './data/npz_data.npz' # npz格式: 用于存储多个numpy数组的压缩格式\n",
    "\n",
    "read_txt = np.loadtxt(txt_path)\n",
    "read_np = np.load(npy_path)\n",
    "read_npz = np.load(npz_path)\n",
    "\n",
    "os.path.getsize(txt_path), os.path.getsize(npy_path), os.path.getsize(npz_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 预测任务：$f(x_1, x_2) → [p_0, p_1]$，其中$x_i$为点坐标，$p_j$为属于第j类的概率\n",
    "- 线性回归：$f(x_1, x_2) = \\sigma([w_1, w_2] \\cdot [x_1, x_2] ^ T +b)$，等价于$\\sigma([w_1, w_2, w_3] \\cdot [x_1, x_2, 1] ^ T)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.62294615, -0.57879556,  0.        ],\n",
       "       [ 1.09961316,  1.55006583,  0.        ],\n",
       "       [ 0.75478522, -1.97615881,  1.        ],\n",
       "       [-1.13634801, -0.0386258 ,  1.        ],\n",
       "       [-0.12266627, -1.85082517,  1.        ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取、检查数据\n",
    "\n",
    "data = read_txt\n",
    "\n",
    "# data = read_np # 这段代码的作用和上一段一样，替换下试试\n",
    "\n",
    "# data = np.concatenate([read_npz['x'], np.expand_dims(read_npz['y'], axis=1)], axis=1) # 这段代码的作用和上一段一样，替换下试试\n",
    "\n",
    "data[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.30700863,  0.33824781,  1.        ],\n",
       "       [ 0.48700455, -0.68155787,  0.        ],\n",
       "       [-1.56096698, -1.82222131,  0.        ],\n",
       "       [-1.57587088, -1.17730481,  0.        ],\n",
       "       [ 0.26944715,  0.00781522,  0.        ]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成随机排列\n",
    "permutation = np.random.permutation(data.shape[0])\n",
    "data = data[permutation]\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data(data[ :, :2], data[ :, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 2), (200, 2), (800,), (200,))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 划分训练集、测试集\n",
    "n_train = 800\n",
    "X = data[ :, :2] # 1000 x 2\n",
    "y = data[ :, 2 ] # 1000\n",
    "X_train = X[ 0 : n_train, : ] # 800 x 2\n",
    "X_test = X[ n_train :, : ] # 200 x 2\n",
    "y_train = y[ 0 : n_train ] # 800\n",
    "y_test = y[ n_train : ] # 200\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手工实现logistic回归\n",
    "\n",
    "- logistic 函数：$f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- 预测函数（简单的线性变换）：$X\\in\\mathbb{R}^{N_{train} \\times 2}, w \\in\\mathbb{R}^{2}, Xw \\in\\mathbb{R}^{N_{train}}$\n",
    "- 对数似然：$\\text{LogLikelihood} = y \\text{log}(\\hat{y}) + (1-y) \\text{log}(1-\\hat{y})$\n",
    "    - 当$y = 0$，有$\\text{Loss} = \\text{log}(1 - \\hat{y})$\n",
    "    - 当$y = 1$，有$\\text{Loss} = \\text{log}(\\hat{y})$\n",
    "- 交叉熵损失：$\\text{Loss} = - [y \\text{log}(\\hat{y}) + (1-y) \\text{log}(1-\\hat{y})]$\n",
    "    - 当$y = 0$，有$\\text{Loss} = - \\text{log}(1 - \\hat{y})$\n",
    "    - 当$y = 1$，有$\\text{Loss} = - \\text{log}(\\hat{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression 的手工实现\n",
    "\n",
    "# logistic 函数\n",
    "def logistic(x): return 1.0 / (1.0 + np.exp(-x)) # N -> N\n",
    "\n",
    "# 预测函数\n",
    "def predict(X_tilde, w): return logistic(np.dot(X_tilde, w)) # N x (D + 1) -> N\n",
    "\n",
    "# 计算平均对数似然\n",
    "def compute_average_ll(X_tilde, y, w):\n",
    "    output_prob = predict(X_tilde, w)\n",
    "    return np.mean(y * np.log(output_prob) + (1 - y) * np.log(1.0 - output_prob)) \n",
    "\n",
    "# 添加偏置项\n",
    "def get_x_tilde(X): return np.concatenate((np.ones((X.shape[ 0 ], 1 )), X), 1)\n",
    "\n",
    "# 梯度下降函数\n",
    "# TODO: 试试把一层的logistic regression改成多层的logistic regression，观察效果是否会提升\n",
    "def fit_w(X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha):\n",
    "    w = np.random.randn(X_tilde_train.shape[ 1 ]) \n",
    "    ll_train = np.zeros(n_steps)\n",
    "    ll_test = np.zeros(n_steps)\n",
    "    for i in range(n_steps):\n",
    "        sigmoid_value = predict(X_tilde_train, w)\n",
    "\n",
    "        w = w + alpha * np.dot(X_tilde_train.T, (y_train - sigmoid_value)) # TODO: 试试加入L2正则化，观察效果是否会提升\n",
    "\n",
    "        ll_train[ i ] = compute_average_ll(X_tilde_train, y_train, w)\n",
    "        ll_test[ i ] = compute_average_ll(X_tilde_test, y_test, w)\n",
    "        print(f'epoch {i+1}: train loss: {-ll_train[ i ]}, test loss: {-ll_test[ i ]}')\n",
    "\n",
    "    return w, ll_train, ll_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss: 1.4979700848457584, test loss: 1.637620957340315\n",
      "epoch 2: train loss: 1.4213832772625787, test loss: 1.5538535497963482\n",
      "epoch 3: train loss: 1.3478134924545802, test loss: 1.4733577784759821\n",
      "epoch 4: train loss: 1.2776004766217677, test loss: 1.3964962042744724\n",
      "epoch 5: train loss: 1.211069242566972, test loss: 1.323614465684891\n",
      "epoch 6: train loss: 1.1485131828908106, test loss: 1.255023458705611\n",
      "epoch 7: train loss: 1.0901764758476609, test loss: 1.190980991893557\n",
      "epoch 8: train loss: 1.0362377806756193, test loss: 1.1316750190362177\n",
      "epoch 9: train loss: 0.9867974257177604, test loss: 1.0772107077513147\n",
      "epoch 10: train loss: 0.9418700328526199, test loss: 1.0276032721351078\n",
      "epoch 11: train loss: 0.9013837779464319, test loss: 0.9827776873472138\n",
      "epoch 12: train loss: 0.8651864146525784, test loss: 0.9425752936472098\n",
      "epoch 13: train loss: 0.8330570753155325, test loss: 0.906766186555272\n",
      "epoch 14: train loss: 0.8047220147733383, test loss: 0.8750654787736326\n",
      "epoch 15: train loss: 0.779872082046217, test loss: 0.8471511893222358\n",
      "epoch 16: train loss: 0.7581798196489955, test loss: 0.8226816723066466\n",
      "epoch 17: train loss: 0.7393145741138025, test loss: 0.8013110027242303\n",
      "epoch 18: train loss: 0.722954652537359, test loss: 0.782701387962667\n",
      "epoch 19: train loss: 0.7087961896473576, test loss: 0.7665322905298853\n",
      "epoch 20: train loss: 0.6965588766957955, test loss: 0.7525064186383201\n",
      "epoch 21: train loss: 0.6859890044560154, test loss: 0.7403530303255634\n",
      "epoch 22: train loss: 0.6768604034555914, test loss: 0.7298291219705646\n",
      "epoch 23: train loss: 0.6689738704723505, test loss: 0.7207190777494644\n",
      "epoch 24: train loss: 0.6621556015932853, test loss: 0.7128332909862065\n",
      "epoch 25: train loss: 0.6562550504445354, test loss: 0.706006171041449\n",
      "epoch 26: train loss: 0.6511425238526507, test loss: 0.7000938471929374\n",
      "epoch 27: train loss: 0.6467067320815661, test loss: 0.6949717890418208\n",
      "epoch 28: train loss: 0.6428524336044518, test loss: 0.6905324878380324\n",
      "epoch 29: train loss: 0.6394982560726309, test loss: 0.6866832858320965\n",
      "epoch 30: train loss: 0.6365747336938358, test loss: 0.6833443995382601\n",
      "epoch 31: train loss: 0.634022573420137, test loss: 0.6804471546891263\n",
      "epoch 32: train loss: 0.6317911448507418, test loss: 0.677932432576707\n",
      "epoch 33: train loss: 0.6298371786548966, test loss: 0.6757493166528599\n",
      "epoch 34: train loss: 0.6281236532717954, test loss: 0.6738539224703841\n",
      "epoch 35: train loss: 0.6266188478823213, test loss: 0.6722083915857472\n",
      "epoch 36: train loss: 0.6252955399045197, test loss: 0.6707800296829542\n",
      "epoch 37: train loss: 0.6241303266683812, test loss: 0.6695405700562037\n",
      "epoch 38: train loss: 0.6231030528983265, test loss: 0.668465545129425\n",
      "epoch 39: train loss: 0.6221963278075946, test loss: 0.6675337505203324\n",
      "epoch 40: train loss: 0.6213951177673329, test loss: 0.6667267880444957\n",
      "epoch 41: train loss: 0.6206864025318827, test loss: 0.6660286758668196\n",
      "epoch 42: train loss: 0.620058884820789, test loss: 0.665425515672682\n",
      "epoch 43: train loss: 0.6195027446567397, test loss: 0.6649052082178705\n",
      "epoch 44: train loss: 0.6190094312392665, test loss: 0.6644572099187654\n",
      "epoch 45: train loss: 0.6185714863115275, test loss: 0.6640723242694061\n",
      "epoch 46: train loss: 0.6181823939726604, test loss: 0.6637425228347463\n",
      "epoch 47: train loss: 0.617836452723799, test loss: 0.6634607913874153\n",
      "epoch 48: train loss: 0.6175286662342164, test loss: 0.6632209974469748\n",
      "epoch 49: train loss: 0.617254649895894, test loss: 0.6630177760636486\n",
      "epoch 50: train loss: 0.6170105507185298, test loss: 0.6628464311788109\n",
      "Confusion Matrix:\n",
      "[[75 19]\n",
      " [47 59]]\n",
      "Accuracy: 0.67\n",
      "Classification Report (scikit-learn):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.80      0.69        94\n",
      "         1.0       0.76      0.56      0.64       106\n",
      "\n",
      "    accuracy                           0.67       200\n",
      "   macro avg       0.69      0.68      0.67       200\n",
      "weighted avg       0.69      0.67      0.67       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 预览决策边界\n",
    "def plot_predictive_distribution(X, y, w, map_inputs = lambda x : x):\n",
    "    xx, yy = plot_data_internal(X, y)\n",
    "    ax = plt.gca()\n",
    "    X_tilde = get_x_tilde(map_inputs(np.concatenate((xx.ravel().reshape((-1, 1)), yy.ravel().reshape((-1, 1))), 1)))\n",
    "    Z = predict(X_tilde, w)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs2 = ax.contour(xx, yy, Z, cmap = 'RdBu', linewidths = 2)\n",
    "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize = 14)\n",
    "    plt.show()\n",
    "\n",
    "alpha = 0.0005 # 学习率，试着自己调整一下\n",
    "n_steps = 50 # epoch迭代次数，试着自己调整一下\n",
    "\n",
    "X_tilde_train = get_x_tilde(X_train)\n",
    "X_tilde_test = get_x_tilde(X_test)\n",
    "w, ll_train, ll_test = fit_w(X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha)\n",
    "\n",
    "X_tilde_test = get_x_tilde(X_test)\n",
    "y_pred_prob = predict(X_tilde_test, w)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy:\", np.mean(y_test == y_pred))\n",
    "# 阅读文档，理解报告里每个指标的含义\n",
    "# https://blog.csdn.net/weixin_62528784/article/details/145379239\n",
    "print(\"Classification Report (scikit-learn):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "plot_loss(-ll_train)\n",
    "plot_loss(-ll_test)\n",
    "plot_predictive_distribution(X, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://s2.loli.net/2025/03/03/QOCGngtfVIM9D6k.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  1.35006783,  2.88835326],\n",
       "        [ 1.        ,  0.83767211, -0.95037442],\n",
       "        [ 1.        ,  2.39939751, -2.2791261 ],\n",
       "        [ 1.        ,  1.96708303, -2.49996167],\n",
       "        [ 1.        ,  0.26956047,  0.02899653]]),\n",
       " array([[ 1.        ,  1.38924382,  2.97721543],\n",
       "        [ 1.        ,  2.21909599, -1.77591382],\n",
       "        [ 1.        ,  1.12729181,  2.973256  ],\n",
       "        [ 1.        ,  0.54871665, -3.05954427],\n",
       "        [ 1.        ,  1.28142947, -2.96677889]]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 核函数：用于扩充数据维度，常常用于SVM\n",
    "# polar: 极坐标核，用来把直角坐标点转换为极坐标点\n",
    "def polar(coords):\n",
    "    x = coords[ :, 0 ]\n",
    "    y = coords[ :, 1 ]\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    theta = np.arctan2(y, x)\n",
    "    return np.column_stack((r, theta))\n",
    "\n",
    "# 极坐标绘图函数\n",
    "def polar_draw(a, b, c):\n",
    "    theta = np.linspace(0, 10 * np.pi, 1000)\n",
    "    # 计算 r 值\n",
    "    r = - (a / b) * theta - (c / b)\n",
    "    # 过滤掉负半径的部分（因为 r 必须是非负数）\n",
    "    valid_idx = r >= 0\n",
    "    r = r[valid_idx]\n",
    "    theta = theta[valid_idx]\n",
    "    # 转换为笛卡尔坐标\n",
    "    x = r * np.cos(theta)\n",
    "    y = r * np.sin(theta)\n",
    "    # 画图\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.legend()\n",
    "    plt.axis(\"equal\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "X_tilde_train = get_x_tilde(polar(X_train))\n",
    "X_tilde_test = get_x_tilde(polar(X_test))\n",
    "X_tilde_train[:5], X_tilde_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss: 1.8260131907387773, test loss: 1.8149399952165606\n",
      "epoch 2: train loss: 1.6123611522762333, test loss: 1.6196876081214848\n",
      "epoch 3: train loss: 1.4133316287382403, test loss: 1.4361584849635785\n",
      "epoch 4: train loss: 1.2318603746323769, test loss: 1.2671819017068766\n",
      "epoch 5: train loss: 1.0719731793671061, test loss: 1.1167497145619136\n",
      "epoch 6: train loss: 0.9381003017572753, test loss: 0.9893450127825386\n",
      "epoch 7: train loss: 0.8333563921112151, test loss: 0.8882593488119451\n",
      "epoch 8: train loss: 0.7574457398987581, test loss: 0.8135823557801222\n",
      "epoch 9: train loss: 0.7061182715986861, test loss: 0.7616954563815709\n",
      "epoch 10: train loss: 0.6730221890133388, test loss: 0.726986386699605\n",
      "epoch 11: train loss: 0.6521481061899528, test loss: 0.7040730858696103\n",
      "epoch 12: train loss: 0.6390230358271373, test loss: 0.6888880005341679\n",
      "epoch 13: train loss: 0.6307076015715986, test loss: 0.6786978007903284\n",
      "epoch 14: train loss: 0.6253738270968896, test loss: 0.6717507341758505\n",
      "epoch 15: train loss: 0.6219042027333899, test loss: 0.6669358953811817\n",
      "epoch 16: train loss: 0.619614467988418, test loss: 0.6635441651041041\n",
      "epoch 17: train loss: 0.6180814560642254, test loss: 0.6611167884132232\n",
      "epoch 18: train loss: 0.6170400047513871, test loss: 0.6593523604239431\n",
      "epoch 19: train loss: 0.6163216442737379, test loss: 0.6580497923817532\n",
      "epoch 20: train loss: 0.6158179023272985, test loss: 0.6570729785535179\n",
      "epoch 21: train loss: 0.6154580762009596, test loss: 0.6563285754217781\n",
      "epoch 22: train loss: 0.6151955738827588, test loss: 0.6557517960418299\n",
      "epoch 23: train loss: 0.6149993950776289, test loss: 0.6552971806124037\n",
      "epoch 24: train loss: 0.6148487406212299, test loss: 0.654932508704648\n",
      "epoch 25: train loss: 0.6147295549861022, test loss: 0.6546347279933069\n",
      "epoch 26: train loss: 0.6146322810980351, test loss: 0.654387197626638\n",
      "epoch 27: train loss: 0.6145503861638426, test loss: 0.6541778009902108\n",
      "epoch 28: train loss: 0.6144793842390054, test loss: 0.6539976408552461\n",
      "epoch 29: train loss: 0.6144161826069556, test loss: 0.6538401291439215\n",
      "epoch 30: train loss: 0.6143586414651927, test loss: 0.6537003468112796\n",
      "epoch 31: train loss: 0.6143052754281366, test loss: 0.6535745902974303\n",
      "epoch 32: train loss: 0.6142550500793077, test loss: 0.6534600478875054\n",
      "epoch 33: train loss: 0.6142072426734656, test loss: 0.6533545671880263\n",
      "epoch 34: train loss: 0.6141613463940659, test loss: 0.6532564869423051\n",
      "epoch 35: train loss: 0.6141170043343539, test loss: 0.6531645145642159\n",
      "epoch 36: train loss: 0.6140739638508992, test loss: 0.6530776363558874\n",
      "epoch 37: train loss: 0.6140320449314793, test loss: 0.6529950512300268\n",
      "epoch 38: train loss: 0.6139911182332465, test loss: 0.6529161214360993\n",
      "epoch 39: train loss: 0.6139510898108965, test loss: 0.6528403356618209\n",
      "epoch 40: train loss: 0.613911890483038, test loss: 0.6527672811972258\n",
      "epoch 41: train loss: 0.613873468420024, test loss: 0.6526966227779641\n",
      "epoch 42: train loss: 0.6138357839725824, test loss: 0.6526280863840601\n",
      "epoch 43: train loss: 0.6137988060610398, test loss: 0.6525614467405938\n",
      "epoch 44: train loss: 0.6137625096525129, test loss: 0.6524965176035419\n",
      "epoch 45: train loss: 0.6137268739972119, test loss: 0.6524331441562962\n",
      "epoch 46: train loss: 0.6136918813947594, test loss: 0.6523711970175367\n",
      "epoch 47: train loss: 0.6136575163307738, test loss: 0.6523105674883578\n",
      "epoch 48: train loss: 0.6136237648722341, test loss: 0.6522511637594758\n",
      "epoch 49: train loss: 0.613590614243767, test loss: 0.6521929078675714\n",
      "epoch 50: train loss: 0.6135580525304678, test loss: 0.6521357332402217\n",
      "Confusion Matrix:\n",
      "[[72 22]\n",
      " [46 60]]\n",
      "Accuracy: 0.66\n",
      "Classification Report (scikit-learn):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.77      0.68        94\n",
      "         1.0       0.73      0.57      0.64       106\n",
      "\n",
      "    accuracy                           0.66       200\n",
      "   macro avg       0.67      0.67      0.66       200\n",
      "weighted avg       0.67      0.66      0.66       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/882dxcks2c78wz1dfm9f0ghm0000gn/T/ipykernel_13468/4045588789.py:29: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha = 0.0005 # 学习率，试着自己调整一下\n",
    "n_steps = 50 # epoch迭代次数，试着自己调整一下\n",
    "\n",
    "# 这里使用极坐标做线性回归，实际上是用阿基米德螺旋线做分类\n",
    "w, ll_train, ll_test = fit_w(X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha)\n",
    "\n",
    "X_tilde_test_polar = get_x_tilde(polar(X_test))\n",
    "y_pred_prob = predict(X_tilde_test_polar, w)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy:\", np.mean(y_test == y_pred))\n",
    "print(\"Classification Report (scikit-learn):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "plot_loss(-ll_train)\n",
    "plot_loss(-ll_test)\n",
    "plot_predictive_distribution(X, y, w, polar)\n",
    "\n",
    "c, b, a = w\n",
    "polar_draw(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.00000000e+000, 1.00000000e+000, 3.37389630e-093, ...,\n",
       "         3.70065937e-142, 2.32903834e-083, 3.07639456e-173],\n",
       "        [1.00000000e+000, 3.37389630e-093, 1.00000000e+000, ...,\n",
       "         1.59955546e-009, 3.82545186e-042, 1.58190296e-047],\n",
       "        [1.00000000e+000, 1.75098806e-103, 4.68885461e-120, ...,\n",
       "         7.37602205e-192, 1.21152343e-240, 7.60511601e-063],\n",
       "        [1.00000000e+000, 3.57902968e-052, 1.80820744e-098, ...,\n",
       "         7.21471014e-166, 1.30103430e-180, 9.82429941e-081],\n",
       "        [1.00000000e+000, 4.60629400e-057, 4.49379273e-012, ...,\n",
       "         1.57491691e-021, 1.75461198e-013, 9.52954068e-098]]),\n",
       " array([[1.00000000e+000, 4.41855390e-001, 1.37283715e-093, ...,\n",
       "         4.58687131e-145, 1.02075508e-090, 9.45440636e-166],\n",
       "        [1.00000000e+000, 1.69993443e-153, 3.79347768e-068, ...,\n",
       "         4.87475523e-113, 1.08555303e-203, 1.22525508e-007],\n",
       "        [1.00000000e+000, 4.83318488e-002, 1.18044997e-072, ...,\n",
       "         1.61548448e-117, 1.10055942e-070, 1.01689325e-146],\n",
       "        [1.00000000e+000, 1.83685291e-016, 9.75959153e-033, ...,\n",
       "         2.57759704e-064, 1.07334270e-041, 1.05310635e-101],\n",
       "        [1.00000000e+000, 1.31464087e-007, 1.03119011e-071, ...,\n",
       "         1.19434854e-123, 3.76712938e-095, 6.15603439e-118]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 核函数：用于扩充数据维度，常常用于SVM\n",
    "# Radial Basis Function (RBF)： 定义为空间中任一点x到某一中心xc之间欧氏距离的单调函数，若两点距离很近则接近1，如果距离很远就接近0\n",
    "# TODO: 尝试使用其他的核函数，观察效果是否会提升\n",
    "def RBF(l, X, Z):\n",
    "    X2 = np.sum(X**2, 1)\n",
    "    Z2 = np.sum(Z**2, 1)\n",
    "    ones_Z = np.ones(Z.shape[ 0 ])\n",
    "    ones_X = np.ones(X.shape[ 0 ])\n",
    "    r2 = np.outer(X2, ones_Z) - 2 * np.dot(X, Z.T) + np.outer(ones_X, Z2)\n",
    "    return np.exp(-0.5 / l**2 * r2)\n",
    "\n",
    "l = 0.1 # 高斯核函数的宽度，试着自己调整一下？\n",
    "\n",
    "X_tilde_train = get_x_tilde(RBF(l, X_train, X_train))\n",
    "X_tilde_test = get_x_tilde(RBF(l, X_test, X_train))\n",
    "\n",
    "X_tilde_train[:5], X_tilde_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss: 0.9951607570239465, test loss: 1.033215990598017\n",
      "epoch 2: train loss: 0.9849715164191305, test loss: 1.0242787015653305\n",
      "epoch 3: train loss: 0.9755295559922996, test loss: 1.0160610404135437\n",
      "epoch 4: train loss: 0.9667500961144734, test loss: 1.008478036253276\n",
      "epoch 5: train loss: 0.9585574750147714, test loss: 1.0014537948763371\n",
      "epoch 6: train loss: 0.9508843803896885, test loss: 0.9949207914825342\n",
      "epoch 7: train loss: 0.9436710947089386, test loss: 0.9888191567469592\n",
      "epoch 8: train loss: 0.93686476880124, test loss: 0.9830959753615389\n",
      "epoch 9: train loss: 0.9304187337038468, test loss: 0.9777046108846398\n",
      "epoch 10: train loss: 0.9242918570758556, test loss: 0.9726040663719602\n",
      "epoch 11: train loss: 0.9184479475976371, test loss: 0.9677583867660874\n",
      "epoch 12: train loss: 0.9128552085888684, test loss: 0.9631361062899747\n",
      "epoch 13: train loss: 0.9074857404595792, test loss: 0.958709742010598\n",
      "epoch 14: train loss: 0.9023150904552384, test loss: 0.954455333203789\n",
      "epoch 15: train loss: 0.897321847373275, test loss: 0.9503520250580846\n",
      "epoch 16: train loss: 0.892487278433074, test loss: 0.9463816945140959\n",
      "epoch 17: train loss: 0.887795005206226, test loss: 0.9425286155687277\n",
      "epoch 18: train loss: 0.8832307154027954, test loss: 0.9387791611155197\n",
      "epoch 19: train loss: 0.8787819073177018, test loss: 0.9351215382901795\n",
      "epoch 20: train loss: 0.874437663833519, test loss: 0.9315455543011703\n",
      "epoch 21: train loss: 0.8701884530245456, test loss: 0.9280424098152679\n",
      "epoch 22: train loss: 0.8660259525908252, test loss: 0.9246045171110514\n",
      "epoch 23: train loss: 0.8619428955541618, test loss: 0.9212253403893753\n",
      "epoch 24: train loss: 0.857932934859457, test loss: 0.9178992558240163\n",
      "epoch 25: train loss: 0.8539905247356286, test loss: 0.9146214291369553\n",
      "epoch 26: train loss: 0.8501108168751815, test loss: 0.9113877086832863\n",
      "epoch 27: train loss: 0.8462895696863888, test loss: 0.9081945322251812\n",
      "epoch 28: train loss: 0.8425230690546429, test loss: 0.9050388457590592\n",
      "epoch 29: train loss: 0.8388080592185904, test loss: 0.9019180329329893\n",
      "epoch 30: train loss: 0.8351416825216401, test loss: 0.8988298537511773\n",
      "epoch 31: train loss: 0.8315214269404541, test loss: 0.8957723914087729\n",
      "epoch 32: train loss: 0.8279450804194465, test loss: 0.8927440062332269\n",
      "epoch 33: train loss: 0.8244106911548958, test loss: 0.8897432958285109\n",
      "epoch 34: train loss: 0.8209165330747801, test loss: 0.8867690606263295\n",
      "epoch 35: train loss: 0.8174610758518418, test loss: 0.8838202741448455\n",
      "epoch 36: train loss: 0.8140429588685774, test loss: 0.8808960573412378\n",
      "epoch 37: train loss: 0.8106609686247783, test loss: 0.8779956565205955\n",
      "epoch 38: train loss: 0.8073140191418264, test loss: 0.87511842433102\n",
      "epoch 39: train loss: 0.8040011349739958, test loss: 0.8722638034343091\n",
      "epoch 40: train loss: 0.8007214364863624, test loss: 0.8694313124939853\n",
      "epoch 41: train loss: 0.7974741271022705, test loss: 0.8666205341684833\n",
      "epoch 42: train loss: 0.7942584822613555, test loss: 0.8638311048377295\n",
      "epoch 43: train loss: 0.7910738398624483, test loss: 0.8610627058267601\n",
      "epoch 44: train loss: 0.7879195919948669, test loss: 0.8583150559210037\n",
      "epoch 45: train loss: 0.7847951777871117, test loss: 0.8555879049949402\n",
      "epoch 46: train loss: 0.7817000772242559, test loss: 0.8528810285994801\n",
      "epoch 47: train loss: 0.7786338058047818, test loss: 0.8501942233740237\n",
      "epoch 48: train loss: 0.7755959099245604, test loss: 0.8475273031671126\n",
      "epoch 49: train loss: 0.7725859628904612, test loss: 0.8448800957652238\n",
      "epoch 50: train loss: 0.7696035614789487, test loss: 0.8422524401428477\n",
      "epoch 51: train loss: 0.7666483229662208, test loss: 0.8396441841587982\n",
      "epoch 52: train loss: 0.7637198825661975, test loss: 0.837055182633974\n",
      "epoch 53: train loss: 0.7608178912211372, test loss: 0.834485295754684\n",
      "epoch 54: train loss: 0.7579420136970224, test loss: 0.8319343877533691\n",
      "epoch 55: train loss: 0.7550919269422548, test loss: 0.8294023258252481\n",
      "epoch 56: train loss: 0.7522673186737524, test loss: 0.8268889792452001\n",
      "epoch 57: train loss: 0.7494678861593619, test loss: 0.8243942186542199\n",
      "epoch 58: train loss: 0.7466933351696828, test loss: 0.8219179154891079\n",
      "epoch 59: train loss: 0.7439433790760256, test loss: 0.8194599415328111\n",
      "epoch 60: train loss: 0.7412177380743717, test loss: 0.8170201685660677\n",
      "epoch 61: train loss: 0.7385161385179287, test loss: 0.814598468103799\n",
      "epoch 62: train loss: 0.7358383123432342, test loss: 0.8121947112021036\n",
      "epoch 63: train loss: 0.7331839965768109, test loss: 0.8098087683237877\n",
      "epoch 64: train loss: 0.730552932911139, test loss: 0.807440509252149\n",
      "epoch 65: train loss: 0.7279448673402535, test loss: 0.8050898030442791\n",
      "epoch 66: train loss: 0.7253595498465947, test loss: 0.8027565180164609\n",
      "epoch 67: train loss: 0.7227967341318902, test loss: 0.8004405217553893\n",
      "epoch 68: train loss: 0.7202561773858374, test loss: 0.7981416811499051\n",
      "epoch 69: train loss: 0.7177376400872181, test loss: 0.7958598624387782\n",
      "epoch 70: train loss: 0.715240885832811, test loss: 0.7935949312707864\n",
      "epoch 71: train loss: 0.7127656811901174, test loss: 0.7913467527739567\n",
      "epoch 72: train loss: 0.7103117955704613, test loss: 0.7891151916313481\n",
      "epoch 73: train loss: 0.7078790011195114, test loss: 0.7869001121612126\n",
      "epoch 74: train loss: 0.7054670726226772, test loss: 0.7847013783997465\n",
      "epoch 75: train loss: 0.7030757874231921, test loss: 0.7825188541849645\n",
      "epoch 76: train loss: 0.7007049253510078, test loss: 0.7803524032405106\n",
      "epoch 77: train loss: 0.6983542686608785, test loss: 0.7782018892584484\n",
      "epoch 78: train loss: 0.6960236019782545, test loss: 0.7760671759802662\n",
      "epoch 79: train loss: 0.6937127122517939, test loss: 0.7739481272755069\n",
      "epoch 80: train loss: 0.6914213887114741, test loss: 0.7718446072175624\n",
      "epoch 81: train loss: 0.6891494228314303, test loss: 0.7697564801562958\n",
      "epoch 82: train loss: 0.6868966082967758, test loss: 0.7676836107872448\n",
      "epoch 83: train loss: 0.6846627409737636, test loss: 0.7656258642172483\n",
      "epoch 84: train loss: 0.682447618882749, test loss: 0.7635831060263986\n",
      "epoch 85: train loss: 0.680251042173485, test loss: 0.7615552023262763\n",
      "epoch 86: train loss: 0.6780728131023573, test loss: 0.7595420198144758\n",
      "epoch 87: train loss: 0.6759127360112228, test loss: 0.7575434258254563\n",
      "epoch 88: train loss: 0.6737706173075663, test loss: 0.7555592883777877\n",
      "epoch 89: train loss: 0.6716462654457348, test loss: 0.7535894762178833\n",
      "epoch 90: train loss: 0.6695394909090456, test loss: 0.7516338588603237\n",
      "epoch 91: train loss: 0.6674501061925945, test loss: 0.7496923066248949\n",
      "epoch 92: train loss: 0.6653779257866244, test loss: 0.7477646906704706\n",
      "epoch 93: train loss: 0.6633227661603286, test loss: 0.7458508830258725\n",
      "epoch 94: train loss: 0.661284445745992, test loss: 0.7439507566178537\n",
      "epoch 95: train loss: 0.6592627849233866, test loss: 0.7420641852963411\n",
      "epoch 96: train loss: 0.6572576060043487, test loss: 0.7401910438570845\n",
      "epoch 97: train loss: 0.6552687332174887, test loss: 0.7383312080618535\n",
      "epoch 98: train loss: 0.65329599269298, test loss: 0.7364845546563156\n",
      "epoch 99: train loss: 0.6513392124473983, test loss: 0.7346509613857387\n",
      "epoch 100: train loss: 0.6493982223685751, test loss: 0.7328303070086466\n",
      "epoch 101: train loss: 0.6474728542004483, test loss: 0.7310224713085559\n",
      "epoch 102: train loss: 0.6455629415278905, test loss: 0.7292273351039181\n",
      "epoch 103: train loss: 0.6436683197615022, test loss: 0.7274447802563861\n",
      "epoch 104: train loss: 0.6417888261223642, test loss: 0.7256746896775184\n",
      "epoch 105: train loss: 0.6399242996267388, test loss: 0.7239169473340289\n",
      "epoch 106: train loss: 0.638074581070722, test loss: 0.7221714382516861\n",
      "epoch 107: train loss: 0.6362395130148435, test loss: 0.7204380485179623\n",
      "epoch 108: train loss: 0.6344189397686164, test loss: 0.7187166652835212\n",
      "epoch 109: train loss: 0.6326127073750384, test loss: 0.7170071767626364\n",
      "epoch 110: train loss: 0.6308206635950505, test loss: 0.715309472232623\n",
      "epoch 111: train loss: 0.6290426578919559, test loss: 0.7136234420323597\n",
      "epoch 112: train loss: 0.6272785414158052, test loss: 0.7119489775599789\n",
      "epoch 113: train loss: 0.6255281669877546, test loss: 0.7102859712697893\n",
      "epoch 114: train loss: 0.6237913890844032, test loss: 0.7086343166685034\n",
      "epoch 115: train loss: 0.6220680638221161, test loss: 0.7069939083108245\n",
      "epoch 116: train loss: 0.620358048941339, test loss: 0.7053646417944577\n",
      "epoch 117: train loss: 0.6186612037909149, test loss: 0.7037464137545913\n",
      "epoch 118: train loss: 0.6169773893124058, test loss: 0.7021391218579073\n",
      "epoch 119: train loss: 0.6153064680244278, test loss: 0.7005426647961621\n",
      "epoch 120: train loss: 0.6136483040070081, test loss: 0.6989569422793839\n",
      "epoch 121: train loss: 0.6120027628859694, test loss: 0.6973818550287265\n",
      "epoch 122: train loss: 0.6103697118173459, test loss: 0.6958173047690203\n",
      "epoch 123: train loss: 0.6087490194718416, test loss: 0.6942631942210526\n",
      "epoch 124: train loss: 0.6071405560193353, test loss: 0.692719427093613\n",
      "epoch 125: train loss: 0.6055441931134353, test loss: 0.6911859080753315\n",
      "epoch 126: train loss: 0.6039598038760956, test loss: 0.689662542826341\n",
      "epoch 127: train loss: 0.6023872628822908, test loss: 0.6881492379697869\n",
      "epoch 128: train loss: 0.6008264461447645, test loss: 0.6866459010832117\n",
      "epoch 129: train loss: 0.599277231098847, test loss: 0.685152440689832\n",
      "epoch 130: train loss: 0.5977394965873523, test loss: 0.6836687662497338\n",
      "epoch 131: train loss: 0.5962131228455579, test loss: 0.6821947881509999\n",
      "epoch 132: train loss: 0.5946979914862704, test loss: 0.6807304177007911\n",
      "epoch 133: train loss: 0.5931939854849807, test loss: 0.6792755671163938\n",
      "epoch 134: train loss: 0.5917009891651133, test loss: 0.6778301495162493\n",
      "epoch 135: train loss: 0.5902188881833726, test loss: 0.6763940789109794\n",
      "epoch 136: train loss: 0.5887475695151901, test loss: 0.6749672701944176\n",
      "epoch 137: train loss: 0.5872869214402727, test loss: 0.6735496391346598\n",
      "epoch 138: train loss: 0.5858368335282574, test loss: 0.6721411023651416\n",
      "epoch 139: train loss: 0.5843971966244753, test loss: 0.6707415773757528\n",
      "epoch 140: train loss: 0.5829679028358224, test loss: 0.6693509825039959\n",
      "epoch 141: train loss: 0.5815488455167465, test loss: 0.6679692369261973\n",
      "epoch 142: train loss: 0.5801399192553444, test loss: 0.666596260648775\n",
      "epoch 143: train loss: 0.5787410198595764, test loss: 0.6652319744995715\n",
      "epoch 144: train loss: 0.5773520443435977, test loss: 0.6638763001192532\n",
      "epoch 145: train loss: 0.5759728909142069, test loss: 0.6625291599527846\n",
      "epoch 146: train loss: 0.5746034589574138, test loss: 0.661190477240977\n",
      "epoch 147: train loss: 0.5732436490251288, test loss: 0.6598601760121189\n",
      "epoch 148: train loss: 0.5718933628219708, test loss: 0.6585381810736871\n",
      "epoch 149: train loss: 0.5705525031921977, test loss: 0.6572244180041431\n",
      "epoch 150: train loss: 0.5692209741067594, test loss: 0.6559188131448161\n",
      "epoch 151: train loss: 0.5678986806504723, test loss: 0.6546212935918735\n",
      "epoch 152: train loss: 0.5665855290093159, test loss: 0.6533317871883801\n",
      "epoch 153: train loss: 0.5652814264578538, test loss: 0.6520502225164487\n",
      "epoch 154: train loss: 0.5639862813467755, test loss: 0.6507765288894788\n",
      "epoch 155: train loss: 0.5627000030905618, test loss: 0.6495106363444872\n",
      "epoch 156: train loss: 0.561422502155273, test loss: 0.6482524756345296\n",
      "epoch 157: train loss: 0.5601536900464587, test loss: 0.6470019782212103\n",
      "epoch 158: train loss: 0.5588934792971894, test loss: 0.6457590762672843\n",
      "epoch 159: train loss: 0.5576417834562107, test loss: 0.6445237026293482\n",
      "epoch 160: train loss: 0.5563985170762173, test loss: 0.6432957908506176\n",
      "epoch 161: train loss: 0.5551635957022482, test loss: 0.6420752751537964\n",
      "epoch 162: train loss: 0.5539369358602013, test loss: 0.6408620904340279\n",
      "epoch 163: train loss: 0.5527184550454691, test loss: 0.6396561722519359\n",
      "epoch 164: train loss: 0.5515080717116896, test loss: 0.6384574568267487\n",
      "epoch 165: train loss: 0.550305705259619, test loss: 0.6372658810295051\n",
      "epoch 166: train loss: 0.5491112760261178, test loss: 0.6360813823763447\n",
      "epoch 167: train loss: 0.5479247052732563, test loss: 0.6349038990218776\n",
      "epoch 168: train loss: 0.5467459151775341, test loss: 0.6337333697526332\n",
      "epoch 169: train loss: 0.5455748288192137, test loss: 0.632569733980587\n",
      "epoch 170: train loss: 0.5444113701717687, test loss: 0.631412931736765\n",
      "epoch 171: train loss: 0.5432554640914441, test loss: 0.6302629036649197\n",
      "epoch 172: train loss: 0.5421070363069296, test loss: 0.6291195910152829\n",
      "epoch 173: train loss: 0.540966013409142, test loss: 0.6279829356383896\n",
      "epoch 174: train loss: 0.5398323228411188, test loss: 0.6268528799789693\n",
      "epoch 175: train loss: 0.5387058928880204, test loss: 0.6257293670699112\n",
      "epoch 176: train loss: 0.5375866526672397, test loss: 0.624612340526292\n",
      "epoch 177: train loss: 0.5364745321186198, test loss: 0.6235017445394729\n",
      "epoch 178: train loss: 0.5353694619947761, test loss: 0.6223975238712599\n",
      "epoch 179: train loss: 0.5342713738515245, test loss: 0.6212996238481269\n",
      "epoch 180: train loss: 0.5331802000384129, test loss: 0.6202079903555016\n",
      "epoch 181: train loss: 0.5320958736893562, test loss: 0.6191225698321103\n",
      "epoch 182: train loss: 0.5310183287133718, test loss: 0.6180433092643836\n",
      "epoch 183: train loss: 0.5299474997854187, test loss: 0.6169701561809184\n",
      "epoch 184: train loss: 0.5288833223373324, test loss: 0.6159030586469967\n",
      "epoch 185: train loss: 0.5278257325488631, test loss: 0.6148419652591604\n",
      "epoch 186: train loss: 0.5267746673388085, test loss: 0.6137868251398395\n",
      "epoch 187: train loss: 0.5257300643562455, test loss: 0.612737587932034\n",
      "epoch 188: train loss: 0.524691861971856, test loss: 0.6116942037940468\n",
      "epoch 189: train loss: 0.5236599992693501, test loss: 0.6106566233942682\n",
      "epoch 190: train loss: 0.5226344160369808, test loss: 0.60962479790601\n",
      "epoch 191: train loss: 0.5216150527591537, test loss: 0.6085986790023883\n",
      "epoch 192: train loss: 0.5206018506081267, test loss: 0.6075782188512544\n",
      "epoch 193: train loss: 0.519594751435804, test loss: 0.6065633701101726\n",
      "epoch 194: train loss: 0.5185936977656175, test loss: 0.6055540859214439\n",
      "epoch 195: train loss: 0.5175986327844982, test loss: 0.6045503199071758\n",
      "epoch 196: train loss: 0.516609500334937, test loss: 0.6035520261643951\n",
      "epoch 197: train loss: 0.5156262449071329, test loss: 0.6025591592602066\n",
      "epoch 198: train loss: 0.5146488116312261, test loss: 0.6015716742269928\n",
      "epoch 199: train loss: 0.5136771462696197, test loss: 0.6005895265576568\n",
      "epoch 200: train loss: 0.512711195209384, test loss: 0.5996126722009076\n",
      "epoch 201: train loss: 0.5117509054547463, test loss: 0.5986410675565851\n",
      "epoch 202: train loss: 0.5107962246196629, test loss: 0.5976746694710267\n",
      "epoch 203: train loss: 0.5098471009204752, test loss: 0.5967134352324731\n",
      "epoch 204: train loss: 0.5089034831686451, test loss: 0.5957573225665148\n",
      "epoch 205: train loss: 0.507965320763573, test loss: 0.5948062896315751\n",
      "epoch 206: train loss: 0.507032563685496, test loss: 0.5938602950144349\n",
      "epoch 207: train loss: 0.5061051624884636, test loss: 0.5929192977257922\n",
      "epoch 208: train loss: 0.5051830682933934, test loss: 0.5919832571958612\n",
      "epoch 209: train loss: 0.5042662327812044, test loss: 0.5910521332700086\n",
      "epoch 210: train loss: 0.5033546081860265, test loss: 0.5901258862044257\n",
      "epoch 211: train loss: 0.5024481472884869, test loss: 0.5892044766618378\n",
      "epoch 212: train loss: 0.5015468034090711, test loss: 0.5882878657072494\n",
      "epoch 213: train loss: 0.5006505304015608, test loss: 0.5873760148037249\n",
      "epoch 214: train loss: 0.49975928264654285, test loss: 0.5864688858082064\n",
      "epoch 215: train loss: 0.4988730150449937, test loss: 0.5855664409673648\n",
      "epoch 216: train loss: 0.49799168301193575, test loss: 0.5846686429134866\n",
      "epoch 217: train loss: 0.4971152424701657, test loss: 0.5837754546603968\n",
      "epoch 218: train loss: 0.4962436498440544, test loss: 0.5828868395994141\n",
      "epoch 219: train loss: 0.4953768620534168, test loss: 0.5820027614953421\n",
      "epoch 220: train loss: 0.4945148365074521, test loss: 0.581123184482494\n",
      "epoch 221: train loss: 0.49365753109875343, test loss: 0.580248073060751\n",
      "epoch 222: train loss: 0.4928049041973857, test loss: 0.579377392091656\n",
      "epoch 223: train loss: 0.4919569146450313, test loss: 0.5785111067945374\n",
      "epoch 224: train loss: 0.491113521749204, test loss: 0.5776491827426711\n",
      "epoch 225: train loss: 0.4902746852775284, test loss: 0.5767915858594715\n",
      "epoch 226: train loss: 0.4894403654520859, test loss: 0.5759382824147184\n",
      "epoch 227: train loss: 0.4886105229438266, test loss: 0.5750892390208147\n",
      "epoch 228: train loss: 0.48778511886704523, test loss: 0.574244422629079\n",
      "epoch 229: train loss: 0.4869641147739212, test loss: 0.5734038005260703\n",
      "epoch 230: train loss: 0.4861474726491236, test loss: 0.5725673403299435\n",
      "epoch 231: train loss: 0.48533515490447726, test loss: 0.5717350099868399\n",
      "epoch 232: train loss: 0.4845271243736927, test loss: 0.5709067777673088\n",
      "epoch 233: train loss: 0.48372334430715713, test loss: 0.5700826122627611\n",
      "epoch 234: train loss: 0.48292377836678674, test loss: 0.5692624823819561\n",
      "epoch 235: train loss: 0.4821283906209402, test loss: 0.5684463573475179\n",
      "epoch 236: train loss: 0.4813371455393912, test loss: 0.5676342066924874\n",
      "epoch 237: train loss: 0.48055000798836117, test loss: 0.5668260002569028\n",
      "epoch 238: train loss: 0.47976694322561114, test loss: 0.5660217081844123\n",
      "epoch 239: train loss: 0.47898791689559084, test loss: 0.565221300918921\n",
      "epoch 240: train loss: 0.4782128950246468, test loss: 0.564424749201265\n",
      "epoch 241: train loss: 0.47744184401628714, test loss: 0.5636320240659207\n",
      "epoch 242: train loss: 0.4766747306465027, test loss: 0.5628430968377428\n",
      "epoch 243: train loss: 0.47591152205914466, test loss: 0.5620579391287344\n",
      "epoch 244: train loss: 0.4751521857613581, test loss: 0.5612765228348482\n",
      "epoch 245: train loss: 0.47439668961906956, test loss: 0.5604988201328178\n",
      "epoch 246: train loss: 0.4736450018525298, test loss: 0.5597248034770216\n",
      "epoch 247: train loss: 0.472897091031911, test loss: 0.5589544455963741\n",
      "epoch 248: train loss: 0.4721529260729556, test loss: 0.5581877194912511\n",
      "epoch 249: train loss: 0.47141247623268, test loss: 0.5574245984304433\n",
      "epoch 250: train loss: 0.47067571110512957, test loss: 0.5566650559481412\n",
      "epoch 251: train loss: 0.4699426006171851, test loss: 0.5559090658409488\n",
      "epoch 252: train loss: 0.46921311502442203, test loss: 0.555156602164929\n",
      "epoch 253: train loss: 0.46848722490701855, test loss: 0.5544076392326773\n",
      "epoch 254: train loss: 0.4677649011657157, test loss: 0.5536621516104274\n",
      "epoch 255: train loss: 0.46704611501782595, test loss: 0.5529201141151829\n",
      "epoch 256: train loss: 0.46633083799329184, test loss: 0.5521815018118826\n",
      "epoch 257: train loss: 0.46561904193079295, test loss: 0.5514462900105919\n",
      "epoch 258: train loss: 0.46491069897390075, test loss: 0.5507144542637248\n",
      "epoch 259: train loss: 0.46420578156728254, test loss: 0.549985970363295\n",
      "epoch 260: train loss: 0.46350426245295046, test loss: 0.5492608143381956\n",
      "epoch 261: train loss: 0.46280611466655985, test loss: 0.5485389624515075\n",
      "epoch 262: train loss: 0.4621113115337509, test loss: 0.5478203911978351\n",
      "epoch 263: train loss: 0.4614198266665393, test loss: 0.5471050773006741\n",
      "epoch 264: train loss: 0.46073163395974887, test loss: 0.546392997709803\n",
      "epoch 265: train loss: 0.4600467075874917, test loss: 0.5456841295987056\n",
      "epoch 266: train loss: 0.4593650219996908, test loss: 0.5449784503620204\n",
      "epoch 267: train loss: 0.4586865519186476, test loss: 0.5442759376130185\n",
      "epoch 268: train loss: 0.45801127233565225, test loss: 0.5435765691811073\n",
      "epoch 269: train loss: 0.4573391585076368, test loss: 0.5428803231093637\n",
      "epoch 270: train loss: 0.4566701859538715, test loss: 0.5421871776520933\n",
      "epoch 271: train loss: 0.4560043304527017, test loss: 0.5414971112724161\n",
      "epoch 272: train loss: 0.4553415680383277, test loss: 0.5408101026398812\n",
      "epoch 273: train loss: 0.4546818749976247, test loss: 0.5401261306281054\n",
      "epoch 274: train loss: 0.45402522786700344, test loss: 0.5394451743124405\n",
      "epoch 275: train loss: 0.453371603429311, test loss: 0.538767212967665\n",
      "epoch 276: train loss: 0.45272097871077166, test loss: 0.5380922260657038\n",
      "epoch 277: train loss: 0.4520733309779664, test loss: 0.5374201932733724\n",
      "epoch 278: train loss: 0.451428637734851, test loss: 0.5367510944501471\n",
      "epoch 279: train loss: 0.45078687671981355, test loss: 0.5360849096459603\n",
      "epoch 280: train loss: 0.4501480259027687, test loss: 0.5354216190990229\n",
      "epoch 281: train loss: 0.4495120634822897, test loss: 0.534761203233669\n",
      "epoch 282: train loss: 0.44887896788277787, test loss: 0.5341036426582279\n",
      "epoch 283: train loss: 0.44824871775166814, test loss: 0.5334489181629194\n",
      "epoch 284: train loss: 0.4476212919566709, test loss: 0.5327970107177734\n",
      "epoch 285: train loss: 0.4469966695830493, test loss: 0.5321479014705759\n",
      "epoch 286: train loss: 0.4463748299309335, test loss: 0.5315015717448359\n",
      "epoch 287: train loss: 0.4457557525126666, test loss: 0.5308580030377787\n",
      "epoch 288: train loss: 0.4451394170501884, test loss: 0.5302171770183617\n",
      "epoch 289: train loss: 0.44452580347245124, test loss: 0.5295790755253131\n",
      "epoch 290: train loss: 0.4439148919128705, test loss: 0.5289436805651958\n",
      "epoch 291: train loss: 0.4433066627068071, test loss: 0.5283109743104921\n",
      "epoch 292: train loss: 0.4427010963890847, test loss: 0.5276809390977126\n",
      "epoch 293: train loss: 0.442098173691538, test loss: 0.527053557425527\n",
      "epoch 294: train loss: 0.4414978755405931, test loss: 0.5264288119529177\n",
      "epoch 295: train loss: 0.4409001830548808, test loss: 0.5258066854973549\n",
      "epoch 296: train loss: 0.44030507754287973, test loss: 0.5251871610329951\n",
      "epoch 297: train loss: 0.4397125405005913, test loss: 0.5245702216888992\n",
      "epoch 298: train loss: 0.43912255360924507, test loss: 0.5239558507472739\n",
      "epoch 299: train loss: 0.4385350987330334, test loss: 0.5233440316417337\n",
      "epoch 300: train loss: 0.437950157916878, test loss: 0.522734747955584\n",
      "epoch 301: train loss: 0.4373677133842234, test loss: 0.5221279834201246\n",
      "epoch 302: train loss: 0.4367877475348611, test loss: 0.5215237219129756\n",
      "epoch 303: train loss: 0.4362102429427816, test loss: 0.5209219474564216\n",
      "epoch 304: train loss: 0.4356351823540555, test loss: 0.5203226442157768\n",
      "epoch 305: train loss: 0.4350625486847424, test loss: 0.5197257964977716\n",
      "epoch 306: train loss: 0.43449232501882634, test loss: 0.5191313887489568\n",
      "epoch 307: train loss: 0.4339244946061801, test loss: 0.5185394055541288\n",
      "epoch 308: train loss: 0.43335904086055566, test loss: 0.5179498316347736\n",
      "epoch 309: train loss: 0.4327959473576008, test loss: 0.5173626518475305\n",
      "epoch 310: train loss: 0.43223519783290326, test loss: 0.5167778511826745\n",
      "epoch 311: train loss: 0.43167677618005923, test loss: 0.5161954147626173\n",
      "epoch 312: train loss: 0.4311206664487688, test loss: 0.5156153278404265\n",
      "epoch 313: train loss: 0.43056685284295554, test loss: 0.5150375757983646\n",
      "epoch 314: train loss: 0.4300153197189121, test loss: 0.5144621441464449\n",
      "epoch 315: train loss: 0.4294660515834693, test loss: 0.5138890185210048\n",
      "epoch 316: train loss: 0.42891903309219054, test loss: 0.513318184683299\n",
      "epoch 317: train loss: 0.4283742490475892, test loss: 0.5127496285181081\n",
      "epoch 318: train loss: 0.4278316843973707, test loss: 0.5121833360323659\n",
      "epoch 319: train loss: 0.42729132423269683, test loss: 0.511619293353802\n",
      "epoch 320: train loss: 0.4267531537864743, test loss: 0.511057486729605\n",
      "epoch 321: train loss: 0.42621715843166513, test loss: 0.5104979025250975\n",
      "epoch 322: train loss: 0.4256833236796197, test loss: 0.5099405272224317\n",
      "epoch 323: train loss: 0.4251516351784321, test loss: 0.5093853474193001\n",
      "epoch 324: train loss: 0.42462207871131724, test loss: 0.5088323498276606\n",
      "epoch 325: train loss: 0.42409464019500964, test loss: 0.5082815212724812\n",
      "epoch 326: train loss: 0.42356930567818346, test loss: 0.5077328486904957\n",
      "epoch 327: train loss: 0.42304606133989325, test loss: 0.5071863191289808\n",
      "epoch 328: train loss: 0.4225248934880356, test loss: 0.5066419197445426\n",
      "epoch 329: train loss: 0.42200578855783183, test loss: 0.5060996378019226\n",
      "epoch 330: train loss: 0.4214887331103294, test loss: 0.505559460672817\n",
      "epoch 331: train loss: 0.4209737138309251, test loss: 0.5050213758347109\n",
      "epoch 332: train loss: 0.4204607175279058, test loss: 0.5044853708697274\n",
      "epoch 333: train loss: 0.4199497311310107, test loss: 0.5039514334634915\n",
      "epoch 334: train loss: 0.4194407416900107, test loss: 0.503419551404007\n",
      "epoch 335: train loss: 0.41893373637330844, test loss: 0.5028897125805495\n",
      "epoch 336: train loss: 0.4184287024665552, test loss: 0.5023619049825713\n",
      "epoch 337: train loss: 0.41792562737128747, test loss: 0.501836116698622\n",
      "epoch 338: train loss: 0.41742449860358133, test loss: 0.5013123359152809\n",
      "epoch 339: train loss: 0.41692530379272363, test loss: 0.5007905509161035\n",
      "epoch 340: train loss: 0.41642803067990264, test loss: 0.5002707500805829\n",
      "epoch 341: train loss: 0.41593266711691435, test loss: 0.49975292188312054\n",
      "epoch 342: train loss: 0.4154392010648869, test loss: 0.4992370548920138\n",
      "epoch 343: train loss: 0.41494762059302154, test loss: 0.4987231377684542\n",
      "epoch 344: train loss: 0.41445791387735026, test loss: 0.4982111592655384\n",
      "epoch 345: train loss: 0.41397006919951, test loss: 0.4977011082272918\n",
      "epoch 346: train loss: 0.413484074945533, test loss: 0.49719297358770476\n",
      "epoch 347: train loss: 0.41299991960465304, test loss: 0.4966867443697808\n",
      "epoch 348: train loss: 0.4125175917681277, test loss: 0.49618240968459587\n",
      "epoch 349: train loss: 0.41203708012807566, test loss: 0.4956799587303709\n",
      "epoch 350: train loss: 0.41155837347633023, test loss: 0.4951793807915543\n",
      "epoch 351: train loss: 0.41108146070330703, test loss: 0.49468066523791854\n",
      "epoch 352: train loss: 0.41060633079688713, test loss: 0.49418380152366437\n",
      "epoch 353: train loss: 0.4101329728413151, test loss: 0.4936887791865392\n",
      "epoch 354: train loss: 0.4096613760161107, test loss: 0.4931955878469666\n",
      "epoch 355: train loss: 0.40919152959499594, test loss: 0.49270421720718405\n",
      "epoch 356: train loss: 0.40872342294483544, test loss: 0.49221465705039336\n",
      "epoch 357: train loss: 0.4082570455245916, test loss: 0.4917268972399231\n",
      "epoch 358: train loss: 0.40779238688429204, test loss: 0.4912409277183973\n",
      "epoch 359: train loss: 0.40732943666401217, test loss: 0.49075673850691914\n",
      "epoch 360: train loss: 0.4068681845928701, test loss: 0.4902743197042615\n",
      "epoch 361: train loss: 0.406408620488035, test loss: 0.48979366148606984\n",
      "epoch 362: train loss: 0.4059507342537485, test loss: 0.489314754104073\n",
      "epoch 363: train loss: 0.40549451588035884, test loss: 0.48883758788530485\n",
      "epoch 364: train loss: 0.4050399554433675, test loss: 0.4883621532313366\n",
      "epoch 365: train loss: 0.4045870431024885, test loss: 0.48788844061751624\n",
      "epoch 366: train loss: 0.4041357691007197, test loss: 0.48741644059221967\n",
      "epoch 367: train loss: 0.40368612376342666, test loss: 0.48694614377610945\n",
      "epoch 368: train loss: 0.4032380974974377, test loss: 0.48647754086140454\n",
      "epoch 369: train loss: 0.40279168079015165, test loss: 0.48601062261115635\n",
      "epoch 370: train loss: 0.4023468642086561, test loss: 0.48554537985853696\n",
      "epoch 371: train loss: 0.40190363839885807, test loss: 0.4850818035061337\n",
      "epoch 372: train loss: 0.40146199408462563, test loss: 0.4846198845252529\n",
      "epoch 373: train loss: 0.40102192206694015, test loss: 0.4841596139552334\n",
      "epoch 374: train loss: 0.4005834132230602, test loss: 0.48370098290276703\n",
      "epoch 375: train loss: 0.40014645850569636, test loss: 0.4832439825412279\n",
      "epoch 376: train loss: 0.39971104894219533, test loss: 0.4827886041100109\n",
      "epoch 377: train loss: 0.3992771756337365, test loss: 0.4823348389138759\n",
      "epoch 378: train loss: 0.39884482975453733, test loss: 0.481882678322303\n",
      "epoch 379: train loss: 0.39841400255107007, test loss: 0.4814321137688535\n",
      "epoch 380: train loss: 0.39798468534128717, test loss: 0.4809831367505391\n",
      "epoch 381: train loss: 0.3975568695138581, test loss: 0.4805357388271995\n",
      "epoch 382: train loss: 0.39713054652741564, test loss: 0.4800899116208862\n",
      "epoch 383: train loss: 0.39670570790981025, test loss: 0.4796456468152562\n",
      "epoch 384: train loss: 0.39628234525737666, test loss: 0.47920293615496923\n",
      "epoch 385: train loss: 0.3958604502342075, test loss: 0.4787617714450967\n",
      "epoch 386: train loss: 0.3954400145714373, test loss: 0.47832214455053434\n",
      "epoch 387: train loss: 0.3950210300665355, test loss: 0.47788404739542373\n",
      "epoch 388: train loss: 0.3946034885826083, test loss: 0.47744747196257975\n",
      "epoch 389: train loss: 0.3941873820477093, test loss: 0.4770124102929263\n",
      "epoch 390: train loss: 0.3937727024541594, test loss: 0.4765788544849372\n",
      "epoch 391: train loss: 0.3933594418578747, test loss: 0.4761467966940849\n",
      "epoch 392: train loss: 0.39294759237770277, test loss: 0.47571622913229544\n",
      "epoch 393: train loss: 0.39253714619476854, test loss: 0.47528714406740946\n",
      "epoch 394: train loss: 0.39212809555182687, test loss: 0.474859533822651\n",
      "epoch 395: train loss: 0.3917204327526243, test loss: 0.47443339077610114\n",
      "epoch 396: train loss: 0.39131415016126864, test loss: 0.47400870736017836\n",
      "epoch 397: train loss: 0.39090924020160583, test loss: 0.4735854760611255\n",
      "epoch 398: train loss: 0.3905056953566063, test loss: 0.47316368941850273\n",
      "epoch 399: train loss: 0.3901035081677563, test loss: 0.47274334002468565\n",
      "epoch 400: train loss: 0.38970267123445995, test loss: 0.4723244205243708\n",
      "epoch 401: train loss: 0.3893031772134463, test loss: 0.4719069236140852\n",
      "epoch 402: train loss: 0.38890501881818507, test loss: 0.4714908420417038\n",
      "epoch 403: train loss: 0.38850818881830934, test loss: 0.4710761686059701\n",
      "epoch 404: train loss: 0.38811268003904514, test loss: 0.4706628961560253\n",
      "epoch 405: train loss: 0.38771848536064885, test loss: 0.47025101759093957\n",
      "epoch 406: train loss: 0.38732559771785063, test loss: 0.4698405258592527\n",
      "epoch 407: train loss: 0.38693401009930556, test loss: 0.46943141395851584\n",
      "epoch 408: train loss: 0.3865437155470509, test loss: 0.4690236749348422\n",
      "epoch 409: train loss: 0.3861547071559711, test loss: 0.46861730188246187\n",
      "epoch 410: train loss: 0.3857669780732676, test loss: 0.4682122879432796\n",
      "epoch 411: train loss: 0.38538052149793695, test loss: 0.46780862630644166\n",
      "epoch 412: train loss: 0.38499533068025443, test loss: 0.4674063102079044\n",
      "epoch 413: train loss: 0.3846113989212643, test loss: 0.4670053329300088\n",
      "epoch 414: train loss: 0.384228719572276, test loss: 0.4666056878010615\n",
      "epoch 415: train loss: 0.3838472860343664, test loss: 0.46620736819491754\n",
      "epoch 416: train loss: 0.3834670917578893, test loss: 0.46581036753057115\n",
      "epoch 417: train loss: 0.3830881302419891, test loss: 0.46541467927174796\n",
      "epoch 418: train loss: 0.3827103950341214, test loss: 0.4650202969265051\n",
      "epoch 419: train loss: 0.38233387972957983, test loss: 0.464627214046834\n",
      "epoch 420: train loss: 0.38195857797102767, test loss: 0.46423542422826697\n",
      "epoch 421: train loss: 0.3815844834480353, test loss: 0.46384492110949066\n",
      "epoch 422: train loss: 0.38121158989662407, test loss: 0.4634556983719621\n",
      "epoch 423: train loss: 0.3808398910988148, test loss: 0.46306774973952886\n",
      "epoch 424: train loss: 0.3804693808821817, test loss: 0.46268106897805483\n",
      "epoch 425: train loss: 0.3801000531194123, test loss: 0.46229564989504934\n",
      "epoch 426: train loss: 0.37973190172787213, test loss: 0.46191148633930135\n",
      "epoch 427: train loss: 0.3793649206691748, test loss: 0.4615285722005151\n",
      "epoch 428: train loss: 0.3789991039487572, test loss: 0.46114690140895404\n",
      "epoch 429: train loss: 0.37863444561545956, test loss: 0.46076646793508447\n",
      "epoch 430: train loss: 0.3782709397611107, test loss: 0.4603872657892269\n",
      "epoch 431: train loss: 0.3779085805201186, test loss: 0.460009289021208\n",
      "epoch 432: train loss: 0.3775473620690647, test loss: 0.45963253172001856\n",
      "epoch 433: train loss: 0.37718727862630447, test loss: 0.4592569880134748\n",
      "epoch 434: train loss: 0.37682832445157105, test loss: 0.45888265206788276\n",
      "epoch 435: train loss: 0.37647049384558534, test loss: 0.4585095180877072\n",
      "epoch 436: train loss: 0.37611378114966926, test loss: 0.45813758031524315\n",
      "epoch 437: train loss: 0.37575818074536427, test loss: 0.4577668330302924\n",
      "epoch 438: train loss: 0.37540368705405386, test loss: 0.4573972705498424\n",
      "epoch 439: train loss: 0.37505029453659106, test loss: 0.45702888722774904\n",
      "epoch 440: train loss: 0.37469799769293005, test loss: 0.4566616774544227\n",
      "epoch 441: train loss: 0.3743467910617618, test loss: 0.45629563565651865\n",
      "epoch 442: train loss: 0.37399666922015407, test loss: 0.4559307562966292\n",
      "epoch 443: train loss: 0.373647626783196, test loss: 0.4555670338729808\n",
      "epoch 444: train loss: 0.3732996584036462, test loss: 0.4552044629191332\n",
      "epoch 445: train loss: 0.3729527587715854, test loss: 0.4548430380036818\n",
      "epoch 446: train loss: 0.37260692261407263, test loss: 0.45448275372996505\n",
      "epoch 447: train loss: 0.3722621446948061, test loss: 0.4541236047357725\n",
      "epoch 448: train loss: 0.37191841981378687, test loss: 0.4537655856930574\n",
      "epoch 449: train loss: 0.3715757428069877, test loss: 0.45340869130765216\n",
      "epoch 450: train loss: 0.37123410854602423, test loss: 0.4530529163189867\n",
      "epoch 451: train loss: 0.3708935119378316, test loss: 0.45269825549981\n",
      "epoch 452: train loss: 0.37055394792434315, test loss: 0.4523447036559139\n",
      "epoch 453: train loss: 0.3702154114821737, test loss: 0.451992255625861\n",
      "epoch 454: train loss: 0.3698778976223059, test loss: 0.4516409062807145\n",
      "epoch 455: train loss: 0.3695414013897814, test loss: 0.4512906505237709\n",
      "epoch 456: train loss: 0.36920591786339324, test loss: 0.450941483290296\n",
      "epoch 457: train loss: 0.36887144215538387, test loss: 0.4505933995472637\n",
      "epoch 458: train loss: 0.36853796941114536, test loss: 0.4502463942930967\n",
      "epoch 459: train loss: 0.36820549480892384, test loss: 0.44990046255741123\n",
      "epoch 460: train loss: 0.3678740135595256, test loss: 0.4495555994007625\n",
      "epoch 461: train loss: 0.3675435209060291, test loss: 0.449211799914395\n",
      "epoch 462: train loss: 0.36721401212349764, test loss: 0.448869059219994\n",
      "epoch 463: train loss: 0.3668854825186964, test loss: 0.44852737246943947\n",
      "epoch 464: train loss: 0.36655792742981347, test loss: 0.4481867348445643\n",
      "epoch 465: train loss: 0.36623134222618176, test loss: 0.4478471415569117\n",
      "epoch 466: train loss: 0.3659057223080061, test loss: 0.4475085878474993\n",
      "epoch 467: train loss: 0.36558106310609234, test loss: 0.4471710689865816\n",
      "epoch 468: train loss: 0.36525736008157933, test loss: 0.44683458027341827\n",
      "epoch 469: train loss: 0.3649346087256747, test loss: 0.4464991170360417\n",
      "epoch 470: train loss: 0.3646128045593923, test loss: 0.4461646746310299\n",
      "epoch 471: train loss: 0.36429194313329405, test loss: 0.44583124844327926\n",
      "epoch 472: train loss: 0.3639720200272329, test loss: 0.4454988338857808\n",
      "epoch 473: train loss: 0.3636530308500999, test loss: 0.4451674263993992\n",
      "epoch 474: train loss: 0.3633349712395734, test loss: 0.4448370214526519\n",
      "epoch 475: train loss: 0.3630178368618717, test loss: 0.44450761454149346\n",
      "epoch 476: train loss: 0.3627016234115068, test loss: 0.44417920118909876\n",
      "epoch 477: train loss: 0.36238632661104275, test loss: 0.44385177694565214\n",
      "epoch 478: train loss: 0.36207194221085537, test loss: 0.44352533738813443\n",
      "epoch 479: train loss: 0.36175846598889494, test loss: 0.44319987812011546\n",
      "epoch 480: train loss: 0.36144589375045105, test loss: 0.44287539477154714\n",
      "epoch 481: train loss: 0.361134221327921, test loss: 0.4425518829985592\n",
      "epoch 482: train loss: 0.3608234445805795, test loss: 0.4422293384832554\n",
      "epoch 483: train loss: 0.36051355939435126, test loss: 0.4419077569335142\n",
      "epoch 484: train loss: 0.36020456168158643, test loss: 0.44158713408279\n",
      "epoch 485: train loss: 0.35989644738083776, test loss: 0.44126746568991565\n",
      "epoch 486: train loss: 0.35958921245664044, test loss: 0.4409487475389082\n",
      "epoch 487: train loss: 0.3592828528992944, test loss: 0.44063097543877605\n",
      "epoch 488: train loss: 0.3589773647246488, test loss: 0.4403141452233279\n",
      "epoch 489: train loss: 0.3586727439738885, test loss: 0.4399982527509836\n",
      "epoch 490: train loss: 0.35836898671332307, test loss: 0.43968329390458677\n",
      "epoch 491: train loss: 0.3580660890341784, test loss: 0.43936926459121906\n",
      "epoch 492: train loss: 0.3577640470523893, test loss: 0.4390561607420175\n",
      "epoch 493: train loss: 0.35746285690839597, test loss: 0.4387439783119912\n",
      "epoch 494: train loss: 0.3571625147669406, test loss: 0.4384327132798417\n",
      "epoch 495: train loss: 0.356863016816868, test loss: 0.43812236164778495\n",
      "epoch 496: train loss: 0.356564359270927, test loss: 0.4378129194413736\n",
      "epoch 497: train loss: 0.35626653836557437, test loss: 0.437504382709323\n",
      "epoch 498: train loss: 0.355969550360781, test loss: 0.43719674752333626\n",
      "epoch 499: train loss: 0.35567339153983957, test loss: 0.43689000997793476\n",
      "epoch 500: train loss: 0.35537805820917434, test loss: 0.436584166190285\n",
      "epoch 501: train loss: 0.35508354669815334, test loss: 0.43627921230003336\n",
      "epoch 502: train loss: 0.3547898533589018, test loss: 0.43597514446913566\n",
      "epoch 503: train loss: 0.3544969745661182, test loss: 0.43567195888169535\n",
      "epoch 504: train loss: 0.3542049067168911, test loss: 0.4353696517437963\n",
      "epoch 505: train loss: 0.3539136462305198, test loss: 0.4350682192833426\n",
      "epoch 506: train loss: 0.3536231895483342, test loss: 0.43476765774989773\n",
      "epoch 507: train loss: 0.3533335331335188, test loss: 0.4344679634145235\n",
      "epoch 508: train loss: 0.35304467347093704, test loss: 0.4341691325696246\n",
      "epoch 509: train loss: 0.35275660706695816, test loss: 0.43387116152879\n",
      "epoch 510: train loss: 0.3524693304492857, test loss: 0.4335740466266395\n",
      "epoch 511: train loss: 0.352182840166787, test loss: 0.43327778421867025\n",
      "epoch 512: train loss: 0.3518971327893256, test loss: 0.4329823706811035\n",
      "epoch 513: train loss: 0.35161220490759443, test loss: 0.43268780241073446\n",
      "epoch 514: train loss: 0.3513280531329507, test loss: 0.43239407582478395\n",
      "epoch 515: train loss: 0.35104467409725326, test loss: 0.4321011873607486\n",
      "epoch 516: train loss: 0.3507620644527005, test loss: 0.4318091334762563\n",
      "epoch 517: train loss: 0.35048022087167036, test loss: 0.43151791064891903\n",
      "epoch 518: train loss: 0.35019914004656216, test loss: 0.4312275153761903\n",
      "epoch 519: train loss: 0.3499188186896396, test loss: 0.43093794417522197\n",
      "epoch 520: train loss: 0.3496392535328752, test loss: 0.4306491935827222\n",
      "epoch 521: train loss: 0.34936044132779714, test loss: 0.43036126015481707\n",
      "epoch 522: train loss: 0.3490823788453363, test loss: 0.4300741404669101\n",
      "epoch 523: train loss: 0.34880506287567564, test loss: 0.4297878311135455\n",
      "epoch 524: train loss: 0.34852849022810106, test loss: 0.42950232870827165\n",
      "epoch 525: train loss: 0.3482526577308535, test loss: 0.4292176298835061\n",
      "epoch 526: train loss: 0.3479775622309823, test loss: 0.4289337312904008\n",
      "epoch 527: train loss: 0.34770320059420035, test loss: 0.4286506295987109\n",
      "epoch 528: train loss: 0.3474295697047406, test loss: 0.4283683214966618\n",
      "epoch 529: train loss: 0.3471566664652134, test loss: 0.4280868036908195\n",
      "epoch 530: train loss: 0.3468844877964662, test loss: 0.42780607290596145\n",
      "epoch 531: train loss: 0.3466130306374434, test loss: 0.42752612588494754\n",
      "epoch 532: train loss: 0.34634229194504895, test loss: 0.4272469593885942\n",
      "epoch 533: train loss: 0.34607226869400903, test loss: 0.4269685701955477\n",
      "epoch 534: train loss: 0.3458029578767362, test loss: 0.4266909551021598\n",
      "epoch 535: train loss: 0.3455343565031963, test loss: 0.42641411092236475\n",
      "epoch 536: train loss: 0.34526646160077407, test loss: 0.4261380344875548\n",
      "epoch 537: train loss: 0.34499927021414245, test loss: 0.42586272264646086\n",
      "epoch 538: train loss: 0.3447327794051317, test loss: 0.42558817226503065\n",
      "epoch 539: train loss: 0.3444669862526005, test loss: 0.42531438022630935\n",
      "epoch 540: train loss: 0.3442018878523075, test loss: 0.4250413434303218\n",
      "epoch 541: train loss: 0.34393748131678464, test loss: 0.4247690587939553\n",
      "epoch 542: train loss: 0.3436737637752121, test loss: 0.4244975232508412\n",
      "epoch 543: train loss: 0.34341073237329295, test loss: 0.4242267337512433\n",
      "epoch 544: train loss: 0.34314838427313077, test loss: 0.4239566872619394\n",
      "epoch 545: train loss: 0.3428867166531068, test loss: 0.42368738076611057\n",
      "epoch 546: train loss: 0.34262572670775937, test loss: 0.4234188112632286\n",
      "epoch 547: train loss: 0.34236541164766415, test loss: 0.42315097576894245\n",
      "epoch 548: train loss: 0.34210576869931486, test loss: 0.42288387131497\n",
      "epoch 549: train loss: 0.34184679510500643, test loss: 0.42261749494898754\n",
      "epoch 550: train loss: 0.3415884881227177, test loss: 0.422351843734521\n",
      "epoch 551: train loss: 0.34133084502599614, test loss: 0.42208691475083876\n",
      "epoch 552: train loss: 0.34107386310384386, test loss: 0.42182270509284364\n",
      "epoch 553: train loss: 0.34081753966060363, test loss: 0.42155921187096856\n",
      "epoch 554: train loss: 0.34056187201584676, test loss: 0.4212964322110709\n",
      "epoch 555: train loss: 0.3403068575042623, test loss: 0.4210343632543268\n",
      "epoch 556: train loss: 0.34005249347554584, test loss: 0.42077300215713076\n",
      "epoch 557: train loss: 0.33979877729429075, test loss: 0.420512346090991\n",
      "epoch 558: train loss: 0.33954570633987996, test loss: 0.42025239224242816\n",
      "epoch 559: train loss: 0.33929327800637865, test loss: 0.41999313781287606\n",
      "epoch 560: train loss: 0.3390414897024272, test loss: 0.4197345800185792\n",
      "epoch 561: train loss: 0.33879033885113713, test loss: 0.4194767160904969\n",
      "epoch 562: train loss: 0.3385398228899852, test loss: 0.4192195432742017\n",
      "epoch 563: train loss: 0.3382899392707111, test loss: 0.41896305882978474\n",
      "epoch 564: train loss: 0.33804068545921423, test loss: 0.41870726003175773\n",
      "epoch 565: train loss: 0.3377920589354522, test loss: 0.4184521441689567\n",
      "epoch 566: train loss: 0.33754405719334046, test loss: 0.4181977085444487\n",
      "epoch 567: train loss: 0.3372966777406515, test loss: 0.41794395047543503\n",
      "epoch 568: train loss: 0.33704991809891705, test loss: 0.41769086729316046\n",
      "epoch 569: train loss: 0.3368037758033293, test loss: 0.41743845634281823\n",
      "epoch 570: train loss: 0.33655824840264403, test loss: 0.41718671498345933\n",
      "epoch 571: train loss: 0.33631333345908376, test loss: 0.4169356405879007\n",
      "epoch 572: train loss: 0.336069028548243, test loss: 0.41668523054263523\n",
      "epoch 573: train loss: 0.33582533125899316, test loss: 0.41643548224774124\n",
      "epoch 574: train loss: 0.3355822391933886, test loss: 0.41618639311679423\n",
      "epoch 575: train loss: 0.3353397499665739, test loss: 0.415937960576778\n",
      "epoch 576: train loss: 0.3350978612066918, test loss: 0.41569018206799724\n",
      "epoch 577: train loss: 0.3348565705547915, test loss: 0.415443055043991\n",
      "epoch 578: train loss: 0.3346158756647382, test loss: 0.4151965769714451\n",
      "epoch 579: train loss: 0.3343757742031233, test loss: 0.4149507453301088\n",
      "epoch 580: train loss: 0.3341362638491756, test loss: 0.41470555761270794\n",
      "epoch 581: train loss: 0.33389734229467266, test loss: 0.4144610113248619\n",
      "epoch 582: train loss: 0.3336590072438538, test loss: 0.41421710398499967\n",
      "epoch 583: train loss: 0.33342125641333303, test loss: 0.413973833124277\n",
      "epoch 584: train loss: 0.33318408753201334, test loss: 0.41373119628649463\n",
      "epoch 585: train loss: 0.3329474983410012, test loss: 0.41348919102801673\n",
      "epoch 586: train loss: 0.33271148659352207, test loss: 0.413247814917689\n",
      "epoch 587: train loss: 0.33247605005483727, test loss: 0.4130070655367596\n",
      "epoch 588: train loss: 0.33224118650216, test loss: 0.4127669404787997\n",
      "epoch 589: train loss: 0.33200689372457354, test loss: 0.412527437349623\n",
      "epoch 590: train loss: 0.3317731695229496, test loss: 0.41228855376720885\n",
      "epoch 591: train loss: 0.33154001170986724, test loss: 0.41205028736162413\n",
      "epoch 592: train loss: 0.3313074181095325, test loss: 0.4118126357749455\n",
      "epoch 593: train loss: 0.3310753865576993, test loss: 0.41157559666118376\n",
      "epoch 594: train loss: 0.3308439149015899, test loss: 0.4113391676862068\n",
      "epoch 595: train loss: 0.330613000999817, test loss: 0.4111033465276661\n",
      "epoch 596: train loss: 0.33038264272230644, test loss: 0.41086813087491897\n",
      "epoch 597: train loss: 0.3301528379502197, test loss: 0.410633518428958\n",
      "epoch 598: train loss: 0.32992358457587817, test loss: 0.4103995069023345\n",
      "epoch 599: train loss: 0.32969488050268686, test loss: 0.41016609401908705\n",
      "epoch 600: train loss: 0.32946672364506013, test loss: 0.40993327751466796\n",
      "epoch 601: train loss: 0.32923911192834693, test loss: 0.40970105513587285\n",
      "epoch 602: train loss: 0.32901204328875694, test loss: 0.4094694246407676\n",
      "epoch 603: train loss: 0.32878551567328756, test loss: 0.4092383837986183\n",
      "epoch 604: train loss: 0.32855952703965174, test loss: 0.4090079303898216\n",
      "epoch 605: train loss: 0.3283340753562052, test loss: 0.4087780622058338\n",
      "epoch 606: train loss: 0.3281091586018759, test loss: 0.4085487770491021\n",
      "epoch 607: train loss: 0.327884774766093, test loss: 0.4083200727329963\n",
      "epoch 608: train loss: 0.32766092184871654, test loss: 0.40809194708174035\n",
      "epoch 609: train loss: 0.3274375978599682, test loss: 0.40786439793034474\n",
      "epoch 610: train loss: 0.32721480082036175, test loss: 0.407637423124539\n",
      "epoch 611: train loss: 0.3269925287606352, test loss: 0.40741102052070544\n",
      "epoch 612: train loss: 0.3267707797216828, test loss: 0.40718518798581355\n",
      "epoch 613: train loss: 0.3265495517544872, test loss: 0.40695992339735326\n",
      "epoch 614: train loss: 0.32632884292005315, test loss: 0.4067352246432707\n",
      "epoch 615: train loss: 0.32610865128934136, test loss: 0.4065110896219041\n",
      "epoch 616: train loss: 0.32588897494320224, test loss: 0.40628751624191695\n",
      "epoch 617: train loss: 0.32566981197231143, test loss: 0.40606450242223757\n",
      "epoch 618: train loss: 0.3254511604771046, test loss: 0.405842046091995\n",
      "epoch 619: train loss: 0.32523301856771364, test loss: 0.4056201451904544\n",
      "epoch 620: train loss: 0.32501538436390304, test loss: 0.40539879766695797\n",
      "epoch 621: train loss: 0.32479825599500667, test loss: 0.40517800148086003\n",
      "epoch 622: train loss: 0.32458163159986514, test loss: 0.40495775460146805\n",
      "epoch 623: train loss: 0.32436550932676433, test loss: 0.40473805500798077\n",
      "epoch 624: train loss: 0.32414988733337297, test loss: 0.4045189006894281\n",
      "epoch 625: train loss: 0.32393476378668207, test loss: 0.4043002896446102\n",
      "epoch 626: train loss: 0.3237201368629442, test loss: 0.4040822198820401\n",
      "epoch 627: train loss: 0.32350600474761365, test loss: 0.40386468941988185\n",
      "epoch 628: train loss: 0.32329236563528624, test loss: 0.40364769628589414\n",
      "epoch 629: train loss: 0.3230792177296406, test loss: 0.40343123851737145\n",
      "epoch 630: train loss: 0.32286655924337937, test loss: 0.4032153141610854\n",
      "epoch 631: train loss: 0.32265438839817107, test loss: 0.4029999212732291\n",
      "epoch 632: train loss: 0.3224427034245922, test loss: 0.40278505791935776\n",
      "epoch 633: train loss: 0.3222315025620699, test loss: 0.40257072217433487\n",
      "epoch 634: train loss: 0.32202078405882534, test loss: 0.4023569121222748\n",
      "epoch 635: train loss: 0.3218105461718173, test loss: 0.402143625856486\n",
      "epoch 636: train loss: 0.3216007871666857, test loss: 0.401930861479418\n",
      "epoch 637: train loss: 0.32139150531769695, test loss: 0.4017186171026055\n",
      "epoch 638: train loss: 0.3211826989076881, test loss: 0.4015068908466128\n",
      "epoch 639: train loss: 0.3209743662280127, test loss: 0.4012956808409814\n",
      "epoch 640: train loss: 0.32076650557848607, test loss: 0.4010849852241756\n",
      "epoch 641: train loss: 0.3205591152673321, test loss: 0.40087480214352894\n",
      "epoch 642: train loss: 0.3203521936111295, test loss: 0.4006651297551916\n",
      "epoch 643: train loss: 0.3201457389347589, test loss: 0.4004559662240781\n",
      "epoch 644: train loss: 0.3199397495713503, test loss: 0.40024730972381434\n",
      "epoch 645: train loss: 0.3197342238622308, test loss: 0.4000391584366865\n",
      "epoch 646: train loss: 0.31952916015687327, test loss: 0.39983151055359\n",
      "epoch 647: train loss: 0.3193245568128445, test loss: 0.3996243642739772\n",
      "epoch 648: train loss: 0.31912041219575454, test loss: 0.39941771780580737\n",
      "epoch 649: train loss: 0.3189167246792061, test loss: 0.3992115693654973\n",
      "epoch 650: train loss: 0.3187134926447445, test loss: 0.3990059171778694\n",
      "epoch 651: train loss: 0.31851071448180746, test loss: 0.39880075947610416\n",
      "epoch 652: train loss: 0.31830838858767635, test loss: 0.3985960945016888\n",
      "epoch 653: train loss: 0.31810651336742657, test loss: 0.3983919205043714\n",
      "epoch 654: train loss: 0.3179050872338793, test loss: 0.39818823574210827\n",
      "epoch 655: train loss: 0.3177041086075531, test loss: 0.3979850384810197\n",
      "epoch 656: train loss: 0.31750357591661593, test loss: 0.39778232699533883\n",
      "epoch 657: train loss: 0.3173034875968377, test loss: 0.3975800995673675\n",
      "epoch 658: train loss: 0.31710384209154313, test loss: 0.3973783544874256\n",
      "epoch 659: train loss: 0.3169046378515648, test loss: 0.3971770900538067\n",
      "epoch 660: train loss: 0.31670587333519684, test loss: 0.3969763045727307\n",
      "epoch 661: train loss: 0.3165075470081485, test loss: 0.3967759963582975\n",
      "epoch 662: train loss: 0.316309657343499, test loss: 0.3965761637324411\n",
      "epoch 663: train loss: 0.3161122028216512, test loss: 0.3963768050248842\n",
      "epoch 664: train loss: 0.3159151819302877, test loss: 0.39617791857309376\n",
      "epoch 665: train loss: 0.31571859316432493, test loss: 0.39597950272223414\n",
      "epoch 666: train loss: 0.3155224350258694, test loss: 0.39578155582512536\n",
      "epoch 667: train loss: 0.31532670602417356, test loss: 0.3955840762421954\n",
      "epoch 668: train loss: 0.315131404675592, test loss: 0.3953870623414397\n",
      "epoch 669: train loss: 0.31493652950353784, test loss: 0.39519051249837434\n",
      "epoch 670: train loss: 0.3147420790384401, test loss: 0.3949944250959953\n",
      "epoch 671: train loss: 0.3145480518177004, test loss: 0.39479879852473354\n",
      "epoch 672: train loss: 0.31435444638565097, test loss: 0.3946036311824137\n",
      "epoch 673: train loss: 0.3141612612935122, test loss: 0.3944089214742101\n",
      "epoch 674: train loss: 0.31396849509935093, test loss: 0.39421466781260617\n",
      "epoch 675: train loss: 0.3137761463680391, test loss: 0.3940208686173516\n",
      "epoch 676: train loss: 0.31358421367121203, test loss: 0.39382752231542056\n",
      "epoch 677: train loss: 0.3133926955872283, test loss: 0.3936346273409717\n",
      "epoch 678: train loss: 0.3132015907011285, test loss: 0.39344218213530513\n",
      "epoch 679: train loss: 0.31301089760459516, test loss: 0.39325018514682347\n",
      "epoch 680: train loss: 0.31282061489591273, test loss: 0.393058634830991\n",
      "epoch 681: train loss: 0.312630741179928, test loss: 0.39286752965029265\n",
      "epoch 682: train loss: 0.3124412750680105, test loss: 0.3926768680741951\n",
      "epoch 683: train loss: 0.3122522151780133, test loss: 0.39248664857910653\n",
      "epoch 684: train loss: 0.31206356013423464, test loss: 0.39229686964833843\n",
      "epoch 685: train loss: 0.3118753085673789, test loss: 0.3921075297720638\n",
      "epoch 686: train loss: 0.31168745911451823, test loss: 0.3919186274472817\n",
      "epoch 687: train loss: 0.31150001041905495, test loss: 0.39173016117777676\n",
      "epoch 688: train loss: 0.3113129611306838, test loss: 0.3915421294740807\n",
      "epoch 689: train loss: 0.31112630990535434, test loss: 0.3913545308534357\n",
      "epoch 690: train loss: 0.3109400554052335, test loss: 0.39116736383975537\n",
      "epoch 691: train loss: 0.3107541962986693, test loss: 0.39098062696358754\n",
      "epoch 692: train loss: 0.3105687312601537, test loss: 0.3907943187620777\n",
      "epoch 693: train loss: 0.3103836589702865, test loss: 0.39060843777893134\n",
      "epoch 694: train loss: 0.310198978115739, test loss: 0.39042298256437663\n",
      "epoch 695: train loss: 0.3100146873892183, test loss: 0.3902379516751298\n",
      "epoch 696: train loss: 0.3098307854894319, test loss: 0.390053343674356\n",
      "epoch 697: train loss: 0.3096472711210519, test loss: 0.38986915713163706\n",
      "epoch 698: train loss: 0.3094641429946802, test loss: 0.3896853906229317\n",
      "epoch 699: train loss: 0.3092813998268139, test loss: 0.3895020427305434\n",
      "epoch 700: train loss: 0.30909904033981, test loss: 0.3893191120430828\n",
      "epoch 701: train loss: 0.3089170632618518, test loss: 0.3891365971554335\n",
      "epoch 702: train loss: 0.30873546732691465, test loss: 0.3889544966687178\n",
      "epoch 703: train loss: 0.3085542512747316, test loss: 0.3887728091902612\n",
      "epoch 704: train loss: 0.30837341385076045, test loss: 0.38859153333355834\n",
      "epoch 705: train loss: 0.30819295380614997, test loss: 0.38841066771823923\n",
      "epoch 706: train loss: 0.30801286989770704, test loss: 0.38823021097003435\n",
      "epoch 707: train loss: 0.30783316088786344, test loss: 0.3880501617207427\n",
      "epoch 708: train loss: 0.30765382554464343, test loss: 0.38787051860819655\n",
      "epoch 709: train loss: 0.3074748626416314, test loss: 0.3876912802762294\n",
      "epoch 710: train loss: 0.3072962709579392, test loss: 0.3875124453746423\n",
      "epoch 711: train loss: 0.3071180492781748, test loss: 0.3873340125591713\n",
      "epoch 712: train loss: 0.3069401963924101, test loss: 0.3871559804914553\n",
      "epoch 713: train loss: 0.30676271109614944, test loss: 0.38697834783900364\n",
      "epoch 714: train loss: 0.30658559219029846, test loss: 0.3868011132751623\n",
      "epoch 715: train loss: 0.30640883848113304, test loss: 0.38662427547908457\n",
      "epoch 716: train loss: 0.3062324487802682, test loss: 0.38644783313569775\n",
      "epoch 717: train loss: 0.3060564219046275, test loss: 0.38627178493567166\n",
      "epoch 718: train loss: 0.3058807566764128, test loss: 0.386096129575388\n",
      "epoch 719: train loss: 0.3057054519230741, test loss: 0.385920865756909\n",
      "epoch 720: train loss: 0.3055305064772789, test loss: 0.38574599218794586\n",
      "epoch 721: train loss: 0.3053559191768834, test loss: 0.38557150758182973\n",
      "epoch 722: train loss: 0.3051816888649017, test loss: 0.3853974106574786\n",
      "epoch 723: train loss: 0.30500781438947766, test loss: 0.3852237001393699\n",
      "epoch 724: train loss: 0.30483429460385464, test loss: 0.3850503747575084\n",
      "epoch 725: train loss: 0.30466112836634734, test loss: 0.38487743324739726\n",
      "epoch 726: train loss: 0.30448831454031244, test loss: 0.384704874350008\n",
      "epoch 727: train loss: 0.30431585199412026, test loss: 0.3845326968117512\n",
      "epoch 728: train loss: 0.3041437396011263, test loss: 0.3843608993844464\n",
      "epoch 729: train loss: 0.3039719762396432, test loss: 0.3841894808252943\n",
      "epoch 730: train loss: 0.30380056079291273, test loss: 0.38401843989684686\n",
      "epoch 731: train loss: 0.3036294921490776, test loss: 0.3838477753669794\n",
      "epoch 732: train loss: 0.3034587692011542, test loss: 0.38367748600886076\n",
      "epoch 733: train loss: 0.3032883908470055, test loss: 0.38350757060092533\n",
      "epoch 734: train loss: 0.30311835598931275, test loss: 0.3833380279268466\n",
      "epoch 735: train loss: 0.30294866353554967, test loss: 0.38316885677550644\n",
      "epoch 736: train loss: 0.30277931239795475, test loss: 0.38300005594096914\n",
      "epoch 737: train loss: 0.3026103014935049, test loss: 0.3828316242224531\n",
      "epoch 738: train loss: 0.3024416297438888, test loss: 0.38266356042430366\n",
      "epoch 739: train loss: 0.3022732960754808, test loss: 0.3824958633559652\n",
      "epoch 740: train loss: 0.3021052994193145, test loss: 0.38232853183195425\n",
      "epoch 741: train loss: 0.30193763871105717, test loss: 0.38216156467183376\n",
      "epoch 742: train loss: 0.30177031289098344, test loss: 0.38199496070018424\n",
      "epoch 743: train loss: 0.30160332090395003, test loss: 0.3818287187465784\n",
      "epoch 744: train loss: 0.30143666169937033, test loss: 0.3816628376455554\n",
      "epoch 745: train loss: 0.30127033423118915, test loss: 0.38149731623659233\n",
      "epoch 746: train loss: 0.30110433745785736, test loss: 0.38133215336408055\n",
      "epoch 747: train loss: 0.30093867034230715, test loss: 0.38116734787729806\n",
      "epoch 748: train loss: 0.30077333185192723, test loss: 0.38100289863038467\n",
      "epoch 749: train loss: 0.30060832095853846, test loss: 0.38083880448231655\n",
      "epoch 750: train loss: 0.3004436366383691, test loss: 0.3806750642968801\n",
      "epoch 751: train loss: 0.3002792778720307, test loss: 0.38051167694264615\n",
      "epoch 752: train loss: 0.3001152436444937, test loss: 0.38034864129294704\n",
      "epoch 753: train loss: 0.2999515329450643, test loss: 0.38018595622584883\n",
      "epoch 754: train loss: 0.29978814476735977, test loss: 0.38002362062412914\n",
      "epoch 755: train loss: 0.29962507810928524, test loss: 0.3798616333752504\n",
      "epoch 756: train loss: 0.29946233197301025, test loss: 0.3796999933713373\n",
      "epoch 757: train loss: 0.2992999053649455, test loss: 0.37953869950915\n",
      "epoch 758: train loss: 0.29913779729571954, test loss: 0.3793777506900632\n",
      "epoch 759: train loss: 0.2989760067801558, test loss: 0.37921714582003857\n",
      "epoch 760: train loss: 0.2988145328372497, test loss: 0.37905688380960356\n",
      "epoch 761: train loss: 0.29865337449014623, test loss: 0.3788969635738264\n",
      "epoch 762: train loss: 0.29849253076611715, test loss: 0.37873738403229296\n",
      "epoch 763: train loss: 0.2983320006965386, test loss: 0.3785781441090839\n",
      "epoch 764: train loss: 0.29817178331686905, test loss: 0.37841924273274896\n",
      "epoch 765: train loss: 0.29801187766662685, test loss: 0.37826067883628683\n",
      "epoch 766: train loss: 0.29785228278936876, test loss: 0.37810245135712106\n",
      "epoch 767: train loss: 0.2976929977326677, test loss: 0.3779445592370764\n",
      "epoch 768: train loss: 0.2975340215480915, test loss: 0.37778700142235755\n",
      "epoch 769: train loss: 0.297375353291181, test loss: 0.37762977686352384\n",
      "epoch 770: train loss: 0.29721699202142865, test loss: 0.3774728845154715\n",
      "epoch 771: train loss: 0.2970589368022578, test loss: 0.3773163233374066\n",
      "epoch 772: train loss: 0.296901186701001, test loss: 0.3771600922928258\n",
      "epoch 773: train loss: 0.29674374078887944, test loss: 0.37700419034949306\n",
      "epoch 774: train loss: 0.2965865981409818, test loss: 0.37684861647941886\n",
      "epoch 775: train loss: 0.2964297578362439, test loss: 0.3766933696588362\n",
      "epoch 776: train loss: 0.2962732189574279, test loss: 0.376538448868183\n",
      "epoch 777: train loss: 0.296116980591102, test loss: 0.3763838530920756\n",
      "epoch 778: train loss: 0.2959610418276199, test loss: 0.3762295813192927\n",
      "epoch 779: train loss: 0.29580540176110115, test loss: 0.3760756325427487\n",
      "epoch 780: train loss: 0.2956500594894107, test loss: 0.3759220057594775\n",
      "epoch 781: train loss: 0.2954950141141391, test loss: 0.3757686999706085\n",
      "epoch 782: train loss: 0.29534026474058284, test loss: 0.37561571418134726\n",
      "epoch 783: train loss: 0.2951858104777246, test loss: 0.3754630474009542\n",
      "epoch 784: train loss: 0.2950316504382138, test loss: 0.3753106986427233\n",
      "epoch 785: train loss: 0.29487778373834717, test loss: 0.37515866692396416\n",
      "epoch 786: train loss: 0.2947242094980495, test loss: 0.3750069512659774\n",
      "epoch 787: train loss: 0.2945709268408547, test loss: 0.3748555506940403\n",
      "epoch 788: train loss: 0.29441793489388624, test loss: 0.37470446423738174\n",
      "epoch 789: train loss: 0.294265232787839, test loss: 0.3745536909291636\n",
      "epoch 790: train loss: 0.29411281965695996, test loss: 0.3744032298064623\n",
      "epoch 791: train loss: 0.29396069463902974, test loss: 0.37425307991024825\n",
      "epoch 792: train loss: 0.293808856875344, test loss: 0.3741032402853658\n",
      "epoch 793: train loss: 0.2936573055106952, test loss: 0.373953709980514\n",
      "epoch 794: train loss: 0.29350603969335404, test loss: 0.37380448804822836\n",
      "epoch 795: train loss: 0.29335505857505156, test loss: 0.37365557354485907\n",
      "epoch 796: train loss: 0.2932043613109609, test loss: 0.37350696553055557\n",
      "epoch 797: train loss: 0.2930539470596793, test loss: 0.37335866306924387\n",
      "epoch 798: train loss: 0.29290381498321055, test loss: 0.37321066522861024\n",
      "epoch 799: train loss: 0.2927539642469471, test loss: 0.3730629710800812\n",
      "epoch 800: train loss: 0.2926043940196522, test loss: 0.37291557969880484\n",
      "epoch 801: train loss: 0.29245510347344295, test loss: 0.37276849016363345\n",
      "epoch 802: train loss: 0.29230609178377237, test loss: 0.3726217015571034\n",
      "epoch 803: train loss: 0.2921573581294126, test loss: 0.37247521296541847\n",
      "epoch 804: train loss: 0.29200890169243743, test loss: 0.37232902347843\n",
      "epoch 805: train loss: 0.2918607216582053, test loss: 0.3721831321896205\n",
      "epoch 806: train loss: 0.2917128172153424, test loss: 0.37203753819608393\n",
      "epoch 807: train loss: 0.2915651875557259, test loss: 0.37189224059850956\n",
      "epoch 808: train loss: 0.29141783187446696, test loss: 0.37174723850116304\n",
      "epoch 809: train loss: 0.2912707493698944, test loss: 0.37160253101186874\n",
      "epoch 810: train loss: 0.291123939243538, test loss: 0.3714581172419928\n",
      "epoch 811: train loss: 0.2909774007001122, test loss: 0.37131399630642464\n",
      "epoch 812: train loss: 0.29083113294749946, test loss: 0.3711701673235613\n",
      "epoch 813: train loss: 0.29068513519673467, test loss: 0.3710266294152889\n",
      "epoch 814: train loss: 0.29053940666198824, test loss: 0.370883381706965\n",
      "epoch 815: train loss: 0.2903939465605509, test loss: 0.3707404233274033\n",
      "epoch 816: train loss: 0.29024875411281703, test loss: 0.3705977534088552\n",
      "epoch 817: train loss: 0.29010382854226946, test loss: 0.37045537108699383\n",
      "epoch 818: train loss: 0.2899591690754633, test loss: 0.37031327550089643\n",
      "epoch 819: train loss: 0.28981477494201063, test loss: 0.3701714657930286\n",
      "epoch 820: train loss: 0.2896706453745649, test loss: 0.37002994110922754\n",
      "epoch 821: train loss: 0.2895267796088053, test loss: 0.36988870059868434\n",
      "epoch 822: train loss: 0.2893831768834217, test loss: 0.3697477434139298\n",
      "epoch 823: train loss: 0.2892398364400993, test loss: 0.3696070687108174\n",
      "epoch 824: train loss: 0.2890967575235035, test loss: 0.36946667564850605\n",
      "epoch 825: train loss: 0.28895393938126496, test loss: 0.36932656338944375\n",
      "epoch 826: train loss: 0.2888113812639642, test loss: 0.3691867310993555\n",
      "epoch 827: train loss: 0.2886690824251176, test loss: 0.36904717794722147\n",
      "epoch 828: train loss: 0.2885270421211615, test loss: 0.36890790310526617\n",
      "epoch 829: train loss: 0.28838525961143857, test loss: 0.36876890574894056\n",
      "epoch 830: train loss: 0.2882437341581828, test loss: 0.3686301850569068\n",
      "epoch 831: train loss: 0.2881024650265045, test loss: 0.36849174021102216\n",
      "epoch 832: train loss: 0.28796145148437685, test loss: 0.3683535703963248\n",
      "epoch 833: train loss: 0.2878206928026209, test loss: 0.36821567480101736\n",
      "epoch 834: train loss: 0.28768018825489156, test loss: 0.3680780526164525\n",
      "epoch 835: train loss: 0.2875399371176634, test loss: 0.3679407030371169\n",
      "epoch 836: train loss: 0.2873999386702166, test loss: 0.36780362526061794\n",
      "epoch 837: train loss: 0.287260192194623, test loss: 0.3676668184876655\n",
      "epoch 838: train loss: 0.2871206969757323, test loss: 0.3675302819220601\n",
      "epoch 839: train loss: 0.2869814523011578, test loss: 0.36739401477067724\n",
      "epoch 840: train loss: 0.28684245746126336, test loss: 0.3672580162434525\n",
      "epoch 841: train loss: 0.28670371174914944, test loss: 0.36712228555336607\n",
      "epoch 842: train loss: 0.28656521446063926, test loss: 0.36698682191643006\n",
      "epoch 843: train loss: 0.2864269648942658, test loss: 0.36685162455167164\n",
      "epoch 844: train loss: 0.2862889623512581, test loss: 0.36671669268112156\n",
      "epoch 845: train loss: 0.28615120613552825, test loss: 0.3665820255297968\n",
      "epoch 846: train loss: 0.2860136955536576, test loss: 0.36644762232568906\n",
      "epoch 847: train loss: 0.2858764299148843, test loss: 0.36631348229974847\n",
      "epoch 848: train loss: 0.2857394085310898, test loss: 0.3661796046858709\n",
      "epoch 849: train loss: 0.28560263071678593, test loss: 0.3660459887208835\n",
      "epoch 850: train loss: 0.2854660957891019, test loss: 0.3659126336445307\n",
      "epoch 851: train loss: 0.2853298030677718, test loss: 0.36577953869946106\n",
      "epoch 852: train loss: 0.2851937518751215, test loss: 0.3656467031312118\n",
      "epoch 853: train loss: 0.2850579415360561, test loss: 0.36551412618819795\n",
      "epoch 854: train loss: 0.28492237137804727, test loss: 0.36538180712169605\n",
      "epoch 855: train loss: 0.2847870407311208, test loss: 0.3652497451858311\n",
      "epoch 856: train loss: 0.2846519489278441, test loss: 0.3651179396375666\n",
      "epoch 857: train loss: 0.2845170953033137, test loss: 0.36498638973668535\n",
      "epoch 858: train loss: 0.2843824791951432, test loss: 0.3648550947457811\n",
      "epoch 859: train loss: 0.2842480999434509, test loss: 0.3647240539302418\n",
      "epoch 860: train loss: 0.2841139568908475, test loss: 0.36459326655824\n",
      "epoch 861: train loss: 0.2839800493824242, test loss: 0.3644627319007174\n",
      "epoch 862: train loss: 0.28384637676574065, test loss: 0.36433244923137115\n",
      "epoch 863: train loss: 0.2837129383908129, test loss: 0.3642024178266442\n",
      "epoch 864: train loss: 0.2835797336101017, test loss: 0.36407263696570874\n",
      "epoch 865: train loss: 0.28344676177850053, test loss: 0.36394310593045703\n",
      "epoch 866: train loss: 0.28331402225332397, test loss: 0.3638138240054844\n",
      "epoch 867: train loss: 0.2831815143942959, test loss: 0.36368479047808167\n",
      "epoch 868: train loss: 0.28304923756353817, test loss: 0.36355600463821863\n",
      "epoch 869: train loss: 0.2829171911255587, test loss: 0.363427465778533\n",
      "epoch 870: train loss: 0.28278537444724017, test loss: 0.3632991731943185\n",
      "epoch 871: train loss: 0.28265378689782905, test loss: 0.36317112618351216\n",
      "epoch 872: train loss: 0.2825224278489235, test loss: 0.3630433240466812\n",
      "epoch 873: train loss: 0.28239129667446267, test loss: 0.36291576608701187\n",
      "epoch 874: train loss: 0.28226039275071535, test loss: 0.36278845161029766\n",
      "epoch 875: train loss: 0.2821297154562688, test loss: 0.3626613799249255\n",
      "epoch 876: train loss: 0.28199926417201765, test loss: 0.3625345503418659\n",
      "epoch 877: train loss: 0.28186903828115306, test loss: 0.3624079621746589\n",
      "epoch 878: train loss: 0.28173903716915183, test loss: 0.36228161473940446\n",
      "epoch 879: train loss: 0.28160926022376515, test loss: 0.36215550735474883\n",
      "epoch 880: train loss: 0.28147970683500817, test loss: 0.36202963934187316\n",
      "epoch 881: train loss: 0.28135037639514915, test loss: 0.3619040100244833\n",
      "epoch 882: train loss: 0.2812212682986986, test loss: 0.36177861872879596\n",
      "epoch 883: train loss: 0.28109238194239905, test loss: 0.3616534647835291\n",
      "epoch 884: train loss: 0.2809637167252141, test loss: 0.3615285475198891\n",
      "epoch 885: train loss: 0.28083527204831815, test loss: 0.36140386627156007\n",
      "epoch 886: train loss: 0.2807070473150859, test loss: 0.3612794203746924\n",
      "epoch 887: train loss: 0.28057904193108196, test loss: 0.36115520916789107\n",
      "epoch 888: train loss: 0.2804512553040506, test loss: 0.36103123199220577\n",
      "epoch 889: train loss: 0.28032368684390535, test loss: 0.36090748819111773\n",
      "epoch 890: train loss: 0.2801963359627189, test loss: 0.36078397711052984\n",
      "epoch 891: train loss: 0.2800692020747132, test loss: 0.3606606980987557\n",
      "epoch 892: train loss: 0.27994228459624876, test loss: 0.3605376505065091\n",
      "epoch 893: train loss: 0.2798155829458154, test loss: 0.36041483368689\n",
      "epoch 894: train loss: 0.27968909654402163, test loss: 0.36029224699537893\n",
      "epoch 895: train loss: 0.27956282481358513, test loss: 0.36016988978982134\n",
      "epoch 896: train loss: 0.27943676717932275, test loss: 0.36004776143041967\n",
      "epoch 897: train loss: 0.2793109230681409, test loss: 0.3599258612797215\n",
      "epoch 898: train loss: 0.27918529190902547, test loss: 0.3598041887026098\n",
      "epoch 899: train loss: 0.2790598731330324, test loss: 0.3596827430662908\n",
      "epoch 900: train loss: 0.27893466617327806, test loss: 0.3595615237402859\n",
      "epoch 901: train loss: 0.2788096704649296, test loss: 0.35944053009641935\n",
      "epoch 902: train loss: 0.2786848854451955, test loss: 0.35931976150880707\n",
      "epoch 903: train loss: 0.278560310553316, test loss: 0.3591992173538501\n",
      "epoch 904: train loss: 0.27843594523055376, test loss: 0.3590788970102194\n",
      "epoch 905: train loss: 0.27831178892018454, test loss: 0.35895879985884965\n",
      "epoch 906: train loss: 0.278187841067488, test loss: 0.35883892528292693\n",
      "epoch 907: train loss: 0.278064101119738, test loss: 0.3587192726678793\n",
      "epoch 908: train loss: 0.277940568526194, test loss: 0.3585998414013663\n",
      "epoch 909: train loss: 0.27781724273809155, test loss: 0.35848063087326965\n",
      "epoch 910: train loss: 0.2776941232086332, test loss: 0.3583616404756831\n",
      "epoch 911: train loss: 0.2775712093929797, test loss: 0.3582428696029011\n",
      "epoch 912: train loss: 0.27744850074824073, test loss: 0.3581243176514126\n",
      "epoch 913: train loss: 0.27732599673346614, test loss: 0.35800598401988637\n",
      "epoch 914: train loss: 0.2772036968096371, test loss: 0.35788786810916506\n",
      "epoch 915: train loss: 0.27708160043965696, test loss: 0.35776996932225413\n",
      "epoch 916: train loss: 0.2769597070883429, test loss: 0.3576522870643123\n",
      "epoch 917: train loss: 0.27683801622241677, test loss: 0.35753482074264153\n",
      "epoch 918: train loss: 0.2767165273104968, test loss: 0.3574175697666789\n",
      "epoch 919: train loss: 0.2765952398230886, test loss: 0.357300533547985\n",
      "epoch 920: train loss: 0.2764741532325767, test loss: 0.3571837115002366\n",
      "epoch 921: train loss: 0.2763532670132161, test loss: 0.3570671030392157\n",
      "epoch 922: train loss: 0.2762325806411237, test loss: 0.3569507075828011\n",
      "epoch 923: train loss: 0.27611209359426997, test loss: 0.3568345245509596\n",
      "epoch 924: train loss: 0.2759918053524701, test loss: 0.3567185533657354\n",
      "epoch 925: train loss: 0.27587171539737626, test loss: 0.35660279345124046\n",
      "epoch 926: train loss: 0.2757518232124688, test loss: 0.3564872442336488\n",
      "epoch 927: train loss: 0.2756321282830485, test loss: 0.35637190514118317\n",
      "epoch 928: train loss: 0.2755126300962277, test loss: 0.3562567756041096\n",
      "epoch 929: train loss: 0.27539332814092254, test loss: 0.3561418550547248\n",
      "epoch 930: train loss: 0.275274221907845, test loss: 0.3560271429273509\n",
      "epoch 931: train loss: 0.2751553108894943, test loss: 0.3559126386583238\n",
      "epoch 932: train loss: 0.2750365945801493, test loss: 0.35579834168598606\n",
      "epoch 933: train loss: 0.27491807247586036, test loss: 0.3556842514506771\n",
      "epoch 934: train loss: 0.2747997440744413, test loss: 0.35557036739472453\n",
      "epoch 935: train loss: 0.27468160887546167, test loss: 0.35545668896243626\n",
      "epoch 936: train loss: 0.2745636663802387, test loss: 0.3553432156000902\n",
      "epoch 937: train loss: 0.2744459160918298, test loss: 0.3552299467559285\n",
      "epoch 938: train loss: 0.2743283575150242, test loss: 0.3551168818801456\n",
      "epoch 939: train loss: 0.274210990156336, test loss: 0.35500402042488133\n",
      "epoch 940: train loss: 0.2740938135239959, test loss: 0.35489136184421355\n",
      "epoch 941: train loss: 0.27397682712794363, test loss: 0.3547789055941473\n",
      "epoch 942: train loss: 0.27386003047982066, test loss: 0.35466665113260887\n",
      "epoch 943: train loss: 0.2737434230929622, test loss: 0.3545545979194349\n",
      "epoch 944: train loss: 0.2736270044823899, test loss: 0.3544427454163667\n",
      "epoch 945: train loss: 0.2735107741648046, test loss: 0.35433109308703964\n",
      "epoch 946: train loss: 0.2733947316585785, test loss: 0.354219640396977\n",
      "epoch 947: train loss: 0.27327887648374793, test loss: 0.3541083868135803\n",
      "epoch 948: train loss: 0.2731632081620059, test loss: 0.3539973318061215\n",
      "epoch 949: train loss: 0.27304772621669504, test loss: 0.3538864748457351\n",
      "epoch 950: train loss: 0.27293243017280006, test loss: 0.35377581540541103\n",
      "epoch 951: train loss: 0.27281731955694044, test loss: 0.353665352959984\n",
      "epoch 952: train loss: 0.2727023938973636, test loss: 0.3535550869861289\n",
      "epoch 953: train loss: 0.2725876527239375, test loss: 0.35344501696234987\n",
      "epoch 954: train loss: 0.27247309556814336, test loss: 0.35333514236897534\n",
      "epoch 955: train loss: 0.272358721963069, test loss: 0.3532254626881465\n",
      "epoch 956: train loss: 0.2722445314434012, test loss: 0.35311597740381323\n",
      "epoch 957: train loss: 0.2721305235454195, test loss: 0.35300668600172375\n",
      "epoch 958: train loss: 0.27201669780698845, test loss: 0.3528975879694191\n",
      "epoch 959: train loss: 0.271903053767551, test loss: 0.3527886827962231\n",
      "epoch 960: train loss: 0.2717895909681217, test loss: 0.3526799699732361\n",
      "epoch 961: train loss: 0.27167630895127975, test loss: 0.35257144899332643\n",
      "epoch 962: train loss: 0.27156320726116223, test loss: 0.3524631193511262\n",
      "epoch 963: train loss: 0.2714502854434571, test loss: 0.35235498054301734\n",
      "epoch 964: train loss: 0.2713375430453967, test loss: 0.35224703206712976\n",
      "epoch 965: train loss: 0.2712249796157509, test loss: 0.35213927342333307\n",
      "epoch 966: train loss: 0.27111259470482063, test loss: 0.3520317041132259\n",
      "epoch 967: train loss: 0.2710003878644309, test loss: 0.3519243236401316\n",
      "epoch 968: train loss: 0.2708883586479246, test loss: 0.35181713150909133\n",
      "epoch 969: train loss: 0.27077650661015523, test loss: 0.35171012722685313\n",
      "epoch 970: train loss: 0.2706648313074815, test loss: 0.35160331030186925\n",
      "epoch 971: train loss: 0.27055333229775974, test loss: 0.3514966802442847\n",
      "epoch 972: train loss: 0.270442009140338, test loss: 0.3513902365659337\n",
      "epoch 973: train loss: 0.27033086139604956, test loss: 0.35128397878032985\n",
      "epoch 974: train loss: 0.2702198886272062, test loss: 0.3511779064026601\n",
      "epoch 975: train loss: 0.27010909039759234, test loss: 0.3510720189497787\n",
      "epoch 976: train loss: 0.26999846627245855, test loss: 0.3509663159401969\n",
      "epoch 977: train loss: 0.2698880158185149, test loss: 0.350860796894081\n",
      "epoch 978: train loss: 0.2697777386039252, test loss: 0.3507554613332396\n",
      "epoch 979: train loss: 0.26966763419830037, test loss: 0.3506503087811215\n",
      "epoch 980: train loss: 0.2695577021726926, test loss: 0.3505453387628061\n",
      "epoch 981: train loss: 0.26944794209958883, test loss: 0.35044055080499836\n",
      "epoch 982: train loss: 0.2693383535529049, test loss: 0.3503359444360181\n",
      "epoch 983: train loss: 0.26922893610797943, test loss: 0.3502315191857995\n",
      "epoch 984: train loss: 0.2691196893415674, test loss: 0.3501272745858788\n",
      "epoch 985: train loss: 0.2690106128318346, test loss: 0.35002321016938964\n",
      "epoch 986: train loss: 0.2689017061583514, test loss: 0.34991932547105725\n",
      "epoch 987: train loss: 0.2687929689020867, test loss: 0.34981562002718997\n",
      "epoch 988: train loss: 0.26868440064540217, test loss: 0.34971209337567466\n",
      "epoch 989: train loss: 0.26857600097204615, test loss: 0.3496087450559676\n",
      "epoch 990: train loss: 0.26846776946714784, test loss: 0.3495055746090904\n",
      "epoch 991: train loss: 0.2683597057172117, test loss: 0.3494025815776212\n",
      "epoch 992: train loss: 0.2682518093101113, test loss: 0.3492997655056918\n",
      "epoch 993: train loss: 0.2681440798350835, test loss: 0.34919712593897473\n",
      "epoch 994: train loss: 0.26803651688272295, test loss: 0.34909466242468457\n",
      "epoch 995: train loss: 0.26792912004497643, test loss: 0.3489923745115663\n",
      "epoch 996: train loss: 0.2678218889151367, test loss: 0.3488902617498907\n",
      "epoch 997: train loss: 0.26771482308783723, test loss: 0.34878832369144674\n",
      "epoch 998: train loss: 0.2676079221590465, test loss: 0.34868655988953806\n",
      "epoch 999: train loss: 0.2675011857260622, test loss: 0.3485849698989739\n",
      "epoch 1000: train loss: 0.2673946133875059, test loss: 0.34848355327606356\n",
      "epoch 1001: train loss: 0.26728820474331727, test loss: 0.348382309578611\n",
      "epoch 1002: train loss: 0.2671819593947488, test loss: 0.3482812383659087\n",
      "epoch 1003: train loss: 0.2670758769443601, test loss: 0.3481803391987303\n",
      "epoch 1004: train loss: 0.26696995699601245, test loss: 0.34807961163932505\n",
      "epoch 1005: train loss: 0.2668641991548636, test loss: 0.3479790552514123\n",
      "epoch 1006: train loss: 0.26675860302736193, test loss: 0.34787866960017466\n",
      "epoch 1007: train loss: 0.2666531682212416, test loss: 0.3477784542522527\n",
      "epoch 1008: train loss: 0.2665478943455167, test loss: 0.34767840877573775\n",
      "epoch 1009: train loss: 0.2664427810104762, test loss: 0.34757853274016753\n",
      "epoch 1010: train loss: 0.2663378278276787, test loss: 0.3474788257165187\n",
      "epoch 1011: train loss: 0.26623303440994683, test loss: 0.347379287277203\n",
      "epoch 1012: train loss: 0.2661284003713625, test loss: 0.34727991699605837\n",
      "epoch 1013: train loss: 0.26602392532726116, test loss: 0.3471807144483447\n",
      "epoch 1014: train loss: 0.26591960889422694, test loss: 0.34708167921074023\n",
      "epoch 1015: train loss: 0.2658154506900875, test loss: 0.3469828108613309\n",
      "epoch 1016: train loss: 0.2657114503339086, test loss: 0.34688410897960886\n",
      "epoch 1017: train loss: 0.2656076074459893, test loss: 0.3467855731464644\n",
      "epoch 1018: train loss: 0.26550392164785686, test loss: 0.34668720294418137\n",
      "epoch 1019: train loss: 0.2654003925622612, test loss: 0.34658899795643117\n",
      "epoch 1020: train loss: 0.2652970198131707, test loss: 0.34649095776826594\n",
      "epoch 1021: train loss: 0.26519380302576645, test loss: 0.34639308196611623\n",
      "epoch 1022: train loss: 0.2650907418264375, test loss: 0.3462953701377799\n",
      "epoch 1023: train loss: 0.2649878358427761, test loss: 0.34619782187242265\n",
      "epoch 1024: train loss: 0.26488508470357247, test loss: 0.3461004367605684\n",
      "epoch 1025: train loss: 0.2647824880388102, test loss: 0.3460032143940947\n",
      "epoch 1026: train loss: 0.2646800454796611, test loss: 0.34590615436622885\n",
      "epoch 1027: train loss: 0.2645777566584804, test loss: 0.3458092562715391\n",
      "epoch 1028: train loss: 0.2644756212088019, test loss: 0.34571251970593364\n",
      "epoch 1029: train loss: 0.2643736387653335, test loss: 0.3456159442666499\n",
      "epoch 1030: train loss: 0.26427180896395164, test loss: 0.34551952955225546\n",
      "epoch 1031: train loss: 0.2641701314416975, test loss: 0.3454232751626354\n",
      "epoch 1032: train loss: 0.2640686058367716, test loss: 0.3453271806989935\n",
      "epoch 1033: train loss: 0.2639672317885291, test loss: 0.3452312457638436\n",
      "epoch 1034: train loss: 0.2638660089374755, test loss: 0.3451354699610048\n",
      "epoch 1035: train loss: 0.26376493692526176, test loss: 0.34503985289559624\n",
      "epoch 1036: train loss: 0.26366401539467943, test loss: 0.3449443941740327\n",
      "epoch 1037: train loss: 0.26356324398965647, test loss: 0.3448490934040166\n",
      "epoch 1038: train loss: 0.2634626223552523, test loss: 0.3447539501945374\n",
      "epoch 1039: train loss: 0.26336215013765335, test loss: 0.34465896415586256\n",
      "epoch 1040: train loss: 0.26326182698416856, test loss: 0.3445641348995342\n",
      "epoch 1041: train loss: 0.26316165254322477, test loss: 0.34446946203836293\n",
      "epoch 1042: train loss: 0.2630616264643623, test loss: 0.3443749451864241\n",
      "epoch 1043: train loss: 0.26296174839823033, test loss: 0.3442805839590521\n",
      "epoch 1044: train loss: 0.2628620179965825, test loss: 0.3441863779728336\n",
      "epoch 1045: train loss: 0.2627624349122726, test loss: 0.34409232684560687\n",
      "epoch 1046: train loss: 0.2626629987992498, test loss: 0.3439984301964514\n",
      "epoch 1047: train loss: 0.2625637093125548, test loss: 0.34390468764568766\n",
      "epoch 1048: train loss: 0.26246456610831503, test loss: 0.3438110988148685\n",
      "epoch 1049: train loss: 0.26236556884374024, test loss: 0.3437176633267778\n",
      "epoch 1050: train loss: 0.2622667171771185, test loss: 0.3436243808054212\n",
      "epoch 1051: train loss: 0.2621680107678118, test loss: 0.34353125087602554\n",
      "epoch 1052: train loss: 0.2620694492762515, test loss: 0.3434382731650315\n",
      "epoch 1053: train loss: 0.26197103236393426, test loss: 0.343345447300089\n",
      "epoch 1054: train loss: 0.26187275969341806, test loss: 0.3432527729100535\n",
      "epoch 1055: train loss: 0.2617746309283174, test loss: 0.34316024962497993\n",
      "epoch 1056: train loss: 0.26167664573329946, test loss: 0.34306787707611847\n",
      "epoch 1057: train loss: 0.26157880377407994, test loss: 0.34297565489591036\n",
      "epoch 1058: train loss: 0.26148110471741876, test loss: 0.3428835827179826\n",
      "epoch 1059: train loss: 0.2613835482311158, test loss: 0.342791660177142\n",
      "epoch 1060: train loss: 0.26128613398400735, test loss: 0.3426998869093743\n",
      "epoch 1061: train loss: 0.2611888616459611, test loss: 0.3426082625518346\n",
      "epoch 1062: train loss: 0.2610917308878729, test loss: 0.34251678674284736\n",
      "epoch 1063: train loss: 0.2609947413816621, test loss: 0.3424254591218968\n",
      "epoch 1064: train loss: 0.260897892800268, test loss: 0.34233427932962845\n",
      "epoch 1065: train loss: 0.26080118481764536, test loss: 0.34224324700783854\n",
      "epoch 1066: train loss: 0.26070461710876075, test loss: 0.34215236179947395\n",
      "epoch 1067: train loss: 0.2606081893495883, test loss: 0.34206162334862467\n",
      "epoch 1068: train loss: 0.26051190121710605, test loss: 0.3419710313005222\n",
      "epoch 1069: train loss: 0.2604157523892916, test loss: 0.341880585301532\n",
      "epoch 1070: train loss: 0.2603197425451185, test loss: 0.34179028499914976\n",
      "epoch 1071: train loss: 0.2602238713645524, test loss: 0.34170013004200045\n",
      "epoch 1072: train loss: 0.2601281385285466, test loss: 0.3416101200798285\n",
      "epoch 1073: train loss: 0.260032543719039, test loss: 0.3415202547634969\n",
      "epoch 1074: train loss: 0.2599370866189475, test loss: 0.3414305337449827\n",
      "epoch 1075: train loss: 0.25984176691216676, test loss: 0.3413409566773708\n",
      "epoch 1076: train loss: 0.2597465842835639, test loss: 0.3412515232148515\n",
      "epoch 1077: train loss: 0.2596515384189751, test loss: 0.34116223301271453\n",
      "epoch 1078: train loss: 0.2595566290052014, test loss: 0.34107308572734696\n",
      "epoch 1079: train loss: 0.25946185573000546, test loss: 0.3409840810162268\n",
      "epoch 1080: train loss: 0.2593672182821074, test loss: 0.3408952185379185\n",
      "epoch 1081: train loss: 0.25927271635118126, test loss: 0.34080649795207235\n",
      "epoch 1082: train loss: 0.25917834962785113, test loss: 0.34071791891941544\n",
      "epoch 1083: train loss: 0.2590841178036877, test loss: 0.3406294811017506\n",
      "epoch 1084: train loss: 0.25899002057120446, test loss: 0.3405411841619512\n",
      "epoch 1085: train loss: 0.258896057623854, test loss: 0.3404530277639577\n",
      "epoch 1086: train loss: 0.2588022286560244, test loss: 0.34036501157277144\n",
      "epoch 1087: train loss: 0.25870853336303584, test loss: 0.3402771352544532\n",
      "epoch 1088: train loss: 0.2586149714411365, test loss: 0.3401893984761179\n",
      "epoch 1089: train loss: 0.25852154258749954, test loss: 0.3401018009059304\n",
      "epoch 1090: train loss: 0.258428246500219, test loss: 0.34001434221310106\n",
      "epoch 1091: train loss: 0.2583350828783069, test loss: 0.3399270220678835\n",
      "epoch 1092: train loss: 0.258242051421689, test loss: 0.33983984014156726\n",
      "epoch 1093: train loss: 0.2581491518312015, test loss: 0.339752796106478\n",
      "epoch 1094: train loss: 0.2580563838085881, test loss: 0.3396658896359694\n",
      "epoch 1095: train loss: 0.2579637470564956, test loss: 0.3395791204044228\n",
      "epoch 1096: train loss: 0.2578712412784713, test loss: 0.3394924880872404\n",
      "epoch 1097: train loss: 0.2577788661789587, test loss: 0.3394059923608431\n",
      "epoch 1098: train loss: 0.25768662146329485, test loss: 0.33931963290266537\n",
      "epoch 1099: train loss: 0.25759450683770646, test loss: 0.3392334093911525\n",
      "epoch 1100: train loss: 0.2575025220093067, test loss: 0.33914732150575644\n",
      "epoch 1101: train loss: 0.2574106666860916, test loss: 0.33906136892693056\n",
      "epoch 1102: train loss: 0.25731894057693694, test loss: 0.338975551336128\n",
      "epoch 1103: train loss: 0.25722734339159475, test loss: 0.3388898684157964\n",
      "epoch 1104: train loss: 0.25713587484068995, test loss: 0.33880431984937476\n",
      "epoch 1105: train loss: 0.2570445346357171, test loss: 0.33871890532128823\n",
      "epoch 1106: train loss: 0.25695332248903713, test loss: 0.33863362451694656\n",
      "epoch 1107: train loss: 0.256862238113874, test loss: 0.3385484771227391\n",
      "epoch 1108: train loss: 0.2567712812243111, test loss: 0.33846346282603074\n",
      "epoch 1109: train loss: 0.25668045153528873, test loss: 0.33837858131515886\n",
      "epoch 1110: train loss: 0.2565897487626001, test loss: 0.3382938322794301\n",
      "epoch 1111: train loss: 0.2564991726228886, test loss: 0.3382092154091138\n",
      "epoch 1112: train loss: 0.25640872283364435, test loss: 0.33812473039544316\n",
      "epoch 1113: train loss: 0.2563183991132011, test loss: 0.33804037693060707\n",
      "epoch 1114: train loss: 0.2562282011807328, test loss: 0.33795615470774876\n",
      "epoch 1115: train loss: 0.25613812875625075, test loss: 0.3378720634209621\n",
      "epoch 1116: train loss: 0.2560481815606004, test loss: 0.3377881027652876\n",
      "epoch 1117: train loss: 0.2559583593154581, test loss: 0.33770427243670764\n",
      "epoch 1118: train loss: 0.2558686617433278, test loss: 0.337620572132146\n",
      "epoch 1119: train loss: 0.2557790885675384, test loss: 0.33753700154946104\n",
      "epoch 1120: train loss: 0.2556896395122402, test loss: 0.33745356038744395\n",
      "epoch 1121: train loss: 0.25560031430240215, test loss: 0.337370248345814\n",
      "epoch 1122: train loss: 0.25551111266380844, test loss: 0.3372870651252173\n",
      "epoch 1123: train loss: 0.2554220343230558, test loss: 0.33720401042722076\n",
      "epoch 1124: train loss: 0.25533307900755026, test loss: 0.3371210839543089\n",
      "epoch 1125: train loss: 0.25524424644550414, test loss: 0.33703828540988345\n",
      "epoch 1126: train loss: 0.25515553636593297, test loss: 0.3369556144982548\n",
      "epoch 1127: train loss: 0.2550669484986527, test loss: 0.33687307092464314\n",
      "epoch 1128: train loss: 0.2549784825742766, test loss: 0.3367906543951731\n",
      "epoch 1129: train loss: 0.2548901383242121, test loss: 0.33670836461686976\n",
      "epoch 1130: train loss: 0.25480191548065806, test loss: 0.3366262012976564\n",
      "epoch 1131: train loss: 0.254713813776602, test loss: 0.3365441641463509\n",
      "epoch 1132: train loss: 0.25462583294581664, test loss: 0.33646225287266174\n",
      "epoch 1133: train loss: 0.2545379727228575, test loss: 0.3363804671871853\n",
      "epoch 1134: train loss: 0.25445023284305957, test loss: 0.33629880680140256\n",
      "epoch 1135: train loss: 0.2543626130425347, test loss: 0.3362172714276751\n",
      "epoch 1136: train loss: 0.2542751130581687, test loss: 0.33613586077924285\n",
      "epoch 1137: train loss: 0.2541877326276185, test loss: 0.336054574570219\n",
      "epoch 1138: train loss: 0.2541004714893089, test loss: 0.3359734125155904\n",
      "epoch 1139: train loss: 0.25401332938243043, test loss: 0.3358923743312083\n",
      "epoch 1140: train loss: 0.2539263060469359, test loss: 0.33581145973379256\n",
      "epoch 1141: train loss: 0.25383940122353776, test loss: 0.3357306684409212\n",
      "epoch 1142: train loss: 0.2537526146537056, test loss: 0.3356500001710315\n",
      "epoch 1143: train loss: 0.2536659460796629, test loss: 0.3355694546434169\n",
      "epoch 1144: train loss: 0.2535793952443848, test loss: 0.3354890315782224\n",
      "epoch 1145: train loss: 0.25349296189159454, test loss: 0.33540873069644045\n",
      "epoch 1146: train loss: 0.25340664576576155, test loss: 0.3353285517199099\n",
      "epoch 1147: train loss: 0.2533204466120983, test loss: 0.33524849437131143\n",
      "epoch 1148: train loss: 0.25323436417655754, test loss: 0.33516855837416687\n",
      "epoch 1149: train loss: 0.25314839820582963, test loss: 0.335088743452832\n",
      "epoch 1150: train loss: 0.25306254844734005, test loss: 0.3350090493324963\n",
      "epoch 1151: train loss: 0.2529768146492463, test loss: 0.3349294757391795\n",
      "epoch 1152: train loss: 0.2528911965604356, test loss: 0.3348500223997279\n",
      "epoch 1153: train loss: 0.25280569393052216, test loss: 0.33477068904181195\n",
      "epoch 1154: train loss: 0.2527203065098443, test loss: 0.3346914753939226\n",
      "epoch 1155: train loss: 0.252635034049462, test loss: 0.3346123811853684\n",
      "epoch 1156: train loss: 0.25254987630115433, test loss: 0.3345334061462741\n",
      "epoch 1157: train loss: 0.25246483301741657, test loss: 0.33445455000757335\n",
      "epoch 1158: train loss: 0.2523799039514579, test loss: 0.3343758125010109\n",
      "epoch 1159: train loss: 0.2522950888571984, test loss: 0.33429719335913644\n",
      "epoch 1160: train loss: 0.2522103874892674, test loss: 0.33421869231530194\n",
      "epoch 1161: train loss: 0.25212579960299936, test loss: 0.3341403091036593\n",
      "epoch 1162: train loss: 0.25204132495443293, test loss: 0.33406204345915824\n",
      "epoch 1163: train loss: 0.25195696330030726, test loss: 0.3339838951175416\n",
      "epoch 1164: train loss: 0.25187271439805997, test loss: 0.3339058638153432\n",
      "epoch 1165: train loss: 0.25178857800582466, test loss: 0.3338279492898848\n",
      "epoch 1166: train loss: 0.251704553882428, test loss: 0.33375015127927427\n",
      "epoch 1167: train loss: 0.2516206417873878, test loss: 0.33367246952240054\n",
      "epoch 1168: train loss: 0.2515368414809099, test loss: 0.33359490375893286\n",
      "epoch 1169: train loss: 0.25145315272388624, test loss: 0.3335174537293173\n",
      "epoch 1170: train loss: 0.25136957527789194, test loss: 0.33344011917477245\n",
      "epoch 1171: train loss: 0.2512861089051832, test loss: 0.3333628998372894\n",
      "epoch 1172: train loss: 0.25120275336869474, test loss: 0.33328579545962617\n",
      "epoch 1173: train loss: 0.25111950843203706, test loss: 0.33320880578530576\n",
      "epoch 1174: train loss: 0.2510363738594945, test loss: 0.33313193055861523\n",
      "epoch 1175: train loss: 0.2509533494160225, test loss: 0.33305516952460024\n",
      "epoch 1176: train loss: 0.2508704348672454, test loss: 0.3329785224290643\n",
      "epoch 1177: train loss: 0.2507876299794537, test loss: 0.3329019890185632\n",
      "epoch 1178: train loss: 0.25070493451960213, test loss: 0.33282556904040606\n",
      "epoch 1179: train loss: 0.25062234825530694, test loss: 0.33274926224265017\n",
      "epoch 1180: train loss: 0.25053987095484376, test loss: 0.3326730683740987\n",
      "epoch 1181: train loss: 0.25045750238714504, test loss: 0.3325969871842973\n",
      "epoch 1182: train loss: 0.25037524232179775, test loss: 0.3325210184235339\n",
      "epoch 1183: train loss: 0.2502930905290413, test loss: 0.332445161842833\n",
      "epoch 1184: train loss: 0.2502110467797648, test loss: 0.33236941719395446\n",
      "epoch 1185: train loss: 0.25012911084550515, test loss: 0.33229378422939104\n",
      "epoch 1186: train loss: 0.25004728249844443, test loss: 0.3322182627023655\n",
      "epoch 1187: train loss: 0.24996556151140775, test loss: 0.33214285236682656\n",
      "epoch 1188: train loss: 0.24988394765786098, test loss: 0.3320675529774496\n",
      "epoch 1189: train loss: 0.24980244071190846, test loss: 0.3319923642896305\n",
      "epoch 1190: train loss: 0.24972104044829066, test loss: 0.3319172860594848\n",
      "epoch 1191: train loss: 0.24963974664238203, test loss: 0.3318423180438451\n",
      "epoch 1192: train loss: 0.24955855907018878, test loss: 0.33176746000025825\n",
      "epoch 1193: train loss: 0.24947747750834648, test loss: 0.3316927116869824\n",
      "epoch 1194: train loss: 0.24939650173411795, test loss: 0.3316180728629841\n",
      "epoch 1195: train loss: 0.2493156315253911, test loss: 0.33154354328793817\n",
      "epoch 1196: train loss: 0.24923486666067668, test loss: 0.33146912272222145\n",
      "epoch 1197: train loss: 0.24915420691910595, test loss: 0.3313948109269138\n",
      "epoch 1198: train loss: 0.2490736520804287, test loss: 0.33132060766379295\n",
      "epoch 1199: train loss: 0.248993201925011, test loss: 0.3312465126953326\n",
      "epoch 1200: train loss: 0.24891285623383297, test loss: 0.3311725257847025\n",
      "epoch 1201: train loss: 0.24883261478848667, test loss: 0.33109864669576083\n",
      "epoch 1202: train loss: 0.24875247737117406, test loss: 0.33102487519305696\n",
      "epoch 1203: train loss: 0.24867244376470451, test loss: 0.3309512110418257\n",
      "epoch 1204: train loss: 0.24859251375249328, test loss: 0.3308776540079862\n",
      "epoch 1205: train loss: 0.24851268711855873, test loss: 0.3308042038581383\n",
      "epoch 1206: train loss: 0.2484329636475206, test loss: 0.33073086035956295\n",
      "epoch 1207: train loss: 0.2483533431245979, test loss: 0.33065762328021564\n",
      "epoch 1208: train loss: 0.24827382533560663, test loss: 0.3305844923887263\n",
      "epoch 1209: train loss: 0.2481944100669579, test loss: 0.33051146745439813\n",
      "epoch 1210: train loss: 0.24811509710565574, test loss: 0.33043854824720326\n",
      "epoch 1211: train loss: 0.24803588623929493, test loss: 0.33036573453777907\n",
      "epoch 1212: train loss: 0.24795677725605927, test loss: 0.3302930260974306\n",
      "epoch 1213: train loss: 0.2478777699447191, test loss: 0.330220422698123\n",
      "epoch 1214: train loss: 0.2477988640946298, test loss: 0.33014792411248206\n",
      "epoch 1215: train loss: 0.24772005949572914, test loss: 0.33007553011379137\n",
      "epoch 1216: train loss: 0.2476413559385356, test loss: 0.3300032404759898\n",
      "epoch 1217: train loss: 0.24756275321414653, test loss: 0.32993105497366815\n",
      "epoch 1218: train loss: 0.24748425111423572, test loss: 0.3298589733820693\n",
      "epoch 1219: train loss: 0.2474058494310517, test loss: 0.3297869954770833\n",
      "epoch 1220: train loss: 0.24732754795741563, test loss: 0.3297151210352473\n",
      "epoch 1221: train loss: 0.24724934648671945, test loss: 0.32964334983374116\n",
      "epoch 1222: train loss: 0.24717124481292374, test loss: 0.3295716816503871\n",
      "epoch 1223: train loss: 0.24709324273055572, test loss: 0.32950011626364495\n",
      "epoch 1224: train loss: 0.24701534003470763, test loss: 0.32942865345261424\n",
      "epoch 1225: train loss: 0.24693753652103442, test loss: 0.3293572929970252\n",
      "epoch 1226: train loss: 0.24685983198575193, test loss: 0.3292860346772442\n",
      "epoch 1227: train loss: 0.2467822262256351, test loss: 0.32921487827426565\n",
      "epoch 1228: train loss: 0.24670471903801583, test loss: 0.3291438235697134\n",
      "epoch 1229: train loss: 0.24662731022078113, test loss: 0.3290728703458352\n",
      "epoch 1230: train loss: 0.24654999957237134, test loss: 0.3290020183855038\n",
      "epoch 1231: train loss: 0.24647278689177804, test loss: 0.3289312674722132\n",
      "epoch 1232: train loss: 0.2463956719785424, test loss: 0.32886061739007494\n",
      "epoch 1233: train loss: 0.24631865463275296, test loss: 0.3287900679238199\n",
      "epoch 1234: train loss: 0.24624173465504412, test loss: 0.3287196188587931\n",
      "epoch 1235: train loss: 0.24616491184659403, test loss: 0.32864926998095034\n",
      "epoch 1236: train loss: 0.2460881860091227, test loss: 0.3285790210768607\n",
      "epoch 1237: train loss: 0.2460115569448905, test loss: 0.32850887193369965\n",
      "epoch 1238: train loss: 0.2459350244566958, test loss: 0.32843882233924915\n",
      "epoch 1239: train loss: 0.24585858834787358, test loss: 0.3283688720818968\n",
      "epoch 1240: train loss: 0.24578224842229343, test loss: 0.32829902095063007\n",
      "epoch 1241: train loss: 0.24570600448435762, test loss: 0.32822926873503716\n",
      "epoch 1242: train loss: 0.24562985633899934, test loss: 0.3281596152253043\n",
      "epoch 1243: train loss: 0.24555380379168112, test loss: 0.3280900602122134\n",
      "epoch 1244: train loss: 0.24547784664839284, test loss: 0.32802060348714007\n",
      "epoch 1245: train loss: 0.2454019847156498, test loss: 0.32795124484205085\n",
      "epoch 1246: train loss: 0.24532621780049124, test loss: 0.32788198406950214\n",
      "epoch 1247: train loss: 0.24525054571047833, test loss: 0.32781282096263836\n",
      "epoch 1248: train loss: 0.24517496825369253, test loss: 0.3277437553151871\n",
      "epoch 1249: train loss: 0.24509948523873384, test loss: 0.3276747869214624\n",
      "epoch 1250: train loss: 0.24502409647471887, test loss: 0.32760591557635765\n",
      "epoch 1251: train loss: 0.24494880177127937, test loss: 0.32753714107534565\n",
      "epoch 1252: train loss: 0.2448736009385602, test loss: 0.3274684632144773\n",
      "epoch 1253: train loss: 0.2447984937872178, test loss: 0.32739988179037705\n",
      "epoch 1254: train loss: 0.2447234801284185, test loss: 0.3273313966002448\n",
      "epoch 1255: train loss: 0.24464855977383648, test loss: 0.32726300744185055\n",
      "epoch 1256: train loss: 0.24457373253565243, test loss: 0.32719471411353324\n",
      "epoch 1257: train loss: 0.24449899822655177, test loss: 0.32712651641419993\n",
      "epoch 1258: train loss: 0.24442435665972273, test loss: 0.32705841414332254\n",
      "epoch 1259: train loss: 0.2443498076488549, test loss: 0.32699040710093635\n",
      "epoch 1260: train loss: 0.2442753510081374, test loss: 0.32692249508763893\n",
      "epoch 1261: train loss: 0.24420098655225744, test loss: 0.32685467790458644\n",
      "epoch 1262: train loss: 0.2441267140963982, test loss: 0.3267869553534929\n",
      "epoch 1263: train loss: 0.2440525334562377, test loss: 0.3267193272366275\n",
      "epoch 1264: train loss: 0.24397844444794672, test loss: 0.32665179335681443\n",
      "epoch 1265: train loss: 0.24390444688818738, test loss: 0.32658435351742854\n",
      "epoch 1266: train loss: 0.24383054059411147, test loss: 0.32651700752239377\n",
      "epoch 1267: train loss: 0.24375672538335877, test loss: 0.32644975517618535\n",
      "epoch 1268: train loss: 0.24368300107405536, test loss: 0.3263825962838217\n",
      "epoch 1269: train loss: 0.24360936748481216, test loss: 0.3263155306508652\n",
      "epoch 1270: train loss: 0.24353582443472313, test loss: 0.32624855808342346\n",
      "epoch 1271: train loss: 0.243462371743364, test loss: 0.3261816783881421\n",
      "epoch 1272: train loss: 0.2433890092307901, test loss: 0.326114891372206\n",
      "epoch 1273: train loss: 0.2433157367175353, test loss: 0.32604819684333775\n",
      "epoch 1274: train loss: 0.24324255402461012, test loss: 0.32598159460979353\n",
      "epoch 1275: train loss: 0.24316946097350034, test loss: 0.32591508448036416\n",
      "epoch 1276: train loss: 0.24309645738616517, test loss: 0.3258486662643702\n",
      "epoch 1277: train loss: 0.24302354308503601, test loss: 0.32578233977166265\n",
      "epoch 1278: train loss: 0.2429507178930146, test loss: 0.32571610481262087\n",
      "epoch 1279: train loss: 0.24287798163347155, test loss: 0.32564996119814793\n",
      "epoch 1280: train loss: 0.24280533413024485, test loss: 0.32558390873967413\n",
      "epoch 1281: train loss: 0.24273277520763833, test loss: 0.3255179472491481\n",
      "epoch 1282: train loss: 0.24266030469042, test loss: 0.3254520765390425\n",
      "epoch 1283: train loss: 0.24258792240382057, test loss: 0.3253862964223471\n",
      "epoch 1284: train loss: 0.24251562817353217, test loss: 0.3253206067125681\n",
      "epoch 1285: train loss: 0.24244342182570627, test loss: 0.3252550072237294\n",
      "epoch 1286: train loss: 0.2423713031869528, test loss: 0.325189497770365\n",
      "epoch 1287: train loss: 0.2422992720843381, test loss: 0.3251240781675228\n",
      "epoch 1288: train loss: 0.24222732834538388, test loss: 0.32505874823076014\n",
      "epoch 1289: train loss: 0.24215547179806543, test loss: 0.32499350777614233\n",
      "epoch 1290: train loss: 0.2420837022708101, test loss: 0.3249283566202413\n",
      "epoch 1291: train loss: 0.24201201959249616, test loss: 0.324863294580134\n",
      "epoch 1292: train loss: 0.24194042359245083, test loss: 0.3247983214734006\n",
      "epoch 1293: train loss: 0.24186891410044914, test loss: 0.32473343711812214\n",
      "epoch 1294: train loss: 0.24179749094671255, test loss: 0.32466864133287926\n",
      "epoch 1295: train loss: 0.2417261539619072, test loss: 0.32460393393675047\n",
      "epoch 1296: train loss: 0.2416549029771425, test loss: 0.3245393147493121\n",
      "epoch 1297: train loss: 0.24158373782396989, test loss: 0.3244747835906328\n",
      "epoch 1298: train loss: 0.2415126583343813, test loss: 0.3244103402812757\n",
      "epoch 1299: train loss: 0.24144166434080752, test loss: 0.3243459846422937\n",
      "epoch 1300: train loss: 0.2413707556761171, test loss: 0.32428171649523163\n",
      "epoch 1301: train loss: 0.24129993217361473, test loss: 0.32421753566212047\n",
      "epoch 1302: train loss: 0.24122919366703968, test loss: 0.32415344196547835\n",
      "epoch 1303: train loss: 0.24115853999056483, test loss: 0.32408943522830713\n",
      "epoch 1304: train loss: 0.24108797097879475, test loss: 0.32402551527409273\n",
      "epoch 1305: train loss: 0.24101748646676466, test loss: 0.3239616819268019\n",
      "epoch 1306: train loss: 0.24094708628993877, test loss: 0.32389793501088143\n",
      "epoch 1307: train loss: 0.24087677028420923, test loss: 0.32383427435125545\n",
      "epoch 1308: train loss: 0.24080653828589432, test loss: 0.32377069977332623\n",
      "epoch 1309: train loss: 0.24073639013173748, test loss: 0.32370721110297024\n",
      "epoch 1310: train loss: 0.2406663256589055, test loss: 0.3236438081665366\n",
      "epoch 1311: train loss: 0.2405963447049876, test loss: 0.3235804907908472\n",
      "epoch 1312: train loss: 0.24052644710799378, test loss: 0.32351725880319365\n",
      "epoch 1313: train loss: 0.24045663270635348, test loss: 0.32345411203133645\n",
      "epoch 1314: train loss: 0.24038690133891427, test loss: 0.32339105030350285\n",
      "epoch 1315: train loss: 0.24031725284494074, test loss: 0.32332807344838627\n",
      "epoch 1316: train loss: 0.2402476870641125, test loss: 0.3232651812951429\n",
      "epoch 1317: train loss: 0.24017820383652363, test loss: 0.32320237367339233\n",
      "epoch 1318: train loss: 0.24010880300268084, test loss: 0.3231396504132145\n",
      "epoch 1319: train loss: 0.24003948440350228, test loss: 0.32307701134514893\n",
      "epoch 1320: train loss: 0.23997024788031632, test loss: 0.3230144563001919\n",
      "epoch 1321: train loss: 0.23990109327485995, test loss: 0.3229519851097982\n",
      "epoch 1322: train loss: 0.2398320204292778, test loss: 0.3228895976058757\n",
      "epoch 1323: train loss: 0.23976302918612066, test loss: 0.3228272936207845\n",
      "epoch 1324: train loss: 0.2396941193883442, test loss: 0.3227650729873391\n",
      "epoch 1325: train loss: 0.23962529087930753, test loss: 0.32270293553880125\n",
      "epoch 1326: train loss: 0.23955654350277228, test loss: 0.3226408811088835\n",
      "epoch 1327: train loss: 0.23948787710290106, test loss: 0.3225789095317454\n",
      "epoch 1328: train loss: 0.23941929152425598, test loss: 0.3225170206419909\n",
      "epoch 1329: train loss: 0.2393507866117978, test loss: 0.32245521427466955\n",
      "epoch 1330: train loss: 0.23928236221088447, test loss: 0.32239349026527336\n",
      "epoch 1331: train loss: 0.23921401816726962, test loss: 0.32233184844973406\n",
      "epoch 1332: train loss: 0.23914575432710183, test loss: 0.3222702886644268\n",
      "epoch 1333: train loss: 0.23907757053692283, test loss: 0.3222088107461606\n",
      "epoch 1334: train loss: 0.23900946664366665, test loss: 0.3221474145321848\n",
      "epoch 1335: train loss: 0.23894144249465796, test loss: 0.32208609986018333\n",
      "epoch 1336: train loss: 0.23887349793761145, test loss: 0.3220248665682739\n",
      "epoch 1337: train loss: 0.23880563282062986, test loss: 0.32196371449500616\n",
      "epoch 1338: train loss: 0.23873784699220335, test loss: 0.32190264347936237\n",
      "epoch 1339: train loss: 0.2386701403012078, test loss: 0.32184165336075327\n",
      "epoch 1340: train loss: 0.23860251259690407, test loss: 0.3217807439790182\n",
      "epoch 1341: train loss: 0.23853496372893632, test loss: 0.32171991517442505\n",
      "epoch 1342: train loss: 0.23846749354733102, test loss: 0.32165916678766415\n",
      "epoch 1343: train loss: 0.23840010190249586, test loss: 0.32159849865985224\n",
      "epoch 1344: train loss: 0.23833278864521815, test loss: 0.32153791063252946\n",
      "epoch 1345: train loss: 0.23826555362666413, test loss: 0.32147740254765367\n",
      "epoch 1346: train loss: 0.23819839669837742, test loss: 0.3214169742476066\n",
      "epoch 1347: train loss: 0.23813131771227788, test loss: 0.3213566255751865\n",
      "epoch 1348: train loss: 0.23806431652066046, test loss: 0.3212963563736097\n",
      "epoch 1349: train loss: 0.2379973929761941, test loss: 0.3212361664865071\n",
      "epoch 1350: train loss: 0.2379305469319204, test loss: 0.32117605575792596\n",
      "epoch 1351: train loss: 0.2378637782412526, test loss: 0.3211160240323243\n",
      "epoch 1352: train loss: 0.23779708675797434, test loss: 0.3210560711545743\n",
      "epoch 1353: train loss: 0.23773047233623834, test loss: 0.3209961969699562\n",
      "epoch 1354: train loss: 0.2376639348305656, test loss: 0.3209364013241609\n",
      "epoch 1355: train loss: 0.23759747409584386, test loss: 0.3208766840632871\n",
      "epoch 1356: train loss: 0.23753108998732678, test loss: 0.32081704503383857\n",
      "epoch 1357: train loss: 0.2374647823606325, test loss: 0.3207574840827252\n",
      "epoch 1358: train loss: 0.2373985510717427, test loss: 0.32069800105726054\n",
      "epoch 1359: train loss: 0.23733239597700148, test loss: 0.32063859580515996\n",
      "epoch 1360: train loss: 0.23726631693311404, test loss: 0.320579268174541\n",
      "epoch 1361: train loss: 0.2372003137971456, test loss: 0.3205200180139198\n",
      "epoch 1362: train loss: 0.23713438642652043, test loss: 0.3204608451722109\n",
      "epoch 1363: train loss: 0.23706853467902064, test loss: 0.3204017494987269\n",
      "epoch 1364: train loss: 0.237002758412785, test loss: 0.32034273084317716\n",
      "epoch 1365: train loss: 0.23693705748630792, test loss: 0.3202837890556637\n",
      "epoch 1366: train loss: 0.23687143175843828, test loss: 0.32022492398668206\n",
      "epoch 1367: train loss: 0.2368058810883784, test loss: 0.3201661354871214\n",
      "epoch 1368: train loss: 0.2367404053356828, test loss: 0.32010742340826054\n",
      "epoch 1369: train loss: 0.23667500436025718, test loss: 0.32004878760176825\n",
      "epoch 1370: train loss: 0.23660967802235752, test loss: 0.3199902279197012\n",
      "epoch 1371: train loss: 0.2365444261825887, test loss: 0.3199317442145023\n",
      "epoch 1372: train loss: 0.23647924870190337, test loss: 0.3198733363390023\n",
      "epoch 1373: train loss: 0.2364141454416014, test loss: 0.3198150041464144\n",
      "epoch 1374: train loss: 0.23634911626332805, test loss: 0.3197567474903358\n",
      "epoch 1375: train loss: 0.2362841610290736, test loss: 0.31969856622474635\n",
      "epoch 1376: train loss: 0.23621927960117176, test loss: 0.319640460204006\n",
      "epoch 1377: train loss: 0.23615447184229893, test loss: 0.3195824292828536\n",
      "epoch 1378: train loss: 0.236089737615473, test loss: 0.3195244733164085\n",
      "epoch 1379: train loss: 0.23602507678405232, test loss: 0.31946659216016426\n",
      "epoch 1380: train loss: 0.2359604892117346, test loss: 0.31940878566999253\n",
      "epoch 1381: train loss: 0.23589597476255597, test loss: 0.31935105370213906\n",
      "epoch 1382: train loss: 0.2358315333008899, test loss: 0.31929339611322133\n",
      "epoch 1383: train loss: 0.2357671646914461, test loss: 0.3192358127602332\n",
      "epoch 1384: train loss: 0.2357028687992696, test loss: 0.31917830350053533\n",
      "epoch 1385: train loss: 0.23563864548973945, test loss: 0.31912086819185953\n",
      "epoch 1386: train loss: 0.23557449462856817, test loss: 0.3190635066923086\n",
      "epoch 1387: train loss: 0.23551041608180018, test loss: 0.3190062188603495\n",
      "epoch 1388: train loss: 0.23544640971581124, test loss: 0.31894900455481723\n",
      "epoch 1389: train loss: 0.23538247539730706, test loss: 0.31889186363491157\n",
      "epoch 1390: train loss: 0.23531861299332263, test loss: 0.31883479596019737\n",
      "epoch 1391: train loss: 0.23525482237122092, test loss: 0.3187778013906004\n",
      "epoch 1392: train loss: 0.23519110339869215, test loss: 0.3187208797864091\n",
      "epoch 1393: train loss: 0.2351274559437524, test loss: 0.3186640310082724\n",
      "epoch 1394: train loss: 0.23506387987474306, test loss: 0.31860725491719843\n",
      "epoch 1395: train loss: 0.23500037506032953, test loss: 0.31855055137455335\n",
      "epoch 1396: train loss: 0.23493694136950036, test loss: 0.3184939202420607\n",
      "epoch 1397: train loss: 0.23487357867156625, test loss: 0.3184373613818004\n",
      "epoch 1398: train loss: 0.23481028683615893, test loss: 0.31838087465620574\n",
      "epoch 1399: train loss: 0.23474706573323048, test loss: 0.3183244599280648\n",
      "epoch 1400: train loss: 0.23468391523305202, test loss: 0.31826811706051766\n",
      "epoch 1401: train loss: 0.2346208352062129, test loss: 0.31821184591705587\n",
      "epoch 1402: train loss: 0.2345578255236199, test loss: 0.3181556463615218\n",
      "epoch 1403: train loss: 0.23449488605649585, test loss: 0.31809951825810595\n",
      "epoch 1404: train loss: 0.234432016676379, test loss: 0.3180434614713473\n",
      "epoch 1405: train loss: 0.234369217255122, test loss: 0.3179874758661325\n",
      "epoch 1406: train loss: 0.23430648766489082, test loss: 0.3179315613076927\n",
      "epoch 1407: train loss: 0.23424382777816405, test loss: 0.31787571766160516\n",
      "epoch 1408: train loss: 0.2341812374677316, test loss: 0.31781994479378994\n",
      "epoch 1409: train loss: 0.2341187166066941, test loss: 0.31776424257050984\n",
      "epoch 1410: train loss: 0.2340562650684617, test loss: 0.3177086108583687\n",
      "epoch 1411: train loss: 0.2339938827267533, test loss: 0.31765304952431195\n",
      "epoch 1412: train loss: 0.2339315694555956, test loss: 0.3175975584356222\n",
      "epoch 1413: train loss: 0.23386932512932213, test loss: 0.3175421374599234\n",
      "epoch 1414: train loss: 0.2338071496225721, test loss: 0.31748678646517337\n",
      "epoch 1415: train loss: 0.23374504281028996, test loss: 0.31743150531966824\n",
      "epoch 1416: train loss: 0.23368300456772417, test loss: 0.3173762938920388\n",
      "epoch 1417: train loss: 0.2336210347704262, test loss: 0.31732115205124967\n",
      "epoch 1418: train loss: 0.23355913329424985, test loss: 0.3172660796665973\n",
      "epoch 1419: train loss: 0.2334973000153502, test loss: 0.3172110766077112\n",
      "epoch 1420: train loss: 0.23343553481018287, test loss: 0.3171561427445524\n",
      "epoch 1421: train loss: 0.23337383755550278, test loss: 0.3171012779474101\n",
      "epoch 1422: train loss: 0.2333122081283636, test loss: 0.31704648208690267\n",
      "epoch 1423: train loss: 0.23325064640611662, test loss: 0.3169917550339772\n",
      "epoch 1424: train loss: 0.23318915226640996, test loss: 0.31693709665990644\n",
      "epoch 1425: train loss: 0.2331277255871878, test loss: 0.31688250683628927\n",
      "epoch 1426: train loss: 0.2330663662466892, test loss: 0.31682798543504775\n",
      "epoch 1427: train loss: 0.23300507412344737, test loss: 0.3167735323284308\n",
      "epoch 1428: train loss: 0.23294384909628896, test loss: 0.3167191473890069\n",
      "epoch 1429: train loss: 0.2328826910443328, test loss: 0.3166648304896677\n",
      "epoch 1430: train loss: 0.23282159984698939, test loss: 0.31661058150362464\n",
      "epoch 1431: train loss: 0.2327605753839599, test loss: 0.3165564003044088\n",
      "epoch 1432: train loss: 0.23269961753523513, test loss: 0.31650228676587067\n",
      "epoch 1433: train loss: 0.23263872618109496, test loss: 0.316448240762177\n",
      "epoch 1434: train loss: 0.23257790120210725, test loss: 0.31639426216781225\n",
      "epoch 1435: train loss: 0.23251714247912703, test loss: 0.3163403508575754\n",
      "epoch 1436: train loss: 0.2324564498932957, test loss: 0.3162865067065825\n",
      "epoch 1437: train loss: 0.23239582332604017, test loss: 0.3162327295902594\n",
      "epoch 1438: train loss: 0.23233526265907195, test loss: 0.31617901938434784\n",
      "epoch 1439: train loss: 0.23227476777438638, test loss: 0.3161253759648997\n",
      "epoch 1440: train loss: 0.23221433855426174, test loss: 0.3160717992082784\n",
      "epoch 1441: train loss: 0.23215397488125838, test loss: 0.3160182889911553\n",
      "epoch 1442: train loss: 0.23209367663821795, test loss: 0.31596484519051243\n",
      "epoch 1443: train loss: 0.2320334437082626, test loss: 0.31591146768363987\n",
      "epoch 1444: train loss: 0.23197327597479395, test loss: 0.3158581563481327\n",
      "epoch 1445: train loss: 0.23191317332149242, test loss: 0.3158049110618937\n",
      "epoch 1446: train loss: 0.23185313563231655, test loss: 0.3157517317031292\n",
      "epoch 1447: train loss: 0.2317931627915019, test loss: 0.3156986181503499\n",
      "epoch 1448: train loss: 0.23173325468356012, test loss: 0.3156455702823707\n",
      "epoch 1449: train loss: 0.2316734111932788, test loss: 0.3155925879783075\n",
      "epoch 1450: train loss: 0.23161363220571984, test loss: 0.3155396711175767\n",
      "epoch 1451: train loss: 0.23155391760621913, test loss: 0.3154868195798967\n",
      "epoch 1452: train loss: 0.23149426728038566, test loss: 0.3154340332452842\n",
      "epoch 1453: train loss: 0.2314346811141006, test loss: 0.3153813119940547\n",
      "epoch 1454: train loss: 0.23137515899351663, test loss: 0.3153286557068201\n",
      "epoch 1455: train loss: 0.23131570080505712, test loss: 0.31527606426449073\n",
      "epoch 1456: train loss: 0.23125630643541506, test loss: 0.3152235375482702\n",
      "epoch 1457: train loss: 0.23119697577155285, test loss: 0.3151710754396595\n",
      "epoch 1458: train loss: 0.2311377087007009, test loss: 0.31511867782045194\n",
      "epoch 1459: train loss: 0.2310785051103572, test loss: 0.31506634457273264\n",
      "epoch 1460: train loss: 0.23101936488828648, test loss: 0.31501407557888106\n",
      "epoch 1461: train loss: 0.2309602879225193, test loss: 0.31496187072156706\n",
      "epoch 1462: train loss: 0.2309012741013513, test loss: 0.3149097298837489\n",
      "epoch 1463: train loss: 0.2308423233133427, test loss: 0.3148576529486772\n",
      "epoch 1464: train loss: 0.23078343544731708, test loss: 0.31480563979988757\n",
      "epoch 1465: train loss: 0.23072461039236095, test loss: 0.3147536903212068\n",
      "epoch 1466: train loss: 0.23066584803782286, test loss: 0.3147018043967458\n",
      "epoch 1467: train loss: 0.2306071482733126, test loss: 0.3146499819109006\n",
      "epoch 1468: train loss: 0.2305485109887004, test loss: 0.31459822274835536\n",
      "epoch 1469: train loss: 0.23048993607411639, test loss: 0.31454652679407613\n",
      "epoch 1470: train loss: 0.23043142341994965, test loss: 0.3144948939333117\n",
      "epoch 1471: train loss: 0.23037297291684744, test loss: 0.3144433240515934\n",
      "epoch 1472: train loss: 0.23031458445571457, test loss: 0.31439181703473523\n",
      "epoch 1473: train loss: 0.2302562579277125, test loss: 0.31434037276883137\n",
      "epoch 1474: train loss: 0.23019799322425874, test loss: 0.31428899114025355\n",
      "epoch 1475: train loss: 0.2301397902370262, test loss: 0.3142376720356562\n",
      "epoch 1476: train loss: 0.23008164885794188, test loss: 0.31418641534196873\n",
      "epoch 1477: train loss: 0.230023568979187, test loss: 0.3141352209463971\n",
      "epoch 1478: train loss: 0.2299655504931955, test loss: 0.31408408873642696\n",
      "epoch 1479: train loss: 0.2299075932926537, test loss: 0.3140330185998166\n",
      "epoch 1480: train loss: 0.2298496972704995, test loss: 0.3139820104245995\n",
      "epoch 1481: train loss: 0.22979186231992163, test loss: 0.31393106409908383\n",
      "epoch 1482: train loss: 0.22973408833435888, test loss: 0.31388017951185093\n",
      "epoch 1483: train loss: 0.2296763752074994, test loss: 0.31382935655175137\n",
      "epoch 1484: train loss: 0.22961872283328014, test loss: 0.31377859510791056\n",
      "epoch 1485: train loss: 0.22956113110588575, test loss: 0.3137278950697226\n",
      "epoch 1486: train loss: 0.22950359991974836, test loss: 0.3136772563268508\n",
      "epoch 1487: train loss: 0.2294461291695464, test loss: 0.3136266787692294\n",
      "epoch 1488: train loss: 0.2293887187502041, test loss: 0.3135761622870578\n",
      "epoch 1489: train loss: 0.22933136855689099, test loss: 0.313525706770805\n",
      "epoch 1490: train loss: 0.2292740784850207, test loss: 0.31347531211120516\n",
      "epoch 1491: train loss: 0.2292168484302507, test loss: 0.3134249781992578\n",
      "epoch 1492: train loss: 0.22915967828848138, test loss: 0.31337470492622854\n",
      "epoch 1493: train loss: 0.2291025679558554, test loss: 0.3133244921836454\n",
      "epoch 1494: train loss: 0.22904551732875703, test loss: 0.3132743398632998\n",
      "epoch 1495: train loss: 0.22898852630381136, test loss: 0.3132242478572481\n",
      "epoch 1496: train loss: 0.2289315947778837, test loss: 0.31317421605780404\n",
      "epoch 1497: train loss: 0.22887472264807884, test loss: 0.3131242443575454\n",
      "epoch 1498: train loss: 0.22881790981174036, test loss: 0.3130743326493087\n",
      "epoch 1499: train loss: 0.2287611561664501, test loss: 0.31302448082618883\n",
      "epoch 1500: train loss: 0.22870446161002708, test loss: 0.3129746887815423\n",
      "epoch 1501: train loss: 0.2286478260405273, test loss: 0.3129249564089791\n",
      "epoch 1502: train loss: 0.2285912493562428, test loss: 0.3128752836023681\n",
      "epoch 1503: train loss: 0.228534731455701, test loss: 0.31282567025583574\n",
      "epoch 1504: train loss: 0.22847827223766398, test loss: 0.312776116263761\n",
      "epoch 1505: train loss: 0.22842187160112798, test loss: 0.31272662152077885\n",
      "epoch 1506: train loss: 0.22836552944532265, test loss: 0.31267718592177907\n",
      "epoch 1507: train loss: 0.22830924566971028, test loss: 0.31262780936190326\n",
      "epoch 1508: train loss: 0.22825302017398522, test loss: 0.31257849173654334\n",
      "epoch 1509: train loss: 0.22819685285807328, test loss: 0.3125292329413482\n",
      "epoch 1510: train loss: 0.22814074362213105, test loss: 0.31248003287221143\n",
      "epoch 1511: train loss: 0.22808469236654505, test loss: 0.3124308914252799\n",
      "epoch 1512: train loss: 0.22802869899193137, test loss: 0.3123818084969507\n",
      "epoch 1513: train loss: 0.2279727633991348, test loss: 0.3123327839838659\n",
      "epoch 1514: train loss: 0.22791688548922828, test loss: 0.31228381778291864\n",
      "epoch 1515: train loss: 0.22786106516351226, test loss: 0.31223490979124824\n",
      "epoch 1516: train loss: 0.22780530232351384, test loss: 0.31218605990623866\n",
      "epoch 1517: train loss: 0.2277495968709866, test loss: 0.3121372680255217\n",
      "epoch 1518: train loss: 0.22769394870790932, test loss: 0.31208853404697323\n",
      "epoch 1519: train loss: 0.22763835773648605, test loss: 0.31203985786871175\n",
      "epoch 1520: train loss: 0.2275828238591448, test loss: 0.31199123938910156\n",
      "epoch 1521: train loss: 0.22752734697853738, test loss: 0.3119426785067486\n",
      "epoch 1522: train loss: 0.22747192699753846, test loss: 0.31189417512049944\n",
      "epoch 1523: train loss: 0.22741656381924522, test loss: 0.3118457291294441\n",
      "epoch 1524: train loss: 0.22736125734697651, test loss: 0.3117973404329116\n",
      "epoch 1525: train loss: 0.2273060074842723, test loss: 0.3117490089304698\n",
      "epoch 1526: train loss: 0.22725081413489298, test loss: 0.3117007345219291\n",
      "epoch 1527: train loss: 0.22719567720281886, test loss: 0.31165251710733466\n",
      "epoch 1528: train loss: 0.22714059659224956, test loss: 0.31160435658697233\n",
      "epoch 1529: train loss: 0.22708557220760317, test loss: 0.3115562528613605\n",
      "epoch 1530: train loss: 0.22703060395351587, test loss: 0.31150820583125866\n",
      "epoch 1531: train loss: 0.2269756917348413, test loss: 0.31146021539765917\n",
      "epoch 1532: train loss: 0.2269208354566497, test loss: 0.3114122814617896\n",
      "epoch 1533: train loss: 0.22686603502422767, test loss: 0.3113644039251126\n",
      "epoch 1534: train loss: 0.22681129034307726, test loss: 0.31131658268932205\n",
      "epoch 1535: train loss: 0.22675660131891548, test loss: 0.3112688176563478\n",
      "epoch 1536: train loss: 0.22670196785767374, test loss: 0.3112211087283483\n",
      "epoch 1537: train loss: 0.2266473898654972, test loss: 0.31117345580771616\n",
      "epoch 1538: train loss: 0.22659286724874406, test loss: 0.3111258587970734\n",
      "epoch 1539: train loss: 0.22653839991398528, test loss: 0.3110783175992722\n",
      "epoch 1540: train loss: 0.22648398776800355, test loss: 0.3110308321173941\n",
      "epoch 1541: train loss: 0.22642963071779307, test loss: 0.3109834022547489\n",
      "epoch 1542: train loss: 0.22637532867055873, test loss: 0.3109360279148753\n",
      "epoch 1543: train loss: 0.2263210815337158, test loss: 0.31088870900153837\n",
      "epoch 1544: train loss: 0.22626688921488872, test loss: 0.3108414454187313\n",
      "epoch 1545: train loss: 0.22621275162191135, test loss: 0.3107942370706715\n",
      "epoch 1546: train loss: 0.22615866866282588, test loss: 0.3107470838618023\n",
      "epoch 1547: train loss: 0.22610464024588217, test loss: 0.31069998569679164\n",
      "epoch 1548: train loss: 0.22605066627953754, test loss: 0.31065294248053266\n",
      "epoch 1549: train loss: 0.2259967466724558, test loss: 0.310605954118139\n",
      "epoch 1550: train loss: 0.22594288133350704, test loss: 0.31055902051495154\n",
      "epoch 1551: train loss: 0.22588907017176685, test loss: 0.31051214157652735\n",
      "epoch 1552: train loss: 0.22583531309651572, test loss: 0.3104653172086496\n",
      "epoch 1553: train loss: 0.22578161001723854, test loss: 0.3104185473173198\n",
      "epoch 1554: train loss: 0.225727960843624, test loss: 0.31037183180876127\n",
      "epoch 1555: train loss: 0.22567436548556427, test loss: 0.31032517058941606\n",
      "epoch 1556: train loss: 0.22562082385315377, test loss: 0.3102785635659417\n",
      "epoch 1557: train loss: 0.22556733585668945, test loss: 0.3102320106452205\n",
      "epoch 1558: train loss: 0.22551390140666963, test loss: 0.3101855117343467\n",
      "epoch 1559: train loss: 0.22546052041379372, test loss: 0.31013906674063396\n",
      "epoch 1560: train loss: 0.2254071927889615, test loss: 0.3100926755716111\n",
      "epoch 1561: train loss: 0.2253539184432727, test loss: 0.3100463381350234\n",
      "epoch 1562: train loss: 0.22530069728802643, test loss: 0.3100000543388302\n",
      "epoch 1563: train loss: 0.2252475292347205, test loss: 0.3099538240912068\n",
      "epoch 1564: train loss: 0.22519441419505093, test loss: 0.3099076473005404\n",
      "epoch 1565: train loss: 0.22514135208091163, test loss: 0.309861523875432\n",
      "epoch 1566: train loss: 0.22508834280439338, test loss: 0.3098154537246961\n",
      "epoch 1567: train loss: 0.22503538627778374, test loss: 0.3097694367573574\n",
      "epoch 1568: train loss: 0.22498248241356633, test loss: 0.3097234728826532\n",
      "epoch 1569: train loss: 0.2249296311244203, test loss: 0.3096775620100308\n",
      "epoch 1570: train loss: 0.2248768323232196, test loss: 0.30963170404914653\n",
      "epoch 1571: train loss: 0.2248240859230328, test loss: 0.3095858989098684\n",
      "epoch 1572: train loss: 0.22477139183712233, test loss: 0.30954014650227213\n",
      "epoch 1573: train loss: 0.2247187499789439, test loss: 0.30949444673663984\n",
      "epoch 1574: train loss: 0.2246661602621462, test loss: 0.309448799523466\n",
      "epoch 1575: train loss: 0.22461362260057016, test loss: 0.3094032047734463\n",
      "epoch 1576: train loss: 0.2245611369082485, test loss: 0.30935766239748674\n",
      "epoch 1577: train loss: 0.22450870309940515, test loss: 0.3093121723066987\n",
      "epoch 1578: train loss: 0.22445632108845487, test loss: 0.30926673441239594\n",
      "epoch 1579: train loss: 0.22440399079000245, test loss: 0.309221348626101\n",
      "epoch 1580: train loss: 0.22435171211884253, test loss: 0.3091760148595386\n",
      "epoch 1581: train loss: 0.2242994849899589, test loss: 0.30913073302463484\n",
      "epoch 1582: train loss: 0.22424730931852388, test loss: 0.3090855030335212\n",
      "epoch 1583: train loss: 0.22419518501989807, test loss: 0.30904032479853194\n",
      "epoch 1584: train loss: 0.22414311200962944, test loss: 0.30899519823220106\n",
      "epoch 1585: train loss: 0.22409109020345336, test loss: 0.3089501232472649\n",
      "epoch 1586: train loss: 0.2240391195172916, test loss: 0.30890509975665836\n",
      "epoch 1587: train loss: 0.22398719986725207, test loss: 0.3088601276735196\n",
      "epoch 1588: train loss: 0.22393533116962822, test loss: 0.3088152069111831\n",
      "epoch 1589: train loss: 0.22388351334089862, test loss: 0.30877033738318466\n",
      "epoch 1590: train loss: 0.2238317462977263, test loss: 0.3087255190032551\n",
      "epoch 1591: train loss: 0.22378002995695845, test loss: 0.30868075168532716\n",
      "epoch 1592: train loss: 0.22372836423562575, test loss: 0.3086360353435266\n",
      "epoch 1593: train loss: 0.22367674905094198, test loss: 0.30859136989217884\n",
      "epoch 1594: train loss: 0.22362518432030343, test loss: 0.30854675524580305\n",
      "epoch 1595: train loss: 0.22357366996128847, test loss: 0.30850219131911344\n",
      "epoch 1596: train loss: 0.22352220589165697, test loss: 0.3084576780270224\n",
      "epoch 1597: train loss: 0.2234707920293501, test loss: 0.30841321528463433\n",
      "epoch 1598: train loss: 0.2234194282924893, test loss: 0.30836880300724573\n",
      "epoch 1599: train loss: 0.22336811459937636, test loss: 0.3083244411103486\n",
      "epoch 1600: train loss: 0.22331685086849248, test loss: 0.30828012950962913\n",
      "epoch 1601: train loss: 0.223265637018498, test loss: 0.3082358681209603\n",
      "epoch 1602: train loss: 0.22321447296823205, test loss: 0.30819165686041156\n",
      "epoch 1603: train loss: 0.22316335863671172, test loss: 0.3081474956442418\n",
      "epoch 1604: train loss: 0.22311229394313187, test loss: 0.3081033843888987\n",
      "epoch 1605: train loss: 0.22306127880686444, test loss: 0.3080593230110226\n",
      "epoch 1606: train loss: 0.2230103131474582, test loss: 0.3080153114274402\n",
      "epoch 1607: train loss: 0.22295939688463803, test loss: 0.3079713495551705\n",
      "epoch 1608: train loss: 0.22290852993830462, test loss: 0.30792743731141725\n",
      "epoch 1609: train loss: 0.22285771222853398, test loss: 0.3078835746135752\n",
      "epoch 1610: train loss: 0.2228069436755767, test loss: 0.3078397613792232\n",
      "epoch 1611: train loss: 0.22275622419985802, test loss: 0.3077959975261285\n",
      "epoch 1612: train loss: 0.22270555372197676, test loss: 0.3077522829722453\n",
      "epoch 1613: train loss: 0.22265493216270532, test loss: 0.3077086176357103\n",
      "epoch 1614: train loss: 0.22260435944298887, test loss: 0.30766500143484915\n",
      "epoch 1615: train loss: 0.2225538354839452, test loss: 0.30762143428816957\n",
      "epoch 1616: train loss: 0.2225033602068639, test loss: 0.30757791611436436\n",
      "epoch 1617: train loss: 0.2224529335332062, test loss: 0.3075344468323085\n",
      "epoch 1618: train loss: 0.22240255538460446, test loss: 0.30749102636106285\n",
      "epoch 1619: train loss: 0.22235222568286162, test loss: 0.307447654619866\n",
      "epoch 1620: train loss: 0.2223019443499506, test loss: 0.3074043315281434\n",
      "epoch 1621: train loss: 0.22225171130801422, test loss: 0.30736105700549954\n",
      "epoch 1622: train loss: 0.22220152647936453, test loss: 0.30731783097171983\n",
      "epoch 1623: train loss: 0.22215138978648216, test loss: 0.3072746533467715\n",
      "epoch 1624: train loss: 0.22210130115201632, test loss: 0.307231524050799\n",
      "epoch 1625: train loss: 0.22205126049878396, test loss: 0.30718844300412856\n",
      "epoch 1626: train loss: 0.22200126774976955, test loss: 0.30714541012726443\n",
      "epoch 1627: train loss: 0.22195132282812444, test loss: 0.3071024253408897\n",
      "epoch 1628: train loss: 0.22190142565716658, test loss: 0.30705948856586596\n",
      "epoch 1629: train loss: 0.22185157616038004, test loss: 0.3070165997232292\n",
      "epoch 1630: train loss: 0.22180177426141448, test loss: 0.30697375873419686\n",
      "epoch 1631: train loss: 0.22175201988408488, test loss: 0.30693096552015825\n",
      "epoch 1632: train loss: 0.22170231295237078, test loss: 0.3068882200026819\n",
      "epoch 1633: train loss: 0.22165265339041626, test loss: 0.30684552210350946\n",
      "epoch 1634: train loss: 0.22160304112252915, test loss: 0.3068028717445593\n",
      "epoch 1635: train loss: 0.22155347607318077, test loss: 0.30676026884792423\n",
      "epoch 1636: train loss: 0.22150395816700552, test loss: 0.30671771333586845\n",
      "epoch 1637: train loss: 0.22145448732880035, test loss: 0.3066752051308337\n",
      "epoch 1638: train loss: 0.22140506348352423, test loss: 0.30663274415542985\n",
      "epoch 1639: train loss: 0.22135568655629811, test loss: 0.3065903303324442\n",
      "epoch 1640: train loss: 0.22130635647240396, test loss: 0.3065479635848316\n",
      "epoch 1641: train loss: 0.2212570731572848, test loss: 0.30650564383572126\n",
      "epoch 1642: train loss: 0.221207836536544, test loss: 0.3064633710084131\n",
      "epoch 1643: train loss: 0.2211586465359449, test loss: 0.3064211450263765\n",
      "epoch 1644: train loss: 0.22110950308141064, test loss: 0.3063789658132514\n",
      "epoch 1645: train loss: 0.22106040609902317, test loss: 0.30633683329284855\n",
      "epoch 1646: train loss: 0.22101135551502346, test loss: 0.3062947473891456\n",
      "epoch 1647: train loss: 0.2209623512558107, test loss: 0.3062527080262898\n",
      "epoch 1648: train loss: 0.220913393247942, test loss: 0.30621071512859815\n",
      "epoch 1649: train loss: 0.22086448141813192, test loss: 0.306168768620553\n",
      "epoch 1650: train loss: 0.2208156156932521, test loss: 0.30612686842680625\n",
      "epoch 1651: train loss: 0.2207667960003309, test loss: 0.3060850144721737\n",
      "epoch 1652: train loss: 0.2207180222665527, test loss: 0.30604320668164003\n",
      "epoch 1653: train loss: 0.220669294419258, test loss: 0.306001444980355\n",
      "epoch 1654: train loss: 0.22062061238594255, test loss: 0.30595972929363335\n",
      "epoch 1655: train loss: 0.220571976094257, test loss: 0.30591805954695467\n",
      "epoch 1656: train loss: 0.2205233854720069, test loss: 0.30587643566596395\n",
      "epoch 1657: train loss: 0.22047484044715165, test loss: 0.30583485757646955\n",
      "epoch 1658: train loss: 0.22042634094780464, test loss: 0.3057933252044432\n",
      "epoch 1659: train loss: 0.2203778869022326, test loss: 0.3057518384760216\n",
      "epoch 1660: train loss: 0.22032947823885515, test loss: 0.305710397317501\n",
      "epoch 1661: train loss: 0.2202811148862446, test loss: 0.30566900165534194\n",
      "epoch 1662: train loss: 0.22023279677312538, test loss: 0.305627651416167\n",
      "epoch 1663: train loss: 0.2201845238283736, test loss: 0.3055863465267576\n",
      "epoch 1664: train loss: 0.22013629598101694, test loss: 0.3055450869140596\n",
      "epoch 1665: train loss: 0.22008811316023377, test loss: 0.30550387250517697\n",
      "epoch 1666: train loss: 0.22003997529535332, test loss: 0.3054627032273747\n",
      "epoch 1667: train loss: 0.2199918823158549, test loss: 0.30542157900807515\n",
      "epoch 1668: train loss: 0.21994383415136753, test loss: 0.30538049977486326\n",
      "epoch 1669: train loss: 0.21989583073166974, test loss: 0.3053394654554797\n",
      "epoch 1670: train loss: 0.2198478719866889, test loss: 0.30529847597782533\n",
      "epoch 1671: train loss: 0.21979995784650122, test loss: 0.3052575312699585\n",
      "epoch 1672: train loss: 0.21975208824133105, test loss: 0.3052166312600925\n",
      "epoch 1673: train loss: 0.21970426310155053, test loss: 0.3051757758766016\n",
      "epoch 1674: train loss: 0.21965648235767923, test loss: 0.3051349650480129\n",
      "epoch 1675: train loss: 0.21960874594038388, test loss: 0.30509419870301174\n",
      "epoch 1676: train loss: 0.21956105378047786, test loss: 0.3050534767704392\n",
      "epoch 1677: train loss: 0.21951340580892084, test loss: 0.3050127991792894\n",
      "epoch 1678: train loss: 0.2194658019568184, test loss: 0.30497216585871384\n",
      "epoch 1679: train loss: 0.2194182421554217, test loss: 0.3049315767380169\n",
      "epoch 1680: train loss: 0.21937072633612703, test loss: 0.3048910317466569\n",
      "epoch 1681: train loss: 0.21932325443047543, test loss: 0.30485053081424734\n",
      "epoch 1682: train loss: 0.2192758263701525, test loss: 0.30481007387055226\n",
      "epoch 1683: train loss: 0.21922844208698763, test loss: 0.30476966084548995\n",
      "epoch 1684: train loss: 0.21918110151295409, test loss: 0.30472929166913004\n",
      "epoch 1685: train loss: 0.21913380458016823, test loss: 0.30468896627169634\n",
      "epoch 1686: train loss: 0.21908655122088955, test loss: 0.30464868458356026\n",
      "epoch 1687: train loss: 0.21903934136751982, test loss: 0.3046084465352481\n",
      "epoch 1688: train loss: 0.21899217495260323, test loss: 0.3045682520574344\n",
      "epoch 1689: train loss: 0.21894505190882554, test loss: 0.3045281010809442\n",
      "epoch 1690: train loss: 0.2188979721690142, test loss: 0.3044879935367535\n",
      "epoch 1691: train loss: 0.21885093566613747, test loss: 0.3044479293559851\n",
      "epoch 1692: train loss: 0.21880394233330447, test loss: 0.3044079084699131\n",
      "epoch 1693: train loss: 0.21875699210376465, test loss: 0.30436793080996094\n",
      "epoch 1694: train loss: 0.21871008491090735, test loss: 0.30432799630769775\n",
      "epoch 1695: train loss: 0.21866322068826158, test loss: 0.3042881048948415\n",
      "epoch 1696: train loss: 0.2186163993694956, test loss: 0.30424825650325715\n",
      "epoch 1697: train loss: 0.21856962088841655, test loss: 0.3042084510649579\n",
      "epoch 1698: train loss: 0.21852288517897012, test loss: 0.3041686885121016\n",
      "epoch 1699: train loss: 0.21847619217524017, test loss: 0.30412896877699364\n",
      "epoch 1700: train loss: 0.21842954181144836, test loss: 0.30408929179208455\n",
      "epoch 1701: train loss: 0.21838293402195386, test loss: 0.30404965748997087\n",
      "epoch 1702: train loss: 0.21833636874125287, test loss: 0.3040100658033934\n",
      "epoch 1703: train loss: 0.21828984590397837, test loss: 0.30397051666523767\n",
      "epoch 1704: train loss: 0.2182433654448999, test loss: 0.3039310100085346\n",
      "epoch 1705: train loss: 0.21819692729892282, test loss: 0.3038915457664556\n",
      "epoch 1706: train loss: 0.21815053140108823, test loss: 0.30385212387232013\n",
      "epoch 1707: train loss: 0.2181041776865728, test loss: 0.3038127442595868\n",
      "epoch 1708: train loss: 0.21805786609068792, test loss: 0.30377340686185866\n",
      "epoch 1709: train loss: 0.2180115965488799, test loss: 0.30373411161288094\n",
      "epoch 1710: train loss: 0.2179653689967291, test loss: 0.3036948584465397\n",
      "epoch 1711: train loss: 0.21791918336995011, test loss: 0.3036556472968647\n",
      "epoch 1712: train loss: 0.21787303960439097, test loss: 0.3036164780980235\n",
      "epoch 1713: train loss: 0.21782693763603306, test loss: 0.303577350784328\n",
      "epoch 1714: train loss: 0.21778087740099072, test loss: 0.30353826529022687\n",
      "epoch 1715: train loss: 0.21773485883551086, test loss: 0.30349922155031295\n",
      "epoch 1716: train loss: 0.21768888187597263, test loss: 0.30346021949931434\n",
      "epoch 1717: train loss: 0.21764294645888718, test loss: 0.30342125907209994\n",
      "epoch 1718: train loss: 0.21759705252089717, test loss: 0.30338234020367905\n",
      "epoch 1719: train loss: 0.21755119999877656, test loss: 0.30334346282919866\n",
      "epoch 1720: train loss: 0.2175053888294302, test loss: 0.3033046268839417\n",
      "epoch 1721: train loss: 0.21745961894989357, test loss: 0.30326583230333276\n",
      "epoch 1722: train loss: 0.21741389029733235, test loss: 0.3032270790229294\n",
      "epoch 1723: train loss: 0.21736820280904204, test loss: 0.3031883669784289\n",
      "epoch 1724: train loss: 0.21732255642244788, test loss: 0.3031496961056652\n",
      "epoch 1725: train loss: 0.21727695107510436, test loss: 0.3031110663406082\n",
      "epoch 1726: train loss: 0.21723138670469477, test loss: 0.3030724776193617\n",
      "epoch 1727: train loss: 0.21718586324903114, test loss: 0.30303392987816913\n",
      "epoch 1728: train loss: 0.21714038064605354, test loss: 0.302995423053404\n",
      "epoch 1729: train loss: 0.21709493883383033, test loss: 0.30295695708158\n",
      "epoch 1730: train loss: 0.21704953775055713, test loss: 0.3029185318993399\n",
      "epoch 1731: train loss: 0.2170041773345571, test loss: 0.30288014744346436\n",
      "epoch 1732: train loss: 0.21695885752428026, test loss: 0.3028418036508668\n",
      "epoch 1733: train loss: 0.21691357825830326, test loss: 0.3028035004585947\n",
      "epoch 1734: train loss: 0.21686833947532919, test loss: 0.30276523780382525\n",
      "epoch 1735: train loss: 0.216823141114187, test loss: 0.3027270156238719\n",
      "epoch 1736: train loss: 0.21677798311383134, test loss: 0.3026888338561804\n",
      "epoch 1737: train loss: 0.2167328654133425, test loss: 0.3026506924383254\n",
      "epoch 1738: train loss: 0.21668778795192536, test loss: 0.30261259130801627\n",
      "epoch 1739: train loss: 0.21664275066890987, test loss: 0.3025745304030917\n",
      "epoch 1740: train loss: 0.21659775350375032, test loss: 0.30253650966152196\n",
      "epoch 1741: train loss: 0.216552796396025, test loss: 0.3024985290214075\n",
      "epoch 1742: train loss: 0.21650787928543613, test loss: 0.3024605884209807\n",
      "epoch 1743: train loss: 0.21646300211180933, test loss: 0.30242268779860126\n",
      "epoch 1744: train loss: 0.2164181648150934, test loss: 0.30238482709276\n",
      "epoch 1745: train loss: 0.21637336733535995, test loss: 0.30234700624207633\n",
      "epoch 1746: train loss: 0.21632860961280323, test loss: 0.30230922518529946\n",
      "epoch 1747: train loss: 0.21628389158773956, test loss: 0.3022714838613053\n",
      "epoch 1748: train loss: 0.21623921320060746, test loss: 0.3022337822090989\n",
      "epoch 1749: train loss: 0.21619457439196665, test loss: 0.3021961201678134\n",
      "epoch 1750: train loss: 0.21614997510249867, test loss: 0.3021584976767096\n",
      "epoch 1751: train loss: 0.2161054152730056, test loss: 0.30212091467517377\n",
      "epoch 1752: train loss: 0.21606089484441057, test loss: 0.3020833711027213\n",
      "epoch 1753: train loss: 0.21601641375775688, test loss: 0.3020458668989931\n",
      "epoch 1754: train loss: 0.21597197195420803, test loss: 0.30200840200375567\n",
      "epoch 1755: train loss: 0.2159275693750473, test loss: 0.3019709763569015\n",
      "epoch 1756: train loss: 0.2158832059616774, test loss: 0.3019335898984484\n",
      "epoch 1757: train loss: 0.2158388816556203, test loss: 0.3018962425685404\n",
      "epoch 1758: train loss: 0.21579459639851697, test loss: 0.3018589343074459\n",
      "epoch 1759: train loss: 0.2157503501321267, test loss: 0.30182166505555597\n",
      "epoch 1760: train loss: 0.21570614279832728, test loss: 0.3017844347533882\n",
      "epoch 1761: train loss: 0.21566197433911452, test loss: 0.3017472433415842\n",
      "epoch 1762: train loss: 0.21561784469660175, test loss: 0.3017100907609067\n",
      "epoch 1763: train loss: 0.21557375381301994, test loss: 0.3016729769522424\n",
      "epoch 1764: train loss: 0.215529701630717, test loss: 0.3016359018566028\n",
      "epoch 1765: train loss: 0.21548568809215773, test loss: 0.3015988654151207\n",
      "epoch 1766: train loss: 0.2154417131399235, test loss: 0.30156186756904935\n",
      "epoch 1767: train loss: 0.2153977767167119, test loss: 0.30152490825976663\n",
      "epoch 1768: train loss: 0.2153538787653364, test loss: 0.301487987428771\n",
      "epoch 1769: train loss: 0.21531001922872622, test loss: 0.30145110501768146\n",
      "epoch 1770: train loss: 0.21526619804992592, test loss: 0.301414260968237\n",
      "epoch 1771: train loss: 0.2152224151720951, test loss: 0.30137745522230186\n",
      "epoch 1772: train loss: 0.21517867053850823, test loss: 0.30134068772185557\n",
      "epoch 1773: train loss: 0.21513496409255425, test loss: 0.3013039584089995\n",
      "epoch 1774: train loss: 0.21509129577773634, test loss: 0.30126726722595476\n",
      "epoch 1775: train loss: 0.21504766553767168, test loss: 0.3012306141150619\n",
      "epoch 1776: train loss: 0.21500407331609098, test loss: 0.3011939990187818\n",
      "epoch 1777: train loss: 0.21496051905683836, test loss: 0.30115742187968947\n",
      "epoch 1778: train loss: 0.21491700270387112, test loss: 0.3011208826404849\n",
      "epoch 1779: train loss: 0.21487352420125938, test loss: 0.3010843812439805\n",
      "epoch 1780: train loss: 0.21483008349318564, test loss: 0.301047917633109\n",
      "epoch 1781: train loss: 0.21478668052394476, test loss: 0.3010114917509227\n",
      "epoch 1782: train loss: 0.21474331523794354, test loss: 0.30097510354058665\n",
      "epoch 1783: train loss: 0.2146999875797006, test loss: 0.3009387529453857\n",
      "epoch 1784: train loss: 0.21465669749384575, test loss: 0.30090243990872045\n",
      "epoch 1785: train loss: 0.21461344492512008, test loss: 0.30086616437410885\n",
      "epoch 1786: train loss: 0.21457022981837562, test loss: 0.30082992628518124\n",
      "epoch 1787: train loss: 0.21452705211857487, test loss: 0.3007937255856909\n",
      "epoch 1788: train loss: 0.2144839117707906, test loss: 0.3007575622194974\n",
      "epoch 1789: train loss: 0.21444080872020588, test loss: 0.30072143613058294\n",
      "epoch 1790: train loss: 0.21439774291211328, test loss: 0.3006853472630405\n",
      "epoch 1791: train loss: 0.21435471429191502, test loss: 0.3006492955610783\n",
      "epoch 1792: train loss: 0.21431172280512242, test loss: 0.30061328096902024\n",
      "epoch 1793: train loss: 0.21426876839735606, test loss: 0.3005773034313011\n",
      "epoch 1794: train loss: 0.21422585101434483, test loss: 0.30054136289247113\n",
      "epoch 1795: train loss: 0.2141829706019262, test loss: 0.30050545929719563\n",
      "epoch 1796: train loss: 0.2141401271060461, test loss: 0.30046959259025036\n",
      "epoch 1797: train loss: 0.21409732047275778, test loss: 0.3004337627165237\n",
      "epoch 1798: train loss: 0.21405455064822257, test loss: 0.30039796962101895\n",
      "epoch 1799: train loss: 0.2140118175787089, test loss: 0.30036221324884743\n",
      "epoch 1800: train loss: 0.21396912121059253, test loss: 0.3003264935452375\n",
      "epoch 1801: train loss: 0.21392646149035582, test loss: 0.30029081045552436\n",
      "epoch 1802: train loss: 0.21388383836458785, test loss: 0.30025516392515833\n",
      "epoch 1803: train loss: 0.21384125177998395, test loss: 0.30021955389969746\n",
      "epoch 1804: train loss: 0.21379870168334555, test loss: 0.3001839803248131\n",
      "epoch 1805: train loss: 0.21375618802157978, test loss: 0.3001484431462855\n",
      "epoch 1806: train loss: 0.21371371074169937, test loss: 0.3001129423100048\n",
      "epoch 1807: train loss: 0.2136712697908222, test loss: 0.3000774777619736\n",
      "epoch 1808: train loss: 0.21362886511617138, test loss: 0.3000420494482996\n",
      "epoch 1809: train loss: 0.2135864966650746, test loss: 0.3000066573152029\n",
      "epoch 1810: train loss: 0.213544164384964, test loss: 0.2999713013090142\n",
      "epoch 1811: train loss: 0.21350186822337622, test loss: 0.29993598137616867\n",
      "epoch 1812: train loss: 0.21345960812795156, test loss: 0.29990069746321224\n",
      "epoch 1813: train loss: 0.2134173840464343, test loss: 0.2998654495168002\n",
      "epoch 1814: train loss: 0.21337519592667198, test loss: 0.2998302374836934\n",
      "epoch 1815: train loss: 0.21333304371661563, test loss: 0.2997950613107603\n",
      "epoch 1816: train loss: 0.21329092736431896, test loss: 0.29975992094497905\n",
      "epoch 1817: train loss: 0.21324884681793863, test loss: 0.29972481633343384\n",
      "epoch 1818: train loss: 0.2132068020257336, test loss: 0.29968974742331467\n",
      "epoch 1819: train loss: 0.2131647929360652, test loss: 0.2996547141619175\n",
      "epoch 1820: train loss: 0.21312281949739664, test loss: 0.2996197164966478\n",
      "epoch 1821: train loss: 0.21308088165829284, test loss: 0.2995847543750154\n",
      "epoch 1822: train loss: 0.21303897936742025, test loss: 0.2995498277446329\n",
      "epoch 1823: train loss: 0.21299711257354645, test loss: 0.29951493655322237\n",
      "epoch 1824: train loss: 0.21295528122554017, test loss: 0.299480080748611\n",
      "epoch 1825: train loss: 0.2129134852723706, test loss: 0.29944526027872614\n",
      "epoch 1826: train loss: 0.21287172466310778, test loss: 0.29941047509160634\n",
      "epoch 1827: train loss: 0.2128299993469216, test loss: 0.29937572513538924\n",
      "epoch 1828: train loss: 0.2127883092730822, test loss: 0.29934101035831984\n",
      "epoch 1829: train loss: 0.21274665439095955, test loss: 0.2993063307087458\n",
      "epoch 1830: train loss: 0.21270503465002283, test loss: 0.29927168613511795\n",
      "epoch 1831: train loss: 0.21266344999984071, test loss: 0.2992370765859896\n",
      "epoch 1832: train loss: 0.21262190039008094, test loss: 0.29920250201002074\n",
      "epoch 1833: train loss: 0.21258038577050983, test loss: 0.29916796235597043\n",
      "epoch 1834: train loss: 0.21253890609099244, test loss: 0.2991334575727029\n",
      "epoch 1835: train loss: 0.21249746130149208, test loss: 0.29909898760918074\n",
      "epoch 1836: train loss: 0.2124560513520701, test loss: 0.2990645524144748\n",
      "epoch 1837: train loss: 0.21241467619288584, test loss: 0.2990301519377504\n",
      "epoch 1838: train loss: 0.212373335774196, test loss: 0.2989957861282807\n",
      "epoch 1839: train loss: 0.2123320300463548, test loss: 0.2989614549354364\n",
      "epoch 1840: train loss: 0.2122907589598136, test loss: 0.2989271583086904\n",
      "epoch 1841: train loss: 0.21224952246512052, test loss: 0.2988928961976171\n",
      "epoch 1842: train loss: 0.2122083205129205, test loss: 0.2988586685518879\n",
      "epoch 1843: train loss: 0.21216715305395478, test loss: 0.2988244753212801\n",
      "epoch 1844: train loss: 0.21212602003906092, test loss: 0.29879031645566595\n",
      "epoch 1845: train loss: 0.21208492141917237, test loss: 0.2987561919050195\n",
      "epoch 1846: train loss: 0.2120438571453182, test loss: 0.29872210161941587\n",
      "epoch 1847: train loss: 0.21200282716862323, test loss: 0.2986880455490249\n",
      "epoch 1848: train loss: 0.2119618314403073, test loss: 0.2986540236441192\n",
      "epoch 1849: train loss: 0.2119208699116855, test loss: 0.29862003585506935\n",
      "epoch 1850: train loss: 0.2118799425341675, test loss: 0.29858608213234433\n",
      "epoch 1851: train loss: 0.2118390492592578, test loss: 0.29855216242651084\n",
      "epoch 1852: train loss: 0.21179819003855502, test loss: 0.29851827668823355\n",
      "epoch 1853: train loss: 0.21175736482375218, test loss: 0.29848442486827503\n",
      "epoch 1854: train loss: 0.21171657356663595, test loss: 0.2984506069174959\n",
      "epoch 1855: train loss: 0.21167581621908674, test loss: 0.2984168227868539\n",
      "epoch 1856: train loss: 0.21163509273307848, test loss: 0.2983830724274014\n",
      "epoch 1857: train loss: 0.21159440306067828, test loss: 0.29834935579029215\n",
      "epoch 1858: train loss: 0.21155374715404635, test loss: 0.2983156728267712\n",
      "epoch 1859: train loss: 0.21151312496543548, test loss: 0.29828202348818583\n",
      "epoch 1860: train loss: 0.21147253644719122, test loss: 0.2982484077259723\n",
      "epoch 1861: train loss: 0.21143198155175125, test loss: 0.2982148254916695\n",
      "epoch 1862: train loss: 0.2113914602316456, test loss: 0.2981812767369067\n",
      "epoch 1863: train loss: 0.21135097243949602, test loss: 0.29814776141341054\n",
      "epoch 1864: train loss: 0.21131051812801602, test loss: 0.2981142794730036\n",
      "epoch 1865: train loss: 0.21127009725001053, test loss: 0.2980808308676032\n",
      "epoch 1866: train loss: 0.21122970975837568, test loss: 0.2980474155492183\n",
      "epoch 1867: train loss: 0.21118935560609875, test loss: 0.2980140334699557\n",
      "epoch 1868: train loss: 0.21114903474625762, test loss: 0.29798068458201393\n",
      "epoch 1869: train loss: 0.21110874713202102, test loss: 0.2979473688376878\n",
      "epoch 1870: train loss: 0.2110684927166477, test loss: 0.2979140861893637\n",
      "epoch 1871: train loss: 0.21102827145348688, test loss: 0.2978808365895232\n",
      "epoch 1872: train loss: 0.21098808329597754, test loss: 0.29784761999073905\n",
      "epoch 1873: train loss: 0.21094792819764852, test loss: 0.2978144363456773\n",
      "epoch 1874: train loss: 0.21090780611211804, test loss: 0.29778128560709943\n",
      "epoch 1875: train loss: 0.21086771699309362, test loss: 0.29774816772785634\n",
      "epoch 1876: train loss: 0.21082766079437196, test loss: 0.29771508266089164\n",
      "epoch 1877: train loss: 0.21078763746983853, test loss: 0.2976820303592419\n",
      "epoch 1878: train loss: 0.2107476469734676, test loss: 0.2976490107760364\n",
      "epoch 1879: train loss: 0.21070768925932165, test loss: 0.29761602386449376\n",
      "epoch 1880: train loss: 0.2106677642815516, test loss: 0.2975830695779252\n",
      "epoch 1881: train loss: 0.2106278719943964, test loss: 0.29755014786973255\n",
      "epoch 1882: train loss: 0.2105880123521827, test loss: 0.2975172586934093\n",
      "epoch 1883: train loss: 0.21054818530932487, test loss: 0.29748440200253956\n",
      "epoch 1884: train loss: 0.21050839082032463, test loss: 0.2974515777507971\n",
      "epoch 1885: train loss: 0.21046862883977085, test loss: 0.2974187858919462\n",
      "epoch 1886: train loss: 0.21042889932233946, test loss: 0.29738602637984185\n",
      "epoch 1887: train loss: 0.21038920222279323, test loss: 0.2973532991684278\n",
      "epoch 1888: train loss: 0.21034953749598137, test loss: 0.29732060421173867\n",
      "epoch 1889: train loss: 0.2103099050968396, test loss: 0.29728794146389576\n",
      "epoch 1890: train loss: 0.21027030498038962, test loss: 0.29725531087911333\n",
      "epoch 1891: train loss: 0.2102307371017393, test loss: 0.2972227124116923\n",
      "epoch 1892: train loss: 0.2101912014160822, test loss: 0.2971901460160224\n",
      "epoch 1893: train loss: 0.21015169787869736, test loss: 0.29715761164658155\n",
      "epoch 1894: train loss: 0.21011222644494928, test loss: 0.2971251092579354\n",
      "epoch 1895: train loss: 0.21007278707028756, test loss: 0.29709263880474074\n",
      "epoch 1896: train loss: 0.21003337971024677, test loss: 0.2970602002417394\n",
      "epoch 1897: train loss: 0.20999400432044624, test loss: 0.29702779352376085\n",
      "epoch 1898: train loss: 0.20995466085658981, test loss: 0.2969954186057216\n",
      "epoch 1899: train loss: 0.20991534927446578, test loss: 0.2969630754426288\n",
      "epoch 1900: train loss: 0.2098760695299465, test loss: 0.29693076398957147\n",
      "epoch 1901: train loss: 0.20983682157898834, test loss: 0.29689848420173026\n",
      "epoch 1902: train loss: 0.20979760537763134, test loss: 0.29686623603436857\n",
      "epoch 1903: train loss: 0.20975842088199925, test loss: 0.29683401944283827\n",
      "epoch 1904: train loss: 0.20971926804829905, test loss: 0.2968018343825763\n",
      "epoch 1905: train loss: 0.209680146832821, test loss: 0.2967696808091064\n",
      "epoch 1906: train loss: 0.20964105719193818, test loss: 0.2967375586780357\n",
      "epoch 1907: train loss: 0.2096019990821066, test loss: 0.29670546794505975\n",
      "epoch 1908: train loss: 0.2095629724598648, test loss: 0.29667340856595814\n",
      "epoch 1909: train loss: 0.20952397728183378, test loss: 0.2966413804965957\n",
      "epoch 1910: train loss: 0.2094850135047166, test loss: 0.29660938369292267\n",
      "epoch 1911: train loss: 0.20944608108529836, test loss: 0.296577418110972\n",
      "epoch 1912: train loss: 0.20940717998044608, test loss: 0.2965454837068626\n",
      "epoch 1913: train loss: 0.20936831014710833, test loss: 0.29651358043679743\n",
      "epoch 1914: train loss: 0.2093294715423151, test loss: 0.2964817082570628\n",
      "epoch 1915: train loss: 0.2092906641231776, test loss: 0.2964498671240305\n",
      "epoch 1916: train loss: 0.20925188784688814, test loss: 0.2964180569941553\n",
      "epoch 1917: train loss: 0.2092131426707199, test loss: 0.2963862778239731\n",
      "epoch 1918: train loss: 0.20917442855202664, test loss: 0.2963545295701048\n",
      "epoch 1919: train loss: 0.2091357454482427, test loss: 0.296322812189257\n",
      "epoch 1920: train loss: 0.2090970933168827, test loss: 0.29629112563821336\n",
      "epoch 1921: train loss: 0.2090584721155413, test loss: 0.2962594698738451\n",
      "epoch 1922: train loss: 0.20901988180189307, test loss: 0.2962278448531049\n",
      "epoch 1923: train loss: 0.20898132233369246, test loss: 0.2961962505330246\n",
      "epoch 1924: train loss: 0.2089427936687733, test loss: 0.29616468687072045\n",
      "epoch 1925: train loss: 0.20890429576504896, test loss: 0.2961331538233908\n",
      "epoch 1926: train loss: 0.2088658285805117, test loss: 0.29610165134831656\n",
      "epoch 1927: train loss: 0.20882739207323311, test loss: 0.29607017940285557\n",
      "epoch 1928: train loss: 0.2087889862013633, test loss: 0.2960387379444522\n",
      "epoch 1929: train loss: 0.20875061092313124, test loss: 0.29600732693062876\n",
      "epoch 1930: train loss: 0.20871226619684413, test loss: 0.29597594631899016\n",
      "epoch 1931: train loss: 0.2086739519808876, test loss: 0.29594459606721835\n",
      "epoch 1932: train loss: 0.20863566823372526, test loss: 0.29591327613308105\n",
      "epoch 1933: train loss: 0.20859741491389866, test loss: 0.2958819864744214\n",
      "epoch 1934: train loss: 0.20855919198002704, test loss: 0.29585072704916615\n",
      "epoch 1935: train loss: 0.20852099939080715, test loss: 0.29581949781531963\n",
      "epoch 1936: train loss: 0.20848283710501314, test loss: 0.2957882987309665\n",
      "epoch 1937: train loss: 0.20844470508149635, test loss: 0.29575712975427143\n",
      "epoch 1938: train loss: 0.20840660327918503, test loss: 0.29572599084347806\n",
      "epoch 1939: train loss: 0.20836853165708435, test loss: 0.2956948819569079\n",
      "epoch 1940: train loss: 0.20833049017427605, test loss: 0.2956638030529639\n",
      "epoch 1941: train loss: 0.20829247878991836, test loss: 0.2956327540901253\n",
      "epoch 1942: train loss: 0.20825449746324573, test loss: 0.2956017350269515\n",
      "epoch 1943: train loss: 0.20821654615356888, test loss: 0.2955707458220783\n",
      "epoch 1944: train loss: 0.20817862482027427, test loss: 0.2955397864342221\n",
      "epoch 1945: train loss: 0.2081407334228242, test loss: 0.2955088568221754\n",
      "epoch 1946: train loss: 0.20810287192075663, test loss: 0.2954779569448086\n",
      "epoch 1947: train loss: 0.20806504027368475, test loss: 0.2954470867610714\n",
      "epoch 1948: train loss: 0.20802723844129717, test loss: 0.2954162462299893\n",
      "epoch 1949: train loss: 0.20798946638335739, test loss: 0.2953854353106643\n",
      "epoch 1950: train loss: 0.20795172405970394, test loss: 0.2953546539622778\n",
      "epoch 1951: train loss: 0.20791401143024998, test loss: 0.2953239021440872\n",
      "epoch 1952: train loss: 0.20787632845498316, test loss: 0.29529317981542375\n",
      "epoch 1953: train loss: 0.2078386750939657, test loss: 0.29526248693569945\n",
      "epoch 1954: train loss: 0.20780105130733376, test loss: 0.29523182346439975\n",
      "epoch 1955: train loss: 0.20776345705529767, test loss: 0.2952011893610875\n",
      "epoch 1956: train loss: 0.20772589229814165, test loss: 0.29517058458540113\n",
      "epoch 1957: train loss: 0.20768835699622357, test loss: 0.2951400090970542\n",
      "epoch 1958: train loss: 0.20765085110997472, test loss: 0.2951094628558363\n",
      "epoch 1959: train loss: 0.20761337459989984, test loss: 0.29507894582161337\n",
      "epoch 1960: train loss: 0.20757592742657685, test loss: 0.2950484579543247\n",
      "epoch 1961: train loss: 0.20753850955065667, test loss: 0.2950179992139864\n",
      "epoch 1962: train loss: 0.2075011209328629, test loss: 0.29498756956068767\n",
      "epoch 1963: train loss: 0.20746376153399212, test loss: 0.29495716895459423\n",
      "epoch 1964: train loss: 0.20742643131491328, test loss: 0.29492679735594424\n",
      "epoch 1965: train loss: 0.20738913023656752, test loss: 0.2948964547250527\n",
      "epoch 1966: train loss: 0.20735185825996838, test loss: 0.29486614102230624\n",
      "epoch 1967: train loss: 0.20731461534620135, test loss: 0.294835856208167\n",
      "epoch 1968: train loss: 0.20727740145642365, test loss: 0.2948056002431699\n",
      "epoch 1969: train loss: 0.20724021655186434, test loss: 0.2947753730879249\n",
      "epoch 1970: train loss: 0.20720306059382387, test loss: 0.29474517470311273\n",
      "epoch 1971: train loss: 0.20716593354367419, test loss: 0.2947150050494913\n",
      "epoch 1972: train loss: 0.20712883536285823, test loss: 0.2946848640878884\n",
      "epoch 1973: train loss: 0.2070917660128902, test loss: 0.294654751779205\n",
      "epoch 1974: train loss: 0.207054725455355, test loss: 0.2946246680844171\n",
      "epoch 1975: train loss: 0.20701771365190827, test loss: 0.2945946129645713\n",
      "epoch 1976: train loss: 0.2069807305642762, test loss: 0.2945645863807859\n",
      "epoch 1977: train loss: 0.20694377615425544, test loss: 0.29453458829425433\n",
      "epoch 1978: train loss: 0.20690685038371265, test loss: 0.29450461866623806\n",
      "epoch 1979: train loss: 0.20686995321458482, test loss: 0.29447467745807476\n",
      "epoch 1980: train loss: 0.20683308460887864, test loss: 0.2944447646311707\n",
      "epoch 1981: train loss: 0.2067962445286706, test loss: 0.29441488014700634\n",
      "epoch 1982: train loss: 0.2067594329361069, test loss: 0.29438502396712857\n",
      "epoch 1983: train loss: 0.2067226497934029, test loss: 0.29435519605316157\n",
      "epoch 1984: train loss: 0.2066858950628434, test loss: 0.2943253963667972\n",
      "epoch 1985: train loss: 0.20664916870678243, test loss: 0.294295624869799\n",
      "epoch 1986: train loss: 0.20661247068764269, test loss: 0.29426588152400046\n",
      "epoch 1987: train loss: 0.20657580096791583, test loss: 0.29423616629130606\n",
      "epoch 1988: train loss: 0.20653915951016216, test loss: 0.29420647913369113\n",
      "epoch 1989: train loss: 0.20650254627701042, test loss: 0.29417682001320083\n",
      "epoch 1990: train loss: 0.20646596123115762, test loss: 0.29414718889195135\n",
      "epoch 1991: train loss: 0.2064294043353692, test loss: 0.29411758573212515\n",
      "epoch 1992: train loss: 0.20639287555247826, test loss: 0.2940880104959787\n",
      "epoch 1993: train loss: 0.20635637484538594, test loss: 0.29405846314583756\n",
      "epoch 1994: train loss: 0.20631990217706117, test loss: 0.2940289436440927\n",
      "epoch 1995: train loss: 0.2062834575105403, test loss: 0.29399945195320903\n",
      "epoch 1996: train loss: 0.20624704080892717, test loss: 0.2939699880357184\n",
      "epoch 1997: train loss: 0.20621065203539282, test loss: 0.293940551854222\n",
      "epoch 1998: train loss: 0.20617429115317534, test loss: 0.29391114337138746\n",
      "epoch 1999: train loss: 0.20613795812557995, test loss: 0.29388176254995513\n",
      "epoch 2000: train loss: 0.20610165291597848, test loss: 0.2938524093527308\n",
      "epoch 2001: train loss: 0.20606537548780945, test loss: 0.2938230837425904\n",
      "epoch 2002: train loss: 0.2060291258045779, test loss: 0.29379378568247405\n",
      "epoch 2003: train loss: 0.2059929038298553, test loss: 0.293764515135396\n",
      "epoch 2004: train loss: 0.20595670952727904, test loss: 0.29373527206443373\n",
      "epoch 2005: train loss: 0.20592054286055292, test loss: 0.2937060564327332\n",
      "epoch 2006: train loss: 0.20588440379344633, test loss: 0.29367686820350936\n",
      "epoch 2007: train loss: 0.2058482922897946, test loss: 0.2936477073400411\n",
      "epoch 2008: train loss: 0.20581220831349845, test loss: 0.2936185738056793\n",
      "epoch 2009: train loss: 0.2057761518285244, test loss: 0.2935894675638375\n",
      "epoch 2010: train loss: 0.20574012279890388, test loss: 0.2935603885779992\n",
      "epoch 2011: train loss: 0.20570412118873377, test loss: 0.2935313368117113\n",
      "epoch 2012: train loss: 0.20566814696217578, test loss: 0.293502312228591\n",
      "epoch 2013: train loss: 0.20563220008345656, test loss: 0.29347331479231853\n",
      "epoch 2014: train loss: 0.20559628051686743, test loss: 0.29344434446664214\n",
      "epoch 2015: train loss: 0.20556038822676442, test loss: 0.29341540121537674\n",
      "epoch 2016: train loss: 0.2055245231775679, test loss: 0.29338648500240117\n",
      "epoch 2017: train loss: 0.20548868533376236, test loss: 0.2933575957916608\n",
      "epoch 2018: train loss: 0.2054528746598966, test loss: 0.2933287335471688\n",
      "epoch 2019: train loss: 0.20541709112058348, test loss: 0.29329989823299957\n",
      "epoch 2020: train loss: 0.20538133468049963, test loss: 0.2932710898132956\n",
      "epoch 2021: train loss: 0.20534560530438536, test loss: 0.29324230825226527\n",
      "epoch 2022: train loss: 0.2053099029570445, test loss: 0.2932135535141803\n",
      "epoch 2023: train loss: 0.20527422760334438, test loss: 0.2931848255633764\n",
      "epoch 2024: train loss: 0.20523857920821562, test loss: 0.29315612436425714\n",
      "epoch 2025: train loss: 0.205202957736652, test loss: 0.29312744988128697\n",
      "epoch 2026: train loss: 0.2051673631537102, test loss: 0.29309880207899885\n",
      "epoch 2027: train loss: 0.20513179542450982, test loss: 0.2930701809219848\n",
      "epoch 2028: train loss: 0.20509625451423313, test loss: 0.293041586374905\n",
      "epoch 2029: train loss: 0.20506074038812502, test loss: 0.2930130184024842\n",
      "epoch 2030: train loss: 0.20502525301149288, test loss: 0.29298447696950747\n",
      "epoch 2031: train loss: 0.20498979234970624, test loss: 0.29295596204082497\n",
      "epoch 2032: train loss: 0.20495435836819687, test loss: 0.2929274735813509\n",
      "epoch 2033: train loss: 0.2049189510324586, test loss: 0.2928990115560629\n",
      "epoch 2034: train loss: 0.20488357030804705, test loss: 0.29287057593000076\n",
      "epoch 2035: train loss: 0.20484821616057963, test loss: 0.2928421666682703\n",
      "epoch 2036: train loss: 0.20481288855573546, test loss: 0.29281378373603517\n",
      "epoch 2037: train loss: 0.20477758745925484, test loss: 0.29278542709852634\n",
      "epoch 2038: train loss: 0.20474231283693975, test loss: 0.292757096721036\n",
      "epoch 2039: train loss: 0.20470706465465308, test loss: 0.29272879256891854\n",
      "epoch 2040: train loss: 0.204671842878319, test loss: 0.29270051460758917\n",
      "epoch 2041: train loss: 0.20463664747392232, test loss: 0.2926722628025293\n",
      "epoch 2042: train loss: 0.204601478407509, test loss: 0.2926440371192778\n",
      "epoch 2043: train loss: 0.20456633564518534, test loss: 0.2926158375234392\n",
      "epoch 2044: train loss: 0.20453121915311837, test loss: 0.29258766398067915\n",
      "epoch 2045: train loss: 0.20449612889753532, test loss: 0.29255951645672107\n",
      "epoch 2046: train loss: 0.20446106484472382, test loss: 0.29253139491735647\n",
      "epoch 2047: train loss: 0.20442602696103152, test loss: 0.2925032993284329\n",
      "epoch 2048: train loss: 0.20439101521286607, test loss: 0.29247522965586\n",
      "epoch 2049: train loss: 0.20435602956669505, test loss: 0.29244718586561047\n",
      "epoch 2050: train loss: 0.20432106998904564, test loss: 0.2924191679237189\n",
      "epoch 2051: train loss: 0.20428613644650462, test loss: 0.29239117579627527\n",
      "epoch 2052: train loss: 0.20425122890571834, test loss: 0.2923632094494352\n",
      "epoch 2053: train loss: 0.20421634733339225, test loss: 0.2923352688494126\n",
      "epoch 2054: train loss: 0.20418149169629124, test loss: 0.2923073539624849\n",
      "epoch 2055: train loss: 0.20414666196123907, test loss: 0.29227946475498384\n",
      "epoch 2056: train loss: 0.20411185809511853, test loss: 0.2922516011933069\n",
      "epoch 2057: train loss: 0.20407708006487113, test loss: 0.29222376324390886\n",
      "epoch 2058: train loss: 0.20404232783749707, test loss: 0.2921959508733049\n",
      "epoch 2059: train loss: 0.20400760138005516, test loss: 0.2921681640480718\n",
      "epoch 2060: train loss: 0.20397290065966253, test loss: 0.29214040273483893\n",
      "epoch 2061: train loss: 0.2039382256434946, test loss: 0.2921126669003054\n",
      "epoch 2062: train loss: 0.203903576298785, test loss: 0.2920849565112224\n",
      "epoch 2063: train loss: 0.20386895259282528, test loss: 0.2920572715344017\n",
      "epoch 2064: train loss: 0.203834354492965, test loss: 0.2920296119367159\n",
      "epoch 2065: train loss: 0.2037997819666114, test loss: 0.2920019776850933\n",
      "epoch 2066: train loss: 0.20376523498122937, test loss: 0.29197436874652566\n",
      "epoch 2067: train loss: 0.2037307135043413, test loss: 0.2919467850880589\n",
      "epoch 2068: train loss: 0.20369621750352715, test loss: 0.2919192266767986\n",
      "epoch 2069: train loss: 0.2036617469464238, test loss: 0.29189169347991045\n",
      "epoch 2070: train loss: 0.20362730180072558, test loss: 0.2918641854646171\n",
      "epoch 2071: train loss: 0.20359288203418358, test loss: 0.2918367025981985\n",
      "epoch 2072: train loss: 0.203558487614606, test loss: 0.2918092448479952\n",
      "epoch 2073: train loss: 0.2035241185098576, test loss: 0.29178181218140264\n",
      "epoch 2074: train loss: 0.20348977468785992, test loss: 0.29175440456587426\n",
      "epoch 2075: train loss: 0.2034554561165909, test loss: 0.2917270219689243\n",
      "epoch 2076: train loss: 0.20342116276408498, test loss: 0.2916996643581203\n",
      "epoch 2077: train loss: 0.20338689459843265, test loss: 0.2916723317010899\n",
      "epoch 2078: train loss: 0.20335265158778085, test loss: 0.29164502396551606\n",
      "epoch 2079: train loss: 0.20331843370033226, test loss: 0.2916177411191406\n",
      "epoch 2080: train loss: 0.2032842409043456, test loss: 0.29159048312976155\n",
      "epoch 2081: train loss: 0.20325007316813526, test loss: 0.291563249965232\n",
      "epoch 2082: train loss: 0.2032159304600714, test loss: 0.29153604159346536\n",
      "epoch 2083: train loss: 0.20318181274857955, test loss: 0.29150885798242837\n",
      "epoch 2084: train loss: 0.20314772000214082, test loss: 0.29148169910014593\n",
      "epoch 2085: train loss: 0.20311365218929148, test loss: 0.2914545649146986\n",
      "epoch 2086: train loss: 0.203079609278623, test loss: 0.29142745539422266\n",
      "epoch 2087: train loss: 0.20304559123878185, test loss: 0.2914003705069102\n",
      "epoch 2088: train loss: 0.20301159803846958, test loss: 0.2913733102210118\n",
      "epoch 2089: train loss: 0.20297762964644228, test loss: 0.29134627450483036\n",
      "epoch 2090: train loss: 0.2029436860315109, test loss: 0.29131926332672725\n",
      "epoch 2091: train loss: 0.20290976716254094, test loss: 0.29129227665511814\n",
      "epoch 2092: train loss: 0.20287587300845236, test loss: 0.291265314458473\n",
      "epoch 2093: train loss: 0.20284200353821935, test loss: 0.2912383767053195\n",
      "epoch 2094: train loss: 0.20280815872087032, test loss: 0.2912114633642387\n",
      "epoch 2095: train loss: 0.20277433852548785, test loss: 0.29118457440386775\n",
      "epoch 2096: train loss: 0.20274054292120852, test loss: 0.2911577097928973\n",
      "epoch 2097: train loss: 0.20270677187722255, test loss: 0.2911308695000737\n",
      "epoch 2098: train loss: 0.2026730253627742, test loss: 0.2911040534941996\n",
      "epoch 2099: train loss: 0.20263930334716102, test loss: 0.2910772617441288\n",
      "epoch 2100: train loss: 0.20260560579973436, test loss: 0.2910504942187712\n",
      "epoch 2101: train loss: 0.20257193268989873, test loss: 0.2910237508870933\n",
      "epoch 2102: train loss: 0.20253828398711213, test loss: 0.2909970317181114\n",
      "epoch 2103: train loss: 0.2025046596608855, test loss: 0.2909703366808995\n",
      "epoch 2104: train loss: 0.2024710596807829, test loss: 0.29094366574458413\n",
      "epoch 2105: train loss: 0.2024374840164214, test loss: 0.2909170188783435\n",
      "epoch 2106: train loss: 0.20240393263747072, test loss: 0.29089039605141453\n",
      "epoch 2107: train loss: 0.20237040551365346, test loss: 0.29086379723308214\n",
      "epoch 2108: train loss: 0.20233690261474457, test loss: 0.290837222392691\n",
      "epoch 2109: train loss: 0.2023034239105717, test loss: 0.29081067149963163\n",
      "epoch 2110: train loss: 0.20226996937101474, test loss: 0.2907841445233551\n",
      "epoch 2111: train loss: 0.2022365389660058, test loss: 0.2907576414333596\n",
      "epoch 2112: train loss: 0.20220313266552922, test loss: 0.29073116219920087\n",
      "epoch 2113: train loss: 0.20216975043962115, test loss: 0.2907047067904849\n",
      "epoch 2114: train loss: 0.20213639225836985, test loss: 0.290678275176871\n",
      "epoch 2115: train loss: 0.20210305809191534, test loss: 0.2906518673280709\n",
      "epoch 2116: train loss: 0.2020697479104492, test loss: 0.29062548321384996\n",
      "epoch 2117: train loss: 0.20203646168421469, test loss: 0.29059912280402495\n",
      "epoch 2118: train loss: 0.20200319938350644, test loss: 0.2905727860684653\n",
      "epoch 2119: train loss: 0.2019699609786705, test loss: 0.2905464729770923\n",
      "epoch 2120: train loss: 0.20193674644010412, test loss: 0.2905201834998795\n",
      "epoch 2121: train loss: 0.20190355573825564, test loss: 0.290493917606853\n",
      "epoch 2122: train loss: 0.2018703888436245, test loss: 0.29046767526808953\n",
      "epoch 2123: train loss: 0.2018372457267609, test loss: 0.290441456453719\n",
      "epoch 2124: train loss: 0.201804126358266, test loss: 0.29041526113392147\n",
      "epoch 2125: train loss: 0.20177103070879154, test loss: 0.29038908927892904\n",
      "epoch 2126: train loss: 0.2017379587490398, test loss: 0.29036294085902403\n",
      "epoch 2127: train loss: 0.20170491044976374, test loss: 0.29033681584454407\n",
      "epoch 2128: train loss: 0.20167188578176629, test loss: 0.29031071420587273\n",
      "epoch 2129: train loss: 0.20163888471590102, test loss: 0.29028463591344894\n",
      "epoch 2130: train loss: 0.20160590722307142, test loss: 0.29025858093776014\n",
      "epoch 2131: train loss: 0.20157295327423108, test loss: 0.29023254924934244\n",
      "epoch 2132: train loss: 0.20154002284038355, test loss: 0.2902065408187893\n",
      "epoch 2133: train loss: 0.2015071158925821, test loss: 0.2901805556167378\n",
      "epoch 2134: train loss: 0.20147423240192977, test loss: 0.2901545936138797\n",
      "epoch 2135: train loss: 0.20144137233957923, test loss: 0.2901286547809568\n",
      "epoch 2136: train loss: 0.2014085356767326, test loss: 0.2901027390887585\n",
      "epoch 2137: train loss: 0.2013757223846414, test loss: 0.29007684650812754\n",
      "epoch 2138: train loss: 0.2013429324346065, test loss: 0.2900509770099539\n",
      "epoch 2139: train loss: 0.20131016579797786, test loss: 0.2900251305651793\n",
      "epoch 2140: train loss: 0.20127742244615454, test loss: 0.28999930714479577\n",
      "epoch 2141: train loss: 0.20124470235058464, test loss: 0.2899735067198421\n",
      "epoch 2142: train loss: 0.20121200548276502, test loss: 0.28994772926141077\n",
      "epoch 2143: train loss: 0.2011793318142414, test loss: 0.28992197474064074\n",
      "epoch 2144: train loss: 0.20114668131660807, test loss: 0.28989624312872236\n",
      "epoch 2145: train loss: 0.20111405396150794, test loss: 0.28987053439689164\n",
      "epoch 2146: train loss: 0.20108144972063235, test loss: 0.28984484851643955\n",
      "epoch 2147: train loss: 0.20104886856572093, test loss: 0.28981918545870006\n",
      "epoch 2148: train loss: 0.20101631046856172, test loss: 0.2897935451950608\n",
      "epoch 2149: train loss: 0.20098377540099066, test loss: 0.2897679276969541\n",
      "epoch 2150: train loss: 0.2009512633348919, test loss: 0.2897423329358665\n",
      "epoch 2151: train loss: 0.20091877424219753, test loss: 0.28971676088332726\n",
      "epoch 2152: train loss: 0.20088630809488728, test loss: 0.28969121151091837\n",
      "epoch 2153: train loss: 0.20085386486498888, test loss: 0.28966568479026944\n",
      "epoch 2154: train loss: 0.20082144452457754, test loss: 0.2896401806930551\n",
      "epoch 2155: train loss: 0.20078904704577596, test loss: 0.28961469919100247\n",
      "epoch 2156: train loss: 0.20075667240075432, test loss: 0.2895892402558851\n",
      "epoch 2157: train loss: 0.20072432056173029, test loss: 0.2895638038595229\n",
      "epoch 2158: train loss: 0.20069199150096836, test loss: 0.28953838997378795\n",
      "epoch 2159: train loss: 0.20065968519078062, test loss: 0.28951299857059576\n",
      "epoch 2160: train loss: 0.20062740160352582, test loss: 0.2894876296219133\n",
      "epoch 2161: train loss: 0.20059514071160975, test loss: 0.2894622830997502\n",
      "epoch 2162: train loss: 0.2005629024874852, test loss: 0.2894369589761685\n",
      "epoch 2163: train loss: 0.20053068690365136, test loss: 0.2894116572232737\n",
      "epoch 2164: train loss: 0.20049849393265431, test loss: 0.2893863778132232\n",
      "epoch 2165: train loss: 0.20046632354708657, test loss: 0.289361120718217\n",
      "epoch 2166: train loss: 0.20043417571958705, test loss: 0.28933588591050335\n",
      "epoch 2167: train loss: 0.20040205042284107, test loss: 0.28931067336238\n",
      "epoch 2168: train loss: 0.2003699476295801, test loss: 0.28928548304618845\n",
      "epoch 2169: train loss: 0.20033786731258182, test loss: 0.2892603149343192\n",
      "epoch 2170: train loss: 0.2003058094446699, test loss: 0.28923516899920776\n",
      "epoch 2171: train loss: 0.2002737739987139, test loss: 0.2892100452133367\n",
      "epoch 2172: train loss: 0.20024176094762944, test loss: 0.2891849435492353\n",
      "epoch 2173: train loss: 0.20020977026437756, test loss: 0.28915986397948096\n",
      "epoch 2174: train loss: 0.20017780192196524, test loss: 0.2891348064766924\n",
      "epoch 2175: train loss: 0.20014585589344486, test loss: 0.2891097710135401\n",
      "epoch 2176: train loss: 0.20011393215191425, test loss: 0.2890847575627367\n",
      "epoch 2177: train loss: 0.20008203067051666, test loss: 0.28905976609704315\n",
      "epoch 2178: train loss: 0.2000501514224407, test loss: 0.2890347965892659\n",
      "epoch 2179: train loss: 0.20001829438091984, test loss: 0.2890098490122548\n",
      "epoch 2180: train loss: 0.19998645951923294, test loss: 0.2889849233389082\n",
      "epoch 2181: train loss: 0.19995464681070374, test loss: 0.2889600195421694\n",
      "epoch 2182: train loss: 0.1999228562287008, test loss: 0.2889351375950271\n",
      "epoch 2183: train loss: 0.1998910877466374, test loss: 0.28891027747051445\n",
      "epoch 2184: train loss: 0.19985934133797187, test loss: 0.2888854391417097\n",
      "epoch 2185: train loss: 0.1998276169762067, test loss: 0.2888606225817384\n",
      "epoch 2186: train loss: 0.19979591463488922, test loss: 0.28883582776377054\n",
      "epoch 2187: train loss: 0.19976423428761095, test loss: 0.2888110546610181\n",
      "epoch 2188: train loss: 0.19973257590800786, test loss: 0.2887863032467417\n",
      "epoch 2189: train loss: 0.1997009394697601, test loss: 0.2887615734942465\n",
      "epoch 2190: train loss: 0.19966932494659198, test loss: 0.2887368653768788\n",
      "epoch 2191: train loss: 0.19963773231227186, test loss: 0.28871217886803313\n",
      "epoch 2192: train loss: 0.199606161540612, test loss: 0.28868751394114783\n",
      "epoch 2193: train loss: 0.1995746126054685, test loss: 0.28866287056970413\n",
      "epoch 2194: train loss: 0.19954308548074134, test loss: 0.28863824872722965\n",
      "epoch 2195: train loss: 0.1995115801403741, test loss: 0.2886136483872944\n",
      "epoch 2196: train loss: 0.1994800965583539, test loss: 0.288589069523513\n",
      "epoch 2197: train loss: 0.1994486347087114, test loss: 0.2885645121095458\n",
      "epoch 2198: train loss: 0.1994171945655207, test loss: 0.28853997611909504\n",
      "epoch 2199: train loss: 0.19938577610289918, test loss: 0.28851546152590646\n",
      "epoch 2200: train loss: 0.19935437929500735, test loss: 0.2884909683037721\n",
      "epoch 2201: train loss: 0.199323004116049, test loss: 0.2884664964265249\n",
      "epoch 2202: train loss: 0.19929165054027076, test loss: 0.28844204586804234\n",
      "epoch 2203: train loss: 0.19926031854196247, test loss: 0.28841761660224763\n",
      "epoch 2204: train loss: 0.1992290080954566, test loss: 0.28839320860310447\n",
      "epoch 2205: train loss: 0.19919771917512852, test loss: 0.2883688218446184\n",
      "epoch 2206: train loss: 0.19916645175539613, test loss: 0.2883444563008434\n",
      "epoch 2207: train loss: 0.19913520581072017, test loss: 0.28832011194587315\n",
      "epoch 2208: train loss: 0.19910398131560364, test loss: 0.2882957887538444\n",
      "epoch 2209: train loss: 0.19907277824459207, test loss: 0.28827148669893676\n",
      "epoch 2210: train loss: 0.19904159657227327, test loss: 0.2882472057553741\n",
      "epoch 2211: train loss: 0.19901043627327739, test loss: 0.2882229458974208\n",
      "epoch 2212: train loss: 0.19897929732227645, test loss: 0.2881987070993865\n",
      "epoch 2213: train loss: 0.19894817969398496, test loss: 0.2881744893356222\n",
      "epoch 2214: train loss: 0.198917083363159, test loss: 0.2881502925805204\n",
      "epoch 2215: train loss: 0.1988860083045968, test loss: 0.288126116808517\n",
      "epoch 2216: train loss: 0.19885495449313823, test loss: 0.28810196199409005\n",
      "epoch 2217: train loss: 0.19882392190366496, test loss: 0.2880778281117599\n",
      "epoch 2218: train loss: 0.19879291051110023, test loss: 0.2880537151360892\n",
      "epoch 2219: train loss: 0.19876192029040887, test loss: 0.28802962304168267\n",
      "epoch 2220: train loss: 0.1987309512165971, test loss: 0.2880055518031854\n",
      "epoch 2221: train loss: 0.1987000032647126, test loss: 0.28798150139528594\n",
      "epoch 2222: train loss: 0.19866907640984427, test loss: 0.28795747179271547\n",
      "epoch 2223: train loss: 0.19863817062712208, test loss: 0.2879334629702429\n",
      "epoch 2224: train loss: 0.1986072858917174, test loss: 0.2879094749026845\n",
      "epoch 2225: train loss: 0.19857642217884233, test loss: 0.28788550756489356\n",
      "epoch 2226: train loss: 0.1985455794637501, test loss: 0.28786156093176546\n",
      "epoch 2227: train loss: 0.1985147577217347, test loss: 0.28783763497823805\n",
      "epoch 2228: train loss: 0.198483956928131, test loss: 0.28781372967929014\n",
      "epoch 2229: train loss: 0.19845317705831436, test loss: 0.2877898450099417\n",
      "epoch 2230: train loss: 0.19842241808770095, test loss: 0.28776598094525346\n",
      "epoch 2231: train loss: 0.19839167999174726, test loss: 0.28774213746032545\n",
      "epoch 2232: train loss: 0.1983609627459504, test loss: 0.28771831453030267\n",
      "epoch 2233: train loss: 0.1983302663258477, test loss: 0.2876945121303681\n",
      "epoch 2234: train loss: 0.19829959070701675, test loss: 0.28767073023574613\n",
      "epoch 2235: train loss: 0.19826893586507544, test loss: 0.2876469688216987\n",
      "epoch 2236: train loss: 0.19823830177568177, test loss: 0.28762322786353656\n",
      "epoch 2237: train loss: 0.19820768841453343, test loss: 0.2875995073366004\n",
      "epoch 2238: train loss: 0.19817709575736853, test loss: 0.2875758072162784\n",
      "epoch 2239: train loss: 0.19814652377996467, test loss: 0.2875521274779977\n",
      "epoch 2240: train loss: 0.19811597245813936, test loss: 0.2875284680972239\n",
      "epoch 2241: train loss: 0.19808544176774986, test loss: 0.2875048290494641\n",
      "epoch 2242: train loss: 0.19805493168469282, test loss: 0.2874812103102666\n",
      "epoch 2243: train loss: 0.19802444218490464, test loss: 0.2874576118552157\n",
      "epoch 2244: train loss: 0.19799397324436108, test loss: 0.2874340336599397\n",
      "epoch 2245: train loss: 0.19796352483907714, test loss: 0.28741047570010414\n",
      "epoch 2246: train loss: 0.19793309694510738, test loss: 0.2873869379514147\n",
      "epoch 2247: train loss: 0.19790268953854528, test loss: 0.2873634203896188\n",
      "epoch 2248: train loss: 0.19787230259552366, test loss: 0.2873399229905013\n",
      "epoch 2249: train loss: 0.19784193609221412, test loss: 0.2873164457298856\n",
      "epoch 2250: train loss: 0.19781159000482748, test loss: 0.2872929885836372\n",
      "epoch 2251: train loss: 0.19778126430961337, test loss: 0.2872695515276588\n",
      "epoch 2252: train loss: 0.19775095898286008, test loss: 0.287246134537895\n",
      "epoch 2253: train loss: 0.1977206740008948, test loss: 0.2872227375903231\n",
      "epoch 2254: train loss: 0.19769040934008322, test loss: 0.28719936066096863\n",
      "epoch 2255: train loss: 0.19766016497682976, test loss: 0.2871760037258878\n",
      "epoch 2256: train loss: 0.19762994088757704, test loss: 0.28715266676118184\n",
      "epoch 2257: train loss: 0.19759973704880643, test loss: 0.2871293497429878\n",
      "epoch 2258: train loss: 0.19756955343703733, test loss: 0.28710605264747896\n",
      "epoch 2259: train loss: 0.1975393900288276, test loss: 0.28708277545087385\n",
      "epoch 2260: train loss: 0.19750924680077311, test loss: 0.2870595181294239\n",
      "epoch 2261: train loss: 0.1974791237295079, test loss: 0.2870362806594219\n",
      "epoch 2262: train loss: 0.197449020791704, test loss: 0.28701306301719565\n",
      "epoch 2263: train loss: 0.19741893796407148, test loss: 0.2869898651791168\n",
      "epoch 2264: train loss: 0.19738887522335802, test loss: 0.28696668712159124\n",
      "epoch 2265: train loss: 0.1973588325463493, test loss: 0.2869435288210632\n",
      "epoch 2266: train loss: 0.19732880990986865, test loss: 0.28692039025401556\n",
      "epoch 2267: train loss: 0.19729880729077695, test loss: 0.28689727139696974\n",
      "epoch 2268: train loss: 0.19726882466597273, test loss: 0.28687417222648615\n",
      "epoch 2269: train loss: 0.19723886201239188, test loss: 0.28685109271915926\n",
      "epoch 2270: train loss: 0.19720891930700785, test loss: 0.2868280328516246\n",
      "epoch 2271: train loss: 0.19717899652683124, test loss: 0.2868049926005558\n",
      "epoch 2272: train loss: 0.19714909364890992, test loss: 0.2867819719426607\n",
      "epoch 2273: train loss: 0.19711921065032903, test loss: 0.2867589708546872\n",
      "epoch 2274: train loss: 0.1970893475082108, test loss: 0.28673598931342215\n",
      "epoch 2275: train loss: 0.19705950419971427, test loss: 0.28671302729568515\n",
      "epoch 2276: train loss: 0.19702968070203572, test loss: 0.28669008477833613\n",
      "epoch 2277: train loss: 0.196999876992408, test loss: 0.286667161738273\n",
      "epoch 2278: train loss: 0.19697009304810095, test loss: 0.2866442581524298\n",
      "epoch 2279: train loss: 0.19694032884642113, test loss: 0.2866213739977765\n",
      "epoch 2280: train loss: 0.1969105843647116, test loss: 0.28659850925132163\n",
      "epoch 2281: train loss: 0.19688085958035212, test loss: 0.28657566389011\n",
      "epoch 2282: train loss: 0.19685115447075885, test loss: 0.2865528378912242\n",
      "epoch 2283: train loss: 0.1968214690133844, test loss: 0.28653003123178133\n",
      "epoch 2284: train loss: 0.19679180318571782, test loss: 0.28650724388893756\n",
      "epoch 2285: train loss: 0.19676215696528424, test loss: 0.28648447583988507\n",
      "epoch 2286: train loss: 0.1967325303296451, test loss: 0.28646172706185224\n",
      "epoch 2287: train loss: 0.19670292325639807, test loss: 0.2864389975321025\n",
      "epoch 2288: train loss: 0.19667333572317658, test loss: 0.28641628722793905\n",
      "epoch 2289: train loss: 0.19664376770765038, test loss: 0.2863935961266972\n",
      "epoch 2290: train loss: 0.19661421918752492, test loss: 0.28637092420575305\n",
      "epoch 2291: train loss: 0.19658469014054145, test loss: 0.2863482714425158\n",
      "epoch 2292: train loss: 0.1965551805444771, test loss: 0.28632563781443204\n",
      "epoch 2293: train loss: 0.19652569037714465, test loss: 0.2863030232989822\n",
      "epoch 2294: train loss: 0.19649621961639244, test loss: 0.28628042787368524\n",
      "epoch 2295: train loss: 0.19646676824010448, test loss: 0.28625785151609634\n",
      "epoch 2296: train loss: 0.19643733622620008, test loss: 0.2862352942038028\n",
      "epoch 2297: train loss: 0.19640792355263412, test loss: 0.28621275591443246\n",
      "epoch 2298: train loss: 0.19637853019739665, test loss: 0.28619023662564613\n",
      "epoch 2299: train loss: 0.19634915613851317, test loss: 0.28616773631513936\n",
      "epoch 2300: train loss: 0.19631980135404414, test loss: 0.2861452549606443\n",
      "epoch 2301: train loss: 0.1962904658220853, test loss: 0.2861227925399298\n",
      "epoch 2302: train loss: 0.1962611495207674, test loss: 0.2861003490307978\n",
      "epoch 2303: train loss: 0.1962318524282562, test loss: 0.28607792441108815\n",
      "epoch 2304: train loss: 0.1962025745227522, test loss: 0.286055518658672\n",
      "epoch 2305: train loss: 0.1961733157824909, test loss: 0.28603313175146045\n",
      "epoch 2306: train loss: 0.19614407618574248, test loss: 0.28601076366739536\n",
      "epoch 2307: train loss: 0.1961148557108119, test loss: 0.28598841438445793\n",
      "epoch 2308: train loss: 0.1960856543360386, test loss: 0.28596608388065864\n",
      "epoch 2309: train loss: 0.1960564720397968, test loss: 0.2859437721340483\n",
      "epoch 2310: train loss: 0.19602730880049493, test loss: 0.28592147912270993\n",
      "epoch 2311: train loss: 0.19599816459657604, test loss: 0.285899204824761\n",
      "epoch 2312: train loss: 0.19596903940651741, test loss: 0.2858769492183555\n",
      "epoch 2313: train loss: 0.19593993320883077, test loss: 0.28585471228167914\n",
      "epoch 2314: train loss: 0.19591084598206177, test loss: 0.28583249399295513\n",
      "epoch 2315: train loss: 0.19588177770479043, test loss: 0.28581029433043964\n",
      "epoch 2316: train loss: 0.1958527283556308, test loss: 0.28578811327242337\n",
      "epoch 2317: train loss: 0.19582369791323093, test loss: 0.28576595079722855\n",
      "epoch 2318: train loss: 0.19579468635627273, test loss: 0.2857438068832184\n",
      "epoch 2319: train loss: 0.19576569366347202, test loss: 0.28572168150878424\n",
      "epoch 2320: train loss: 0.19573671981357849, test loss: 0.2856995746523547\n",
      "epoch 2321: train loss: 0.19570776478537538, test loss: 0.2856774862923882\n",
      "epoch 2322: train loss: 0.1956788285576799, test loss: 0.2856554164073849\n",
      "epoch 2323: train loss: 0.19564991110934252, test loss: 0.28563336497586994\n",
      "epoch 2324: train loss: 0.1956210124192475, test loss: 0.2856113319764087\n",
      "epoch 2325: train loss: 0.19559213246631238, test loss: 0.28558931738759613\n",
      "epoch 2326: train loss: 0.19556327122948822, test loss: 0.2855673211880657\n",
      "epoch 2327: train loss: 0.1955344286877593, test loss: 0.28554534335648035\n",
      "epoch 2328: train loss: 0.19550560482014334, test loss: 0.2855233838715372\n",
      "epoch 2329: train loss: 0.19547679960569103, test loss: 0.2855014427119684\n",
      "epoch 2330: train loss: 0.1954480130234863, test loss: 0.28547951985653713\n",
      "epoch 2331: train loss: 0.1954192450526463, test loss: 0.2854576152840427\n",
      "epoch 2332: train loss: 0.1953904956723208, test loss: 0.285435728973317\n",
      "epoch 2333: train loss: 0.19536176486169285, test loss: 0.2854138609032239\n",
      "epoch 2334: train loss: 0.19533305259997824, test loss: 0.28539201105265827\n",
      "epoch 2335: train loss: 0.19530435886642558, test loss: 0.28537017940055537\n",
      "epoch 2336: train loss: 0.19527568364031608, test loss: 0.28534836592587665\n",
      "epoch 2337: train loss: 0.1952470269009639, test loss: 0.2853265706076175\n",
      "epoch 2338: train loss: 0.1952183886277156, test loss: 0.2853047934248106\n",
      "epoch 2339: train loss: 0.19518976879995026, test loss: 0.28528303435651575\n",
      "epoch 2340: train loss: 0.19516116739707953, test loss: 0.2852612933818298\n",
      "epoch 2341: train loss: 0.19513258439854755, test loss: 0.28523957047987997\n",
      "epoch 2342: train loss: 0.19510401978383057, test loss: 0.28521786562982543\n",
      "epoch 2343: train loss: 0.1950754735324373, test loss: 0.28519617881086\n",
      "epoch 2344: train loss: 0.19504694562390867, test loss: 0.2851745100022094\n",
      "epoch 2345: train loss: 0.19501843603781768, test loss: 0.2851528591831316\n",
      "epoch 2346: train loss: 0.19498994475376946, test loss: 0.28513122633291643\n",
      "epoch 2347: train loss: 0.19496147175140124, test loss: 0.28510961143088875\n",
      "epoch 2348: train loss: 0.19493301701038207, test loss: 0.2850880144563989\n",
      "epoch 2349: train loss: 0.19490458051041298, test loss: 0.2850664353888355\n",
      "epoch 2350: train loss: 0.19487616223122686, test loss: 0.28504487420762037\n",
      "epoch 2351: train loss: 0.19484776215258834, test loss: 0.2850233308922011\n",
      "epoch 2352: train loss: 0.19481938025429382, test loss: 0.2850018054220618\n",
      "epoch 2353: train loss: 0.19479101651617128, test loss: 0.28498029777671824\n",
      "epoch 2354: train loss: 0.19476267091808025, test loss: 0.28495880793571654\n",
      "epoch 2355: train loss: 0.19473434343991197, test loss: 0.2849373358786345\n",
      "epoch 2356: train loss: 0.1947060340615889, test loss: 0.2849158815850835\n",
      "epoch 2357: train loss: 0.19467774276306504, test loss: 0.2848944450347047\n",
      "epoch 2358: train loss: 0.19464946952432574, test loss: 0.2848730262071736\n",
      "epoch 2359: train loss: 0.1946212143253876, test loss: 0.28485162508219297\n",
      "epoch 2360: train loss: 0.19459297714629833, test loss: 0.28483024163949894\n",
      "epoch 2361: train loss: 0.19456475796713707, test loss: 0.2848088758588615\n",
      "epoch 2362: train loss: 0.19453655676801368, test loss: 0.28478752772007937\n",
      "epoch 2363: train loss: 0.19450837352906938, test loss: 0.2847661972029821\n",
      "epoch 2364: train loss: 0.19448020823047607, test loss: 0.28474488428743217\n",
      "epoch 2365: train loss: 0.1944520608524369, test loss: 0.28472358895332095\n",
      "epoch 2366: train loss: 0.19442393137518552, test loss: 0.284702311180576\n",
      "epoch 2367: train loss: 0.19439581977898654, test loss: 0.2846810509491494\n",
      "epoch 2368: train loss: 0.19436772604413538, test loss: 0.2846598082390267\n",
      "epoch 2369: train loss: 0.19433965015095794, test loss: 0.28463858303022815\n",
      "epoch 2370: train loss: 0.19431159207981075, test loss: 0.28461737530279857\n",
      "epoch 2371: train loss: 0.1942835518110811, test loss: 0.2845961850368163\n",
      "epoch 2372: train loss: 0.1942555293251865, test loss: 0.2845750122123918\n",
      "epoch 2373: train loss: 0.19422752460257497, test loss: 0.28455385680966644\n",
      "epoch 2374: train loss: 0.194199537623725, test loss: 0.28453271880880726\n",
      "epoch 2375: train loss: 0.1941715683691453, test loss: 0.28451159819001803\n",
      "epoch 2376: train loss: 0.19414361681937473, test loss: 0.2844904949335298\n",
      "epoch 2377: train loss: 0.1941156829549826, test loss: 0.28446940901960505\n",
      "epoch 2378: train loss: 0.19408776675656803, test loss: 0.28444834042853456\n",
      "epoch 2379: train loss: 0.19405986820476046, test loss: 0.28442728914064064\n",
      "epoch 2380: train loss: 0.19403198728021923, test loss: 0.28440625513627876\n",
      "epoch 2381: train loss: 0.19400412396363356, test loss: 0.28438523839583074\n",
      "epoch 2382: train loss: 0.19397627823572283, test loss: 0.2843642388997084\n",
      "epoch 2383: train loss: 0.19394845007723593, test loss: 0.28434325662835763\n",
      "epoch 2384: train loss: 0.19392063946895172, test loss: 0.28432229156224936\n",
      "epoch 2385: train loss: 0.19389284639167864, test loss: 0.28430134368188903\n",
      "epoch 2386: train loss: 0.19386507082625506, test loss: 0.28428041296780804\n",
      "epoch 2387: train loss: 0.19383731275354857, test loss: 0.28425949940056894\n",
      "epoch 2388: train loss: 0.19380957215445668, test loss: 0.28423860296076486\n",
      "epoch 2389: train loss: 0.19378184900990603, test loss: 0.28421772362902054\n",
      "epoch 2390: train loss: 0.19375414330085303, test loss: 0.28419686138598615\n",
      "epoch 2391: train loss: 0.1937264550082832, test loss: 0.28417601621234195\n",
      "epoch 2392: train loss: 0.19369878411321145, test loss: 0.2841551880888013\n",
      "epoch 2393: train loss: 0.19367113059668206, test loss: 0.2841343769961044\n",
      "epoch 2394: train loss: 0.19364349443976836, test loss: 0.2841135829150215\n",
      "epoch 2395: train loss: 0.19361587562357294, test loss: 0.28409280582635094\n",
      "epoch 2396: train loss: 0.1935882741292273, test loss: 0.28407204571092215\n",
      "epoch 2397: train loss: 0.19356068993789216, test loss: 0.2840513025495936\n",
      "epoch 2398: train loss: 0.19353312303075718, test loss: 0.28403057632325357\n",
      "epoch 2399: train loss: 0.1935055733890408, test loss: 0.2840098670128159\n",
      "epoch 2400: train loss: 0.1934780409939905, test loss: 0.28398917459922773\n",
      "epoch 2401: train loss: 0.19345052582688244, test loss: 0.2839684990634624\n",
      "epoch 2402: train loss: 0.19342302786902155, test loss: 0.2839478403865243\n",
      "epoch 2403: train loss: 0.19339554710174148, test loss: 0.2839271985494465\n",
      "epoch 2404: train loss: 0.1933680835064046, test loss: 0.28390657353328846\n",
      "epoch 2405: train loss: 0.19334063706440163, test loss: 0.2838859653191409\n",
      "epoch 2406: train loss: 0.19331320775715202, test loss: 0.28386537388812216\n",
      "epoch 2407: train loss: 0.19328579556610362, test loss: 0.28384479922137984\n",
      "epoch 2408: train loss: 0.19325840047273268, test loss: 0.28382424130009143\n",
      "epoch 2409: train loss: 0.19323102245854387, test loss: 0.28380370010545874\n",
      "epoch 2410: train loss: 0.19320366150507007, test loss: 0.283783175618717\n",
      "epoch 2411: train loss: 0.19317631759387247, test loss: 0.28376266782112836\n",
      "epoch 2412: train loss: 0.1931489907065405, test loss: 0.28374217669398055\n",
      "epoch 2413: train loss: 0.19312168082469164, test loss: 0.28372170221859405\n",
      "epoch 2414: train loss: 0.19309438792997152, test loss: 0.28370124437631394\n",
      "epoch 2415: train loss: 0.19306711200405374, test loss: 0.2836808031485154\n",
      "epoch 2416: train loss: 0.19303985302864, test loss: 0.28366037851660153\n",
      "epoch 2417: train loss: 0.19301261098545974, test loss: 0.28363997046200495\n",
      "epoch 2418: train loss: 0.19298538585627042, test loss: 0.28361957896618345\n",
      "epoch 2419: train loss: 0.1929581776228573, test loss: 0.28359920401062483\n",
      "epoch 2420: train loss: 0.19293098626703326, test loss: 0.28357884557684443\n",
      "epoch 2421: train loss: 0.1929038117706391, test loss: 0.28355850364638463\n",
      "epoch 2422: train loss: 0.19287665411554314, test loss: 0.2835381782008168\n",
      "epoch 2423: train loss: 0.19284951328364136, test loss: 0.28351786922173994\n",
      "epoch 2424: train loss: 0.1928223892568572, test loss: 0.28349757669078196\n",
      "epoch 2425: train loss: 0.19279528201714172, test loss: 0.28347730058959475\n",
      "epoch 2426: train loss: 0.19276819154647334, test loss: 0.2834570408998622\n",
      "epoch 2427: train loss: 0.1927411178268579, test loss: 0.2834367976032932\n",
      "epoch 2428: train loss: 0.19271406084032855, test loss: 0.2834165706816243\n",
      "epoch 2429: train loss: 0.19268702056894577, test loss: 0.2833963601166204\n",
      "epoch 2430: train loss: 0.19265999699479722, test loss: 0.2833761658900737\n",
      "epoch 2431: train loss: 0.19263299009999776, test loss: 0.28335598798380274\n",
      "epoch 2432: train loss: 0.19260599986668944, test loss: 0.28333582637965715\n",
      "epoch 2433: train loss: 0.19257902627704124, test loss: 0.2833156810595075\n",
      "epoch 2434: train loss: 0.19255206931324928, test loss: 0.2832955520052552\n",
      "epoch 2435: train loss: 0.1925251289575367, test loss: 0.28327543919882997\n",
      "epoch 2436: train loss: 0.1924982051921534, test loss: 0.283255342622188\n",
      "epoch 2437: train loss: 0.19247129799937623, test loss: 0.2832352622573089\n",
      "epoch 2438: train loss: 0.1924444073615089, test loss: 0.28321519808620516\n",
      "epoch 2439: train loss: 0.1924175332608818, test loss: 0.28319515009091295\n",
      "epoch 2440: train loss: 0.1923906756798521, test loss: 0.2831751182534941\n",
      "epoch 2441: train loss: 0.19236383460080358, test loss: 0.2831551025560395\n",
      "epoch 2442: train loss: 0.19233701000614667, test loss: 0.283135102980667\n",
      "epoch 2443: train loss: 0.19231020187831838, test loss: 0.2831151195095187\n",
      "epoch 2444: train loss: 0.19228341019978212, test loss: 0.2830951521247681\n",
      "epoch 2445: train loss: 0.19225663495302797, test loss: 0.2830752008086092\n",
      "epoch 2446: train loss: 0.1922298761205721, test loss: 0.2830552655432674\n",
      "epoch 2447: train loss: 0.19220313368495742, test loss: 0.2830353463109927\n",
      "epoch 2448: train loss: 0.1921764076287528, test loss: 0.28301544309406285\n",
      "epoch 2449: train loss: 0.1921496979345536, test loss: 0.28299555587478026\n",
      "epoch 2450: train loss: 0.19212300458498127, test loss: 0.2829756846354731\n",
      "epoch 2451: train loss: 0.19209632756268344, test loss: 0.2829558293585015\n",
      "epoch 2452: train loss: 0.19206966685033386, test loss: 0.2829359900262447\n",
      "epoch 2453: train loss: 0.1920430224306324, test loss: 0.2829161666211107\n",
      "epoch 2454: train loss: 0.1920163942863048, test loss: 0.28289635912553507\n",
      "epoch 2455: train loss: 0.1919897824001028, test loss: 0.28287656752197987\n",
      "epoch 2456: train loss: 0.1919631867548042, test loss: 0.28285679179293227\n",
      "epoch 2457: train loss: 0.19193660733321244, test loss: 0.28283703192090387\n",
      "epoch 2458: train loss: 0.19191004411815688, test loss: 0.2828172878884334\n",
      "epoch 2459: train loss: 0.19188349709249258, test loss: 0.2827975596780863\n",
      "epoch 2460: train loss: 0.19185696623910053, test loss: 0.2827778472724553\n",
      "epoch 2461: train loss: 0.19183045154088701, test loss: 0.28275815065415555\n",
      "epoch 2462: train loss: 0.19180395298078431, test loss: 0.2827384698058298\n",
      "epoch 2463: train loss: 0.1917774705417499, test loss: 0.2827188047101458\n",
      "epoch 2464: train loss: 0.19175100420676705, test loss: 0.28269915534979817\n",
      "epoch 2465: train loss: 0.19172455395884438, test loss: 0.28267952170750626\n",
      "epoch 2466: train loss: 0.191698119781016, test loss: 0.2826599037660166\n",
      "epoch 2467: train loss: 0.1916717016563413, test loss: 0.2826403015080968\n",
      "epoch 2468: train loss: 0.19164529956790508, test loss: 0.28262071491654744\n",
      "epoch 2469: train loss: 0.19161891349881727, test loss: 0.2826011439741867\n",
      "epoch 2470: train loss: 0.19159254343221327, test loss: 0.2825815886638647\n",
      "epoch 2471: train loss: 0.19156618935125352, test loss: 0.2825620489684497\n",
      "epoch 2472: train loss: 0.19153985123912343, test loss: 0.2825425248708454\n",
      "epoch 2473: train loss: 0.1915135290790338, test loss: 0.28252301635397037\n",
      "epoch 2474: train loss: 0.19148722285422032, test loss: 0.2825035234007734\n",
      "epoch 2475: train loss: 0.19146093254794358, test loss: 0.28248404599423077\n",
      "epoch 2476: train loss: 0.19143465814348928, test loss: 0.2824645841173394\n",
      "epoch 2477: train loss: 0.19140839962416792, test loss: 0.2824451377531214\n",
      "epoch 2478: train loss: 0.19138215697331484, test loss: 0.2824257068846287\n",
      "epoch 2479: train loss: 0.19135593017429028, test loss: 0.28240629149493257\n",
      "epoch 2480: train loss: 0.19132971921047912, test loss: 0.28238689156713137\n",
      "epoch 2481: train loss: 0.19130352406529091, test loss: 0.28236750708435104\n",
      "epoch 2482: train loss: 0.19127734472216004, test loss: 0.2823481380297383\n",
      "epoch 2483: train loss: 0.1912511811645454, test loss: 0.28232878438646636\n",
      "epoch 2484: train loss: 0.19122503337593044, test loss: 0.282309446137734\n",
      "epoch 2485: train loss: 0.19119890133982312, test loss: 0.2822901232667624\n",
      "epoch 2486: train loss: 0.191172785039756, test loss: 0.2822708157567999\n",
      "epoch 2487: train loss: 0.19114668445928587, test loss: 0.28225152359111816\n",
      "epoch 2488: train loss: 0.19112059958199407, test loss: 0.282232246753013\n",
      "epoch 2489: train loss: 0.19109453039148616, test loss: 0.2822129852258074\n",
      "epoch 2490: train loss: 0.19106847687139208, test loss: 0.2821937389928422\n",
      "epoch 2491: train loss: 0.191042439005366, test loss: 0.2821745080374931\n",
      "epoch 2492: train loss: 0.1910164167770862, test loss: 0.2821552923431498\n",
      "epoch 2493: train loss: 0.1909904101702552, test loss: 0.28213609189323285\n",
      "epoch 2494: train loss: 0.19096441916859974, test loss: 0.2821169066711852\n",
      "epoch 2495: train loss: 0.1909384437558703, test loss: 0.28209773666047255\n",
      "epoch 2496: train loss: 0.1909124839158417, test loss: 0.28207858184458684\n",
      "epoch 2497: train loss: 0.19088653963231256, test loss: 0.28205944220704376\n",
      "epoch 2498: train loss: 0.19086061088910547, test loss: 0.282040317731383\n",
      "epoch 2499: train loss: 0.19083469767006697, test loss: 0.2820212084011671\n",
      "epoch 2500: train loss: 0.19080879995906724, test loss: 0.28200211419998483\n",
      "epoch 2501: train loss: 0.1907829177400005, test loss: 0.28198303511144573\n",
      "epoch 2502: train loss: 0.19075705099678456, test loss: 0.28196397111918786\n",
      "epoch 2503: train loss: 0.19073119971336105, test loss: 0.281944922206869\n",
      "epoch 2504: train loss: 0.19070536387369505, test loss: 0.28192588835817134\n",
      "epoch 2505: train loss: 0.1906795434617755, test loss: 0.28190686955680405\n",
      "epoch 2506: train loss: 0.19065373846161485, test loss: 0.28188786578649583\n",
      "epoch 2507: train loss: 0.1906279488572489, test loss: 0.2818688770310024\n",
      "epoch 2508: train loss: 0.19060217463273715, test loss: 0.2818499032741015\n",
      "epoch 2509: train loss: 0.19057641577216247, test loss: 0.2818309444995948\n",
      "epoch 2510: train loss: 0.1905506722596311, test loss: 0.2818120006913068\n",
      "epoch 2511: train loss: 0.1905249440792726, test loss: 0.2817930718330876\n",
      "epoch 2512: train loss: 0.19049923121523993, test loss: 0.2817741579088073\n",
      "epoch 2513: train loss: 0.19047353365170924, test loss: 0.28175525890236297\n",
      "epoch 2514: train loss: 0.19044785137287995, test loss: 0.2817363747976732\n",
      "epoch 2515: train loss: 0.19042218436297462, test loss: 0.28171750557868186\n",
      "epoch 2516: train loss: 0.19039653260623893, test loss: 0.281698651229352\n",
      "epoch 2517: train loss: 0.19037089608694177, test loss: 0.2816798117336745\n",
      "epoch 2518: train loss: 0.1903452747893749, test loss: 0.28166098707566034\n",
      "epoch 2519: train loss: 0.19031966869785322, test loss: 0.28164217723934565\n",
      "epoch 2520: train loss: 0.1902940777967145, test loss: 0.2816233822087905\n",
      "epoch 2521: train loss: 0.19026850207031956, test loss: 0.28160460196807224\n",
      "epoch 2522: train loss: 0.19024294150305196, test loss: 0.2815858365012994\n",
      "epoch 2523: train loss: 0.19021739607931815, test loss: 0.28156708579259626\n",
      "epoch 2524: train loss: 0.19019186578354735, test loss: 0.2815483498261161\n",
      "epoch 2525: train loss: 0.19016635060019163, test loss: 0.2815296285860326\n",
      "epoch 2526: train loss: 0.19014085051372556, test loss: 0.2815109220565396\n",
      "epoch 2527: train loss: 0.1901153655086466, test loss: 0.2814922302218572\n",
      "epoch 2528: train loss: 0.19008989556947475, test loss: 0.28147355306622956\n",
      "epoch 2529: train loss: 0.19006444068075254, test loss: 0.28145489057391876\n",
      "epoch 2530: train loss: 0.19003900082704506, test loss: 0.2814362427292118\n",
      "epoch 2531: train loss: 0.1900135759929399, test loss: 0.28141760951642103\n",
      "epoch 2532: train loss: 0.18998816616304715, test loss: 0.2813989909198788\n",
      "epoch 2533: train loss: 0.1899627713219993, test loss: 0.28138038692393946\n",
      "epoch 2534: train loss: 0.18993739145445113, test loss: 0.28136179751298046\n",
      "epoch 2535: train loss: 0.18991202654507988, test loss: 0.281343222671402\n",
      "epoch 2536: train loss: 0.189886676578585, test loss: 0.28132466238363013\n",
      "epoch 2537: train loss: 0.18986134153968812, test loss: 0.2813061166341064\n",
      "epoch 2538: train loss: 0.1898360214131333, test loss: 0.2812875854072987\n",
      "epoch 2539: train loss: 0.18981071618368653, test loss: 0.28126906868769885\n",
      "epoch 2540: train loss: 0.18978542583613603, test loss: 0.2812505664598179\n",
      "epoch 2541: train loss: 0.18976015035529215, test loss: 0.2812320787081901\n",
      "epoch 2542: train loss: 0.18973488972598715, test loss: 0.28121360541737206\n",
      "epoch 2543: train loss: 0.18970964393307546, test loss: 0.2811951465719431\n",
      "epoch 2544: train loss: 0.1896844129614333, test loss: 0.2811767021565037\n",
      "epoch 2545: train loss: 0.18965919679595888, test loss: 0.2811582721556785\n",
      "epoch 2546: train loss: 0.18963399542157236, test loss: 0.28113985655410906\n",
      "epoch 2547: train loss: 0.18960880882321568, test loss: 0.2811214553364666\n",
      "epoch 2548: train loss: 0.18958363698585257, test loss: 0.2811030684874354\n",
      "epoch 2549: train loss: 0.18955847989446853, test loss: 0.2810846959917308\n",
      "epoch 2550: train loss: 0.18953333753407076, test loss: 0.28106633783408386\n",
      "epoch 2551: train loss: 0.1895082098896882, test loss: 0.2810479939992499\n",
      "epoch 2552: train loss: 0.1894830969463714, test loss: 0.28102966447200495\n",
      "epoch 2553: train loss: 0.18945799868919244, test loss: 0.28101134923714627\n",
      "epoch 2554: train loss: 0.18943291510324506, test loss: 0.2809930482794944\n",
      "epoch 2555: train loss: 0.1894078461736445, test loss: 0.2809747615838926\n",
      "epoch 2556: train loss: 0.18938279188552745, test loss: 0.28095648913520144\n",
      "epoch 2557: train loss: 0.18935775222405204, test loss: 0.2809382309183081\n",
      "epoch 2558: train loss: 0.1893327271743978, test loss: 0.280919986918118\n",
      "epoch 2559: train loss: 0.18930771672176575, test loss: 0.28090175711955867\n",
      "epoch 2560: train loss: 0.18928272085137798, test loss: 0.28088354150758144\n",
      "epoch 2561: train loss: 0.1892577395484781, test loss: 0.28086534006715547\n",
      "epoch 2562: train loss: 0.1892327727983308, test loss: 0.2808471527832747\n",
      "epoch 2563: train loss: 0.1892078205862221, test loss: 0.28082897964095094\n",
      "epoch 2564: train loss: 0.18918288289745913, test loss: 0.2808108206252202\n",
      "epoch 2565: train loss: 0.18915795971737012, test loss: 0.2807926757211369\n",
      "epoch 2566: train loss: 0.18913305103130448, test loss: 0.28077454491378223\n",
      "epoch 2567: train loss: 0.18910815682463256, test loss: 0.2807564281882534\n",
      "epoch 2568: train loss: 0.18908327708274583, test loss: 0.28073832552966943\n",
      "epoch 2569: train loss: 0.18905841179105656, test loss: 0.2807202369231724\n",
      "epoch 2570: train loss: 0.18903356093499823, test loss: 0.28070216235392353\n",
      "epoch 2571: train loss: 0.18900872450002504, test loss: 0.28068410180710807\n",
      "epoch 2572: train loss: 0.18898390247161195, test loss: 0.2806660552679277\n",
      "epoch 2573: train loss: 0.188959094835255, test loss: 0.28064802272160916\n",
      "epoch 2574: train loss: 0.18893430157647073, test loss: 0.28063000415339945\n",
      "epoch 2575: train loss: 0.18890952268079675, test loss: 0.28061199954856225\n",
      "epoch 2576: train loss: 0.1888847581337911, test loss: 0.2805940088923899\n",
      "epoch 2577: train loss: 0.18886000792103252, test loss: 0.2805760321701888\n",
      "epoch 2578: train loss: 0.18883527202812062, test loss: 0.28055806936728833\n",
      "epoch 2579: train loss: 0.1888105504406753, test loss: 0.28054012046903964\n",
      "epoch 2580: train loss: 0.18878584314433716, test loss: 0.28052218546081364\n",
      "epoch 2581: train loss: 0.18876115012476735, test loss: 0.28050426432800246\n",
      "epoch 2582: train loss: 0.18873647136764737, test loss: 0.2804863570560181\n",
      "epoch 2583: train loss: 0.18871180685867942, test loss: 0.2804684636302929\n",
      "epoch 2584: train loss: 0.1886871565835858, test loss: 0.28045058403628026\n",
      "epoch 2585: train loss: 0.18866252052810936, test loss: 0.28043271825945654\n",
      "epoch 2586: train loss: 0.18863789867801325, test loss: 0.2804148662853141\n",
      "epoch 2587: train loss: 0.1886132910190809, test loss: 0.2803970280993681\n",
      "epoch 2588: train loss: 0.188588697537116, test loss: 0.28037920368715585\n",
      "epoch 2589: train loss: 0.18856411821794247, test loss: 0.28036139303422997\n",
      "epoch 2590: train loss: 0.18853955304740438, test loss: 0.28034359612616927\n",
      "epoch 2591: train loss: 0.18851500201136606, test loss: 0.2803258129485704\n",
      "epoch 2592: train loss: 0.18849046509571182, test loss: 0.28030804348704846\n",
      "epoch 2593: train loss: 0.18846594228634608, test loss: 0.28029028772724185\n",
      "epoch 2594: train loss: 0.18844143356919332, test loss: 0.28027254565480636\n",
      "epoch 2595: train loss: 0.1884169389301981, test loss: 0.2802548172554218\n",
      "epoch 2596: train loss: 0.1883924583553247, test loss: 0.28023710251478184\n",
      "epoch 2597: train loss: 0.18836799183055766, test loss: 0.28021940141860646\n",
      "epoch 2598: train loss: 0.18834353934190115, test loss: 0.2802017139526335\n",
      "epoch 2599: train loss: 0.18831910087537934, test loss: 0.2801840401026204\n",
      "epoch 2600: train loss: 0.18829467641703623, test loss: 0.28016637985434545\n",
      "epoch 2601: train loss: 0.18827026595293542, test loss: 0.2801487331936035\n",
      "epoch 2602: train loss: 0.1882458694691605, test loss: 0.2801311001062131\n",
      "epoch 2603: train loss: 0.18822148695181462, test loss: 0.28011348057801333\n",
      "epoch 2604: train loss: 0.18819711838702066, test loss: 0.28009587459486074\n",
      "epoch 2605: train loss: 0.18817276376092118, test loss: 0.28007828214263103\n",
      "epoch 2606: train loss: 0.18814842305967822, test loss: 0.28006070320722265\n",
      "epoch 2607: train loss: 0.18812409626947357, test loss: 0.28004313777455175\n",
      "epoch 2608: train loss: 0.1880997833765084, test loss: 0.2800255858305537\n",
      "epoch 2609: train loss: 0.18807548436700347, test loss: 0.28000804736118357\n",
      "epoch 2610: train loss: 0.18805119922719904, test loss: 0.2799905223524188\n",
      "epoch 2611: train loss: 0.18802692794335463, test loss: 0.2799730107902543\n",
      "epoch 2612: train loss: 0.1880026705017494, test loss: 0.27995551266070257\n",
      "epoch 2613: train loss: 0.18797842688868166, test loss: 0.27993802794979905\n",
      "epoch 2614: train loss: 0.18795419709046915, test loss: 0.27992055664359966\n",
      "epoch 2615: train loss: 0.18792998109344894, test loss: 0.2799030987281739\n",
      "epoch 2616: train loss: 0.18790577888397714, test loss: 0.279885654189615\n",
      "epoch 2617: train loss: 0.18788159044842934, test loss: 0.27986822301403586\n",
      "epoch 2618: train loss: 0.18785741577320025, test loss: 0.2798508051875686\n",
      "epoch 2619: train loss: 0.1878332548447036, test loss: 0.2798334006963614\n",
      "epoch 2620: train loss: 0.18780910764937248, test loss: 0.27981600952658575\n",
      "epoch 2621: train loss: 0.18778497417365878, test loss: 0.2797986316644303\n",
      "epoch 2622: train loss: 0.18776085440403364, test loss: 0.2797812670961017\n",
      "epoch 2623: train loss: 0.18773674832698717, test loss: 0.27976391580782917\n",
      "epoch 2624: train loss: 0.18771265592902836, test loss: 0.27974657778585915\n",
      "epoch 2625: train loss: 0.18768857719668539, test loss: 0.2797292530164585\n",
      "epoch 2626: train loss: 0.1876645121165051, test loss: 0.27971194148590867\n",
      "epoch 2627: train loss: 0.18764046067505333, test loss: 0.2796946431805159\n",
      "epoch 2628: train loss: 0.1876164228589147, test loss: 0.2796773580866043\n",
      "epoch 2629: train loss: 0.18759239865469268, test loss: 0.2796600861905112\n",
      "epoch 2630: train loss: 0.18756838804900966, test loss: 0.27964282747860214\n",
      "epoch 2631: train loss: 0.1875443910285065, test loss: 0.27962558193725295\n",
      "epoch 2632: train loss: 0.1875204075798429, test loss: 0.27960834955286396\n",
      "epoch 2633: train loss: 0.18749643768969734, test loss: 0.2795911303118525\n",
      "epoch 2634: train loss: 0.18747248134476677, test loss: 0.27957392420065447\n",
      "epoch 2635: train loss: 0.18744853853176685, test loss: 0.27955673120572333\n",
      "epoch 2636: train loss: 0.18742460923743187, test loss: 0.2795395513135345\n",
      "epoch 2637: train loss: 0.18740069344851448, test loss: 0.2795223845105814\n",
      "epoch 2638: train loss: 0.18737679115178602, test loss: 0.27950523078337247\n",
      "epoch 2639: train loss: 0.18735290233403618, test loss: 0.2794880901184394\n",
      "epoch 2640: train loss: 0.18732902698207318, test loss: 0.279470962502327\n",
      "epoch 2641: train loss: 0.18730516508272366, test loss: 0.27945384792160666\n",
      "epoch 2642: train loss: 0.1872813166228326, test loss: 0.2794367463628613\n",
      "epoch 2643: train loss: 0.18725748158926325, test loss: 0.27941965781269423\n",
      "epoch 2644: train loss: 0.18723365996889732, test loss: 0.2794025822577288\n",
      "epoch 2645: train loss: 0.18720985174863475, test loss: 0.27938551968460545\n",
      "epoch 2646: train loss: 0.18718605691539367, test loss: 0.279368470079984\n",
      "epoch 2647: train loss: 0.18716227545611044, test loss: 0.27935143343054053\n",
      "epoch 2648: train loss: 0.18713850735773974, test loss: 0.27933440972297324\n",
      "epoch 2649: train loss: 0.1871147526072542, test loss: 0.2793173989439929\n",
      "epoch 2650: train loss: 0.18709101119164465, test loss: 0.27930040108033544\n",
      "epoch 2651: train loss: 0.18706728309792012, test loss: 0.27928341611875085\n",
      "epoch 2652: train loss: 0.18704356831310748, test loss: 0.27926644404600565\n",
      "epoch 2653: train loss: 0.18701986682425176, test loss: 0.27924948484888984\n",
      "epoch 2654: train loss: 0.18699617861841594, test loss: 0.27923253851420793\n",
      "epoch 2655: train loss: 0.1869725036826811, test loss: 0.27921560502878146\n",
      "epoch 2656: train loss: 0.18694884200414588, test loss: 0.2791986843794543\n",
      "epoch 2657: train loss: 0.18692519356992726, test loss: 0.27918177655308596\n",
      "epoch 2658: train loss: 0.18690155836715977, test loss: 0.27916488153655256\n",
      "epoch 2659: train loss: 0.18687793638299588, test loss: 0.2791479993167485\n",
      "epoch 2660: train loss: 0.18685432760460594, test loss: 0.2791311298805914\n",
      "epoch 2661: train loss: 0.18683073201917783, test loss: 0.27911427321500853\n",
      "epoch 2662: train loss: 0.18680714961391742, test loss: 0.2790974293069516\n",
      "epoch 2663: train loss: 0.18678358037604817, test loss: 0.27908059814338665\n",
      "epoch 2664: train loss: 0.1867600242928112, test loss: 0.2790637797113005\n",
      "epoch 2665: train loss: 0.1867364813514653, test loss: 0.279046973997694\n",
      "epoch 2666: train loss: 0.18671295153928696, test loss: 0.27903018098958776\n",
      "epoch 2667: train loss: 0.18668943484356995, test loss: 0.2790134006740223\n",
      "epoch 2668: train loss: 0.18666593125162595, test loss: 0.27899663303805\n",
      "epoch 2669: train loss: 0.186642440750784, test loss: 0.2789798780687478\n",
      "epoch 2670: train loss: 0.18661896332839056, test loss: 0.27896313575320586\n",
      "epoch 2671: train loss: 0.18659549897180974, test loss: 0.2789464060785338\n",
      "epoch 2672: train loss: 0.18657204766842278, test loss: 0.2789296890318573\n",
      "epoch 2673: train loss: 0.18654860940562856, test loss: 0.2789129846003209\n",
      "epoch 2674: train loss: 0.1865251841708433, test loss: 0.27889629277108574\n",
      "epoch 2675: train loss: 0.18650177195150036, test loss: 0.27887961353133145\n",
      "epoch 2676: train loss: 0.18647837273505072, test loss: 0.278862946868256\n",
      "epoch 2677: train loss: 0.18645498650896236, test loss: 0.2788462927690706\n",
      "epoch 2678: train loss: 0.18643161326072055, test loss: 0.27882965122100833\n",
      "epoch 2679: train loss: 0.1864082529778279, test loss: 0.27881302221131893\n",
      "epoch 2680: train loss: 0.18638490564780405, test loss: 0.27879640572726466\n",
      "epoch 2681: train loss: 0.18636157125818592, test loss: 0.2787798017561344\n",
      "epoch 2682: train loss: 0.18633824979652744, test loss: 0.27876321028522466\n",
      "epoch 2683: train loss: 0.18631494125039968, test loss: 0.2787466313018556\n",
      "epoch 2684: train loss: 0.18629164560739078, test loss: 0.2787300647933623\n",
      "epoch 2685: train loss: 0.1862683628551059, test loss: 0.27871351074709516\n",
      "epoch 2686: train loss: 0.1862450929811671, test loss: 0.27869696915042613\n",
      "epoch 2687: train loss: 0.18622183597321362, test loss: 0.2786804399907414\n",
      "epoch 2688: train loss: 0.18619859181890155, test loss: 0.2786639232554441\n",
      "epoch 2689: train loss: 0.18617536050590375, test loss: 0.27864741893195544\n",
      "epoch 2690: train loss: 0.18615214202191016, test loss: 0.27863092700771314\n",
      "epoch 2691: train loss: 0.18612893635462746, test loss: 0.27861444747017144\n",
      "epoch 2692: train loss: 0.18610574349177916, test loss: 0.2785979803068043\n",
      "epoch 2693: train loss: 0.18608256342110557, test loss: 0.27858152550509835\n",
      "epoch 2694: train loss: 0.18605939613036385, test loss: 0.2785650830525611\n",
      "epoch 2695: train loss: 0.1860362416073277, test loss: 0.2785486529367143\n",
      "epoch 2696: train loss: 0.18601309983978775, test loss: 0.27853223514509756\n",
      "epoch 2697: train loss: 0.1859899708155512, test loss: 0.27851582966526695\n",
      "epoch 2698: train loss: 0.1859668545224418, test loss: 0.27849943648479747\n",
      "epoch 2699: train loss: 0.18594375094830004, test loss: 0.2784830555912766\n",
      "epoch 2700: train loss: 0.18592066008098299, test loss: 0.27846668697231197\n",
      "epoch 2701: train loss: 0.1858975819083642, test loss: 0.2784503306155267\n",
      "epoch 2702: train loss: 0.18587451641833388, test loss: 0.27843398650856216\n",
      "epoch 2703: train loss: 0.18585146359879864, test loss: 0.278417654639075\n",
      "epoch 2704: train loss: 0.18582842343768152, test loss: 0.27840133499473646\n",
      "epoch 2705: train loss: 0.1858053959229221, test loss: 0.27838502756323913\n",
      "epoch 2706: train loss: 0.18578238104247644, test loss: 0.2783687323322889\n",
      "epoch 2707: train loss: 0.18575937878431673, test loss: 0.27835244928960745\n",
      "epoch 2708: train loss: 0.18573638913643173, test loss: 0.2783361784229366\n",
      "epoch 2709: train loss: 0.18571341208682654, test loss: 0.2783199197200309\n",
      "epoch 2710: train loss: 0.1856904476235224, test loss: 0.27830367316866395\n",
      "epoch 2711: train loss: 0.185667495734557, test loss: 0.27828743875662393\n",
      "epoch 2712: train loss: 0.1856445564079841, test loss: 0.27827121647171765\n",
      "epoch 2713: train loss: 0.18562162963187379, test loss: 0.278255006301764\n",
      "epoch 2714: train loss: 0.18559871539431236, test loss: 0.2782388082346056\n",
      "epoch 2715: train loss: 0.18557581368340215, test loss: 0.27822262225809447\n",
      "epoch 2716: train loss: 0.18555292448726174, test loss: 0.27820644836010044\n",
      "epoch 2717: train loss: 0.18553004779402577, test loss: 0.278190286528512\n",
      "epoch 2718: train loss: 0.18550718359184493, test loss: 0.27817413675123115\n",
      "epoch 2719: train loss: 0.18548433186888602, test loss: 0.2781579990161787\n",
      "epoch 2720: train loss: 0.18546149261333178, test loss: 0.2781418733112912\n",
      "epoch 2721: train loss: 0.18543866581338098, test loss: 0.27812575962451924\n",
      "epoch 2722: train loss: 0.18541585145724837, test loss: 0.2781096579438306\n",
      "epoch 2723: train loss: 0.18539304953316463, test loss: 0.2780935682572096\n",
      "epoch 2724: train loss: 0.1853702600293763, test loss: 0.2780774905526564\n",
      "epoch 2725: train loss: 0.18534748293414588, test loss: 0.278061424818187\n",
      "epoch 2726: train loss: 0.18532471823575164, test loss: 0.2780453710418344\n",
      "epoch 2727: train loss: 0.1853019659224877, test loss: 0.27802932921164647\n",
      "epoch 2728: train loss: 0.185279225982664, test loss: 0.2780132993156853\n",
      "epoch 2729: train loss: 0.1852564984046063, test loss: 0.2779972813420346\n",
      "epoch 2730: train loss: 0.185233783176656, test loss: 0.27798127527878697\n",
      "epoch 2731: train loss: 0.18521108028717023, test loss: 0.27796528111405766\n",
      "epoch 2732: train loss: 0.18518838972452187, test loss: 0.27794929883597136\n",
      "epoch 2733: train loss: 0.1851657114770994, test loss: 0.2779333284326733\n",
      "epoch 2734: train loss: 0.185143045533307, test loss: 0.2779173698923236\n",
      "epoch 2735: train loss: 0.1851203918815644, test loss: 0.2779014232030943\n",
      "epoch 2736: train loss: 0.18509775051030694, test loss: 0.27788548835317795\n",
      "epoch 2737: train loss: 0.18507512140798554, test loss: 0.2778695653307826\n",
      "epoch 2738: train loss: 0.18505250456306652, test loss: 0.27785365412412866\n",
      "epoch 2739: train loss: 0.18502989996403188, test loss: 0.27783775472145494\n",
      "epoch 2740: train loss: 0.18500730759937895, test loss: 0.27782186711101464\n",
      "epoch 2741: train loss: 0.18498472745762054, test loss: 0.2778059912810772\n",
      "epoch 2742: train loss: 0.18496215952728495, test loss: 0.277790127219927\n",
      "epoch 2743: train loss: 0.18493960379691587, test loss: 0.27777427491586476\n",
      "epoch 2744: train loss: 0.1849170602550722, test loss: 0.2777584343572064\n",
      "epoch 2745: train loss: 0.18489452889032834, test loss: 0.2777426055322827\n",
      "epoch 2746: train loss: 0.18487200969127393, test loss: 0.2777267884294413\n",
      "epoch 2747: train loss: 0.18484950264651395, test loss: 0.2777109830370432\n",
      "epoch 2748: train loss: 0.18482700774466856, test loss: 0.2776951893434667\n",
      "epoch 2749: train loss: 0.1848045249743733, test loss: 0.2776794073371066\n",
      "epoch 2750: train loss: 0.18478205432427874, test loss: 0.2776636370063708\n",
      "epoch 2751: train loss: 0.18475959578305073, test loss: 0.2776478783396798\n",
      "epoch 2752: train loss: 0.18473714933937033, test loss: 0.27763213132547715\n",
      "epoch 2753: train loss: 0.1847147149819336, test loss: 0.2776163959522147\n",
      "epoch 2754: train loss: 0.18469229269945178, test loss: 0.27760067220836426\n",
      "epoch 2755: train loss: 0.18466988248065114, test loss: 0.27758496008240846\n",
      "epoch 2756: train loss: 0.18464748431427314, test loss: 0.27756925956284867\n",
      "epoch 2757: train loss: 0.18462509818907408, test loss: 0.27755357063820063\n",
      "epoch 2758: train loss: 0.18460272409382533, test loss: 0.27753789329699374\n",
      "epoch 2759: train loss: 0.1845803620173133, test loss: 0.2775222275277748\n",
      "epoch 2760: train loss: 0.18455801194833932, test loss: 0.2775065733191039\n",
      "epoch 2761: train loss: 0.1845356738757196, test loss: 0.2774909306595588\n",
      "epoch 2762: train loss: 0.18451334778828524, test loss: 0.2774752995377278\n",
      "epoch 2763: train loss: 0.18449103367488232, test loss: 0.27745967994221754\n",
      "epoch 2764: train loss: 0.18446873152437163, test loss: 0.27744407186165\n",
      "epoch 2765: train loss: 0.18444644132562885, test loss: 0.2774284752846606\n",
      "epoch 2766: train loss: 0.18442416306754453, test loss: 0.27741289019990256\n",
      "epoch 2767: train loss: 0.18440189673902382, test loss: 0.2773973165960374\n",
      "epoch 2768: train loss: 0.1843796423289867, test loss: 0.2773817544617495\n",
      "epoch 2769: train loss: 0.184357399826368, test loss: 0.2773662037857321\n",
      "epoch 2770: train loss: 0.18433516922011706, test loss: 0.2773506645566984\n",
      "epoch 2771: train loss: 0.18431295049919796, test loss: 0.27733513676337174\n",
      "epoch 2772: train loss: 0.18429074365258938, test loss: 0.27731962039449326\n",
      "epoch 2773: train loss: 0.18426854866928472, test loss: 0.2773041154388176\n",
      "epoch 2774: train loss: 0.18424636553829188, test loss: 0.27728862188511433\n",
      "epoch 2775: train loss: 0.18422419424863343, test loss: 0.27727313972216916\n",
      "epoch 2776: train loss: 0.1842020347893464, test loss: 0.27725766893877973\n",
      "epoch 2777: train loss: 0.18417988714948236, test loss: 0.2772422095237622\n",
      "epoch 2778: train loss: 0.1841577513181074, test loss: 0.27722676146594244\n",
      "epoch 2779: train loss: 0.184135627284302, test loss: 0.27721132475416627\n",
      "epoch 2780: train loss: 0.18411351503716133, test loss: 0.2771958993772888\n",
      "epoch 2781: train loss: 0.18409141456579461, test loss: 0.27718048532418393\n",
      "epoch 2782: train loss: 0.18406932585932573, test loss: 0.27716508258373856\n",
      "epoch 2783: train loss: 0.18404724890689295, test loss: 0.27714969114485544\n",
      "epoch 2784: train loss: 0.1840251836976487, test loss: 0.2771343109964491\n",
      "epoch 2785: train loss: 0.18400313022075987, test loss: 0.2771189421274498\n",
      "epoch 2786: train loss: 0.1839810884654077, test loss: 0.2771035845268037\n",
      "epoch 2787: train loss: 0.18395905842078755, test loss: 0.2770882381834706\n",
      "epoch 2788: train loss: 0.18393704007610914, test loss: 0.27707290308642213\n",
      "epoch 2789: train loss: 0.18391503342059637, test loss: 0.27705757922464924\n",
      "epoch 2790: train loss: 0.18389303844348742, test loss: 0.2770422665871526\n",
      "epoch 2791: train loss: 0.18387105513403454, test loss: 0.2770269651629514\n",
      "epoch 2792: train loss: 0.1838490834815042, test loss: 0.27701167494107326\n",
      "epoch 2793: train loss: 0.18382712347517705, test loss: 0.27699639591056846\n",
      "epoch 2794: train loss: 0.1838051751043477, test loss: 0.27698112806049263\n",
      "epoch 2795: train loss: 0.183783238358325, test loss: 0.27696587137992296\n",
      "epoch 2796: train loss: 0.18376131322643172, test loss: 0.2769506258579474\n",
      "epoch 2797: train loss: 0.18373939969800482, test loss: 0.276935391483667\n",
      "epoch 2798: train loss: 0.18371749776239518, test loss: 0.2769201682461989\n",
      "epoch 2799: train loss: 0.18369560740896762, test loss: 0.2769049561346754\n",
      "epoch 2800: train loss: 0.18367372862710105, test loss: 0.2768897551382406\n",
      "epoch 2801: train loss: 0.18365186140618825, test loss: 0.276874565246055\n",
      "epoch 2802: train loss: 0.1836300057356359, test loss: 0.27685938644728836\n",
      "epoch 2803: train loss: 0.18360816160486462, test loss: 0.2768442187311317\n",
      "epoch 2804: train loss: 0.18358632900330882, test loss: 0.27682906208678615\n",
      "epoch 2805: train loss: 0.18356450792041695, test loss: 0.2768139165034647\n",
      "epoch 2806: train loss: 0.183542698345651, test loss: 0.276798781970398\n",
      "epoch 2807: train loss: 0.18352090026848702, test loss: 0.27678365847683045\n",
      "epoch 2808: train loss: 0.1834991136784147, test loss: 0.27676854601201717\n",
      "epoch 2809: train loss: 0.18347733856493753, test loss: 0.27675344456523154\n",
      "epoch 2810: train loss: 0.18345557491757272, test loss: 0.27673835412575704\n",
      "epoch 2811: train loss: 0.18343382272585118, test loss: 0.27672327468289454\n",
      "epoch 2812: train loss: 0.18341208197931747, test loss: 0.27670820622595493\n",
      "epoch 2813: train loss: 0.18339035266752987, test loss: 0.2766931487442667\n",
      "epoch 2814: train loss: 0.18336863478006035, test loss: 0.2766781022271686\n",
      "epoch 2815: train loss: 0.1833469283064943, test loss: 0.2766630666640161\n",
      "epoch 2816: train loss: 0.18332523323643102, test loss: 0.27664804204417714\n",
      "epoch 2817: train loss: 0.18330354955948303, test loss: 0.2766330283570318\n",
      "epoch 2818: train loss: 0.1832818772652766, test loss: 0.27661802559198007\n",
      "epoch 2819: train loss: 0.18326021634345152, test loss: 0.2766030337384267\n",
      "epoch 2820: train loss: 0.18323856678366104, test loss: 0.2765880527857958\n",
      "epoch 2821: train loss: 0.1832169285755719, test loss: 0.27657308272352565\n",
      "epoch 2822: train loss: 0.18319530170886422, test loss: 0.2765581235410637\n",
      "epoch 2823: train loss: 0.18317368617323168, test loss: 0.27654317522787575\n",
      "epoch 2824: train loss: 0.18315208195838142, test loss: 0.2765282377734384\n",
      "epoch 2825: train loss: 0.1831304890540337, test loss: 0.2765133111672433\n",
      "epoch 2826: train loss: 0.18310890744992248, test loss: 0.2764983953987928\n",
      "epoch 2827: train loss: 0.18308733713579473, test loss: 0.27648349045760834\n",
      "epoch 2828: train loss: 0.18306577810141106, test loss: 0.2764685963332179\n",
      "epoch 2829: train loss: 0.18304423033654516, test loss: 0.2764537130151697\n",
      "epoch 2830: train loss: 0.18302269383098405, test loss: 0.2764388404930186\n",
      "epoch 2831: train loss: 0.1830011685745281, test loss: 0.2764239787563392\n",
      "epoch 2832: train loss: 0.1829796545569908, test loss: 0.2764091277947154\n",
      "epoch 2833: train loss: 0.18295815176819893, test loss: 0.27639428759774787\n",
      "epoch 2834: train loss: 0.18293666019799232, test loss: 0.2763794581550449\n",
      "epoch 2835: train loss: 0.18291517983622418, test loss: 0.27636463945623563\n",
      "epoch 2836: train loss: 0.18289371067276072, test loss: 0.27634983149095727\n",
      "epoch 2837: train loss: 0.18287225269748128, test loss: 0.2763350342488592\n",
      "epoch 2838: train loss: 0.18285080590027838, test loss: 0.2763202477196114\n",
      "epoch 2839: train loss: 0.18282937027105745, test loss: 0.27630547189288884\n",
      "epoch 2840: train loss: 0.18280794579973722, test loss: 0.2762907067583837\n",
      "epoch 2841: train loss: 0.18278653247624932, test loss: 0.2762759523058012\n",
      "epoch 2842: train loss: 0.1827651302905383, test loss: 0.2762612085248613\n",
      "epoch 2843: train loss: 0.18274373923256193, test loss: 0.27624647540529224\n",
      "epoch 2844: train loss: 0.18272235929229075, test loss: 0.27623175293684027\n",
      "epoch 2845: train loss: 0.18270099045970842, test loss: 0.2762170411092621\n",
      "epoch 2846: train loss: 0.18267963272481133, test loss: 0.27620233991232956\n",
      "epoch 2847: train loss: 0.18265828607760887, test loss: 0.27618764933582646\n",
      "epoch 2848: train loss: 0.18263695050812345, test loss: 0.2761729693695467\n",
      "epoch 2849: train loss: 0.18261562600639014, test loss: 0.2761583000033026\n",
      "epoch 2850: train loss: 0.1825943125624569, test loss: 0.2761436412269166\n",
      "epoch 2851: train loss: 0.1825730101663846, test loss: 0.2761289930302231\n",
      "epoch 2852: train loss: 0.18255171880824683, test loss: 0.27611435540307405\n",
      "epoch 2853: train loss: 0.18253043847813002, test loss: 0.27609972833532864\n",
      "epoch 2854: train loss: 0.18250916916613327, test loss: 0.27608511181686185\n",
      "epoch 2855: train loss: 0.18248791086236849, test loss: 0.2760705058375632\n",
      "epoch 2856: train loss: 0.1824666635569602, test loss: 0.27605591038733085\n",
      "epoch 2857: train loss: 0.1824454272400458, test loss: 0.27604132545607707\n",
      "epoch 2858: train loss: 0.18242420190177522, test loss: 0.2760267510337312\n",
      "epoch 2859: train loss: 0.18240298753231102, test loss: 0.27601218711023134\n",
      "epoch 2860: train loss: 0.18238178412182854, test loss: 0.275997633675529\n",
      "epoch 2861: train loss: 0.18236059166051555, test loss: 0.2759830907195885\n",
      "epoch 2862: train loss: 0.18233941013857255, test loss: 0.27596855823238664\n",
      "epoch 2863: train loss: 0.18231823954621248, test loss: 0.27595403620391534\n",
      "epoch 2864: train loss: 0.18229707987366098, test loss: 0.2759395246241762\n",
      "epoch 2865: train loss: 0.18227593111115617, test loss: 0.2759250234831852\n",
      "epoch 2866: train loss: 0.18225479324894844, test loss: 0.2759105327709703\n",
      "epoch 2867: train loss: 0.1822336662773011, test loss: 0.27589605247757154\n",
      "epoch 2868: train loss: 0.1822125501864896, test loss: 0.2758815825930443\n",
      "epoch 2869: train loss: 0.18219144496680187, test loss: 0.27586712310745287\n",
      "epoch 2870: train loss: 0.1821703506085384, test loss: 0.2758526740108782\n",
      "epoch 2871: train loss: 0.18214926710201199, test loss: 0.27583823529340995\n",
      "epoch 2872: train loss: 0.18212819443754774, test loss: 0.2758238069451526\n",
      "epoch 2873: train loss: 0.1821071326054834, test loss: 0.27580938895622237\n",
      "epoch 2874: train loss: 0.18208608159616865, test loss: 0.2757949813167476\n",
      "epoch 2875: train loss: 0.18206504139996582, test loss: 0.2757805840168713\n",
      "epoch 2876: train loss: 0.18204401200724948, test loss: 0.27576619704674565\n",
      "epoch 2877: train loss: 0.18202299340840633, test loss: 0.2757518203965386\n",
      "epoch 2878: train loss: 0.18200198559383543, test loss: 0.2757374540564279\n",
      "epoch 2879: train loss: 0.18198098855394818, test loss: 0.2757230980166047\n",
      "epoch 2880: train loss: 0.181960002279168, test loss: 0.27570875226727354\n",
      "epoch 2881: train loss: 0.18193902675993073, test loss: 0.27569441679865003\n",
      "epoch 2882: train loss: 0.18191806198668414, test loss: 0.27568009160096063\n",
      "epoch 2883: train loss: 0.18189710794988834, test loss: 0.27566577666444986\n",
      "epoch 2884: train loss: 0.18187616464001558, test loss: 0.2756514719793666\n",
      "epoch 2885: train loss: 0.18185523204755008, test loss: 0.2756371775359797\n",
      "epoch 2886: train loss: 0.1818343101629884, test loss: 0.2756228933245665\n",
      "epoch 2887: train loss: 0.18181339897683899, test loss: 0.2756086193354128\n",
      "epoch 2888: train loss: 0.1817924984796223, test loss: 0.27559435555882567\n",
      "epoch 2889: train loss: 0.18177160866187111, test loss: 0.275580101985116\n",
      "epoch 2890: train loss: 0.18175072951412996, test loss: 0.2755658586046124\n",
      "epoch 2891: train loss: 0.18172986102695546, test loss: 0.27555162540765415\n",
      "epoch 2892: train loss: 0.18170900319091626, test loss: 0.27553740238458796\n",
      "epoch 2893: train loss: 0.1816881559965929, test loss: 0.27552318952578114\n",
      "epoch 2894: train loss: 0.1816673194345779, test loss: 0.2755089868216074\n",
      "epoch 2895: train loss: 0.18164649349547576, test loss: 0.2754947942624528\n",
      "epoch 2896: train loss: 0.18162567816990272, test loss: 0.27548061183871797\n",
      "epoch 2897: train loss: 0.18160487344848708, test loss: 0.2754664395408131\n",
      "epoch 2898: train loss: 0.1815840793218689, test loss: 0.2754522773591636\n",
      "epoch 2899: train loss: 0.18156329578070007, test loss: 0.27543812528420347\n",
      "epoch 2900: train loss: 0.18154252281564445, test loss: 0.27542398330637957\n",
      "epoch 2901: train loss: 0.1815217604173775, test loss: 0.27540985141615243\n",
      "epoch 2902: train loss: 0.18150100857658663, test loss: 0.27539572960399405\n",
      "epoch 2903: train loss: 0.18148026728397101, test loss: 0.27538161786038606\n",
      "epoch 2904: train loss: 0.18145953653024144, test loss: 0.2753675161758245\n",
      "epoch 2905: train loss: 0.1814388163061205, test loss: 0.27535342454081646\n",
      "epoch 2906: train loss: 0.1814181066023426, test loss: 0.27533934294587936\n",
      "epoch 2907: train loss: 0.18139740740965363, test loss: 0.27532527138154866\n",
      "epoch 2908: train loss: 0.1813767187188113, test loss: 0.2753112098383619\n",
      "epoch 2909: train loss: 0.18135604052058504, test loss: 0.27529715830687596\n",
      "epoch 2910: train loss: 0.18133537280575568, test loss: 0.27528311677765743\n",
      "epoch 2911: train loss: 0.18131471556511586, test loss: 0.275269085241286\n",
      "epoch 2912: train loss: 0.18129406878946974, test loss: 0.27525506368834846\n",
      "epoch 2913: train loss: 0.18127343246963307, test loss: 0.27524105210944927\n",
      "epoch 2914: train loss: 0.18125280659643317, test loss: 0.2752270504951998\n",
      "epoch 2915: train loss: 0.18123219116070893, test loss: 0.2752130588362271\n",
      "epoch 2916: train loss: 0.18121158615331065, test loss: 0.275199077123167\n",
      "epoch 2917: train loss: 0.1811909915651002, test loss: 0.27518510534666746\n",
      "epoch 2918: train loss: 0.18117040738695103, test loss: 0.275171143497392\n",
      "epoch 2919: train loss: 0.1811498336097479, test loss: 0.27515719156600776\n",
      "epoch 2920: train loss: 0.1811292702243871, test loss: 0.2751432495432016\n",
      "epoch 2921: train loss: 0.18110871722177635, test loss: 0.2751293174196676\n",
      "epoch 2922: train loss: 0.18108817459283472, test loss: 0.2751153951861123\n",
      "epoch 2923: train loss: 0.18106764232849273, test loss: 0.27510148283325286\n",
      "epoch 2924: train loss: 0.18104712041969223, test loss: 0.275087580351822\n",
      "epoch 2925: train loss: 0.1810266088573865, test loss: 0.27507368773256036\n",
      "epoch 2926: train loss: 0.18100610763254005, test loss: 0.275059804966219\n",
      "epoch 2927: train loss: 0.1809856167361288, test loss: 0.27504593204356315\n",
      "epoch 2928: train loss: 0.18096513615913992, test loss: 0.27503206895537\n",
      "epoch 2929: train loss: 0.18094466589257188, test loss: 0.2750182156924242\n",
      "epoch 2930: train loss: 0.18092420592743438, test loss: 0.27500437224552876\n",
      "epoch 2931: train loss: 0.18090375625474842, test loss: 0.2749905386054898\n",
      "epoch 2932: train loss: 0.18088331686554615, test loss: 0.27497671476312985\n",
      "epoch 2933: train loss: 0.180862887750871, test loss: 0.2749629007092837\n",
      "epoch 2934: train loss: 0.1808424689017776, test loss: 0.27494909643479415\n",
      "epoch 2935: train loss: 0.18082206030933165, test loss: 0.27493530193051785\n",
      "epoch 2936: train loss: 0.18080166196461012, test loss: 0.27492151718732083\n",
      "epoch 2937: train loss: 0.18078127385870105, test loss: 0.27490774219608255\n",
      "epoch 2938: train loss: 0.18076089598270353, test loss: 0.2748939769476927\n",
      "epoch 2939: train loss: 0.180740528327728, test loss: 0.2748802214330506\n",
      "epoch 2940: train loss: 0.18072017088489573, test loss: 0.27486647564307\n",
      "epoch 2941: train loss: 0.18069982364533907, test loss: 0.2748527395686745\n",
      "epoch 2942: train loss: 0.1806794866002016, test loss: 0.2748390132007972\n",
      "epoch 2943: train loss: 0.1806591597406378, test loss: 0.27482529653038446\n",
      "epoch 2944: train loss: 0.18063884305781308, test loss: 0.2748115895483963\n",
      "epoch 2945: train loss: 0.18061853654290402, test loss: 0.27479789224579776\n",
      "epoch 2946: train loss: 0.18059824018709814, test loss: 0.27478420461356867\n",
      "epoch 2947: train loss: 0.1805779539815938, test loss: 0.2747705266427015\n",
      "epoch 2948: train loss: 0.18055767791760033, test loss: 0.2747568583241952\n",
      "epoch 2949: train loss: 0.18053741198633813, test loss: 0.2747431996490647\n",
      "epoch 2950: train loss: 0.1805171561790383, test loss: 0.27472955060833426\n",
      "epoch 2951: train loss: 0.18049691048694305, test loss: 0.2747159111930376\n",
      "epoch 2952: train loss: 0.18047667490130528, test loss: 0.2747022813942202\n",
      "epoch 2953: train loss: 0.1804564494133888, test loss: 0.2746886612029411\n",
      "epoch 2954: train loss: 0.18043623401446815, test loss: 0.2746750506102657\n",
      "epoch 2955: train loss: 0.18041602869582904, test loss: 0.27466144960727445\n",
      "epoch 2956: train loss: 0.18039583344876753, test loss: 0.2746478581850583\n",
      "epoch 2957: train loss: 0.18037564826459074, test loss: 0.27463427633471865\n",
      "epoch 2958: train loss: 0.1803554731346165, test loss: 0.2746207040473653\n",
      "epoch 2959: train loss: 0.18033530805017337, test loss: 0.2746071413141219\n",
      "epoch 2960: train loss: 0.18031515300260056, test loss: 0.27459358812612217\n",
      "epoch 2961: train loss: 0.1802950079832482, test loss: 0.2745800444745129\n",
      "epoch 2962: train loss: 0.18027487298347694, test loss: 0.274566510350447\n",
      "epoch 2963: train loss: 0.18025474799465824, test loss: 0.274552985745092\n",
      "epoch 2964: train loss: 0.18023463300817397, test loss: 0.2745394706496252\n",
      "epoch 2965: train loss: 0.180214528015417, test loss: 0.2745259650552343\n",
      "epoch 2966: train loss: 0.18019443300779067, test loss: 0.27451246895311976\n",
      "epoch 2967: train loss: 0.18017434797670875, test loss: 0.2744989823344896\n",
      "epoch 2968: train loss: 0.18015427291359593, test loss: 0.27448550519056547\n",
      "epoch 2969: train loss: 0.18013420780988723, test loss: 0.2744720375125775\n",
      "epoch 2970: train loss: 0.1801141526570284, test loss: 0.2744585792917684\n",
      "epoch 2971: train loss: 0.1800941074464756, test loss: 0.27444513051939246\n",
      "epoch 2972: train loss: 0.18007407216969554, test loss: 0.2744316911867108\n",
      "epoch 2973: train loss: 0.18005404681816561, test loss: 0.2744182612849977\n",
      "epoch 2974: train loss: 0.1800340313833734, test loss: 0.2744048408055406\n",
      "epoch 2975: train loss: 0.18001402585681728, test loss: 0.2743914297396322\n",
      "epoch 2976: train loss: 0.17999403023000582, test loss: 0.2743780280785785\n",
      "epoch 2977: train loss: 0.17997404449445825, test loss: 0.2743646358136987\n",
      "epoch 2978: train loss: 0.17995406864170405, test loss: 0.2743512529363187\n",
      "epoch 2979: train loss: 0.1799341026632832, test loss: 0.2743378794377773\n",
      "epoch 2980: train loss: 0.17991414655074614, test loss: 0.2743245153094221\n",
      "epoch 2981: train loss: 0.1798942002956535, test loss: 0.2743111605426121\n",
      "epoch 2982: train loss: 0.1798742638895765, test loss: 0.2742978151287174\n",
      "epoch 2983: train loss: 0.17985433732409653, test loss: 0.2742844790591194\n",
      "epoch 2984: train loss: 0.17983442059080532, test loss: 0.2742711523252072\n",
      "epoch 2985: train loss: 0.17981451368130508, test loss: 0.2742578349183826\n",
      "epoch 2986: train loss: 0.17979461658720808, test loss: 0.2742445268300564\n",
      "epoch 2987: train loss: 0.179774729300137, test loss: 0.27423122805165295\n",
      "epoch 2988: train loss: 0.17975485181172485, test loss: 0.27421793857460336\n",
      "epoch 2989: train loss: 0.17973498411361472, test loss: 0.27420465839035146\n",
      "epoch 2990: train loss: 0.17971512619746005, test loss: 0.27419138749034877\n",
      "epoch 2991: train loss: 0.17969527805492447, test loss: 0.27417812586606066\n",
      "epoch 2992: train loss: 0.17967543967768176, test loss: 0.27416487350896235\n",
      "epoch 2993: train loss: 0.17965561105741593, test loss: 0.2741516304105361\n",
      "epoch 2994: train loss: 0.17963579218582115, test loss: 0.2741383965622795\n",
      "epoch 2995: train loss: 0.17961598305460175, test loss: 0.2741251719556946\n",
      "epoch 2996: train loss: 0.17959618365547214, test loss: 0.27411195658230114\n",
      "epoch 2997: train loss: 0.17957639398015693, test loss: 0.2740987504336203\n",
      "epoch 2998: train loss: 0.17955661402039078, test loss: 0.27408555350119135\n",
      "epoch 2999: train loss: 0.17953684376791842, test loss: 0.2740723657765623\n",
      "epoch 3000: train loss: 0.17951708321449467, test loss: 0.2740591872512852\n",
      "epoch 3001: train loss: 0.17949733235188442, test loss: 0.2740460179169311\n",
      "epoch 3002: train loss: 0.17947759117186257, test loss: 0.2740328577650748\n",
      "epoch 3003: train loss: 0.1794578596662141, test loss: 0.274019706787305\n",
      "epoch 3004: train loss: 0.1794381378267339, test loss: 0.2740065649752193\n",
      "epoch 3005: train loss: 0.17941842564522695, test loss: 0.27399343232042583\n",
      "epoch 3006: train loss: 0.1793987231135081, test loss: 0.2739803088145408\n",
      "epoch 3007: train loss: 0.1793790302234023, test loss: 0.27396719444919254\n",
      "epoch 3008: train loss: 0.17935934696674427, test loss: 0.273954089216019\n",
      "epoch 3009: train loss: 0.17933967333537879, test loss: 0.27394099310667186\n",
      "epoch 3010: train loss: 0.17932000932116055, test loss: 0.27392790611280765\n",
      "epoch 3011: train loss: 0.179300354915954, test loss: 0.2739148282260916\n",
      "epoch 3012: train loss: 0.17928071011163366, test loss: 0.27390175943820716\n",
      "epoch 3013: train loss: 0.1792610749000838, test loss: 0.2738886997408398\n",
      "epoch 3014: train loss: 0.17924144927319854, test loss: 0.27387564912568996\n",
      "epoch 3015: train loss: 0.17922183322288185, test loss: 0.27386260758446546\n",
      "epoch 3016: train loss: 0.17920222674104758, test loss: 0.273849575108885\n",
      "epoch 3017: train loss: 0.17918262981961927, test loss: 0.27383655169067905\n",
      "epoch 3018: train loss: 0.17916304245053039, test loss: 0.27382353732158277\n",
      "epoch 3019: train loss: 0.179143464625724, test loss: 0.27381053199334615\n",
      "epoch 3020: train loss: 0.17912389633715306, test loss: 0.2737975356977287\n",
      "epoch 3021: train loss: 0.1791043375767803, test loss: 0.2737845484264998\n",
      "epoch 3022: train loss: 0.17908478833657795, test loss: 0.27377157017143483\n",
      "epoch 3023: train loss: 0.17906524860852827, test loss: 0.2737586009243248\n",
      "epoch 3024: train loss: 0.17904571838462296, test loss: 0.2737456406769683\n",
      "epoch 3025: train loss: 0.17902619765686353, test loss: 0.27373268942117035\n",
      "epoch 3026: train loss: 0.17900668641726106, test loss: 0.27371974714875047\n",
      "epoch 3027: train loss: 0.17898718465783645, test loss: 0.2737068138515349\n",
      "epoch 3028: train loss: 0.1789676923706201, test loss: 0.2736938895213641\n",
      "epoch 3029: train loss: 0.178948209547652, test loss: 0.27368097415008386\n",
      "epoch 3030: train loss: 0.17892873618098185, test loss: 0.2736680677295524\n",
      "epoch 3031: train loss: 0.17890927226266892, test loss: 0.2736551702516349\n",
      "epoch 3032: train loss: 0.17888981778478197, test loss: 0.2736422817082079\n",
      "epoch 3033: train loss: 0.17887037273939946, test loss: 0.27362940209116104\n",
      "epoch 3034: train loss: 0.17885093711860922, test loss: 0.2736165313923867\n",
      "epoch 3035: train loss: 0.17883151091450877, test loss: 0.2736036696037931\n",
      "epoch 3036: train loss: 0.17881209411920504, test loss: 0.2735908167172959\n",
      "epoch 3037: train loss: 0.17879268672481458, test loss: 0.2735779727248189\n",
      "epoch 3038: train loss: 0.17877328872346332, test loss: 0.2735651376182978\n",
      "epoch 3039: train loss: 0.17875390010728665, test loss: 0.27355231138967817\n",
      "epoch 3040: train loss: 0.17873452086842953, test loss: 0.2735394940309122\n",
      "epoch 3041: train loss: 0.17871515099904622, test loss: 0.2735266855339668\n",
      "epoch 3042: train loss: 0.17869579049130052, test loss: 0.27351388589081166\n",
      "epoch 3043: train loss: 0.17867643933736566, test loss: 0.2735010950934331\n",
      "epoch 3044: train loss: 0.17865709752942407, test loss: 0.27348831313382294\n",
      "epoch 3045: train loss: 0.17863776505966783, test loss: 0.2734755400039823\n",
      "epoch 3046: train loss: 0.17861844192029822, test loss: 0.2734627756959253\n",
      "epoch 3047: train loss: 0.17859912810352585, test loss: 0.2734500202016717\n",
      "epoch 3048: train loss: 0.17857982360157088, test loss: 0.2734372735132515\n",
      "epoch 3049: train loss: 0.1785605284066625, test loss: 0.2734245356227073\n",
      "epoch 3050: train loss: 0.17854124251103942, test loss: 0.27341180652208746\n",
      "epoch 3051: train loss: 0.17852196590694963, test loss: 0.27339908620345393\n",
      "epoch 3052: train loss: 0.17850269858665033, test loss: 0.2733863746588708\n",
      "epoch 3053: train loss: 0.17848344054240797, test loss: 0.273373671880421\n",
      "epoch 3054: train loss: 0.1784641917664984, test loss: 0.2733609778601906\n",
      "epoch 3055: train loss: 0.1784449522512065, test loss: 0.2733482925902748\n",
      "epoch 3056: train loss: 0.17842572198882653, test loss: 0.27333561606278395\n",
      "epoch 3057: train loss: 0.17840650097166194, test loss: 0.2733229482698323\n",
      "epoch 3058: train loss: 0.17838728919202532, test loss: 0.2733102892035447\n",
      "epoch 3059: train loss: 0.1783680866422384, test loss: 0.27329763885605635\n",
      "epoch 3060: train loss: 0.17834889331463227, test loss: 0.273284997219511\n",
      "epoch 3061: train loss: 0.178329709201547, test loss: 0.27327236428606316\n",
      "epoch 3062: train loss: 0.1783105342953318, test loss: 0.2732597400478736\n",
      "epoch 3063: train loss: 0.1782913685883451, test loss: 0.2732471244971173\n",
      "epoch 3064: train loss: 0.1782722120729544, test loss: 0.27323451762597456\n",
      "epoch 3065: train loss: 0.17825306474153627, test loss: 0.2732219194266346\n",
      "epoch 3066: train loss: 0.17823392658647638, test loss: 0.2732093298912995\n",
      "epoch 3067: train loss: 0.17821479760016948, test loss: 0.27319674901217733\n",
      "epoch 3068: train loss: 0.17819567777501935, test loss: 0.2731841767814861\n",
      "epoch 3069: train loss: 0.17817656710343888, test loss: 0.2731716131914544\n",
      "epoch 3070: train loss: 0.17815746557784984, test loss: 0.27315905823431774\n",
      "epoch 3071: train loss: 0.17813837319068315, test loss: 0.2731465119023252\n",
      "epoch 3072: train loss: 0.1781192899343787, test loss: 0.2731339741877293\n",
      "epoch 3073: train loss: 0.17810021580138533, test loss: 0.27312144508279607\n",
      "epoch 3074: train loss: 0.17808115078416087, test loss: 0.2731089245797971\n",
      "epoch 3075: train loss: 0.17806209487517208, test loss: 0.2730964126710182\n",
      "epoch 3076: train loss: 0.17804304806689472, test loss: 0.2730839093487482\n",
      "epoch 3077: train loss: 0.17802401035181348, test loss: 0.2730714146052902\n",
      "epoch 3078: train loss: 0.17800498172242185, test loss: 0.2730589284329543\n",
      "epoch 3079: train loss: 0.1779859621712224, test loss: 0.2730464508240582\n",
      "epoch 3080: train loss: 0.17796695169072638, test loss: 0.27303398177093025\n",
      "epoch 3081: train loss: 0.17794795027345414, test loss: 0.2730215212659103\n",
      "epoch 3082: train loss: 0.17792895791193467, test loss: 0.2730090693013424\n",
      "epoch 3083: train loss: 0.17790997459870603, test loss: 0.2729966258695838\n",
      "epoch 3084: train loss: 0.17789100032631494, test loss: 0.27298419096299686\n",
      "epoch 3085: train loss: 0.177872035087317, test loss: 0.2729717645739558\n",
      "epoch 3086: train loss: 0.17785307887427662, test loss: 0.27295934669484434\n",
      "epoch 3087: train loss: 0.177834131679767, test loss: 0.27294693731805375\n",
      "epoch 3088: train loss: 0.17781519349637015, test loss: 0.27293453643598176\n",
      "epoch 3089: train loss: 0.1777962643166768, test loss: 0.27292214404104054\n",
      "epoch 3090: train loss: 0.17777734413328644, test loss: 0.272909760125648\n",
      "epoch 3091: train loss: 0.17775843293880725, test loss: 0.2728973846822323\n",
      "epoch 3092: train loss: 0.17773953072585635, test loss: 0.2728850177032258\n",
      "epoch 3093: train loss: 0.17772063748705924, test loss: 0.27287265918107784\n",
      "epoch 3094: train loss: 0.17770175321505044, test loss: 0.27286030910824144\n",
      "epoch 3095: train loss: 0.17768287790247292, test loss: 0.27284796747717693\n",
      "epoch 3096: train loss: 0.17766401154197844, test loss: 0.2728356342803578\n",
      "epoch 3097: train loss: 0.1776451541262274, test loss: 0.27282330951026657\n",
      "epoch 3098: train loss: 0.17762630564788887, test loss: 0.27281099315938867\n",
      "epoch 3099: train loss: 0.17760746609964045, test loss: 0.2727986852202261\n",
      "epoch 3100: train loss: 0.17758863547416848, test loss: 0.27278638568528213\n",
      "epoch 3101: train loss: 0.17756981376416786, test loss: 0.27277409454707546\n",
      "epoch 3102: train loss: 0.17755100096234208, test loss: 0.2727618117981313\n",
      "epoch 3103: train loss: 0.17753219706140322, test loss: 0.2727495374309794\n",
      "epoch 3104: train loss: 0.1775134020540719, test loss: 0.2727372714381641\n",
      "epoch 3105: train loss: 0.17749461593307728, test loss: 0.27272501381223846\n",
      "epoch 3106: train loss: 0.17747583869115718, test loss: 0.27271276454575855\n",
      "epoch 3107: train loss: 0.1774570703210578, test loss: 0.27270052363129244\n",
      "epoch 3108: train loss: 0.17743831081553396, test loss: 0.27268829106142045\n",
      "epoch 3109: train loss: 0.17741956016734892, test loss: 0.2726760668287275\n",
      "epoch 3110: train loss: 0.1774008183692744, test loss: 0.2726638509258051\n",
      "epoch 3111: train loss: 0.17738208541409073, test loss: 0.27265164334526043\n",
      "epoch 3112: train loss: 0.17736336129458657, test loss: 0.2726394440797004\n",
      "epoch 3113: train loss: 0.1773446460035591, test loss: 0.27262725312174935\n",
      "epoch 3114: train loss: 0.17732593953381387, test loss: 0.2726150704640356\n",
      "epoch 3115: train loss: 0.17730724187816496, test loss: 0.2726028960991955\n",
      "epoch 3116: train loss: 0.17728855302943475, test loss: 0.2725907300198761\n",
      "epoch 3117: train loss: 0.17726987298045413, test loss: 0.2725785722187336\n",
      "epoch 3118: train loss: 0.17725120172406228, test loss: 0.2725664226884288\n",
      "epoch 3119: train loss: 0.17723253925310672, test loss: 0.27255428142163285\n",
      "epoch 3120: train loss: 0.1772138855604436, test loss: 0.27254214841102964\n",
      "epoch 3121: train loss: 0.17719524063893702, test loss: 0.27253002364930795\n",
      "epoch 3122: train loss: 0.17717660448145975, test loss: 0.2725179071291624\n",
      "epoch 3123: train loss: 0.17715797708089262, test loss: 0.2725057988432998\n",
      "epoch 3124: train loss: 0.177139358430125, test loss: 0.272493698784438\n",
      "epoch 3125: train loss: 0.17712074852205448, test loss: 0.2724816069452958\n",
      "epoch 3126: train loss: 0.17710214734958676, test loss: 0.2724695233186071\n",
      "epoch 3127: train loss: 0.17708355490563615, test loss: 0.2724574478971102\n",
      "epoch 3128: train loss: 0.17706497118312492, test loss: 0.2724453806735563\n",
      "epoch 3129: train loss: 0.1770463961749837, test loss: 0.2724333216407009\n",
      "epoch 3130: train loss: 0.17702782987415142, test loss: 0.2724212707913092\n",
      "epoch 3131: train loss: 0.1770092722735751, test loss: 0.2724092281181553\n",
      "epoch 3132: train loss: 0.17699072336621005, test loss: 0.27239719361401843\n",
      "epoch 3133: train loss: 0.1769721831450198, test loss: 0.2723851672716945\n",
      "epoch 3134: train loss: 0.17695365160297605, test loss: 0.2723731490839783\n",
      "epoch 3135: train loss: 0.17693512873305864, test loss: 0.2723611390436772\n",
      "epoch 3136: train loss: 0.1769166145282555, test loss: 0.2723491371436106\n",
      "epoch 3137: train loss: 0.17689810898156288, test loss: 0.2723371433765986\n",
      "epoch 3138: train loss: 0.1768796120859851, test loss: 0.2723251577354751\n",
      "epoch 3139: train loss: 0.17686112383453448, test loss: 0.27231318021307765\n",
      "epoch 3140: train loss: 0.1768426442202317, test loss: 0.272301210802261\n",
      "epoch 3141: train loss: 0.17682417323610522, test loss: 0.27228924949587685\n",
      "epoch 3142: train loss: 0.17680571087519187, test loss: 0.2722772962867937\n",
      "epoch 3143: train loss: 0.17678725713053645, test loss: 0.2722653511678857\n",
      "epoch 3144: train loss: 0.1767688119951918, test loss: 0.27225341413203225\n",
      "epoch 3145: train loss: 0.17675037546221875, test loss: 0.2722414851721248\n",
      "epoch 3146: train loss: 0.17673194752468635, test loss: 0.272229564281063\n",
      "epoch 3147: train loss: 0.17671352817567154, test loss: 0.2722176514517512\n",
      "epoch 3148: train loss: 0.17669511740825933, test loss: 0.27220574667710656\n",
      "epoch 3149: train loss: 0.17667671521554262, test loss: 0.27219384995005\n",
      "epoch 3150: train loss: 0.1766583215906225, test loss: 0.2721819612635153\n",
      "epoch 3151: train loss: 0.17663993652660787, test loss: 0.27217008061043907\n",
      "epoch 3152: train loss: 0.1766215600166156, test loss: 0.2721582079837713\n",
      "epoch 3153: train loss: 0.17660319205377067, test loss: 0.27214634337646654\n",
      "epoch 3154: train loss: 0.17658483263120583, test loss: 0.27213448678148733\n",
      "epoch 3155: train loss: 0.17656648174206185, test loss: 0.2721226381918096\n",
      "epoch 3156: train loss: 0.17654813937948746, test loss: 0.2721107976004093\n",
      "epoch 3157: train loss: 0.17652980553663908, test loss: 0.2720989650002776\n",
      "epoch 3158: train loss: 0.1765114802066813, test loss: 0.2720871403844084\n",
      "epoch 3159: train loss: 0.17649316338278645, test loss: 0.2720753237458081\n",
      "epoch 3160: train loss: 0.1764748550581348, test loss: 0.27206351507748633\n",
      "epoch 3161: train loss: 0.17645655522591425, test loss: 0.2720517143724668\n",
      "epoch 3162: train loss: 0.1764382638793209, test loss: 0.27203992162377594\n",
      "epoch 3163: train loss: 0.17641998101155848, test loss: 0.272028136824451\n",
      "epoch 3164: train loss: 0.1764017066158385, test loss: 0.27201635996753637\n",
      "epoch 3165: train loss: 0.17638344068538048, test loss: 0.2720045910460841\n",
      "epoch 3166: train loss: 0.17636518321341155, test loss: 0.2719928300531553\n",
      "epoch 3167: train loss: 0.1763469341931667, test loss: 0.27198107698181895\n",
      "epoch 3168: train loss: 0.1763286936178887, test loss: 0.27196933182515104\n",
      "epoch 3169: train loss: 0.1763104614808281, test loss: 0.27195759457623536\n",
      "epoch 3170: train loss: 0.17629223777524308, test loss: 0.2719458652281659\n",
      "epoch 3171: train loss: 0.17627402249439986, test loss: 0.2719341437740406\n",
      "epoch 3172: train loss: 0.17625581563157208, test loss: 0.27192243020696905\n",
      "epoch 3173: train loss: 0.1762376171800412, test loss: 0.2719107245200691\n",
      "epoch 3174: train loss: 0.17621942713309643, test loss: 0.2718990267064616\n",
      "epoch 3175: train loss: 0.17620124548403468, test loss: 0.27188733675927945\n",
      "epoch 3176: train loss: 0.1761830722261605, test loss: 0.2718756546716631\n",
      "epoch 3177: train loss: 0.17616490735278612, test loss: 0.27186398043676113\n",
      "epoch 3178: train loss: 0.17614675085723142, test loss: 0.27185231404772725\n",
      "epoch 3179: train loss: 0.17612860273282402, test loss: 0.27184065549772735\n",
      "epoch 3180: train loss: 0.17611046297289903, test loss: 0.2718290047799294\n",
      "epoch 3181: train loss: 0.1760923315707993, test loss: 0.27181736188751493\n",
      "epoch 3182: train loss: 0.17607420851987532, test loss: 0.2718057268136692\n",
      "epoch 3183: train loss: 0.176056093813485, test loss: 0.27179409955158573\n",
      "epoch 3184: train loss: 0.1760379874449941, test loss: 0.27178248009447115\n",
      "epoch 3185: train loss: 0.1760198894077758, test loss: 0.2717708684355333\n",
      "epoch 3186: train loss: 0.17600179969521082, test loss: 0.27175926456798843\n",
      "epoch 3187: train loss: 0.1759837183006876, test loss: 0.27174766848506304\n",
      "epoch 3188: train loss: 0.175965645217602, test loss: 0.2717360801799929\n",
      "epoch 3189: train loss: 0.17594758043935743, test loss: 0.2717244996460164\n",
      "epoch 3190: train loss: 0.17592952395936487, test loss: 0.27171292687638476\n",
      "epoch 3191: train loss: 0.1759114757710428, test loss: 0.27170136186435184\n",
      "epoch 3192: train loss: 0.17589343586781717, test loss: 0.2716898046031864\n",
      "epoch 3193: train loss: 0.17587540424312148, test loss: 0.27167825508615384\n",
      "epoch 3194: train loss: 0.17585738089039665, test loss: 0.27166671330654\n",
      "epoch 3195: train loss: 0.17583936580309117, test loss: 0.2716551792576287\n",
      "epoch 3196: train loss: 0.1758213589746608, test loss: 0.27164365293271536\n",
      "epoch 3197: train loss: 0.17580336039856903, test loss: 0.2716321343251038\n",
      "epoch 3198: train loss: 0.17578537006828646, test loss: 0.27162062342810295\n",
      "epoch 3199: train loss: 0.1757673879772914, test loss: 0.2716091202350328\n",
      "epoch 3200: train loss: 0.1757494141190695, test loss: 0.2715976247392152\n",
      "epoch 3201: train loss: 0.17573144848711358, test loss: 0.271586136933985\n",
      "epoch 3202: train loss: 0.17571349107492423, test loss: 0.27157465681268383\n",
      "epoch 3203: train loss: 0.1756955418760092, test loss: 0.27156318436865867\n",
      "epoch 3204: train loss: 0.17567760088388354, test loss: 0.27155171959526675\n",
      "epoch 3205: train loss: 0.17565966809206995, test loss: 0.27154026248586993\n",
      "epoch 3206: train loss: 0.17564174349409817, test loss: 0.2715288130338381\n",
      "epoch 3207: train loss: 0.17562382708350546, test loss: 0.2715173712325536\n",
      "epoch 3208: train loss: 0.17560591885383636, test loss: 0.27150593707539916\n",
      "epoch 3209: train loss: 0.17558801879864278, test loss: 0.27149451055576757\n",
      "epoch 3210: train loss: 0.17557012691148377, test loss: 0.2714830916670611\n",
      "epoch 3211: train loss: 0.17555224318592585, test loss: 0.27147168040268765\n",
      "epoch 3212: train loss: 0.17553436761554278, test loss: 0.2714602767560647\n",
      "epoch 3213: train loss: 0.17551650019391551, test loss: 0.27144888072061607\n",
      "epoch 3214: train loss: 0.17549864091463246, test loss: 0.2714374922897674\n",
      "epoch 3215: train loss: 0.17548078977128903, test loss: 0.27142611145696116\n",
      "epoch 3216: train loss: 0.17546294675748805, test loss: 0.2714147382156446\n",
      "epoch 3217: train loss: 0.17544511186683948, test loss: 0.27140337255926766\n",
      "epoch 3218: train loss: 0.17542728509296068, test loss: 0.2713920144812899\n",
      "epoch 3219: train loss: 0.17540946642947597, test loss: 0.27138066397518124\n",
      "epoch 3220: train loss: 0.175391655870017, test loss: 0.2713693210344184\n",
      "epoch 3221: train loss: 0.17537385340822265, test loss: 0.2713579856524835\n",
      "epoch 3222: train loss: 0.1753560590377389, test loss: 0.2713466578228641\n",
      "epoch 3223: train loss: 0.17533827275221894, test loss: 0.27133533753905903\n",
      "epoch 3224: train loss: 0.17532049454532309, test loss: 0.27132402479457335\n",
      "epoch 3225: train loss: 0.17530272441071884, test loss: 0.27131271958291686\n",
      "epoch 3226: train loss: 0.1752849623420808, test loss: 0.2713014218976144\n",
      "epoch 3227: train loss: 0.1752672083330907, test loss: 0.2712901317321892\n",
      "epoch 3228: train loss: 0.17524946237743744, test loss: 0.2712788490801736\n",
      "epoch 3229: train loss: 0.17523172446881696, test loss: 0.2712675739351124\n",
      "epoch 3230: train loss: 0.1752139946009323, test loss: 0.2712563062905515\n",
      "epoch 3231: train loss: 0.1751962727674936, test loss: 0.27124504614004913\n",
      "epoch 3232: train loss: 0.17517855896221815, test loss: 0.2712337934771653\n",
      "epoch 3233: train loss: 0.17516085317883012, test loss: 0.2712225482954746\n",
      "epoch 3234: train loss: 0.17514315541106096, test loss: 0.2712113105885525\n",
      "epoch 3235: train loss: 0.17512546565264894, test loss: 0.27120008034998333\n",
      "epoch 3236: train loss: 0.17510778389733958, test loss: 0.27118885757336064\n",
      "epoch 3237: train loss: 0.17509011013888517, test loss: 0.27117764225228147\n",
      "epoch 3238: train loss: 0.1750724443710453, test loss: 0.27116643438035604\n",
      "epoch 3239: train loss: 0.17505478658758633, test loss: 0.2711552339511946\n",
      "epoch 3240: train loss: 0.1750371367822817, test loss: 0.2711440409584216\n",
      "epoch 3241: train loss: 0.17501949494891186, test loss: 0.2711328553956618\n",
      "epoch 3242: train loss: 0.17500186108126417, test loss: 0.27112167725655234\n",
      "epoch 3243: train loss: 0.17498423517313302, test loss: 0.2711105065347341\n",
      "epoch 3244: train loss: 0.17496661721831966, test loss: 0.2710993432238581\n",
      "epoch 3245: train loss: 0.1749490072106324, test loss: 0.2710881873175812\n",
      "epoch 3246: train loss: 0.17493140514388636, test loss: 0.27107703880956424\n",
      "epoch 3247: train loss: 0.17491381101190362, test loss: 0.2710658976934841\n",
      "epoch 3248: train loss: 0.17489622480851327, test loss: 0.27105476396301353\n",
      "epoch 3249: train loss: 0.17487864652755114, test loss: 0.2710436376118385\n",
      "epoch 3250: train loss: 0.17486107616286004, test loss: 0.2710325186336529\n",
      "epoch 3251: train loss: 0.17484351370828966, test loss: 0.27102140702215427\n",
      "epoch 3252: train loss: 0.1748259591576965, test loss: 0.2710103027710493\n",
      "epoch 3253: train loss: 0.17480841250494403, test loss: 0.2709992058740534\n",
      "epoch 3254: train loss: 0.17479087374390245, test loss: 0.2709881163248845\n",
      "epoch 3255: train loss: 0.17477334286844892, test loss: 0.2709770341172715\n",
      "epoch 3256: train loss: 0.17475581987246736, test loss: 0.2709659592449494\n",
      "epoch 3257: train loss: 0.17473830474984842, test loss: 0.27095489170165554\n",
      "epoch 3258: train loss: 0.17472079749448977, test loss: 0.2709438314811456\n",
      "epoch 3259: train loss: 0.17470329810029572, test loss: 0.270932778577168\n",
      "epoch 3260: train loss: 0.1746858065611774, test loss: 0.27092173298348976\n",
      "epoch 3261: train loss: 0.1746683228710528, test loss: 0.27091069469387885\n",
      "epoch 3262: train loss: 0.17465084702384653, test loss: 0.2708996637021106\n",
      "epoch 3263: train loss: 0.1746333790134902, test loss: 0.27088864000197005\n",
      "epoch 3264: train loss: 0.17461591883392188, test loss: 0.2708776235872462\n",
      "epoch 3265: train loss: 0.17459846647908658, test loss: 0.27086661445173627\n",
      "epoch 3266: train loss: 0.174581021942936, test loss: 0.27085561258924673\n",
      "epoch 3267: train loss: 0.17456358521942852, test loss: 0.27084461799358756\n",
      "epoch 3268: train loss: 0.17454615630252932, test loss: 0.27083363065857397\n",
      "epoch 3269: train loss: 0.17452873518621018, test loss: 0.2708226505780348\n",
      "epoch 3270: train loss: 0.17451132186444962, test loss: 0.2708116777457989\n",
      "epoch 3271: train loss: 0.17449391633123285, test loss: 0.2708007121557065\n",
      "epoch 3272: train loss: 0.17447651858055174, test loss: 0.27078975380160136\n",
      "epoch 3273: train loss: 0.17445912860640483, test loss: 0.2707788026773388\n",
      "epoch 3274: train loss: 0.17444174640279733, test loss: 0.2707678587767751\n",
      "epoch 3275: train loss: 0.17442437196374108, test loss: 0.27075692209377616\n",
      "epoch 3276: train loss: 0.1744070052832545, test loss: 0.2707459926222182\n",
      "epoch 3277: train loss: 0.17438964635536272, test loss: 0.2707350703559777\n",
      "epoch 3278: train loss: 0.17437229517409755, test loss: 0.27072415528894184\n",
      "epoch 3279: train loss: 0.17435495173349713, test loss: 0.27071324741500397\n",
      "epoch 3280: train loss: 0.17433761602760656, test loss: 0.27070234672806587\n",
      "epoch 3281: train loss: 0.17432028805047722, test loss: 0.27069145322203103\n",
      "epoch 3282: train loss: 0.17430296779616722, test loss: 0.27068056689081454\n",
      "epoch 3283: train loss: 0.17428565525874126, test loss: 0.27066968772833955\n",
      "epoch 3284: train loss: 0.1742683504322705, test loss: 0.2706588157285294\n",
      "epoch 3285: train loss: 0.17425105331083277, test loss: 0.2706479508853195\n",
      "epoch 3286: train loss: 0.1742337638885123, test loss: 0.27063709319265017\n",
      "epoch 3287: train loss: 0.17421648215939997, test loss: 0.2706262426444691\n",
      "epoch 3288: train loss: 0.17419920811759312, test loss: 0.270615399234728\n",
      "epoch 3289: train loss: 0.1741819417571956, test loss: 0.27060456295739094\n",
      "epoch 3290: train loss: 0.17416468307231775, test loss: 0.27059373380642215\n",
      "epoch 3291: train loss: 0.1741474320570765, test loss: 0.2705829117757998\n",
      "epoch 3292: train loss: 0.17413018870559516, test loss: 0.27057209685949984\n",
      "epoch 3293: train loss: 0.17411295301200355, test loss: 0.27056128905151416\n",
      "epoch 3294: train loss: 0.17409572497043796, test loss: 0.2705504883458343\n",
      "epoch 3295: train loss: 0.17407850457504115, test loss: 0.2705396947364614\n",
      "epoch 3296: train loss: 0.17406129181996227, test loss: 0.2705289082174035\n",
      "epoch 3297: train loss: 0.174044086699357, test loss: 0.27051812878267373\n",
      "epoch 3298: train loss: 0.17402688920738732, test loss: 0.2705073564262917\n",
      "epoch 3299: train loss: 0.17400969933822183, test loss: 0.27049659114228564\n",
      "epoch 3300: train loss: 0.17399251708603522, test loss: 0.27048583292469136\n",
      "epoch 3301: train loss: 0.17397534244500895, test loss: 0.2704750817675466\n",
      "epoch 3302: train loss: 0.1739581754093306, test loss: 0.27046433766489925\n",
      "epoch 3303: train loss: 0.17394101597319428, test loss: 0.2704536006108048\n",
      "epoch 3304: train loss: 0.17392386413080035, test loss: 0.2704428705993186\n",
      "epoch 3305: train loss: 0.1739067198763557, test loss: 0.2704321476245123\n",
      "epoch 3306: train loss: 0.1738895832040734, test loss: 0.27042143168045546\n",
      "epoch 3307: train loss: 0.17387245410817292, test loss: 0.2704107227612298\n",
      "epoch 3308: train loss: 0.17385533258288027, test loss: 0.27040002086092163\n",
      "epoch 3309: train loss: 0.1738382186224274, test loss: 0.2703893259736257\n",
      "epoch 3310: train loss: 0.1738211122210529, test loss: 0.2703786380934357\n",
      "epoch 3311: train loss: 0.17380401337300153, test loss: 0.27036795721446033\n",
      "epoch 3312: train loss: 0.17378692207252439, test loss: 0.27035728333081444\n",
      "epoch 3313: train loss: 0.17376983831387885, test loss: 0.2703466164366137\n",
      "epoch 3314: train loss: 0.17375276209132856, test loss: 0.2703359565259856\n",
      "epoch 3315: train loss: 0.17373569339914355, test loss: 0.27032530359305995\n",
      "epoch 3316: train loss: 0.1737186322315999, test loss: 0.2703146576319729\n",
      "epoch 3317: train loss: 0.1737015785829801, test loss: 0.2703040186368741\n",
      "epoch 3318: train loss: 0.1736845324475729, test loss: 0.2702933866019138\n",
      "epoch 3319: train loss: 0.1736674938196732, test loss: 0.27028276152124603\n",
      "epoch 3320: train loss: 0.17365046269358217, test loss: 0.27027214338903555\n",
      "epoch 3321: train loss: 0.17363343906360726, test loss: 0.27026153219945503\n",
      "epoch 3322: train loss: 0.173616422924062, test loss: 0.2702509279466798\n",
      "epoch 3323: train loss: 0.17359941426926626, test loss: 0.2702403306248938\n",
      "epoch 3324: train loss: 0.17358241309354597, test loss: 0.27022974022828383\n",
      "epoch 3325: train loss: 0.17356541939123338, test loss: 0.2702191567510489\n",
      "epoch 3326: train loss: 0.17354843315666685, test loss: 0.2702085801873899\n",
      "epoch 3327: train loss: 0.17353145438419088, test loss: 0.27019801053151515\n",
      "epoch 3328: train loss: 0.17351448306815612, test loss: 0.27018744777764014\n",
      "epoch 3329: train loss: 0.1734975192029195, test loss: 0.27017689191998606\n",
      "epoch 3330: train loss: 0.17348056278284393, test loss: 0.27016634295278086\n",
      "epoch 3331: train loss: 0.17346361380229858, test loss: 0.2701558008702569\n",
      "epoch 3332: train loss: 0.1734466722556586, test loss: 0.27014526566665764\n",
      "epoch 3333: train loss: 0.1734297381373054, test loss: 0.27013473733622545\n",
      "epoch 3334: train loss: 0.1734128114416264, test loss: 0.27012421587321583\n",
      "epoch 3335: train loss: 0.17339589216301518, test loss: 0.27011370127188694\n",
      "epoch 3336: train loss: 0.17337898029587137, test loss: 0.27010319352650547\n",
      "epoch 3337: train loss: 0.17336207583460073, test loss: 0.27009269263134106\n",
      "epoch 3338: train loss: 0.173345178773615, test loss: 0.2700821985806731\n",
      "epoch 3339: train loss: 0.1733282891073321, test loss: 0.27007171136878527\n",
      "epoch 3340: train loss: 0.1733114068301759, test loss: 0.2700612309899684\n",
      "epoch 3341: train loss: 0.17329453193657635, test loss: 0.270050757438519\n",
      "epoch 3342: train loss: 0.17327766442096948, test loss: 0.27004029070873947\n",
      "epoch 3343: train loss: 0.17326080427779725, test loss: 0.2700298307949393\n",
      "epoch 3344: train loss: 0.1732439515015078, test loss: 0.270019377691435\n",
      "epoch 3345: train loss: 0.17322710608655512, test loss: 0.27000893139254506\n",
      "epoch 3346: train loss: 0.17321026802739933, test loss: 0.26999849189259983\n",
      "epoch 3347: train loss: 0.17319343731850645, test loss: 0.26998805918593194\n",
      "epoch 3348: train loss: 0.17317661395434847, test loss: 0.26997763326688196\n",
      "epoch 3349: train loss: 0.1731597979294035, test loss: 0.26996721412979646\n",
      "epoch 3350: train loss: 0.1731429892381555, test loss: 0.26995680176902853\n",
      "epoch 3351: train loss: 0.17312618787509443, test loss: 0.26994639617893434\n",
      "epoch 3352: train loss: 0.17310939383471613, test loss: 0.2699359973538789\n",
      "epoch 3353: train loss: 0.17309260711152252, test loss: 0.2699256052882347\n",
      "epoch 3354: train loss: 0.1730758277000214, test loss: 0.26991521997637824\n",
      "epoch 3355: train loss: 0.17305905559472642, test loss: 0.2699048414126908\n",
      "epoch 3356: train loss: 0.17304229079015723, test loss: 0.2698944695915641\n",
      "epoch 3357: train loss: 0.1730255332808394, test loss: 0.2698841045073915\n",
      "epoch 3358: train loss: 0.17300878306130435, test loss: 0.2698737461545771\n",
      "epoch 3359: train loss: 0.17299204012608946, test loss: 0.26986339452752595\n",
      "epoch 3360: train loss: 0.17297530446973788, test loss: 0.2698530496206496\n",
      "epoch 3361: train loss: 0.17295857608679874, test loss: 0.26984271142837185\n",
      "epoch 3362: train loss: 0.1729418549718271, test loss: 0.26983237994511605\n",
      "epoch 3363: train loss: 0.17292514111938367, test loss: 0.26982205516531504\n",
      "epoch 3364: train loss: 0.1729084345240352, test loss: 0.26981173708340506\n",
      "epoch 3365: train loss: 0.17289173518035425, test loss: 0.2698014256938315\n",
      "epoch 3366: train loss: 0.17287504308291915, test loss: 0.26979112099104235\n",
      "epoch 3367: train loss: 0.1728583582263141, test loss: 0.2697808229694938\n",
      "epoch 3368: train loss: 0.1728416806051291, test loss: 0.26977053162364684\n",
      "epoch 3369: train loss: 0.17282501021396002, test loss: 0.2697602469479734\n",
      "epoch 3370: train loss: 0.17280834704740847, test loss: 0.2697499689369427\n",
      "epoch 3371: train loss: 0.17279169110008186, test loss: 0.2697396975850373\n",
      "epoch 3372: train loss: 0.17277504236659347, test loss: 0.2697294328867408\n",
      "epoch 3373: train loss: 0.17275840084156222, test loss: 0.26971917483654695\n",
      "epoch 3374: train loss: 0.17274176651961298, test loss: 0.26970892342895175\n",
      "epoch 3375: train loss: 0.17272513939537618, test loss: 0.2696986786584598\n",
      "epoch 3376: train loss: 0.17270851946348814, test loss: 0.26968844051958074\n",
      "epoch 3377: train loss: 0.1726919067185909, test loss: 0.2696782090068304\n",
      "epoch 3378: train loss: 0.17267530115533225, test loss: 0.26966798411472775\n",
      "epoch 3379: train loss: 0.17265870276836565, test loss: 0.2696577658378037\n",
      "epoch 3380: train loss: 0.17264211155235035, test loss: 0.26964755417058844\n",
      "epoch 3381: train loss: 0.17262552750195134, test loss: 0.2696373491076225\n",
      "epoch 3382: train loss: 0.17260895061183917, test loss: 0.26962715064345205\n",
      "epoch 3383: train loss: 0.1725923808766903, test loss: 0.26961695877262726\n",
      "epoch 3384: train loss: 0.1725758182911867, test loss: 0.26960677348970213\n",
      "epoch 3385: train loss: 0.17255926285001608, test loss: 0.2695965947892425\n",
      "epoch 3386: train loss: 0.1725427145478719, test loss: 0.2695864226658165\n",
      "epoch 3387: train loss: 0.1725261733794532, test loss: 0.269576257113999\n",
      "epoch 3388: train loss: 0.1725096393394647, test loss: 0.26956609812836957\n",
      "epoch 3389: train loss: 0.17249311242261675, test loss: 0.2695559457035138\n",
      "epoch 3390: train loss: 0.17247659262362547, test loss: 0.2695457998340234\n",
      "epoch 3391: train loss: 0.17246007993721243, test loss: 0.26953566051449734\n",
      "epoch 3392: train loss: 0.17244357435810492, test loss: 0.26952552773953814\n",
      "epoch 3393: train loss: 0.17242707588103584, test loss: 0.26951540150375514\n",
      "epoch 3394: train loss: 0.17241058450074376, test loss: 0.26950528180176503\n",
      "epoch 3395: train loss: 0.1723941002119728, test loss: 0.26949516862818723\n",
      "epoch 3396: train loss: 0.1723776230094726, test loss: 0.2694850619776496\n",
      "epoch 3397: train loss: 0.17236115288799858, test loss: 0.26947496184478437\n",
      "epoch 3398: train loss: 0.17234468984231158, test loss: 0.2694648682242273\n",
      "epoch 3399: train loss: 0.17232823386717797, test loss: 0.26945478111062693\n",
      "epoch 3400: train loss: 0.17231178495736996, test loss: 0.26944470049862973\n",
      "epoch 3401: train loss: 0.17229534310766495, test loss: 0.26943462638289084\n",
      "epoch 3402: train loss: 0.17227890831284629, test loss: 0.26942455875807475\n",
      "epoch 3403: train loss: 0.17226248056770246, test loss: 0.269414497618847\n",
      "epoch 3404: train loss: 0.1722460598670278, test loss: 0.2694044429598773\n",
      "epoch 3405: train loss: 0.172229646205622, test loss: 0.26939439477584837\n",
      "epoch 3406: train loss: 0.17221323957829035, test loss: 0.26938435306144337\n",
      "epoch 3407: train loss: 0.17219683997984359, test loss: 0.26937431781134974\n",
      "epoch 3408: train loss: 0.17218044740509802, test loss: 0.2693642890202676\n",
      "epoch 3409: train loss: 0.17216406184887542, test loss: 0.26935426668289336\n",
      "epoch 3410: train loss: 0.17214768330600305, test loss: 0.26934425079393604\n",
      "epoch 3411: train loss: 0.17213131177131358, test loss: 0.26933424134810735\n",
      "epoch 3412: train loss: 0.1721149472396454, test loss: 0.26932423834012703\n",
      "epoch 3413: train loss: 0.17209858970584208, test loss: 0.2693142417647176\n",
      "epoch 3414: train loss: 0.17208223916475276, test loss: 0.26930425161660987\n",
      "epoch 3415: train loss: 0.1720658956112321, test loss: 0.269294267890538\n",
      "epoch 3416: train loss: 0.17204955904014, test loss: 0.26928429058124254\n",
      "epoch 3417: train loss: 0.17203322944634208, test loss: 0.2692743196834704\n",
      "epoch 3418: train loss: 0.17201690682470924, test loss: 0.2692643551919713\n",
      "epoch 3419: train loss: 0.17200059117011773, test loss: 0.2692543971015074\n",
      "epoch 3420: train loss: 0.17198428247744932, test loss: 0.2692444454068404\n",
      "epoch 3421: train loss: 0.1719679807415912, test loss: 0.2692345001027367\n",
      "epoch 3422: train loss: 0.17195168595743582, test loss: 0.269224561183973\n",
      "epoch 3423: train loss: 0.17193539811988112, test loss: 0.2692146286453274\n",
      "epoch 3424: train loss: 0.17191911722383055, test loss: 0.26920470248158795\n",
      "epoch 3425: train loss: 0.17190284326419267, test loss: 0.26919478268754427\n",
      "epoch 3426: train loss: 0.17188657623588163, test loss: 0.26918486925799023\n",
      "epoch 3427: train loss: 0.17187031613381676, test loss: 0.26917496218773407\n",
      "epoch 3428: train loss: 0.17185406295292296, test loss: 0.2691650614715783\n",
      "epoch 3429: train loss: 0.17183781668813025, test loss: 0.2691551671043407\n",
      "epoch 3430: train loss: 0.17182157733437414, test loss: 0.26914527908083613\n",
      "epoch 3431: train loss: 0.17180534488659546, test loss: 0.2691353973958908\n",
      "epoch 3432: train loss: 0.1717891193397403, test loss: 0.2691255220443319\n",
      "epoch 3433: train loss: 0.1717729006887601, test loss: 0.26911565302099805\n",
      "epoch 3434: train loss: 0.17175668892861157, test loss: 0.2691057903207318\n",
      "epoch 3435: train loss: 0.17174048405425688, test loss: 0.26909593393837283\n",
      "epoch 3436: train loss: 0.17172428606066326, test loss: 0.26908608386877897\n",
      "epoch 3437: train loss: 0.17170809494280342, test loss: 0.2690762401068045\n",
      "epoch 3438: train loss: 0.17169191069565518, test loss: 0.2690664026473125\n",
      "epoch 3439: train loss: 0.17167573331420183, test loss: 0.26905657148517315\n",
      "epoch 3440: train loss: 0.1716595627934318, test loss: 0.26904674661525624\n",
      "epoch 3441: train loss: 0.17164339912833884, test loss: 0.26903692803244345\n",
      "epoch 3442: train loss: 0.1716272423139219, test loss: 0.269027115731619\n",
      "epoch 3443: train loss: 0.17161109234518512, test loss: 0.2690173097076732\n",
      "epoch 3444: train loss: 0.17159494921713808, test loss: 0.2690075099554998\n",
      "epoch 3445: train loss: 0.17157881292479538, test loss: 0.2689977164700016\n",
      "epoch 3446: train loss: 0.17156268346317685, test loss: 0.2689879292460833\n",
      "epoch 3447: train loss: 0.17154656082730782, test loss: 0.2689781482786593\n",
      "epoch 3448: train loss: 0.17153044501221848, test loss: 0.2689683735626419\n",
      "epoch 3449: train loss: 0.17151433601294439, test loss: 0.26895860509295483\n",
      "epoch 3450: train loss: 0.17149823382452628, test loss: 0.26894884286453025\n",
      "epoch 3451: train loss: 0.17148213844201002, test loss: 0.2689390868722949\n",
      "epoch 3452: train loss: 0.17146604986044683, test loss: 0.26892933711119255\n",
      "epoch 3453: train loss: 0.17144996807489285, test loss: 0.2689195935761625\n",
      "epoch 3454: train loss: 0.1714338930804096, test loss: 0.26890985626215774\n",
      "epoch 3455: train loss: 0.17141782487206364, test loss: 0.2689001251641299\n",
      "epoch 3456: train loss: 0.17140176344492672, test loss: 0.2688904002770413\n",
      "epoch 3457: train loss: 0.17138570879407575, test loss: 0.2688806815958558\n",
      "epoch 3458: train loss: 0.17136966091459271, test loss: 0.26887096911554237\n",
      "epoch 3459: train loss: 0.17135361980156483, test loss: 0.26886126283108086\n",
      "epoch 3460: train loss: 0.17133758545008437, test loss: 0.26885156273744953\n",
      "epoch 3461: train loss: 0.17132155785524872, test loss: 0.26884186882963534\n",
      "epoch 3462: train loss: 0.17130553701216045, test loss: 0.2688321811026283\n",
      "epoch 3463: train loss: 0.1712895229159271, test loss: 0.2688224995514272\n",
      "epoch 3464: train loss: 0.17127351556166143, test loss: 0.26881282417103536\n",
      "epoch 3465: train loss: 0.17125751494448124, test loss: 0.2688031549564604\n",
      "epoch 3466: train loss: 0.17124152105950943, test loss: 0.2687934919027119\n",
      "epoch 3467: train loss: 0.17122553390187392, test loss: 0.2687838350048116\n",
      "epoch 3468: train loss: 0.1712095534667078, test loss: 0.2687741842577802\n",
      "epoch 3469: train loss: 0.17119357974914912, test loss: 0.2687645396566471\n",
      "epoch 3470: train loss: 0.17117761274434107, test loss: 0.2687549011964464\n",
      "epoch 3471: train loss: 0.17116165244743178, test loss: 0.2687452688722188\n",
      "epoch 3472: train loss: 0.1711456988535745, test loss: 0.26873564267900474\n",
      "epoch 3473: train loss: 0.1711297519579276, test loss: 0.2687260226118567\n",
      "epoch 3474: train loss: 0.17111381175565427, test loss: 0.26871640866582885\n",
      "epoch 3475: train loss: 0.17109787824192288, test loss: 0.2687068008359798\n",
      "epoch 3476: train loss: 0.1710819514119067, test loss: 0.2686971991173758\n",
      "epoch 3477: train loss: 0.17106603126078415, test loss: 0.2686876035050879\n",
      "epoch 3478: train loss: 0.17105011778373858, test loss: 0.26867801399419\n",
      "epoch 3479: train loss: 0.1710342109759582, test loss: 0.2686684305797623\n",
      "epoch 3480: train loss: 0.17101831083263647, test loss: 0.2686588532568923\n",
      "epoch 3481: train loss: 0.17100241734897167, test loss: 0.2686492820206721\n",
      "epoch 3482: train loss: 0.17098653052016696, test loss: 0.26863971686619414\n",
      "epoch 3483: train loss: 0.17097065034143077, test loss: 0.2686301577885607\n",
      "epoch 3484: train loss: 0.17095477680797613, test loss: 0.2686206047828811\n",
      "epoch 3485: train loss: 0.17093890991502128, test loss: 0.26861105784426365\n",
      "epoch 3486: train loss: 0.1709230496577893, test loss: 0.26860151696782475\n",
      "epoch 3487: train loss: 0.1709071960315082, test loss: 0.2685919821486911\n",
      "epoch 3488: train loss: 0.170891349031411, test loss: 0.2685824533819827\n",
      "epoch 3489: train loss: 0.17087550865273557, test loss: 0.2685729306628376\n",
      "epoch 3490: train loss: 0.17085967489072482, test loss: 0.26856341398639033\n",
      "epoch 3491: train loss: 0.17084384774062628, test loss: 0.2685539033477818\n",
      "epoch 3492: train loss: 0.17082802719769277, test loss: 0.26854439874216296\n",
      "epoch 3493: train loss: 0.17081221325718177, test loss: 0.2685349001646814\n",
      "epoch 3494: train loss: 0.17079640591435571, test loss: 0.268525407610497\n",
      "epoch 3495: train loss: 0.1707806051644819, test loss: 0.2685159210747745\n",
      "epoch 3496: train loss: 0.17076481100283253, test loss: 0.2685064405526778\n",
      "epoch 3497: train loss: 0.17074902342468473, test loss: 0.26849696603938106\n",
      "epoch 3498: train loss: 0.17073324242532034, test loss: 0.26848749753006407\n",
      "epoch 3499: train loss: 0.1707174680000263, test loss: 0.26847803501990386\n",
      "epoch 3500: train loss: 0.17070170014409405, test loss: 0.26846857850409545\n",
      "epoch 3501: train loss: 0.17068593885282027, test loss: 0.2684591279778251\n",
      "epoch 3502: train loss: 0.17067018412150628, test loss: 0.26844968343629655\n",
      "epoch 3503: train loss: 0.17065443594545818, test loss: 0.26844024487470625\n",
      "epoch 3504: train loss: 0.17063869431998702, test loss: 0.2684308122882675\n",
      "epoch 3505: train loss: 0.17062295924040857, test loss: 0.2684213856721896\n",
      "epoch 3506: train loss: 0.1706072307020435, test loss: 0.26841196502169323\n",
      "epoch 3507: train loss: 0.17059150870021733, test loss: 0.2684025503319995\n",
      "epoch 3508: train loss: 0.17057579323026015, test loss: 0.26839314159833444\n",
      "epoch 3509: train loss: 0.17056008428750716, test loss: 0.26838373881593536\n",
      "epoch 3510: train loss: 0.17054438186729803, test loss: 0.2683743419800358\n",
      "epoch 3511: train loss: 0.17052868596497753, test loss: 0.26836495108588004\n",
      "epoch 3512: train loss: 0.17051299657589497, test loss: 0.2683555661287187\n",
      "epoch 3513: train loss: 0.17049731369540447, test loss: 0.26834618710379865\n",
      "epoch 3514: train loss: 0.17048163731886504, test loss: 0.26833681400638104\n",
      "epoch 3515: train loss: 0.17046596744164028, test loss: 0.2683274468317283\n",
      "epoch 3516: train loss: 0.1704503040590987, test loss: 0.2683180855751056\n",
      "epoch 3517: train loss: 0.1704346471666134, test loss: 0.26830873023178836\n",
      "epoch 3518: train loss: 0.1704189967595623, test loss: 0.2682993807970525\n",
      "epoch 3519: train loss: 0.17040335283332808, test loss: 0.26829003726617784\n",
      "epoch 3520: train loss: 0.17038771538329808, test loss: 0.268280699634457\n",
      "epoch 3521: train loss: 0.17037208440486437, test loss: 0.2682713678971749\n",
      "epoch 3522: train loss: 0.17035645989342377, test loss: 0.26826204204963444\n",
      "epoch 3523: train loss: 0.17034084184437778, test loss: 0.2682527220871333\n",
      "epoch 3524: train loss: 0.1703252302531326, test loss: 0.2682434080049822\n",
      "epoch 3525: train loss: 0.17030962511509912, test loss: 0.26823409979848906\n",
      "epoch 3526: train loss: 0.1702940264256929, test loss: 0.2682247974629713\n",
      "epoch 3527: train loss: 0.17027843418033428, test loss: 0.26821550099375024\n",
      "epoch 3528: train loss: 0.17026284837444813, test loss: 0.26820621038615156\n",
      "epoch 3529: train loss: 0.1702472690034641, test loss: 0.2681969256355079\n",
      "epoch 3530: train loss: 0.1702316960628164, test loss: 0.268187646737154\n",
      "epoch 3531: train loss: 0.170216129547944, test loss: 0.2681783736864276\n",
      "epoch 3532: train loss: 0.17020056945429046, test loss: 0.26816910647867875\n",
      "epoch 3533: train loss: 0.17018501577730402, test loss: 0.2681598451092554\n",
      "epoch 3534: train loss: 0.17016946851243747, test loss: 0.2681505895735126\n",
      "epoch 3535: train loss: 0.1701539276551484, test loss: 0.26814133986681143\n",
      "epoch 3536: train loss: 0.17013839320089885, test loss: 0.2681320959845145\n",
      "epoch 3537: train loss: 0.1701228651451556, test loss: 0.268122857921992\n",
      "epoch 3538: train loss: 0.17010734348338988, test loss: 0.26811362567461977\n",
      "epoch 3539: train loss: 0.1700918282110778, test loss: 0.26810439923777435\n",
      "epoch 3540: train loss: 0.17007631932369974, test loss: 0.26809517860684196\n",
      "epoch 3541: train loss: 0.170060816816741, test loss: 0.2680859637772104\n",
      "epoch 3542: train loss: 0.1700453206856912, test loss: 0.2680767547442732\n",
      "epoch 3543: train loss: 0.1700298309260447, test loss: 0.26806755150342676\n",
      "epoch 3544: train loss: 0.17001434753330044, test loss: 0.26805835405007433\n",
      "epoch 3545: train loss: 0.16999887050296172, test loss: 0.2680491623796263\n",
      "epoch 3546: train loss: 0.16998339983053676, test loss: 0.26803997648749317\n",
      "epoch 3547: train loss: 0.169967935511538, test loss: 0.26803079636909144\n",
      "epoch 3548: train loss: 0.16995247754148265, test loss: 0.2680216220198416\n",
      "epoch 3549: train loss: 0.16993702591589238, test loss: 0.2680124534351756\n",
      "epoch 3550: train loss: 0.16992158063029336, test loss: 0.26800329061051825\n",
      "epoch 3551: train loss: 0.16990614168021637, test loss: 0.2679941335413096\n",
      "epoch 3552: train loss: 0.1698907090611967, test loss: 0.2679849822229912\n",
      "epoch 3553: train loss: 0.16987528276877412, test loss: 0.2679758366510043\n",
      "epoch 3554: train loss: 0.16985986279849297, test loss: 0.2679666968208011\n",
      "epoch 3555: train loss: 0.16984444914590205, test loss: 0.2679575627278376\n",
      "epoch 3556: train loss: 0.16982904180655475, test loss: 0.2679484343675717\n",
      "epoch 3557: train loss: 0.16981364077600883, test loss: 0.26793931173546554\n",
      "epoch 3558: train loss: 0.16979824604982663, test loss: 0.2679301948269922\n",
      "epoch 3559: train loss: 0.16978285762357495, test loss: 0.2679210836376231\n",
      "epoch 3560: train loss: 0.16976747549282512, test loss: 0.2679119781628364\n",
      "epoch 3561: train loss: 0.1697520996531528, test loss: 0.26790287839811483\n",
      "epoch 3562: train loss: 0.1697367301001383, test loss: 0.2678937843389456\n",
      "epoch 3563: train loss: 0.16972136682936628, test loss: 0.26788469598082015\n",
      "epoch 3564: train loss: 0.16970600983642592, test loss: 0.26787561331923726\n",
      "epoch 3565: train loss: 0.16969065911691075, test loss: 0.26786653634969604\n",
      "epoch 3566: train loss: 0.1696753146664188, test loss: 0.2678574650677031\n",
      "epoch 3567: train loss: 0.16965997648055264, test loss: 0.26784839946876887\n",
      "epoch 3568: train loss: 0.16964464455491907, test loss: 0.2678393395484096\n",
      "epoch 3569: train loss: 0.16962931888512947, test loss: 0.2678302853021433\n",
      "epoch 3570: train loss: 0.16961399946679967, test loss: 0.2678212367254967\n",
      "epoch 3571: train loss: 0.16959868629554975, test loss: 0.2678121938139942\n",
      "epoch 3572: train loss: 0.16958337936700432, test loss: 0.2678031565631746\n",
      "epoch 3573: train loss: 0.1695680786767923, test loss: 0.26779412496857247\n",
      "epoch 3574: train loss: 0.16955278422054718, test loss: 0.267785099025732\n",
      "epoch 3575: train loss: 0.16953749599390672, test loss: 0.26777607873019976\n",
      "epoch 3576: train loss: 0.169522213992513, test loss: 0.2677670640775284\n",
      "epoch 3577: train loss: 0.16950693821201263, test loss: 0.26775805506327444\n",
      "epoch 3578: train loss: 0.1694916686480565, test loss: 0.2677490516829956\n",
      "epoch 3579: train loss: 0.1694764052962999, test loss: 0.2677400539322619\n",
      "epoch 3580: train loss: 0.1694611481524025, test loss: 0.26773106180663986\n",
      "epoch 3581: train loss: 0.1694458972120282, test loss: 0.2677220753017046\n",
      "epoch 3582: train loss: 0.16943065247084546, test loss: 0.2677130944130383\n",
      "epoch 3583: train loss: 0.169415413924527, test loss: 0.26770411913621894\n",
      "epoch 3584: train loss: 0.16940018156874978, test loss: 0.26769514946683926\n",
      "epoch 3585: train loss: 0.1693849553991952, test loss: 0.2676861854004907\n",
      "epoch 3586: train loss: 0.16936973541154907, test loss: 0.26767722693276813\n",
      "epoch 3587: train loss: 0.16935452160150122, test loss: 0.26766827405927485\n",
      "epoch 3588: train loss: 0.16933931396474614, test loss: 0.26765932677561877\n",
      "epoch 3589: train loss: 0.1693241124969824, test loss: 0.26765038507740835\n",
      "epoch 3590: train loss: 0.16930891719391308, test loss: 0.26764144896025854\n",
      "epoch 3591: train loss: 0.16929372805124532, test loss: 0.26763251841978997\n",
      "epoch 3592: train loss: 0.16927854506469076, test loss: 0.2676235934516244\n",
      "epoch 3593: train loss: 0.1692633682299652, test loss: 0.2676146740513902\n",
      "epoch 3594: train loss: 0.16924819754278883, test loss: 0.2676057602147252\n",
      "epoch 3595: train loss: 0.16923303299888598, test loss: 0.2675968519372607\n",
      "epoch 3596: train loss: 0.1692178745939854, test loss: 0.26758794921464163\n",
      "epoch 3597: train loss: 0.16920272232381997, test loss: 0.2675790520425124\n",
      "epoch 3598: train loss: 0.16918757618412694, test loss: 0.2675701604165283\n",
      "epoch 3599: train loss: 0.16917243617064778, test loss: 0.2675612743323395\n",
      "epoch 3600: train loss: 0.1691573022791282, test loss: 0.26755239378560636\n",
      "epoch 3601: train loss: 0.16914217450531815, test loss: 0.2675435187719939\n",
      "epoch 3602: train loss: 0.16912705284497187, test loss: 0.26753464928717025\n",
      "epoch 3603: train loss: 0.16911193729384771, test loss: 0.26752578532680926\n",
      "epoch 3604: train loss: 0.16909682784770838, test loss: 0.26751692688658624\n",
      "epoch 3605: train loss: 0.16908172450232073, test loss: 0.26750807396218135\n",
      "epoch 3606: train loss: 0.1690666272534559, test loss: 0.2674992265492829\n",
      "epoch 3607: train loss: 0.16905153609688925, test loss: 0.26749038464358277\n",
      "epoch 3608: train loss: 0.16903645102840018, test loss: 0.2674815482407722\n",
      "epoch 3609: train loss: 0.1690213720437724, test loss: 0.26747271733655403\n",
      "epoch 3610: train loss: 0.16900629913879392, test loss: 0.2674638919266279\n",
      "epoch 3611: train loss: 0.16899123230925678, test loss: 0.26745507200670404\n",
      "epoch 3612: train loss: 0.16897617155095732, test loss: 0.26744625757249146\n",
      "epoch 3613: train loss: 0.1689611168596959, test loss: 0.2674374486197112\n",
      "epoch 3614: train loss: 0.16894606823127722, test loss: 0.26742864514408216\n",
      "epoch 3615: train loss: 0.1689310256615101, test loss: 0.26741984714132633\n",
      "epoch 3616: train loss: 0.1689159891462075, test loss: 0.2674110546071807\n",
      "epoch 3617: train loss: 0.16890095868118649, test loss: 0.2674022675373707\n",
      "epoch 3618: train loss: 0.16888593426226842, test loss: 0.26739348592763923\n",
      "epoch 3619: train loss: 0.16887091588527858, test loss: 0.2673847097737302\n",
      "epoch 3620: train loss: 0.16885590354604674, test loss: 0.26737593907138635\n",
      "epoch 3621: train loss: 0.16884089724040638, test loss: 0.26736717381636144\n",
      "epoch 3622: train loss: 0.16882589696419545, test loss: 0.26735841400440985\n",
      "epoch 3623: train loss: 0.16881090271325583, test loss: 0.26734965963129126\n",
      "epoch 3624: train loss: 0.1687959144834337, test loss: 0.26734091069276944\n",
      "epoch 3625: train loss: 0.1687809322705791, test loss: 0.26733216718461406\n",
      "epoch 3626: train loss: 0.1687659560705464, test loss: 0.2673234291025979\n",
      "epoch 3627: train loss: 0.168750985879194, test loss: 0.2673146964424961\n",
      "epoch 3628: train loss: 0.16873602169238436, test loss: 0.2673059692000905\n",
      "epoch 3629: train loss: 0.16872106350598404, test loss: 0.26729724737116456\n",
      "epoch 3630: train loss: 0.1687061113158638, test loss: 0.26728853095151095\n",
      "epoch 3631: train loss: 0.16869116511789828, test loss: 0.2672798199369236\n",
      "epoch 3632: train loss: 0.1686762249079664, test loss: 0.2672711143231998\n",
      "epoch 3633: train loss: 0.16866129068195104, test loss: 0.2672624141061401\n",
      "epoch 3634: train loss: 0.16864636243573913, test loss: 0.26725371928155217\n",
      "epoch 3635: train loss: 0.1686314401652217, test loss: 0.26724502984524856\n",
      "epoch 3636: train loss: 0.1686165238662939, test loss: 0.26723634579304273\n",
      "epoch 3637: train loss: 0.1686016135348548, test loss: 0.2672276671207524\n",
      "epoch 3638: train loss: 0.16858670916680757, test loss: 0.2672189938242046\n",
      "epoch 3639: train loss: 0.16857181075805946, test loss: 0.26721032589922644\n",
      "epoch 3640: train loss: 0.16855691830452174, test loss: 0.2672016633416471\n",
      "epoch 3641: train loss: 0.16854203180210967, test loss: 0.2671930061473056\n",
      "epoch 3642: train loss: 0.16852715124674247, test loss: 0.2671843543120406\n",
      "epoch 3643: train loss: 0.16851227663434365, test loss: 0.2671757078316994\n",
      "epoch 3644: train loss: 0.1684974079608404, test loss: 0.2671670667021243\n",
      "epoch 3645: train loss: 0.16848254522216408, test loss: 0.2671584309191763\n",
      "epoch 3646: train loss: 0.1684676884142501, test loss: 0.26714980047870496\n",
      "epoch 3647: train loss: 0.16845283753303775, test loss: 0.26714117537657744\n",
      "epoch 3648: train loss: 0.16843799257447045, test loss: 0.2671325556086543\n",
      "epoch 3649: train loss: 0.16842315353449547, test loss: 0.26712394117080934\n",
      "epoch 3650: train loss: 0.16840832040906414, test loss: 0.2671153320589126\n",
      "epoch 3651: train loss: 0.1683934931941317, test loss: 0.26710672826884585\n",
      "epoch 3652: train loss: 0.1683786718856575, test loss: 0.26709812979648767\n",
      "epoch 3653: train loss: 0.1683638564796047, test loss: 0.2670895366377255\n",
      "epoch 3654: train loss: 0.16834904697194053, test loss: 0.267080948788448\n",
      "epoch 3655: train loss: 0.16833424335863612, test loss: 0.26707236624455183\n",
      "epoch 3656: train loss: 0.16831944563566656, test loss: 0.2670637890019345\n",
      "epoch 3657: train loss: 0.1683046537990109, test loss: 0.267055217056499\n",
      "epoch 3658: train loss: 0.16828986784465222, test loss: 0.26704665040415115\n",
      "epoch 3659: train loss: 0.16827508776857733, test loss: 0.2670380890408022\n",
      "epoch 3660: train loss: 0.16826031356677718, test loss: 0.2670295329623683\n",
      "epoch 3661: train loss: 0.16824554523524646, test loss: 0.26702098216476655\n",
      "epoch 3662: train loss: 0.16823078276998393, test loss: 0.2670124366439191\n",
      "epoch 3663: train loss: 0.16821602616699224, test loss: 0.2670038963957577\n",
      "epoch 3664: train loss: 0.16820127542227786, test loss: 0.26699536141620805\n",
      "epoch 3665: train loss: 0.16818653053185134, test loss: 0.26698683170121046\n",
      "epoch 3666: train loss: 0.16817179149172695, test loss: 0.26697830724670074\n",
      "epoch 3667: train loss: 0.1681570582979229, test loss: 0.26696978804862653\n",
      "epoch 3668: train loss: 0.1681423309464614, test loss: 0.2669612741029297\n",
      "epoch 3669: train loss: 0.1681276094333684, test loss: 0.2669527654055654\n",
      "epoch 3670: train loss: 0.1681128937546739, test loss: 0.2669442619524873\n",
      "epoch 3671: train loss: 0.1680981839064115, test loss: 0.26693576373965755\n",
      "epoch 3672: train loss: 0.16808347988461902, test loss: 0.26692727076303924\n",
      "epoch 3673: train loss: 0.168068781685338, test loss: 0.266918783018599\n",
      "epoch 3674: train loss: 0.16805408930461366, test loss: 0.26691030050231024\n",
      "epoch 3675: train loss: 0.1680394027384953, test loss: 0.2669018232101458\n",
      "epoch 3676: train loss: 0.16802472198303606, test loss: 0.2668933511380876\n",
      "epoch 3677: train loss: 0.16801004703429281, test loss: 0.26688488428211965\n",
      "epoch 3678: train loss: 0.16799537788832639, test loss: 0.26687642263823014\n",
      "epoch 3679: train loss: 0.16798071454120136, test loss: 0.26686796620240966\n",
      "epoch 3680: train loss: 0.16796605698898623, test loss: 0.26685951497065563\n",
      "epoch 3681: train loss: 0.16795140522775318, test loss: 0.26685106893896604\n",
      "epoch 3682: train loss: 0.1679367592535784, test loss: 0.2668426281033441\n",
      "epoch 3683: train loss: 0.1679221190625417, test loss: 0.2668341924598007\n",
      "epoch 3684: train loss: 0.16790748465072688, test loss: 0.26682576200434743\n",
      "epoch 3685: train loss: 0.16789285601422146, test loss: 0.2668173367329955\n",
      "epoch 3686: train loss: 0.16787823314911685, test loss: 0.2668089166417705\n",
      "epoch 3687: train loss: 0.1678636160515081, test loss: 0.266800501726691\n",
      "epoch 3688: train loss: 0.1678490047174941, test loss: 0.2667920919837892\n",
      "epoch 3689: train loss: 0.16783439914317763, test loss: 0.26678368740909364\n",
      "epoch 3690: train loss: 0.16781979932466526, test loss: 0.26677528799864164\n",
      "epoch 3691: train loss: 0.16780520525806714, test loss: 0.26676689374847123\n",
      "epoch 3692: train loss: 0.16779061693949743, test loss: 0.2667585046546266\n",
      "epoch 3693: train loss: 0.16777603436507388, test loss: 0.26675012071315535\n",
      "epoch 3694: train loss: 0.16776145753091812, test loss: 0.26674174192010874\n",
      "epoch 3695: train loss: 0.1677468864331555, test loss: 0.2667333682715423\n",
      "epoch 3696: train loss: 0.16773232106791508, test loss: 0.26672499976351377\n",
      "epoch 3697: train loss: 0.16771776143132974, test loss: 0.26671663639208837\n",
      "epoch 3698: train loss: 0.16770320751953613, test loss: 0.26670827815333115\n",
      "epoch 3699: train loss: 0.16768865932867455, test loss: 0.2666999250433151\n",
      "epoch 3700: train loss: 0.16767411685488906, test loss: 0.266691577058113\n",
      "epoch 3701: train loss: 0.16765958009432747, test loss: 0.2666832341938027\n",
      "epoch 3702: train loss: 0.16764504904314134, test loss: 0.2666748964464687\n",
      "epoch 3703: train loss: 0.16763052369748588, test loss: 0.2666665638121986\n",
      "epoch 3704: train loss: 0.16761600405352012, test loss: 0.2666582362870819\n",
      "epoch 3705: train loss: 0.16760149010740671, test loss: 0.2666499138672097\n",
      "epoch 3706: train loss: 0.16758698185531207, test loss: 0.26664159654868547\n",
      "epoch 3707: train loss: 0.1675724792934062, test loss: 0.2666332843276073\n",
      "epoch 3708: train loss: 0.1675579824178631, test loss: 0.2666249772000814\n",
      "epoch 3709: train loss: 0.16754349122486, test loss: 0.26661667516221854\n",
      "epoch 3710: train loss: 0.1675290057105783, test loss: 0.2666083782101306\n",
      "epoch 3711: train loss: 0.1675145258712027, test loss: 0.26660008633993953\n",
      "epoch 3712: train loss: 0.16750005170292182, test loss: 0.2665917995477607\n",
      "epoch 3713: train loss: 0.16748558320192786, test loss: 0.2665835178297208\n",
      "epoch 3714: train loss: 0.16747112036441666, test loss: 0.26657524118195214\n",
      "epoch 3715: train loss: 0.1674566631865879, test loss: 0.2665669696005849\n",
      "epoch 3716: train loss: 0.1674422116646446, test loss: 0.2665587030817543\n",
      "epoch 3717: train loss: 0.16742776579479376, test loss: 0.2665504416216018\n",
      "epoch 3718: train loss: 0.1674133255732459, test loss: 0.2665421852162747\n",
      "epoch 3719: train loss: 0.16739889099621513, test loss: 0.26653393386191504\n",
      "epoch 3720: train loss: 0.16738446205991928, test loss: 0.2665256875546798\n",
      "epoch 3721: train loss: 0.16737003876057982, test loss: 0.26651744629072094\n",
      "epoch 3722: train loss: 0.16735562109442184, test loss: 0.2665092100662024\n",
      "epoch 3723: train loss: 0.16734120905767397, test loss: 0.266500978877282\n",
      "epoch 3724: train loss: 0.16732680264656857, test loss: 0.26649275272013173\n",
      "epoch 3725: train loss: 0.16731240185734164, test loss: 0.26648453159091884\n",
      "epoch 3726: train loss: 0.16729800668623274, test loss: 0.26647631548581907\n",
      "epoch 3727: train loss: 0.16728361712948497, test loss: 0.26646810440100877\n",
      "epoch 3728: train loss: 0.16726923318334525, test loss: 0.26645989833267536\n",
      "epoch 3729: train loss: 0.16725485484406385, test loss: 0.26645169727699913\n",
      "epoch 3730: train loss: 0.16724048210789477, test loss: 0.26644350123017185\n",
      "epoch 3731: train loss: 0.1672261149710956, test loss: 0.26643531018838607\n",
      "epoch 3732: train loss: 0.1672117534299275, test loss: 0.2664271241478419\n",
      "epoch 3733: train loss: 0.16719739748065524, test loss: 0.26641894310473696\n",
      "epoch 3734: train loss: 0.1671830471195471, test loss: 0.2664107670552778\n",
      "epoch 3735: train loss: 0.167168702342875, test loss: 0.2664025959956715\n",
      "epoch 3736: train loss: 0.16715436314691445, test loss: 0.2663944299221291\n",
      "epoch 3737: train loss: 0.1671400295279444, test loss: 0.2663862688308688\n",
      "epoch 3738: train loss: 0.16712570148224748, test loss: 0.2663781127181096\n",
      "epoch 3739: train loss: 0.16711137900610987, test loss: 0.266369961580074\n",
      "epoch 3740: train loss: 0.16709706209582123, test loss: 0.2663618154129881\n",
      "epoch 3741: train loss: 0.16708275074767479, test loss: 0.26635367421308365\n",
      "epoch 3742: train loss: 0.16706844495796744, test loss: 0.26634553797659644\n",
      "epoch 3743: train loss: 0.16705414472299943, test loss: 0.266337406699763\n",
      "epoch 3744: train loss: 0.16703985003907462, test loss: 0.2663292803788243\n",
      "epoch 3745: train loss: 0.16702556090250043, test loss: 0.2663211590100276\n",
      "epoch 3746: train loss: 0.1670112773095878, test loss: 0.2663130425896212\n",
      "epoch 3747: train loss: 0.16699699925665115, test loss: 0.2663049311138559\n",
      "epoch 3748: train loss: 0.1669827267400084, test loss: 0.2662968245789933\n",
      "epoch 3749: train loss: 0.16696845975598104, test loss: 0.2662887229812894\n",
      "epoch 3750: train loss: 0.16695419830089414, test loss: 0.2662806263170083\n",
      "epoch 3751: train loss: 0.16693994237107604, test loss: 0.26627253458241873\n",
      "epoch 3752: train loss: 0.1669256919628588, test loss: 0.26626444777379166\n",
      "epoch 3753: train loss: 0.16691144707257785, test loss: 0.26625636588739937\n",
      "epoch 3754: train loss: 0.16689720769657218, test loss: 0.26624828891952595\n",
      "epoch 3755: train loss: 0.16688297383118425, test loss: 0.2662402168664486\n",
      "epoch 3756: train loss: 0.16686874547275996, test loss: 0.2662321497244546\n",
      "epoch 3757: train loss: 0.16685452261764872, test loss: 0.26622408748983367\n",
      "epoch 3758: train loss: 0.16684030526220348, test loss: 0.26621603015888\n",
      "epoch 3759: train loss: 0.16682609340278048, test loss: 0.2662079777278888\n",
      "epoch 3760: train loss: 0.1668118870357396, test loss: 0.2661999301931568\n",
      "epoch 3761: train loss: 0.16679768615744409, test loss: 0.2661918875509935\n",
      "epoch 3762: train loss: 0.1667834907642607, test loss: 0.26618384979770454\n",
      "epoch 3763: train loss: 0.16676930085255953, test loss: 0.26617581692960207\n",
      "epoch 3764: train loss: 0.16675511641871435, test loss: 0.26616778894299825\n",
      "epoch 3765: train loss: 0.1667409374591021, test loss: 0.26615976583421186\n",
      "epoch 3766: train loss: 0.16672676397010336, test loss: 0.26615174759956756\n",
      "epoch 3767: train loss: 0.16671259594810198, test loss: 0.26614373423538856\n",
      "epoch 3768: train loss: 0.16669843338948542, test loss: 0.2661357257380037\n",
      "epoch 3769: train loss: 0.16668427629064447, test loss: 0.2661277221037468\n",
      "epoch 3770: train loss: 0.16667012464797332, test loss: 0.2661197233289535\n",
      "epoch 3771: train loss: 0.16665597845786956, test loss: 0.2661117294099627\n",
      "epoch 3772: train loss: 0.1666418377167343, test loss: 0.26610374034312106\n",
      "epoch 3773: train loss: 0.16662770242097197, test loss: 0.2660957561247729\n",
      "epoch 3774: train loss: 0.16661357256699041, test loss: 0.26608777675127\n",
      "epoch 3775: train loss: 0.16659944815120095, test loss: 0.2660798022189653\n",
      "epoch 3776: train loss: 0.16658532917001814, test loss: 0.2660718325242178\n",
      "epoch 3777: train loss: 0.16657121561986013, test loss: 0.2660638676633859\n",
      "epoch 3778: train loss: 0.16655710749714828, test loss: 0.2660559076328392\n",
      "epoch 3779: train loss: 0.1665430047983074, test loss: 0.2660479524289391\n",
      "epoch 3780: train loss: 0.1665289075197657, test loss: 0.2660400020480633\n",
      "epoch 3781: train loss: 0.16651481565795478, test loss: 0.2660320564865868\n",
      "epoch 3782: train loss: 0.16650072920930958, test loss: 0.26602411574088675\n",
      "epoch 3783: train loss: 0.16648664817026834, test loss: 0.26601617980734366\n",
      "epoch 3784: train loss: 0.16647257253727282, test loss: 0.2660082486823492\n",
      "epoch 3785: train loss: 0.16645850230676792, test loss: 0.26600032236228577\n",
      "epoch 3786: train loss: 0.16644443747520213, test loss: 0.2659924008435526\n",
      "epoch 3787: train loss: 0.16643037803902716, test loss: 0.2659844841225404\n",
      "epoch 3788: train loss: 0.16641632399469802, test loss: 0.26597657219565407\n",
      "epoch 3789: train loss: 0.16640227533867322, test loss: 0.2659686650592957\n",
      "epoch 3790: train loss: 0.16638823206741443, test loss: 0.2659607627098712\n",
      "epoch 3791: train loss: 0.16637419417738677, test loss: 0.2659528651437927\n",
      "epoch 3792: train loss: 0.16636016166505868, test loss: 0.26594497235746944\n",
      "epoch 3793: train loss: 0.1663461345269019, test loss: 0.2659370843473257\n",
      "epoch 3794: train loss: 0.1663321127593914, test loss: 0.2659292011097786\n",
      "epoch 3795: train loss: 0.16631809635900566, test loss: 0.26592132264125196\n",
      "epoch 3796: train loss: 0.16630408532222632, test loss: 0.26591344893817426\n",
      "epoch 3797: train loss: 0.16629007964553838, test loss: 0.2659055799969783\n",
      "epoch 3798: train loss: 0.16627607932543018, test loss: 0.26589771581409666\n",
      "epoch 3799: train loss: 0.16626208435839332, test loss: 0.2658898563859694\n",
      "epoch 3800: train loss: 0.1662480947409226, test loss: 0.2658820017090371\n",
      "epoch 3801: train loss: 0.16623411046951633, test loss: 0.26587415177974605\n",
      "epoch 3802: train loss: 0.16622013154067594, test loss: 0.2658663065945435\n",
      "epoch 3803: train loss: 0.1662061579509062, test loss: 0.2658584661498812\n",
      "epoch 3804: train loss: 0.16619218969671515, test loss: 0.26585063044221635\n",
      "epoch 3805: train loss: 0.1661782267746141, test loss: 0.2658427994680061\n",
      "epoch 3806: train loss: 0.1661642691811177, test loss: 0.26583497322371197\n",
      "epoch 3807: train loss: 0.1661503169127437, test loss: 0.265827151705803\n",
      "epoch 3808: train loss: 0.1661363699660133, test loss: 0.26581933491074805\n",
      "epoch 3809: train loss: 0.16612242833745086, test loss: 0.265811522835014\n",
      "epoch 3810: train loss: 0.16610849202358405, test loss: 0.2658037154750841\n",
      "epoch 3811: train loss: 0.16609456102094375, test loss: 0.2657959128274333\n",
      "epoch 3812: train loss: 0.166080635326064, test loss: 0.26578811488854703\n",
      "epoch 3813: train loss: 0.16606671493548233, test loss: 0.26578032165490845\n",
      "epoch 3814: train loss: 0.16605279984573926, test loss: 0.2657725331230084\n",
      "epoch 3815: train loss: 0.16603889005337868, test loss: 0.2657647492893414\n",
      "epoch 3816: train loss: 0.16602498555494777, test loss: 0.2657569701504005\n",
      "epoch 3817: train loss: 0.1660110863469967, test loss: 0.2657491957026913\n",
      "epoch 3818: train loss: 0.16599719242607908, test loss: 0.26574142594271133\n",
      "epoch 3819: train loss: 0.16598330378875165, test loss: 0.2657336608669668\n",
      "epoch 3820: train loss: 0.1659694204315744, test loss: 0.2657259004719708\n",
      "epoch 3821: train loss: 0.1659555423511105, test loss: 0.26571814475423783\n",
      "epoch 3822: train loss: 0.16594166954392645, test loss: 0.26571039371028005\n",
      "epoch 3823: train loss: 0.1659278020065917, test loss: 0.26570264733661747\n",
      "epoch 3824: train loss: 0.16591393973567914, test loss: 0.2656949056297784\n",
      "epoch 3825: train loss: 0.16590008272776477, test loss: 0.26568716858628466\n",
      "epoch 3826: train loss: 0.16588623097942773, test loss: 0.2656794362026684\n",
      "epoch 3827: train loss: 0.16587238448725045, test loss: 0.26567170847546373\n",
      "epoch 3828: train loss: 0.16585854324781849, test loss: 0.2656639854012035\n",
      "epoch 3829: train loss: 0.16584470725772057, test loss: 0.2656562669764326\n",
      "epoch 3830: train loss: 0.16583087651354864, test loss: 0.2656485531976908\n",
      "epoch 3831: train loss: 0.16581705101189775, test loss: 0.26564084406152544\n",
      "epoch 3832: train loss: 0.16580323074936623, test loss: 0.26563313956448825\n",
      "epoch 3833: train loss: 0.16578941572255546, test loss: 0.26562543970313146\n",
      "epoch 3834: train loss: 0.16577560592807006, test loss: 0.26561774447401254\n",
      "epoch 3835: train loss: 0.16576180136251772, test loss: 0.265610053873688\n",
      "epoch 3836: train loss: 0.16574800202250944, test loss: 0.2656023678987263\n",
      "epoch 3837: train loss: 0.16573420790465918, test loss: 0.26559468654568824\n",
      "epoch 3838: train loss: 0.16572041900558418, test loss: 0.26558700981114863\n",
      "epoch 3839: train loss: 0.1657066353219048, test loss: 0.2655793376916784\n",
      "epoch 3840: train loss: 0.16569285685024446, test loss: 0.2655716701838533\n",
      "epoch 3841: train loss: 0.16567908358722988, test loss: 0.26556400728425555\n",
      "epoch 3842: train loss: 0.16566531552949065, test loss: 0.2655563489894644\n",
      "epoch 3843: train loss: 0.16565155267365975, test loss: 0.26554869529606934\n",
      "epoch 3844: train loss: 0.16563779501637313, test loss: 0.26554104620065805\n",
      "epoch 3845: train loss: 0.16562404255427, test loss: 0.26553340169982326\n",
      "epoch 3846: train loss: 0.16561029528399238, test loss: 0.2655257617901652\n",
      "epoch 3847: train loss: 0.1655965532021858, test loss: 0.26551812646827755\n",
      "epoch 3848: train loss: 0.16558281630549865, test loss: 0.2655104957307646\n",
      "epoch 3849: train loss: 0.16556908459058245, test loss: 0.26550286957423386\n",
      "epoch 3850: train loss: 0.16555535805409188, test loss: 0.26549524799529467\n",
      "epoch 3851: train loss: 0.16554163669268473, test loss: 0.26548763099055533\n",
      "epoch 3852: train loss: 0.16552792050302173, test loss: 0.26548001855663417\n",
      "epoch 3853: train loss: 0.1655142094817669, test loss: 0.26547241069015315\n",
      "epoch 3854: train loss: 0.16550050362558724, test loss: 0.2654648073877305\n",
      "epoch 3855: train loss: 0.1654868029311528, test loss: 0.26545720864599026\n",
      "epoch 3856: train loss: 0.16547310739513688, test loss: 0.2654496144615635\n",
      "epoch 3857: train loss: 0.16545941701421557, test loss: 0.2654420248310824\n",
      "epoch 3858: train loss: 0.16544573178506827, test loss: 0.265434439751182\n",
      "epoch 3859: train loss: 0.16543205170437736, test loss: 0.26542685921849823\n",
      "epoch 3860: train loss: 0.16541837676882828, test loss: 0.265419283229673\n",
      "epoch 3861: train loss: 0.16540470697510956, test loss: 0.265411711781354\n",
      "epoch 3862: train loss: 0.1653910423199127, test loss: 0.2654041448701836\n",
      "epoch 3863: train loss: 0.16537738279993242, test loss: 0.26539658249281867\n",
      "epoch 3864: train loss: 0.1653637284118663, test loss: 0.26538902464591124\n",
      "epoch 3865: train loss: 0.165350079152415, test loss: 0.2653814713261174\n",
      "epoch 3866: train loss: 0.1653364350182824, test loss: 0.26537392253009506\n",
      "epoch 3867: train loss: 0.16532279600617522, test loss: 0.2653663782545164\n",
      "epoch 3868: train loss: 0.16530916211280328, test loss: 0.2653588384960427\n",
      "epoch 3869: train loss: 0.16529553333487942, test loss: 0.26535130325134404\n",
      "epoch 3870: train loss: 0.16528190966911954, test loss: 0.26534377251709507\n",
      "epoch 3871: train loss: 0.16526829111224248, test loss: 0.26533624628997265\n",
      "epoch 3872: train loss: 0.16525467766097018, test loss: 0.26532872456665507\n",
      "epoch 3873: train loss: 0.16524106931202767, test loss: 0.26532120734382614\n",
      "epoch 3874: train loss: 0.16522746606214267, test loss: 0.26531369461817095\n",
      "epoch 3875: train loss: 0.1652138679080463, test loss: 0.265306186386379\n",
      "epoch 3876: train loss: 0.1652002748464725, test loss: 0.26529868264514506\n",
      "epoch 3877: train loss: 0.1651866868741582, test loss: 0.2652911833911624\n",
      "epoch 3878: train loss: 0.1651731039878433, test loss: 0.26528368862112833\n",
      "epoch 3879: train loss: 0.16515952618427082, test loss: 0.26527619833174515\n",
      "epoch 3880: train loss: 0.16514595346018662, test loss: 0.26526871251971895\n",
      "epoch 3881: train loss: 0.16513238581233966, test loss: 0.2652612311817573\n",
      "epoch 3882: train loss: 0.1651188232374819, test loss: 0.2652537543145722\n",
      "epoch 3883: train loss: 0.16510526573236808, test loss: 0.2652462819148772\n",
      "epoch 3884: train loss: 0.1650917132937562, test loss: 0.26523881397938887\n",
      "epoch 3885: train loss: 0.16507816591840693, test loss: 0.26523135050482793\n",
      "epoch 3886: train loss: 0.16506462360308427, test loss: 0.26522389148791903\n",
      "epoch 3887: train loss: 0.16505108634455481, test loss: 0.26521643692538865\n",
      "epoch 3888: train loss: 0.1650375541395883, test loss: 0.26520898681396865\n",
      "epoch 3889: train loss: 0.16502402698495747, test loss: 0.2652015411503866\n",
      "epoch 3890: train loss: 0.1650105048774379, test loss: 0.2651940999313814\n",
      "epoch 3891: train loss: 0.16499698781380823, test loss: 0.265186663153693\n",
      "epoch 3892: train loss: 0.16498347579084993, test loss: 0.26517923081406564\n",
      "epoch 3893: train loss: 0.16496996880534745, test loss: 0.2651718029092407\n",
      "epoch 3894: train loss: 0.1649564668540883, test loss: 0.2651643794359683\n",
      "epoch 3895: train loss: 0.1649429699338628, test loss: 0.2651569603909987\n",
      "epoch 3896: train loss: 0.1649294780414641, test loss: 0.26514954577108957\n",
      "epoch 3897: train loss: 0.1649159911736886, test loss: 0.2651421355729952\n",
      "epoch 3898: train loss: 0.1649025093273353, test loss: 0.26513472979347696\n",
      "epoch 3899: train loss: 0.16488903249920633, test loss: 0.26512732842930203\n",
      "epoch 3900: train loss: 0.16487556068610662, test loss: 0.2651199314772328\n",
      "epoch 3901: train loss: 0.16486209388484405, test loss: 0.26511253893403947\n",
      "epoch 3902: train loss: 0.16484863209222947, test loss: 0.26510515079649977\n",
      "epoch 3903: train loss: 0.16483517530507658, test loss: 0.2650977670613846\n",
      "epoch 3904: train loss: 0.164821723520202, test loss: 0.26509038772547566\n",
      "epoch 3905: train loss: 0.1648082767344252, test loss: 0.2650830127855563\n",
      "epoch 3906: train loss: 0.1647948349445686, test loss: 0.26507564223840663\n",
      "epoch 3907: train loss: 0.16478139814745751, test loss: 0.26506827608081995\n",
      "epoch 3908: train loss: 0.1647679663399202, test loss: 0.2650609143095861\n",
      "epoch 3909: train loss: 0.16475453951878766, test loss: 0.2650535569214984\n",
      "epoch 3910: train loss: 0.1647411176808939, test loss: 0.26504620391335465\n",
      "epoch 3911: train loss: 0.16472770082307575, test loss: 0.26503885528195636\n",
      "epoch 3912: train loss: 0.16471428894217297, test loss: 0.265031511024106\n",
      "epoch 3913: train loss: 0.1647008820350282, test loss: 0.2650241711366092\n",
      "epoch 3914: train loss: 0.16468748009848674, test loss: 0.26501683561627476\n",
      "epoch 3915: train loss: 0.1646740831293971, test loss: 0.2650095044599184\n",
      "epoch 3916: train loss: 0.16466069112461043, test loss: 0.2650021776643526\n",
      "epoch 3917: train loss: 0.16464730408098072, test loss: 0.2649948552263973\n",
      "epoch 3918: train loss: 0.16463392199536492, test loss: 0.26498753714287276\n",
      "epoch 3919: train loss: 0.16462054486462285, test loss: 0.2649802234106032\n",
      "epoch 3920: train loss: 0.16460717268561711, test loss: 0.26497291402641926\n",
      "epoch 3921: train loss: 0.16459380545521313, test loss: 0.26496560898714827\n",
      "epoch 3922: train loss: 0.16458044317027926, test loss: 0.264958308289625\n",
      "epoch 3923: train loss: 0.16456708582768656, test loss: 0.26495101193068327\n",
      "epoch 3924: train loss: 0.1645537334243091, test loss: 0.2649437199071659\n",
      "epoch 3925: train loss: 0.1645403859570237, test loss: 0.2649364322159126\n",
      "epoch 3926: train loss: 0.16452704342270988, test loss: 0.26492914885377045\n",
      "epoch 3927: train loss: 0.16451370581825026, test loss: 0.2649218698175859\n",
      "epoch 3928: train loss: 0.16450037314053, test loss: 0.26491459510421117\n",
      "epoch 3929: train loss: 0.16448704538643735, test loss: 0.2649073247105011\n",
      "epoch 3930: train loss: 0.1644737225528631, test loss: 0.2649000586333149\n",
      "epoch 3931: train loss: 0.16446040463670109, test loss: 0.2648927968695085\n",
      "epoch 3932: train loss: 0.16444709163484783, test loss: 0.26488553941594506\n",
      "epoch 3933: train loss: 0.16443378354420268, test loss: 0.2648782862694949\n",
      "epoch 3934: train loss: 0.1644204803616678, test loss: 0.2648710374270246\n",
      "epoch 3935: train loss: 0.16440718208414806, test loss: 0.2648637928854056\n",
      "epoch 3936: train loss: 0.16439388870855137, test loss: 0.2648565526415112\n",
      "epoch 3937: train loss: 0.16438060023178813, test loss: 0.2648493166922222\n",
      "epoch 3938: train loss: 0.16436731665077176, test loss: 0.26484208503441886\n",
      "epoch 3939: train loss: 0.16435403796241835, test loss: 0.26483485766498194\n",
      "epoch 3940: train loss: 0.16434076416364682, test loss: 0.26482763458080183\n",
      "epoch 3941: train loss: 0.1643274952513788, test loss: 0.2648204157787672\n",
      "epoch 3942: train loss: 0.16431423122253874, test loss: 0.26481320125577\n",
      "epoch 3943: train loss: 0.164300972074054, test loss: 0.2648059910087033\n",
      "epoch 3944: train loss: 0.16428771780285442, test loss: 0.26479878503446863\n",
      "epoch 3945: train loss: 0.1642744684058728, test loss: 0.26479158332996583\n",
      "epoch 3946: train loss: 0.16426122388004472, test loss: 0.26478438589209863\n",
      "epoch 3947: train loss: 0.16424798422230846, test loss: 0.2647771927177754\n",
      "epoch 3948: train loss: 0.16423474942960503, test loss: 0.26477000380390214\n",
      "epoch 3949: train loss: 0.1642215194988782, test loss: 0.26476281914739763\n",
      "epoch 3950: train loss: 0.16420829442707457, test loss: 0.26475563874517377\n",
      "epoch 3951: train loss: 0.16419507421114346, test loss: 0.2647484625941467\n",
      "epoch 3952: train loss: 0.16418185884803677, test loss: 0.26474129069124497\n",
      "epoch 3953: train loss: 0.16416864833470943, test loss: 0.26473412303338706\n",
      "epoch 3954: train loss: 0.16415544266811888, test loss: 0.26472695961750037\n",
      "epoch 3955: train loss: 0.16414224184522536, test loss: 0.2647198004405183\n",
      "epoch 3956: train loss: 0.16412904586299182, test loss: 0.2647126454993717\n",
      "epoch 3957: train loss: 0.164115854718384, test loss: 0.2647054947909956\n",
      "epoch 3958: train loss: 0.16410266840837034, test loss: 0.26469834831233086\n",
      "epoch 3959: train loss: 0.16408948692992198, test loss: 0.26469120606031726\n",
      "epoch 3960: train loss: 0.16407631028001277, test loss: 0.2646840680318984\n",
      "epoch 3961: train loss: 0.16406313845561926, test loss: 0.2646769342240243\n",
      "epoch 3962: train loss: 0.1640499714537208, test loss: 0.2646698046336432\n",
      "epoch 3963: train loss: 0.16403680927129932, test loss: 0.26466267925770803\n",
      "epoch 3964: train loss: 0.1640236519053396, test loss: 0.26465555809317426\n",
      "epoch 3965: train loss: 0.164010499352829, test loss: 0.2646484411370026\n",
      "epoch 3966: train loss: 0.1639973516107576, test loss: 0.2646413283861524\n",
      "epoch 3967: train loss: 0.1639842086761182, test loss: 0.26463421983758767\n",
      "epoch 3968: train loss: 0.16397107054590634, test loss: 0.2646271154882775\n",
      "epoch 3969: train loss: 0.16395793721712018, test loss: 0.2646200153351916\n",
      "epoch 3970: train loss: 0.16394480868676056, test loss: 0.2646129193753013\n",
      "epoch 3971: train loss: 0.16393168495183108, test loss: 0.26460582760558254\n",
      "epoch 3972: train loss: 0.16391856600933785, test loss: 0.2645987400230153\n",
      "epoch 3973: train loss: 0.16390545185628988, test loss: 0.2645916566245785\n",
      "epoch 3974: train loss: 0.1638923424896987, test loss: 0.2645845774072563\n",
      "epoch 3975: train loss: 0.16387923790657852, test loss: 0.2645775023680381\n",
      "epoch 3976: train loss: 0.16386613810394635, test loss: 0.2645704315039126\n",
      "epoch 3977: train loss: 0.1638530430788217, test loss: 0.26456336481187087\n",
      "epoch 3978: train loss: 0.16383995282822678, test loss: 0.2645563022889085\n",
      "epoch 3979: train loss: 0.1638268673491865, test loss: 0.26454924393202406\n",
      "epoch 3980: train loss: 0.16381378663872845, test loss: 0.26454218973821947\n",
      "epoch 3981: train loss: 0.16380071069388275, test loss: 0.26453513970449505\n",
      "epoch 3982: train loss: 0.16378763951168232, test loss: 0.26452809382786213\n",
      "epoch 3983: train loss: 0.16377457308916263, test loss: 0.2645210521053263\n",
      "epoch 3984: train loss: 0.16376151142336176, test loss: 0.26451401453390133\n",
      "epoch 3985: train loss: 0.16374845451132053, test loss: 0.26450698111059695\n",
      "epoch 3986: train loss: 0.16373540235008235, test loss: 0.26449995183243985\n",
      "epoch 3987: train loss: 0.1637223549366933, test loss: 0.2644929266964441\n",
      "epoch 3988: train loss: 0.1637093122682019, test loss: 0.2644859056996336\n",
      "epoch 3989: train loss: 0.1636962743416596, test loss: 0.26447888883903514\n",
      "epoch 3990: train loss: 0.1636832411541202, test loss: 0.2644718761116786\n",
      "epoch 3991: train loss: 0.16367021270264032, test loss: 0.26446486751459175\n",
      "epoch 3992: train loss: 0.16365718898427908, test loss: 0.2644578630448084\n",
      "epoch 3993: train loss: 0.1636441699960983, test loss: 0.26445086269937124\n",
      "epoch 3994: train loss: 0.16363115573516226, test loss: 0.26444386647531687\n",
      "epoch 3995: train loss: 0.16361814619853804, test loss: 0.26443687436968344\n",
      "epoch 3996: train loss: 0.1636051413832952, test loss: 0.2644298863795212\n",
      "epoch 3997: train loss: 0.16359214128650593, test loss: 0.26442290250187866\n",
      "epoch 3998: train loss: 0.16357914590524503, test loss: 0.2644159227338026\n",
      "epoch 3999: train loss: 0.16356615523658988, test loss: 0.26440894707234824\n",
      "epoch 4000: train loss: 0.1635531692776205, test loss: 0.2644019755145714\n",
      "epoch 4001: train loss: 0.16354018802541936, test loss: 0.2643950080575299\n",
      "epoch 4002: train loss: 0.16352721147707178, test loss: 0.26438804469828936\n",
      "epoch 4003: train loss: 0.16351423962966535, test loss: 0.264381085433907\n",
      "epoch 4004: train loss: 0.1635012724802905, test loss: 0.26437413026145573\n",
      "epoch 4005: train loss: 0.16348831002604008, test loss: 0.2643671791780037\n",
      "epoch 4006: train loss: 0.1634753522640096, test loss: 0.2643602321806214\n",
      "epoch 4007: train loss: 0.16346239919129707, test loss: 0.26435328926638485\n",
      "epoch 4008: train loss: 0.16344945080500314, test loss: 0.26434635043237364\n",
      "epoch 4009: train loss: 0.16343650710223095, test loss: 0.2643394156756642\n",
      "epoch 4010: train loss: 0.16342356808008623, test loss: 0.2643324849933444\n",
      "epoch 4011: train loss: 0.16341063373567744, test loss: 0.264325558382498\n",
      "epoch 4012: train loss: 0.16339770406611526, test loss: 0.2643186358402109\n",
      "epoch 4013: train loss: 0.16338477906851318, test loss: 0.26431171736357845\n",
      "epoch 4014: train loss: 0.16337185873998714, test loss: 0.2643048029496933\n",
      "epoch 4015: train loss: 0.16335894307765572, test loss: 0.2642978925956536\n",
      "epoch 4016: train loss: 0.16334603207863985, test loss: 0.2642909862985542\n",
      "epoch 4017: train loss: 0.16333312574006328, test loss: 0.2642840840555009\n",
      "epoch 4018: train loss: 0.1633202240590521, test loss: 0.26427718586359733\n",
      "epoch 4019: train loss: 0.16330732703273498, test loss: 0.2642702917199498\n",
      "epoch 4020: train loss: 0.16329443465824306, test loss: 0.26426340162167034\n",
      "epoch 4021: train loss: 0.16328154693271021, test loss: 0.2642565155658703\n",
      "epoch 4022: train loss: 0.16326866385327257, test loss: 0.26424963354966435\n",
      "epoch 4023: train loss: 0.16325578541706903, test loss: 0.26424275557017257\n",
      "epoch 4024: train loss: 0.16324291162124088, test loss: 0.2642358816245132\n",
      "epoch 4025: train loss: 0.1632300424629319, test loss: 0.264229011709812\n",
      "epoch 4026: train loss: 0.1632171779392885, test loss: 0.264222145823192\n",
      "epoch 4027: train loss: 0.1632043180474595, test loss: 0.264215283961785\n",
      "epoch 4028: train loss: 0.16319146278459634, test loss: 0.26420842612271916\n",
      "epoch 4029: train loss: 0.1631786121478528, test loss: 0.2642015723031317\n",
      "epoch 4030: train loss: 0.16316576613438527, test loss: 0.26419472250015547\n",
      "epoch 4031: train loss: 0.16315292474135268, test loss: 0.2641878767109318\n",
      "epoch 4032: train loss: 0.16314008796591645, test loss: 0.26418103493260103\n",
      "epoch 4033: train loss: 0.16312725580524032, test loss: 0.2641741971623105\n",
      "epoch 4034: train loss: 0.16311442825649075, test loss: 0.2641673633972031\n",
      "epoch 4035: train loss: 0.16310160531683665, test loss: 0.26416053363443254\n",
      "epoch 4036: train loss: 0.16308878698344922, test loss: 0.2641537078711485\n",
      "epoch 4037: train loss: 0.1630759732535024, test loss: 0.2641468861045068\n",
      "epoch 4038: train loss: 0.1630631641241725, test loss: 0.2641400683316624\n",
      "epoch 4039: train loss: 0.1630503595926382, test loss: 0.2641332545497811\n",
      "epoch 4040: train loss: 0.1630375596560809, test loss: 0.26412644475602143\n",
      "epoch 4041: train loss: 0.16302476431168422, test loss: 0.26411963894754853\n",
      "epoch 4042: train loss: 0.16301197355663435, test loss: 0.26411283712153144\n",
      "epoch 4043: train loss: 0.16299918738812014, test loss: 0.26410603927513954\n",
      "epoch 4044: train loss: 0.16298640580333257, test loss: 0.2640992454055503\n",
      "epoch 4045: train loss: 0.16297362879946525, test loss: 0.2640924555099343\n",
      "epoch 4046: train loss: 0.16296085637371427, test loss: 0.2640856695854701\n",
      "epoch 4047: train loss: 0.16294808852327808, test loss: 0.26407888762934034\n",
      "epoch 4048: train loss: 0.1629353252453577, test loss: 0.26407210963873146\n",
      "epoch 4049: train loss: 0.16292256653715653, test loss: 0.2640653356108247\n",
      "epoch 4050: train loss: 0.16290981239588043, test loss: 0.2640585655428089\n",
      "epoch 4051: train loss: 0.1628970628187377, test loss: 0.26405179943187823\n",
      "epoch 4052: train loss: 0.16288431780293902, test loss: 0.26404503727522566\n",
      "epoch 4053: train loss: 0.16287157734569768, test loss: 0.2640382790700458\n",
      "epoch 4054: train loss: 0.16285884144422919, test loss: 0.2640315248135383\n",
      "epoch 4055: train loss: 0.16284611009575164, test loss: 0.2640247745029063\n",
      "epoch 4056: train loss: 0.1628333832974856, test loss: 0.26401802813535324\n",
      "epoch 4057: train loss: 0.16282066104665383, test loss: 0.2640112857080844\n",
      "epoch 4058: train loss: 0.16280794334048174, test loss: 0.26400454721830796\n",
      "epoch 4059: train loss: 0.1627952301761971, test loss: 0.263997812663238\n",
      "epoch 4060: train loss: 0.16278252155103004, test loss: 0.2639910820400881\n",
      "epoch 4061: train loss: 0.16276981746221317, test loss: 0.2639843553460765\n",
      "epoch 4062: train loss: 0.16275711790698144, test loss: 0.2639776325784201\n",
      "epoch 4063: train loss: 0.16274442288257235, test loss: 0.2639709137343413\n",
      "epoch 4064: train loss: 0.16273173238622557, test loss: 0.2639641988110661\n",
      "epoch 4065: train loss: 0.1627190464151835, test loss: 0.26395748780582073\n",
      "epoch 4066: train loss: 0.1627063649666907, test loss: 0.26395078071583256\n",
      "epoch 4067: train loss: 0.16269368803799417, test loss: 0.26394407753833754\n",
      "epoch 4068: train loss: 0.16268101562634332, test loss: 0.2639373782705653\n",
      "epoch 4069: train loss: 0.16266834772899003, test loss: 0.26393068290975885\n",
      "epoch 4070: train loss: 0.16265568434318847, test loss: 0.2639239914531529\n",
      "epoch 4071: train loss: 0.16264302546619525, test loss: 0.2639173038979932\n",
      "epoch 4072: train loss: 0.16263037109526934, test loss: 0.26391062024152057\n",
      "epoch 4073: train loss: 0.16261772122767212, test loss: 0.2639039404809847\n",
      "epoch 4074: train loss: 0.1626050758606673, test loss: 0.263897264613636\n",
      "epoch 4075: train loss: 0.16259243499152107, test loss: 0.2638905926367245\n",
      "epoch 4076: train loss: 0.16257979861750183, test loss: 0.2638839245475064\n",
      "epoch 4077: train loss: 0.1625671667358805, test loss: 0.2638772603432365\n",
      "epoch 4078: train loss: 0.16255453934393038, test loss: 0.2638706000211785\n",
      "epoch 4079: train loss: 0.162541916438927, test loss: 0.26386394357859133\n",
      "epoch 4080: train loss: 0.16252929801814836, test loss: 0.263857291012741\n",
      "epoch 4081: train loss: 0.1625166840788748, test loss: 0.26385064232089345\n",
      "epoch 4082: train loss: 0.16250407461838903, test loss: 0.26384399750032267\n",
      "epoch 4083: train loss: 0.16249146963397607, test loss: 0.2638373565482914\n",
      "epoch 4084: train loss: 0.16247886912292323, test loss: 0.2638307194620837\n",
      "epoch 4085: train loss: 0.1624662730825204, test loss: 0.26382408623897374\n",
      "epoch 4086: train loss: 0.16245368151005962, test loss: 0.26381745687623914\n",
      "epoch 4087: train loss: 0.16244109440283538, test loss: 0.2638108313711654\n",
      "epoch 4088: train loss: 0.1624285117581444, test loss: 0.26380420972103136\n",
      "epoch 4089: train loss: 0.16241593357328582, test loss: 0.2637975919231314\n",
      "epoch 4090: train loss: 0.16240335984556117, test loss: 0.2637909779747475\n",
      "epoch 4091: train loss: 0.16239079057227418, test loss: 0.2637843678731755\n",
      "epoch 4092: train loss: 0.162378225750731, test loss: 0.26377776161571054\n",
      "epoch 4093: train loss: 0.1623656653782401, test loss: 0.26377115919965005\n",
      "epoch 4094: train loss: 0.16235310945211226, test loss: 0.2637645606222903\n",
      "epoch 4095: train loss: 0.16234055796966057, test loss: 0.2637579658809338\n",
      "epoch 4096: train loss: 0.1623280109282005, test loss: 0.26375137497288653\n",
      "epoch 4097: train loss: 0.16231546832504976, test loss: 0.26374478789545147\n",
      "epoch 4098: train loss: 0.1623029301575285, test loss: 0.2637382046459426\n",
      "epoch 4099: train loss: 0.16229039642295898, test loss: 0.26373162522166704\n",
      "epoch 4100: train loss: 0.16227786711866599, test loss: 0.26372504961994314\n",
      "epoch 4101: train loss: 0.1622653422419765, test loss: 0.2637184778380815\n",
      "epoch 4102: train loss: 0.16225282179021983, test loss: 0.2637119098734077\n",
      "epoch 4103: train loss: 0.16224030576072748, test loss: 0.2637053457232353\n",
      "epoch 4104: train loss: 0.16222779415083352, test loss: 0.26369878538489566\n",
      "epoch 4105: train loss: 0.1622152869578741, test loss: 0.2636922288557105\n",
      "epoch 4106: train loss: 0.16220278417918763, test loss: 0.26368567613300814\n",
      "epoch 4107: train loss: 0.16219028581211506, test loss: 0.26367912721412123\n",
      "epoch 4108: train loss: 0.1621777918539994, test loss: 0.26367258209638456\n",
      "epoch 4109: train loss: 0.16216530230218595, test loss: 0.2636660407771289\n",
      "epoch 4110: train loss: 0.1621528171540225, test loss: 0.2636595032536943\n",
      "epoch 4111: train loss: 0.16214033640685888, test loss: 0.2636529695234271\n",
      "epoch 4112: train loss: 0.16212786005804738, test loss: 0.2636464395836612\n",
      "epoch 4113: train loss: 0.1621153881049424, test loss: 0.2636399134317483\n",
      "epoch 4114: train loss: 0.16210292054490083, test loss: 0.2636333910650334\n",
      "epoch 4115: train loss: 0.16209045737528172, test loss: 0.26362687248086863\n",
      "epoch 4116: train loss: 0.1620779985934462, test loss: 0.2636203576766055\n",
      "epoch 4117: train loss: 0.1620655441967581, test loss: 0.26361384664959897\n",
      "epoch 4118: train loss: 0.162053094182583, test loss: 0.2636073393972055\n",
      "epoch 4119: train loss: 0.16204064854828926, test loss: 0.26360083591678607\n",
      "epoch 4120: train loss: 0.16202820729124706, test loss: 0.26359433620570355\n",
      "epoch 4121: train loss: 0.16201577040882906, test loss: 0.263587840261319\n",
      "epoch 4122: train loss: 0.16200333789841018, test loss: 0.2635813480810055\n",
      "epoch 4123: train loss: 0.16199090975736752, test loss: 0.2635748596621253\n",
      "epoch 4124: train loss: 0.16197848598308046, test loss: 0.26356837500205466\n",
      "epoch 4125: train loss: 0.1619660665729306, test loss: 0.26356189409816594\n",
      "epoch 4126: train loss: 0.16195365152430186, test loss: 0.2635554169478374\n",
      "epoch 4127: train loss: 0.16194124083458025, test loss: 0.26354894354844377\n",
      "epoch 4128: train loss: 0.16192883450115417, test loss: 0.2635424738973701\n",
      "epoch 4129: train loss: 0.16191643252141424, test loss: 0.26353600799199733\n",
      "epoch 4130: train loss: 0.16190403489275323, test loss: 0.26352954582971483\n",
      "epoch 4131: train loss: 0.16189164161256617, test loss: 0.26352308740790753\n",
      "epoch 4132: train loss: 0.16187925267825037, test loss: 0.2635166327239662\n",
      "epoch 4133: train loss: 0.1618668680872053, test loss: 0.2635101817752831\n",
      "epoch 4134: train loss: 0.1618544878368327, test loss: 0.2635037345592559\n",
      "epoch 4135: train loss: 0.1618421119245365, test loss: 0.26349729107328135\n",
      "epoch 4136: train loss: 0.16182974034772293, test loss: 0.2634908513147583\n",
      "epoch 4137: train loss: 0.1618173731038003, test loss: 0.26348441528109157\n",
      "epoch 4138: train loss: 0.16180501019017923, test loss: 0.26347798296968356\n",
      "epoch 4139: train loss: 0.16179265160427253, test loss: 0.26347155437793973\n",
      "epoch 4140: train loss: 0.16178029734349522, test loss: 0.2634651295032738\n",
      "epoch 4141: train loss: 0.16176794740526446, test loss: 0.26345870834309204\n",
      "epoch 4142: train loss: 0.16175560178699977, test loss: 0.2634522908948137\n",
      "epoch 4143: train loss: 0.16174326048612273, test loss: 0.26344587715585044\n",
      "epoch 4144: train loss: 0.1617309235000571, test loss: 0.2634394671236247\n",
      "epoch 4145: train loss: 0.16171859082622903, test loss: 0.2634330607955533\n",
      "epoch 4146: train loss: 0.16170626246206665, test loss: 0.26342665816906463\n",
      "epoch 4147: train loss: 0.16169393840500038, test loss: 0.26342025924157964\n",
      "epoch 4148: train loss: 0.16168161865246286, test loss: 0.2634138640105266\n",
      "epoch 4149: train loss: 0.16166930320188883, test loss: 0.2634074724733392\n",
      "epoch 4150: train loss: 0.16165699205071524, test loss: 0.26340108462744644\n",
      "epoch 4151: train loss: 0.16164468519638128, test loss: 0.2633947004702852\n",
      "epoch 4152: train loss: 0.1616323826363282, test loss: 0.2633883199992893\n",
      "epoch 4153: train loss: 0.16162008436799966, test loss: 0.26338194321190256\n",
      "epoch 4154: train loss: 0.16160779038884118, test loss: 0.2633755701055637\n",
      "epoch 4155: train loss: 0.1615955006963007, test loss: 0.26336920067771785\n",
      "epoch 4156: train loss: 0.1615832152878282, test loss: 0.2633628349258099\n",
      "epoch 4157: train loss: 0.16157093416087587, test loss: 0.2633564728472889\n",
      "epoch 4158: train loss: 0.1615586573128981, test loss: 0.26335011443960715\n",
      "epoch 4159: train loss: 0.16154638474135136, test loss: 0.2633437597002172\n",
      "epoch 4160: train loss: 0.16153411644369434, test loss: 0.2633374086265713\n",
      "epoch 4161: train loss: 0.1615218524173879, test loss: 0.263331061216133\n",
      "epoch 4162: train loss: 0.16150959265989498, test loss: 0.26332471746635816\n",
      "epoch 4163: train loss: 0.16149733716868078, test loss: 0.2633183773747064\n",
      "epoch 4164: train loss: 0.16148508594121247, test loss: 0.2633120409386488\n",
      "epoch 4165: train loss: 0.1614728389749596, test loss: 0.26330570815564813\n",
      "epoch 4166: train loss: 0.16146059626739373, test loss: 0.26329937902317524\n",
      "epoch 4167: train loss: 0.16144835781598857, test loss: 0.26329305353869786\n",
      "epoch 4168: train loss: 0.16143612361821993, test loss: 0.2632867316996942\n",
      "epoch 4169: train loss: 0.16142389367156595, test loss: 0.2632804135036373\n",
      "epoch 4170: train loss: 0.1614116679735067, test loss: 0.26327409894800247\n",
      "epoch 4171: train loss: 0.1613994465215244, test loss: 0.26326778803027706\n",
      "epoch 4172: train loss: 0.1613872293131035, test loss: 0.2632614807479373\n",
      "epoch 4173: train loss: 0.1613750163457306, test loss: 0.2632551770984717\n",
      "epoch 4174: train loss: 0.16136280761689423, test loss: 0.26324887707936834\n",
      "epoch 4175: train loss: 0.16135060312408528, test loss: 0.26324258068811257\n",
      "epoch 4176: train loss: 0.16133840286479662, test loss: 0.26323628792219594\n",
      "epoch 4177: train loss: 0.16132620683652324, test loss: 0.26322999877911507\n",
      "epoch 4178: train loss: 0.16131401503676235, test loss: 0.2632237132563656\n",
      "epoch 4179: train loss: 0.1613018274630131, test loss: 0.2632174313514436\n",
      "epoch 4180: train loss: 0.161289644112777, test loss: 0.26321115306185083\n",
      "epoch 4181: train loss: 0.1612774649835574, test loss: 0.26320487838509066\n",
      "epoch 4182: train loss: 0.16126529007286, test loss: 0.2631986073186682\n",
      "epoch 4183: train loss: 0.16125311937819242, test loss: 0.26319233986009044\n",
      "epoch 4184: train loss: 0.16124095289706447, test loss: 0.26318607600686267\n",
      "epoch 4185: train loss: 0.161228790626988, test loss: 0.2631798157565037\n",
      "epoch 4186: train loss: 0.16121663256547708, test loss: 0.2631735591065224\n",
      "epoch 4187: train loss: 0.16120447871004775, test loss: 0.2631673060544351\n",
      "epoch 4188: train loss: 0.1611923290582182, test loss: 0.2631610565977615\n",
      "epoch 4189: train loss: 0.16118018360750874, test loss: 0.26315481073402386\n",
      "epoch 4190: train loss: 0.16116804235544166, test loss: 0.26314856846073925\n",
      "epoch 4191: train loss: 0.16115590529954146, test loss: 0.26314232977543717\n",
      "epoch 4192: train loss: 0.16114377243733471, test loss: 0.2631360946756444\n",
      "epoch 4193: train loss: 0.16113164376634992, test loss: 0.2631298631588857\n",
      "epoch 4194: train loss: 0.1611195192841179, test loss: 0.2631236352226991\n",
      "epoch 4195: train loss: 0.16110739898817134, test loss: 0.2631174108646146\n",
      "epoch 4196: train loss: 0.16109528287604513, test loss: 0.2631111900821688\n",
      "epoch 4197: train loss: 0.16108317094527613, test loss: 0.26310497287289936\n",
      "epoch 4198: train loss: 0.1610710631934034, test loss: 0.26309875923434656\n",
      "epoch 4199: train loss: 0.16105895961796793, test loss: 0.2630925491640521\n",
      "epoch 4200: train loss: 0.16104686021651296, test loss: 0.2630863426595608\n",
      "epoch 4201: train loss: 0.1610347649865836, test loss: 0.26308013971841965\n",
      "epoch 4202: train loss: 0.16102267392572706, test loss: 0.2630739403381804\n",
      "epoch 4203: train loss: 0.1610105870314927, test loss: 0.26306774451639287\n",
      "epoch 4204: train loss: 0.1609985043014319, test loss: 0.26306155225060607\n",
      "epoch 4205: train loss: 0.16098642573309804, test loss: 0.263055363538381\n",
      "epoch 4206: train loss: 0.1609743513240466, test loss: 0.26304917837726927\n",
      "epoch 4207: train loss: 0.16096228107183513, test loss: 0.26304299676483717\n",
      "epoch 4208: train loss: 0.1609502149740232, test loss: 0.2630368186986447\n",
      "epoch 4209: train loss: 0.1609381530281724, test loss: 0.26303064417625294\n",
      "epoch 4210: train loss: 0.1609260952318464, test loss: 0.26302447319523214\n",
      "epoch 4211: train loss: 0.16091404158261086, test loss: 0.26301830575314844\n",
      "epoch 4212: train loss: 0.16090199207803357, test loss: 0.2630121418475748\n",
      "epoch 4213: train loss: 0.16088994671568432, test loss: 0.26300598147608\n",
      "epoch 4214: train loss: 0.16087790549313485, test loss: 0.262999824636241\n",
      "epoch 4215: train loss: 0.1608658684079591, test loss: 0.2629936713256372\n",
      "epoch 4216: train loss: 0.16085383545773282, test loss: 0.26298752154184496\n",
      "epoch 4217: train loss: 0.16084180664003397, test loss: 0.2629813752824447\n",
      "epoch 4218: train loss: 0.1608297819524425, test loss: 0.2629752325450231\n",
      "epoch 4219: train loss: 0.1608177613925403, test loss: 0.262969093327164\n",
      "epoch 4220: train loss: 0.1608057449579114, test loss: 0.2629629576264581\n",
      "epoch 4221: train loss: 0.1607937326461417, test loss: 0.26295682544048765\n",
      "epoch 4222: train loss: 0.16078172445481928, test loss: 0.262950696766851\n",
      "epoch 4223: train loss: 0.16076972038153414, test loss: 0.26294457160314416\n",
      "epoch 4224: train loss: 0.1607577204238783, test loss: 0.26293844994695775\n",
      "epoch 4225: train loss: 0.1607457245794458, test loss: 0.2629323317958942\n",
      "epoch 4226: train loss: 0.1607337328458327, test loss: 0.2629262171475522\n",
      "epoch 4227: train loss: 0.16072174522063698, test loss: 0.2629201059995323\n",
      "epoch 4228: train loss: 0.16070976170145876, test loss: 0.2629139983494447\n",
      "epoch 4229: train loss: 0.1606977822859001, test loss: 0.26290789419489125\n",
      "epoch 4230: train loss: 0.16068580697156506, test loss: 0.26290179353348564\n",
      "epoch 4231: train loss: 0.16067383575605965, test loss: 0.2628956963628359\n",
      "epoch 4232: train loss: 0.16066186863699194, test loss: 0.2628896026805522\n",
      "epoch 4233: train loss: 0.16064990561197193, test loss: 0.26288351248425523\n",
      "epoch 4234: train loss: 0.1606379466786117, test loss: 0.2628774257715633\n",
      "epoch 4235: train loss: 0.16062599183452528, test loss: 0.2628713425400912\n",
      "epoch 4236: train loss: 0.16061404107732855, test loss: 0.26286526278746153\n",
      "epoch 4237: train loss: 0.16060209440463966, test loss: 0.26285918651130025\n",
      "epoch 4238: train loss: 0.16059015181407849, test loss: 0.26285311370923303\n",
      "epoch 4239: train loss: 0.16057821330326694, test loss: 0.2628470443788863\n",
      "epoch 4240: train loss: 0.160566278869829, test loss: 0.2628409785178891\n",
      "epoch 4241: train loss: 0.16055434851139055, test loss: 0.2628349161238763\n",
      "epoch 4242: train loss: 0.16054242222557946, test loss: 0.26282885719448057\n",
      "epoch 4243: train loss: 0.16053050001002553, test loss: 0.2628228017273367\n",
      "epoch 4244: train loss: 0.16051858186236068, test loss: 0.2628167497200868\n",
      "epoch 4245: train loss: 0.16050666778021852, test loss: 0.2628107011703702\n",
      "epoch 4246: train loss: 0.16049475776123492, test loss: 0.26280465607582754\n",
      "epoch 4247: train loss: 0.16048285180304755, test loss: 0.26279861443410246\n",
      "epoch 4248: train loss: 0.16047094990329605, test loss: 0.2627925762428448\n",
      "epoch 4249: train loss: 0.160459052059622, test loss: 0.26278654149970093\n",
      "epoch 4250: train loss: 0.16044715826966907, test loss: 0.2627805102023242\n",
      "epoch 4251: train loss: 0.1604352685310827, test loss: 0.26277448234836703\n",
      "epoch 4252: train loss: 0.16042338284151042, test loss: 0.2627684579354824\n",
      "epoch 4253: train loss: 0.16041150119860162, test loss: 0.2627624369613259\n",
      "epoch 4254: train loss: 0.1603996236000077, test loss: 0.2627564194235606\n",
      "epoch 4255: train loss: 0.160387750043382, test loss: 0.2627504053198465\n",
      "epoch 4256: train loss: 0.16037588052637972, test loss: 0.26274439464784577\n",
      "epoch 4257: train loss: 0.16036401504665812, test loss: 0.2627383874052235\n",
      "epoch 4258: train loss: 0.16035215360187627, test loss: 0.26273238358964685\n",
      "epoch 4259: train loss: 0.16034029618969534, test loss: 0.2627263831987866\n",
      "epoch 4260: train loss: 0.16032844280777828, test loss: 0.26272038623031096\n",
      "epoch 4261: train loss: 0.16031659345379004, test loss: 0.2627143926818986\n",
      "epoch 4262: train loss: 0.16030474812539747, test loss: 0.2627084025512211\n",
      "epoch 4263: train loss: 0.16029290682026942, test loss: 0.262702415835957\n",
      "epoch 4264: train loss: 0.16028106953607665, test loss: 0.26269643253378333\n",
      "epoch 4265: train loss: 0.16026923627049172, test loss: 0.26269045264238505\n",
      "epoch 4266: train loss: 0.16025740702118924, test loss: 0.2626844761594462\n",
      "epoch 4267: train loss: 0.16024558178584566, test loss: 0.2626785030826503\n",
      "epoch 4268: train loss: 0.1602337605621395, test loss: 0.26267253340968366\n",
      "epoch 4269: train loss: 0.160221943347751, test loss: 0.26266656713823816\n",
      "epoch 4270: train loss: 0.16021013014036245, test loss: 0.2626606042660065\n",
      "epoch 4271: train loss: 0.16019832093765796, test loss: 0.2626546447906782\n",
      "epoch 4272: train loss: 0.16018651573732356, test loss: 0.262648688709955\n",
      "epoch 4273: train loss: 0.1601747145370473, test loss: 0.2626427360215299\n",
      "epoch 4274: train loss: 0.160162917334519, test loss: 0.2626367867231055\n",
      "epoch 4275: train loss: 0.16015112412743043, test loss: 0.2626308408123795\n",
      "epoch 4276: train loss: 0.16013933491347532, test loss: 0.2626248982870597\n",
      "epoch 4277: train loss: 0.16012754969034915, test loss: 0.26261895914485023\n",
      "epoch 4278: train loss: 0.1601157684557495, test loss: 0.26261302338346204\n",
      "epoch 4279: train loss: 0.16010399120737567, test loss: 0.2626070910005997\n",
      "epoch 4280: train loss: 0.1600922179429289, test loss: 0.26260116199397776\n",
      "epoch 4281: train loss: 0.1600804486601124, test loss: 0.26259523636131105\n",
      "epoch 4282: train loss: 0.16006868335663121, test loss: 0.2625893141003127\n",
      "epoch 4283: train loss: 0.1600569220301922, test loss: 0.2625833952087011\n",
      "epoch 4284: train loss: 0.16004516467850421, test loss: 0.2625774796841993\n",
      "epoch 4285: train loss: 0.1600334112992779, test loss: 0.26257156752452493\n",
      "epoch 4286: train loss: 0.16002166189022593, test loss: 0.2625656587274054\n",
      "epoch 4287: train loss: 0.16000991644906265, test loss: 0.26255975329056386\n",
      "epoch 4288: train loss: 0.15999817497350444, test loss: 0.26255385121172753\n",
      "epoch 4289: train loss: 0.1599864374612695, test loss: 0.2625479524886284\n",
      "epoch 4290: train loss: 0.1599747039100779, test loss: 0.26254205711899703\n",
      "epoch 4291: train loss: 0.15996297431765158, test loss: 0.2625361651005691\n",
      "epoch 4292: train loss: 0.15995124868171437, test loss: 0.2625302764310771\n",
      "epoch 4293: train loss: 0.15993952699999192, test loss: 0.26252439110825776\n",
      "epoch 4294: train loss: 0.1599278092702118, test loss: 0.2625185091298569\n",
      "epoch 4295: train loss: 0.15991609549010344, test loss: 0.26251263049361034\n",
      "epoch 4296: train loss: 0.15990438565739803, test loss: 0.2625067551972647\n",
      "epoch 4297: train loss: 0.15989267976982877, test loss: 0.26250088323856435\n",
      "epoch 4298: train loss: 0.15988097782513058, test loss: 0.2624950146152564\n",
      "epoch 4299: train loss: 0.15986927982104038, test loss: 0.2624891493250917\n",
      "epoch 4300: train loss: 0.15985758575529677, test loss: 0.26248328736581955\n",
      "epoch 4301: train loss: 0.15984589562564028, test loss: 0.26247742873519386\n",
      "epoch 4302: train loss: 0.15983420942981344, test loss: 0.26247157343097177\n",
      "epoch 4303: train loss: 0.15982252716556034, test loss: 0.26246572145091007\n",
      "epoch 4304: train loss: 0.15981084883062707, test loss: 0.26245987279276495\n",
      "epoch 4305: train loss: 0.15979917442276165, test loss: 0.26245402745430085\n",
      "epoch 4306: train loss: 0.15978750393971372, test loss: 0.2624481854332811\n",
      "epoch 4307: train loss: 0.15977583737923495, test loss: 0.26244234672747063\n",
      "epoch 4308: train loss: 0.15976417473907875, test loss: 0.2624365113346315\n",
      "epoch 4309: train loss: 0.1597525160170004, test loss: 0.26243067925254004\n",
      "epoch 4310: train loss: 0.159740861210757, test loss: 0.26242485047896397\n",
      "epoch 4311: train loss: 0.15972921031810747, test loss: 0.2624190250116751\n",
      "epoch 4312: train loss: 0.15971756333681256, test loss: 0.26241320284844866\n",
      "epoch 4313: train loss: 0.15970592026463495, test loss: 0.2624073839870626\n",
      "epoch 4314: train loss: 0.15969428109933895, test loss: 0.2624015684252956\n",
      "epoch 4315: train loss: 0.1596826458386908, test loss: 0.26239575616092603\n",
      "epoch 4316: train loss: 0.15967101448045862, test loss: 0.26238994719173947\n",
      "epoch 4317: train loss: 0.15965938702241222, test loss: 0.2623841415155158\n",
      "epoch 4318: train loss: 0.15964776346232334, test loss: 0.26237833913004743\n",
      "epoch 4319: train loss: 0.15963614379796545, test loss: 0.26237254003311683\n",
      "epoch 4320: train loss: 0.1596245280271139, test loss: 0.2623667442225182\n",
      "epoch 4321: train loss: 0.1596129161475458, test loss: 0.2623609516960434\n",
      "epoch 4322: train loss: 0.1596013081570401, test loss: 0.2623551624514818\n",
      "epoch 4323: train loss: 0.15958970405337755, test loss: 0.262349376486637\n",
      "epoch 4324: train loss: 0.15957810383434068, test loss: 0.2623435937992983\n",
      "epoch 4325: train loss: 0.15956650749771387, test loss: 0.2623378143872731\n",
      "epoch 4326: train loss: 0.15955491504128327, test loss: 0.2623320382483572\n",
      "epoch 4327: train loss: 0.1595433264628368, test loss: 0.26232626538035964\n",
      "epoch 4328: train loss: 0.1595317417601643, test loss: 0.26232049578108\n",
      "epoch 4329: train loss: 0.1595201609310572, test loss: 0.26231472944833123\n",
      "epoch 4330: train loss: 0.15950858397330897, test loss: 0.26230896637991924\n",
      "epoch 4331: train loss: 0.15949701088471463, test loss: 0.262303206573656\n",
      "epoch 4332: train loss: 0.15948544166307121, test loss: 0.262297450027355\n",
      "epoch 4333: train loss: 0.15947387630617732, test loss: 0.2622916967388292\n",
      "epoch 4334: train loss: 0.1594623148118335, test loss: 0.262285946705898\n",
      "epoch 4335: train loss: 0.15945075717784207, test loss: 0.26228019992638\n",
      "epoch 4336: train loss: 0.15943920340200704, test loss: 0.26227445639809327\n",
      "epoch 4337: train loss: 0.1594276534821343, test loss: 0.2622687161188623\n",
      "epoch 4338: train loss: 0.15941610741603143, test loss: 0.2622629790865125\n",
      "epoch 4339: train loss: 0.15940456520150786, test loss: 0.2622572452988679\n",
      "epoch 4340: train loss: 0.15939302683637474, test loss: 0.26225151475375597\n",
      "epoch 4341: train loss: 0.15938149231844503, test loss: 0.26224578744900895\n",
      "epoch 4342: train loss: 0.15936996164553346, test loss: 0.26224006338245975\n",
      "epoch 4343: train loss: 0.15935843481545653, test loss: 0.2622343425519388\n",
      "epoch 4344: train loss: 0.15934691182603242, test loss: 0.26222862495528454\n",
      "epoch 4345: train loss: 0.1593353926750812, test loss: 0.26222291059033154\n",
      "epoch 4346: train loss: 0.1593238773604247, test loss: 0.26221719945492133\n",
      "epoch 4347: train loss: 0.15931236587988637, test loss: 0.2622114915468967\n",
      "epoch 4348: train loss: 0.15930085823129156, test loss: 0.2622057868640988\n",
      "epoch 4349: train loss: 0.15928935441246733, test loss: 0.2622000854043713\n",
      "epoch 4350: train loss: 0.15927785442124248, test loss: 0.2621943871655615\n",
      "epoch 4351: train loss: 0.1592663582554476, test loss: 0.26218869214552154\n",
      "epoch 4352: train loss: 0.15925486591291502, test loss: 0.2621830003420971\n",
      "epoch 4353: train loss: 0.1592433773914788, test loss: 0.26217731175314485\n",
      "epoch 4354: train loss: 0.15923189268897475, test loss: 0.26217162637651503\n",
      "epoch 4355: train loss: 0.15922041180324042, test loss: 0.26216594421006584\n",
      "epoch 4356: train loss: 0.1592089347321152, test loss: 0.26216026525165503\n",
      "epoch 4357: train loss: 0.15919746147344013, test loss: 0.2621545894991449\n",
      "epoch 4358: train loss: 0.1591859920250579, test loss: 0.26214891695039305\n",
      "epoch 4359: train loss: 0.15917452638481314, test loss: 0.26214324760326346\n",
      "epoch 4360: train loss: 0.15916306455055212, test loss: 0.26213758145562255\n",
      "epoch 4361: train loss: 0.15915160652012278, test loss: 0.262131918505339\n",
      "epoch 4362: train loss: 0.15914015229137493, test loss: 0.2621262587502791\n",
      "epoch 4363: train loss: 0.15912870186216005, test loss: 0.2621206021883136\n",
      "epoch 4364: train loss: 0.1591172552303313, test loss: 0.2621149488173191\n",
      "epoch 4365: train loss: 0.15910581239374363, test loss: 0.26210929863516386\n",
      "epoch 4366: train loss: 0.15909437335025373, test loss: 0.26210365163973043\n",
      "epoch 4367: train loss: 0.1590829380977199, test loss: 0.26209800782889353\n",
      "epoch 4368: train loss: 0.1590715066340023, test loss: 0.2620923672005325\n",
      "epoch 4369: train loss: 0.15906007895696278, test loss: 0.2620867297525315\n",
      "epoch 4370: train loss: 0.15904865506446483, test loss: 0.2620810954827746\n",
      "epoch 4371: train loss: 0.15903723495437375, test loss: 0.26207546438914325\n",
      "epoch 4372: train loss: 0.15902581862455648, test loss: 0.2620698364695259\n",
      "epoch 4373: train loss: 0.15901440607288175, test loss: 0.262064211721812\n",
      "epoch 4374: train loss: 0.15900299729721992, test loss: 0.2620585901438954\n",
      "epoch 4375: train loss: 0.15899159229544316, test loss: 0.2620529717336684\n",
      "epoch 4376: train loss: 0.15898019106542527, test loss: 0.26204735648902144\n",
      "epoch 4377: train loss: 0.15896879360504174, test loss: 0.2620417444078521\n",
      "epoch 4378: train loss: 0.1589573999121698, test loss: 0.2620361354880599\n",
      "epoch 4379: train loss: 0.1589460099846884, test loss: 0.26203052972754326\n",
      "epoch 4380: train loss: 0.15893462382047827, test loss: 0.2620249271242064\n",
      "epoch 4381: train loss: 0.1589232414174216, test loss: 0.26201932767595215\n",
      "epoch 4382: train loss: 0.15891186277340247, test loss: 0.2620137313806824\n",
      "epoch 4383: train loss: 0.15890048788630662, test loss: 0.2620081382363068\n",
      "epoch 4384: train loss: 0.15888911675402148, test loss: 0.2620025482407347\n",
      "epoch 4385: train loss: 0.1588777493744361, test loss: 0.26199696139187423\n",
      "epoch 4386: train loss: 0.1588663857454414, test loss: 0.2619913776876417\n",
      "epoch 4387: train loss: 0.15885502586492975, test loss: 0.2619857971259506\n",
      "epoch 4388: train loss: 0.15884366973079542, test loss: 0.26198021970471413\n",
      "epoch 4389: train loss: 0.1588323173409342, test loss: 0.2619746454218509\n",
      "epoch 4390: train loss: 0.1588209686932437, test loss: 0.2619690742752832\n",
      "epoch 4391: train loss: 0.15880962378562308, test loss: 0.2619635062629292\n",
      "epoch 4392: train loss: 0.15879828261597326, test loss: 0.2619579413827142\n",
      "epoch 4393: train loss: 0.15878694518219688, test loss: 0.2619523796325622\n",
      "epoch 4394: train loss: 0.15877561148219815, test loss: 0.26194682101040206\n",
      "epoch 4395: train loss: 0.15876428151388303, test loss: 0.2619412655141574\n",
      "epoch 4396: train loss: 0.1587529552751591, test loss: 0.2619357131417627\n",
      "epoch 4397: train loss: 0.15874163276393563, test loss: 0.26193016389114787\n",
      "epoch 4398: train loss: 0.15873031397812362, test loss: 0.2619246177602495\n",
      "epoch 4399: train loss: 0.15871899891563565, test loss: 0.2619190747470026\n",
      "epoch 4400: train loss: 0.158707687574386, test loss: 0.261913534849339\n",
      "epoch 4401: train loss: 0.15869637995229063, test loss: 0.26190799806520315\n",
      "epoch 4402: train loss: 0.1586850760472671, test loss: 0.2619024643925359\n",
      "epoch 4403: train loss: 0.15867377585723472, test loss: 0.2618969338292763\n",
      "epoch 4404: train loss: 0.15866247938011438, test loss: 0.2618914063733751\n",
      "epoch 4405: train loss: 0.15865118661382868, test loss: 0.2618858820227698\n",
      "epoch 4406: train loss: 0.15863989755630187, test loss: 0.26188036077541627\n",
      "epoch 4407: train loss: 0.15862861220545982, test loss: 0.2618748426292601\n",
      "epoch 4408: train loss: 0.15861733055923005, test loss: 0.26186932758225\n",
      "epoch 4409: train loss: 0.1586060526155418, test loss: 0.26186381563234606\n",
      "epoch 4410: train loss: 0.15859477837232588, test loss: 0.26185830677749866\n",
      "epoch 4411: train loss: 0.15858350782751476, test loss: 0.2618528010156643\n",
      "epoch 4412: train loss: 0.15857224097904263, test loss: 0.2618472983448046\n",
      "epoch 4413: train loss: 0.15856097782484516, test loss: 0.2618417987628755\n",
      "epoch 4414: train loss: 0.15854971836285986, test loss: 0.2618363022678398\n",
      "epoch 4415: train loss: 0.15853846259102572, test loss: 0.2618308088576638\n",
      "epoch 4416: train loss: 0.1585272105072835, test loss: 0.2618253185303099\n",
      "epoch 4417: train loss: 0.15851596210957544, test loss: 0.261819831283747\n",
      "epoch 4418: train loss: 0.15850471739584557, test loss: 0.26181434711594215\n",
      "epoch 4419: train loss: 0.15849347636403943, test loss: 0.26180886602486725\n",
      "epoch 4420: train loss: 0.1584822390121043, test loss: 0.2618033880084937\n",
      "epoch 4421: train loss: 0.158471005337989, test loss: 0.2617979130647954\n",
      "epoch 4422: train loss: 0.158459775339644, test loss: 0.26179244119174894\n",
      "epoch 4423: train loss: 0.15844854901502142, test loss: 0.26178697238733234\n",
      "epoch 4424: train loss: 0.15843732636207503, test loss: 0.26178150664952404\n",
      "epoch 4425: train loss: 0.15842610737876012, test loss: 0.2617760439763024\n",
      "epoch 4426: train loss: 0.15841489206303372, test loss: 0.2617705843656514\n",
      "epoch 4427: train loss: 0.15840368041285444, test loss: 0.26176512781555894\n",
      "epoch 4428: train loss: 0.15839247242618237, test loss: 0.2617596743240042\n",
      "epoch 4429: train loss: 0.15838126810097947, test loss: 0.2617542238889818\n",
      "epoch 4430: train loss: 0.15837006743520912, test loss: 0.26174877650847717\n",
      "epoch 4431: train loss: 0.15835887042683638, test loss: 0.26174333218048246\n",
      "epoch 4432: train loss: 0.15834767707382796, test loss: 0.2617378909029889\n",
      "epoch 4433: train loss: 0.15833648737415204, test loss: 0.26173245267399586\n",
      "epoch 4434: train loss: 0.15832530132577857, test loss: 0.26172701749149285\n",
      "epoch 4435: train loss: 0.15831411892667901, test loss: 0.2617215853534838\n",
      "epoch 4436: train loss: 0.1583029401748265, test loss: 0.2617161562579618\n",
      "epoch 4437: train loss: 0.15829176506819567, test loss: 0.2617107302029356\n",
      "epoch 4438: train loss: 0.15828059360476282, test loss: 0.2617053071864046\n",
      "epoch 4439: train loss: 0.15826942578250588, test loss: 0.2616998872063716\n",
      "epoch 4440: train loss: 0.15825826159940432, test loss: 0.261694470260845\n",
      "epoch 4441: train loss: 0.15824710105343923, test loss: 0.2616890563478346\n",
      "epoch 4442: train loss: 0.1582359441425933, test loss: 0.26168364546534995\n",
      "epoch 4443: train loss: 0.15822479086485075, test loss: 0.26167823761139836\n",
      "epoch 4444: train loss: 0.15821364121819748, test loss: 0.2616728327839949\n",
      "epoch 4445: train loss: 0.15820249520062096, test loss: 0.26166743098115647\n",
      "epoch 4446: train loss: 0.15819135281011018, test loss: 0.2616620322008993\n",
      "epoch 4447: train loss: 0.15818021404465582, test loss: 0.2616566364412409\n",
      "epoch 4448: train loss: 0.15816907890225004, test loss: 0.26165124370020093\n",
      "epoch 4449: train loss: 0.15815794738088668, test loss: 0.26164585397579887\n",
      "epoch 4450: train loss: 0.15814681947856107, test loss: 0.2616404672660614\n",
      "epoch 4451: train loss: 0.1581356951932702, test loss: 0.26163508356901083\n",
      "epoch 4452: train loss: 0.15812457452301257, test loss: 0.26162970288267823\n",
      "epoch 4453: train loss: 0.15811345746578825, test loss: 0.26162432520508544\n",
      "epoch 4454: train loss: 0.15810234401959897, test loss: 0.26161895053426754\n",
      "epoch 4455: train loss: 0.15809123418244803, test loss: 0.2616135788682518\n",
      "epoch 4456: train loss: 0.15808012795234017, test loss: 0.2616082102050767\n",
      "epoch 4457: train loss: 0.15806902532728176, test loss: 0.26160284454277355\n",
      "epoch 4458: train loss: 0.15805792630528084, test loss: 0.26159748187937754\n",
      "epoch 4459: train loss: 0.15804683088434693, test loss: 0.26159212221293415\n",
      "epoch 4460: train loss: 0.1580357390624911, test loss: 0.26158676554147287\n",
      "epoch 4461: train loss: 0.15802465083772596, test loss: 0.26158141186304185\n",
      "epoch 4462: train loss: 0.1580135662080658, test loss: 0.26157606117568577\n",
      "epoch 4463: train loss: 0.15800248517152635, test loss: 0.26157071347744315\n",
      "epoch 4464: train loss: 0.15799140772612494, test loss: 0.2615653687663659\n",
      "epoch 4465: train loss: 0.15798033386988045, test loss: 0.2615600270405015\n",
      "epoch 4466: train loss: 0.15796926360081337, test loss: 0.2615546882978964\n",
      "epoch 4467: train loss: 0.1579581969169457, test loss: 0.2615493525366044\n",
      "epoch 4468: train loss: 0.15794713381630093, test loss: 0.2615440197546801\n",
      "epoch 4469: train loss: 0.1579360742969042, test loss: 0.2615386899501746\n",
      "epoch 4470: train loss: 0.1579250183567822, test loss: 0.26153336312114706\n",
      "epoch 4471: train loss: 0.15791396599396298, test loss: 0.26152803926565554\n",
      "epoch 4472: train loss: 0.15790291720647648, test loss: 0.2615227183817582\n",
      "epoch 4473: train loss: 0.15789187199235383, test loss: 0.2615174004675175\n",
      "epoch 4474: train loss: 0.1578808303496279, test loss: 0.2615120855209955\n",
      "epoch 4475: train loss: 0.1578697922763331, test loss: 0.26150677354025853\n",
      "epoch 4476: train loss: 0.1578587577705053, test loss: 0.26150146452337153\n",
      "epoch 4477: train loss: 0.1578477268301819, test loss: 0.2614961584683993\n",
      "epoch 4478: train loss: 0.15783669945340192, test loss: 0.261490855373416\n",
      "epoch 4479: train loss: 0.15782567563820593, test loss: 0.26148555523649086\n",
      "epoch 4480: train loss: 0.15781465538263587, test loss: 0.2614802580556983\n",
      "epoch 4481: train loss: 0.1578036386847354, test loss: 0.2614749638291103\n",
      "epoch 4482: train loss: 0.1577926255425496, test loss: 0.2614696725548019\n",
      "epoch 4483: train loss: 0.15778161595412507, test loss: 0.2614643842308513\n",
      "epoch 4484: train loss: 0.15777060991751, test loss: 0.26145909885534097\n",
      "epoch 4485: train loss: 0.1577596074307541, test loss: 0.26145381642634924\n",
      "epoch 4486: train loss: 0.15774860849190853, test loss: 0.261448536941956\n",
      "epoch 4487: train loss: 0.15773761309902604, test loss: 0.26144326040025195\n",
      "epoch 4488: train loss: 0.15772662125016088, test loss: 0.2614379867993172\n",
      "epoch 4489: train loss: 0.15771563294336885, test loss: 0.26143271613723845\n",
      "epoch 4490: train loss: 0.15770464817670718, test loss: 0.26142744841210924\n",
      "epoch 4491: train loss: 0.15769366694823467, test loss: 0.26142218362201386\n",
      "epoch 4492: train loss: 0.1576826892560117, test loss: 0.2614169217650533\n",
      "epoch 4493: train loss: 0.15767171509810005, test loss: 0.2614116628393126\n",
      "epoch 4494: train loss: 0.1576607444725631, test loss: 0.26140640684288863\n",
      "epoch 4495: train loss: 0.1576497773774656, test loss: 0.2614011537738828\n",
      "epoch 4496: train loss: 0.15763881381087402, test loss: 0.2613959036303903\n",
      "epoch 4497: train loss: 0.15762785377085614, test loss: 0.26139065641051007\n",
      "epoch 4498: train loss: 0.15761689725548136, test loss: 0.2613854121123456\n",
      "epoch 4499: train loss: 0.15760594426282054, test loss: 0.2613801707339995\n",
      "epoch 4500: train loss: 0.15759499479094605, test loss: 0.2613749322735778\n",
      "epoch 4501: train loss: 0.15758404883793178, test loss: 0.26136969672918375\n",
      "epoch 4502: train loss: 0.15757310640185307, test loss: 0.26136446409892905\n",
      "epoch 4503: train loss: 0.15756216748078683, test loss: 0.2613592343809237\n",
      "epoch 4504: train loss: 0.15755123207281135, test loss: 0.2613540075732745\n",
      "epoch 4505: train loss: 0.15754030017600656, test loss: 0.2613487836740976\n",
      "epoch 4506: train loss: 0.15752937178845372, test loss: 0.2613435626815045\n",
      "epoch 4507: train loss: 0.15751844690823577, test loss: 0.26133834459361543\n",
      "epoch 4508: train loss: 0.15750752553343694, test loss: 0.26133312940854636\n",
      "epoch 4509: train loss: 0.15749660766214307, test loss: 0.26132791712441333\n",
      "epoch 4510: train loss: 0.15748569329244155, test loss: 0.2613227077393405\n",
      "epoch 4511: train loss: 0.15747478242242108, test loss: 0.2613175012514483\n",
      "epoch 4512: train loss: 0.1574638750501719, test loss: 0.26131229765886127\n",
      "epoch 4513: train loss: 0.15745297117378582, test loss: 0.2613070969597047\n",
      "epoch 4514: train loss: 0.15744207079135605, test loss: 0.2613018991521058\n",
      "epoch 4515: train loss: 0.15743117390097733, test loss: 0.2612967042341925\n",
      "epoch 4516: train loss: 0.1574202805007458, test loss: 0.2612915122040944\n",
      "epoch 4517: train loss: 0.15740939058875913, test loss: 0.2612863230599442\n",
      "epoch 4518: train loss: 0.15739850416311651, test loss: 0.2612811367998758\n",
      "epoch 4519: train loss: 0.15738762122191846, test loss: 0.2612759534220206\n",
      "epoch 4520: train loss: 0.15737674176326716, test loss: 0.2612707729245177\n",
      "epoch 4521: train loss: 0.1573658657852661, test loss: 0.26126559530550664\n",
      "epoch 4522: train loss: 0.1573549932860203, test loss: 0.2612604205631221\n",
      "epoch 4523: train loss: 0.1573441242636362, test loss: 0.26125524869551003\n",
      "epoch 4524: train loss: 0.1573332587162218, test loss: 0.2612500797008096\n",
      "epoch 4525: train loss: 0.15732239664188655, test loss: 0.2612449135771652\n",
      "epoch 4526: train loss: 0.1573115380387413, test loss: 0.2612397503227253\n",
      "epoch 4527: train loss: 0.15730068290489835, test loss: 0.2612345899356332\n",
      "epoch 4528: train loss: 0.15728983123847154, test loss: 0.2612294324140414\n",
      "epoch 4529: train loss: 0.15727898303757612, test loss: 0.2612242777560958\n",
      "epoch 4530: train loss: 0.1572681383003287, test loss: 0.26121912595995134\n",
      "epoch 4531: train loss: 0.1572572970248476, test loss: 0.2612139770237593\n",
      "epoch 4532: train loss: 0.15724645920925234, test loss: 0.26120883094567793\n",
      "epoch 4533: train loss: 0.157235624851664, test loss: 0.26120368772386193\n",
      "epoch 4534: train loss: 0.15722479395020517, test loss: 0.2611985473564704\n",
      "epoch 4535: train loss: 0.15721396650299974, test loss: 0.2611934098416575\n",
      "epoch 4536: train loss: 0.15720314250817313, test loss: 0.2611882751775913\n",
      "epoch 4537: train loss: 0.15719232196385222, test loss: 0.26118314336243015\n",
      "epoch 4538: train loss: 0.15718150486816534, test loss: 0.2611780143943409\n",
      "epoch 4539: train loss: 0.15717069121924218, test loss: 0.2611728882714878\n",
      "epoch 4540: train loss: 0.15715988101521397, test loss: 0.26116776499203537\n",
      "epoch 4541: train loss: 0.15714907425421337, test loss: 0.26116264455415567\n",
      "epoch 4542: train loss: 0.1571382709343744, test loss: 0.26115752695602024\n",
      "epoch 4543: train loss: 0.15712747105383257, test loss: 0.26115241219579716\n",
      "epoch 4544: train loss: 0.15711667461072487, test loss: 0.2611473002716613\n",
      "epoch 4545: train loss: 0.15710588160318964, test loss: 0.26114219118178933\n",
      "epoch 4546: train loss: 0.1570950920293667, test loss: 0.26113708492435495\n",
      "epoch 4547: train loss: 0.1570843058873973, test loss: 0.2611319814975344\n",
      "epoch 4548: train loss: 0.15707352317542406, test loss: 0.26112688089951186\n",
      "epoch 4549: train loss: 0.1570627438915911, test loss: 0.2611217831284632\n",
      "epoch 4550: train loss: 0.157051968034044, test loss: 0.26111668818257583\n",
      "epoch 4551: train loss: 0.1570411956009297, test loss: 0.26111159606002926\n",
      "epoch 4552: train loss: 0.15703042659039654, test loss: 0.26110650675901004\n",
      "epoch 4553: train loss: 0.15701966100059425, test loss: 0.26110142027770766\n",
      "epoch 4554: train loss: 0.15700889882967423, test loss: 0.261096336614309\n",
      "epoch 4555: train loss: 0.156998140075789, test loss: 0.261091255767001\n",
      "epoch 4556: train loss: 0.15698738473709264, test loss: 0.2610861777339802\n",
      "epoch 4557: train loss: 0.15697663281174062, test loss: 0.26108110251343464\n",
      "epoch 4558: train loss: 0.15696588429788982, test loss: 0.26107603010356356\n",
      "epoch 4559: train loss: 0.15695513919369863, test loss: 0.26107096050255857\n",
      "epoch 4560: train loss: 0.15694439749732664, test loss: 0.2610658937086187\n",
      "epoch 4561: train loss: 0.15693365920693508, test loss: 0.2610608297199453\n",
      "epoch 4562: train loss: 0.1569229243206864, test loss: 0.2610557685347321\n",
      "epoch 4563: train loss: 0.15691219283674462, test loss: 0.2610507101511893\n",
      "epoch 4564: train loss: 0.1569014647532751, test loss: 0.2610456545675152\n",
      "epoch 4565: train loss: 0.15689074006844458, test loss: 0.26104060178191524\n",
      "epoch 4566: train loss: 0.15688001878042118, test loss: 0.26103555179259896\n",
      "epoch 4567: train loss: 0.15686930088737452, test loss: 0.26103050459776767\n",
      "epoch 4568: train loss: 0.15685858638747555, test loss: 0.26102546019563433\n",
      "epoch 4569: train loss: 0.15684787527889665, test loss: 0.26102041858441544\n",
      "epoch 4570: train loss: 0.15683716755981159, test loss: 0.2610153797623142\n",
      "epoch 4571: train loss: 0.1568264632283955, test loss: 0.26101034372754844\n",
      "epoch 4572: train loss: 0.15681576228282498, test loss: 0.2610053104783319\n",
      "epoch 4573: train loss: 0.156805064721278, test loss: 0.2610002800128858\n",
      "epoch 4574: train loss: 0.15679437054193382, test loss: 0.2609952523294225\n",
      "epoch 4575: train loss: 0.1567836797429733, test loss: 0.2609902274261653\n",
      "epoch 4576: train loss: 0.15677299232257844, test loss: 0.2609852053013343\n",
      "epoch 4577: train loss: 0.15676230827893284, test loss: 0.2609801859531501\n",
      "epoch 4578: train loss: 0.15675162761022143, test loss: 0.26097516937984094\n",
      "epoch 4579: train loss: 0.15674095031463042, test loss: 0.2609701555796314\n",
      "epoch 4580: train loss: 0.15673027639034756, test loss: 0.2609651445507464\n",
      "epoch 4581: train loss: 0.15671960583556185, test loss: 0.2609601362914152\n",
      "epoch 4582: train loss: 0.15670893864846375, test loss: 0.2609551307998683\n",
      "epoch 4583: train loss: 0.1566982748272451, test loss: 0.26095012807433643\n",
      "epoch 4584: train loss: 0.15668761437009907, test loss: 0.2609451281130528\n",
      "epoch 4585: train loss: 0.15667695727522027, test loss: 0.26094013091425344\n",
      "epoch 4586: train loss: 0.15666630354080463, test loss: 0.2609351364761708\n",
      "epoch 4587: train loss: 0.15665565316504948, test loss: 0.26093014479704546\n",
      "epoch 4588: train loss: 0.15664500614615354, test loss: 0.26092515587511417\n",
      "epoch 4589: train loss: 0.15663436248231688, test loss: 0.26092016970861664\n",
      "epoch 4590: train loss: 0.15662372217174095, test loss: 0.2609151862957968\n",
      "epoch 4591: train loss: 0.1566130852126285, test loss: 0.2609102056348966\n",
      "epoch 4592: train loss: 0.1566024516031838, test loss: 0.260905227724159\n",
      "epoch 4593: train loss: 0.15659182134161237, test loss: 0.26090025256183547\n",
      "epoch 4594: train loss: 0.15658119442612115, test loss: 0.260895280146165\n",
      "epoch 4595: train loss: 0.15657057085491835, test loss: 0.26089031047540273\n",
      "epoch 4596: train loss: 0.15655995062621367, test loss: 0.2608853435477963\n",
      "epoch 4597: train loss: 0.15654933373821808, test loss: 0.2608803793615979\n",
      "epoch 4598: train loss: 0.15653872018914397, test loss: 0.2608754179150633\n",
      "epoch 4599: train loss: 0.15652810997720504, test loss: 0.26087045920644236\n",
      "epoch 4600: train loss: 0.15651750310061638, test loss: 0.2608655032339975\n",
      "epoch 4601: train loss: 0.15650689955759442, test loss: 0.2608605499959797\n",
      "epoch 4602: train loss: 0.15649629934635695, test loss: 0.2608555994906518\n",
      "epoch 4603: train loss: 0.15648570246512306, test loss: 0.26085065171627136\n",
      "epoch 4604: train loss: 0.15647510891211336, test loss: 0.26084570667110496\n",
      "epoch 4605: train loss: 0.1564645186855496, test loss: 0.2608407643534085\n",
      "epoch 4606: train loss: 0.156453931783655, test loss: 0.2608358247614537\n",
      "epoch 4607: train loss: 0.15644334820465408, test loss: 0.2608308878935059\n",
      "epoch 4608: train loss: 0.15643276794677274, test loss: 0.26082595374782686\n",
      "epoch 4609: train loss: 0.15642219100823823, test loss: 0.2608210223226914\n",
      "epoch 4610: train loss: 0.1564116173872791, test loss: 0.2608160936163668\n",
      "epoch 4611: train loss: 0.15640104708212527, test loss: 0.2608111676271275\n",
      "epoch 4612: train loss: 0.15639048009100803, test loss: 0.2608062443532455\n",
      "epoch 4613: train loss: 0.15637991641215995, test loss: 0.26080132379299464\n",
      "epoch 4614: train loss: 0.15636935604381497, test loss: 0.26079640594465303\n",
      "epoch 4615: train loss: 0.15635879898420835, test loss: 0.2607914908064954\n",
      "epoch 4616: train loss: 0.1563482452315767, test loss: 0.26078657837679925\n",
      "epoch 4617: train loss: 0.15633769478415802, test loss: 0.26078166865385155\n",
      "epoch 4618: train loss: 0.15632714764019148, test loss: 0.2607767616359293\n",
      "epoch 4619: train loss: 0.15631660379791779, test loss: 0.26077185732131447\n",
      "epoch 4620: train loss: 0.15630606325557886, test loss: 0.26076695570829606\n",
      "epoch 4621: train loss: 0.15629552601141788, test loss: 0.2607620567951552\n",
      "epoch 4622: train loss: 0.15628499206367955, test loss: 0.26075716058018417\n",
      "epoch 4623: train loss: 0.1562744614106098, test loss: 0.26075226706166527\n",
      "epoch 4624: train loss: 0.15626393405045577, test loss: 0.26074737623789795\n",
      "epoch 4625: train loss: 0.1562534099814661, test loss: 0.26074248810716527\n",
      "epoch 4626: train loss: 0.15624288920189067, test loss: 0.26073760266776225\n",
      "epoch 4627: train loss: 0.15623237170998072, test loss: 0.2607327199179845\n",
      "epoch 4628: train loss: 0.15622185750398873, test loss: 0.26072783985612963\n",
      "epoch 4629: train loss: 0.1562113465821686, test loss: 0.26072296248049204\n",
      "epoch 4630: train loss: 0.15620083894277548, test loss: 0.2607180877893714\n",
      "epoch 4631: train loss: 0.15619033458406587, test loss: 0.26071321578106676\n",
      "epoch 4632: train loss: 0.1561798335042975, test loss: 0.2607083464538825\n",
      "epoch 4633: train loss: 0.15616933570172964, test loss: 0.26070347980611597\n",
      "epoch 4634: train loss: 0.1561588411746225, test loss: 0.2606986158360772\n",
      "epoch 4635: train loss: 0.15614834992123805, test loss: 0.2606937545420662\n",
      "epoch 4636: train loss: 0.15613786193983914, test loss: 0.26068889592239364\n",
      "epoch 4637: train loss: 0.15612737722869022, test loss: 0.260684039975368\n",
      "epoch 4638: train loss: 0.15611689578605692, test loss: 0.26067918669929496\n",
      "epoch 4639: train loss: 0.15610641761020624, test loss: 0.26067433609248686\n",
      "epoch 4640: train loss: 0.15609594269940644, test loss: 0.2606694881532632\n",
      "epoch 4641: train loss: 0.15608547105192708, test loss: 0.2606646428799282\n",
      "epoch 4642: train loss: 0.15607500266603902, test loss: 0.2606598002708021\n",
      "epoch 4643: train loss: 0.15606453754001445, test loss: 0.26065496032420216\n",
      "epoch 4644: train loss: 0.15605407567212687, test loss: 0.26065012303844254\n",
      "epoch 4645: train loss: 0.15604361706065104, test loss: 0.2606452884118462\n",
      "epoch 4646: train loss: 0.15603316170386297, test loss: 0.2606404564427331\n",
      "epoch 4647: train loss: 0.1560227096000401, test loss: 0.2606356271294243\n",
      "epoch 4648: train loss: 0.15601226074746108, test loss: 0.26063080047024373\n",
      "epoch 4649: train loss: 0.1560018151444058, test loss: 0.26062597646351426\n",
      "epoch 4650: train loss: 0.1559913727891556, test loss: 0.26062115510756556\n",
      "epoch 4651: train loss: 0.1559809336799929, test loss: 0.2606163364007251\n",
      "epoch 4652: train loss: 0.1559704978152016, test loss: 0.26061152034131835\n",
      "epoch 4653: train loss: 0.15596006519306682, test loss: 0.26060670692767807\n",
      "epoch 4654: train loss: 0.15594963581187488, test loss: 0.26060189615813373\n",
      "epoch 4655: train loss: 0.15593920966991356, test loss: 0.26059708803102244\n",
      "epoch 4656: train loss: 0.15592878676547173, test loss: 0.260592282544677\n",
      "epoch 4657: train loss: 0.15591836709683965, test loss: 0.26058747969743074\n",
      "epoch 4658: train loss: 0.15590795066230886, test loss: 0.260582679487622\n",
      "epoch 4659: train loss: 0.15589753746017224, test loss: 0.26057788191359016\n",
      "epoch 4660: train loss: 0.1558871274887238, test loss: 0.2605730869736749\n",
      "epoch 4661: train loss: 0.15587672074625888, test loss: 0.26056829466621595\n",
      "epoch 4662: train loss: 0.15586631723107425, test loss: 0.26056350498956\n",
      "epoch 4663: train loss: 0.15585591694146764, test loss: 0.26055871794204366\n",
      "epoch 4664: train loss: 0.15584551987573836, test loss: 0.2605539335220212\n",
      "epoch 4665: train loss: 0.1558351260321869, test loss: 0.26054915172782867\n",
      "epoch 4666: train loss: 0.1558247354091149, test loss: 0.26054437255782564\n",
      "epoch 4667: train loss: 0.1558143480048254, test loss: 0.260539596010352\n",
      "epoch 4668: train loss: 0.15580396381762268, test loss: 0.2605348220837624\n",
      "epoch 4669: train loss: 0.1557935828458123, test loss: 0.2605300507764079\n",
      "epoch 4670: train loss: 0.15578320508770105, test loss: 0.26052528208664183\n",
      "epoch 4671: train loss: 0.15577283054159696, test loss: 0.2605205160128187\n",
      "epoch 4672: train loss: 0.1557624592058094, test loss: 0.260515752553296\n",
      "epoch 4673: train loss: 0.15575209107864899, test loss: 0.26051099170642755\n",
      "epoch 4674: train loss: 0.15574172615842755, test loss: 0.26050623347057467\n",
      "epoch 4675: train loss: 0.1557313644434582, test loss: 0.2605014778440991\n",
      "epoch 4676: train loss: 0.1557210059320553, test loss: 0.2604967248253596\n",
      "epoch 4677: train loss: 0.15571065062253456, test loss: 0.26049197441271793\n",
      "epoch 4678: train loss: 0.15570029851321282, test loss: 0.2604872266045404\n",
      "epoch 4679: train loss: 0.15568994960240823, test loss: 0.2604824813991927\n",
      "epoch 4680: train loss: 0.15567960388844018, test loss: 0.26047773879503816\n",
      "epoch 4681: train loss: 0.15566926136962933, test loss: 0.2604729987904467\n",
      "epoch 4682: train loss: 0.1556589220442976, test loss: 0.2604682613837879\n",
      "epoch 4683: train loss: 0.15564858591076816, test loss: 0.26046352657343036\n",
      "epoch 4684: train loss: 0.15563825296736533, test loss: 0.26045879435774993\n",
      "epoch 4685: train loss: 0.15562792321241484, test loss: 0.26045406473511884\n",
      "epoch 4686: train loss: 0.15561759664424357, test loss: 0.2604493377039086\n",
      "epoch 4687: train loss: 0.15560727326117965, test loss: 0.26044461326249635\n",
      "epoch 4688: train loss: 0.15559695306155252, test loss: 0.2604398914092612\n",
      "epoch 4689: train loss: 0.15558663604369272, test loss: 0.2604351721425809\n",
      "epoch 4690: train loss: 0.15557632220593218, test loss: 0.26043045546083454\n",
      "epoch 4691: train loss: 0.15556601154660402, test loss: 0.26042574136240043\n",
      "epoch 4692: train loss: 0.15555570406404254, test loss: 0.260421029845668\n",
      "epoch 4693: train loss: 0.1555453997565834, test loss: 0.26041632090901595\n",
      "epoch 4694: train loss: 0.15553509862256337, test loss: 0.26041161455083156\n",
      "epoch 4695: train loss: 0.15552480066032054, test loss: 0.2604069107695003\n",
      "epoch 4696: train loss: 0.1555145058681942, test loss: 0.26040220956340854\n",
      "epoch 4697: train loss: 0.15550421424452487, test loss: 0.2603975109309479\n",
      "epoch 4698: train loss: 0.15549392578765434, test loss: 0.2603928148705063\n",
      "epoch 4699: train loss: 0.15548364049592556, test loss: 0.26038812138047923\n",
      "epoch 4700: train loss: 0.1554733583676828, test loss: 0.26038343045925283\n",
      "epoch 4701: train loss: 0.1554630794012715, test loss: 0.26037874210523015\n",
      "epoch 4702: train loss: 0.15545280359503832, test loss: 0.26037405631679983\n",
      "epoch 4703: train loss: 0.15544253094733118, test loss: 0.260369373092359\n",
      "epoch 4704: train loss: 0.1554322614564992, test loss: 0.26036469243030924\n",
      "epoch 4705: train loss: 0.1554219951208927, test loss: 0.2603600143290508\n",
      "epoch 4706: train loss: 0.1554117319388633, test loss: 0.26035533878697886\n",
      "epoch 4707: train loss: 0.15540147190876383, test loss: 0.26035066580250255\n",
      "epoch 4708: train loss: 0.15539121502894823, test loss: 0.2603459953740191\n",
      "epoch 4709: train loss: 0.15538096129777176, test loss: 0.26034132749993216\n",
      "epoch 4710: train loss: 0.15537071071359093, test loss: 0.2603366621786553\n",
      "epoch 4711: train loss: 0.15536046327476336, test loss: 0.26033199940859264\n",
      "epoch 4712: train loss: 0.1553502189796479, test loss: 0.2603273391881467\n",
      "epoch 4713: train loss: 0.15533997782660472, test loss: 0.2603226815157338\n",
      "epoch 4714: train loss: 0.1553297398139951, test loss: 0.2603180263897623\n",
      "epoch 4715: train loss: 0.15531950494018154, test loss: 0.2603133738086447\n",
      "epoch 4716: train loss: 0.15530927320352783, test loss: 0.2603087237707993\n",
      "epoch 4717: train loss: 0.15529904460239888, test loss: 0.26030407627463104\n",
      "epoch 4718: train loss: 0.15528881913516088, test loss: 0.2602994313185647\n",
      "epoch 4719: train loss: 0.15527859680018113, test loss: 0.2602947889010147\n",
      "epoch 4720: train loss: 0.15526837759582823, test loss: 0.26029014902040004\n",
      "epoch 4721: train loss: 0.15525816152047195, test loss: 0.26028551167514175\n",
      "epoch 4722: train loss: 0.15524794857248325, test loss: 0.2602808768636588\n",
      "epoch 4723: train loss: 0.15523773875023433, test loss: 0.2602762445843734\n",
      "epoch 4724: train loss: 0.1552275320520986, test loss: 0.2602716148357132\n",
      "epoch 4725: train loss: 0.15521732847645053, test loss: 0.26026698761609796\n",
      "epoch 4726: train loss: 0.155207128021666, test loss: 0.26026236292396104\n",
      "epoch 4727: train loss: 0.15519693068612195, test loss: 0.2602577407577231\n",
      "epoch 4728: train loss: 0.15518673646819653, test loss: 0.26025312111581506\n",
      "epoch 4729: train loss: 0.15517654536626913, test loss: 0.2602485039966718\n",
      "epoch 4730: train loss: 0.15516635737872037, test loss: 0.2602438893987192\n",
      "epoch 4731: train loss: 0.1551561725039319, test loss: 0.2602392773203922\n",
      "epoch 4732: train loss: 0.1551459907402867, test loss: 0.2602346677601231\n",
      "epoch 4733: train loss: 0.15513581208616894, test loss: 0.2602300607163515\n",
      "epoch 4734: train loss: 0.15512563653996392, test loss: 0.26022545618750903\n",
      "epoch 4735: train loss: 0.1551154641000582, test loss: 0.26022085417203356\n",
      "epoch 4736: train loss: 0.15510529476483942, test loss: 0.26021625466836773\n",
      "epoch 4737: train loss: 0.15509512853269647, test loss: 0.2602116576749499\n",
      "epoch 4738: train loss: 0.15508496540201955, test loss: 0.2602070631902188\n",
      "epoch 4739: train loss: 0.15507480537119972, test loss: 0.2602024712126228\n",
      "epoch 4740: train loss: 0.1550646484386296, test loss: 0.2601978817406009\n",
      "epoch 4741: train loss: 0.1550544946027027, test loss: 0.260193294772602\n",
      "epoch 4742: train loss: 0.1550443438618139, test loss: 0.2601887103070697\n",
      "epoch 4743: train loss: 0.15503419621435907, test loss: 0.2601841283424542\n",
      "epoch 4744: train loss: 0.15502405165873553, test loss: 0.2601795488772011\n",
      "epoch 4745: train loss: 0.15501391019334151, test loss: 0.2601749719097662\n",
      "epoch 4746: train loss: 0.15500377181657654, test loss: 0.26017039743859416\n",
      "epoch 4747: train loss: 0.1549936365268414, test loss: 0.2601658254621452\n",
      "epoch 4748: train loss: 0.15498350432253777, test loss: 0.2601612559788682\n",
      "epoch 4749: train loss: 0.15497337520206883, test loss: 0.26015668898721755\n",
      "epoch 4750: train loss: 0.15496324916383877, test loss: 0.26015212448565284\n",
      "epoch 4751: train loss: 0.15495312620625296, test loss: 0.26014756247263393\n",
      "epoch 4752: train loss: 0.15494300632771793, test loss: 0.2601430029466146\n",
      "epoch 4753: train loss: 0.15493288952664142, test loss: 0.26013844590605645\n",
      "epoch 4754: train loss: 0.15492277580143227, test loss: 0.26013389134941933\n",
      "epoch 4755: train loss: 0.15491266515050056, test loss: 0.26012933927516935\n",
      "epoch 4756: train loss: 0.15490255757225754, test loss: 0.2601247896817713\n",
      "epoch 4757: train loss: 0.1548924530651155, test loss: 0.260120242567689\n",
      "epoch 4758: train loss: 0.15488235162748804, test loss: 0.26011569793138506\n",
      "epoch 4759: train loss: 0.1548722532577899, test loss: 0.2601111557713309\n",
      "epoch 4760: train loss: 0.1548621579544368, test loss: 0.2601066160859953\n",
      "epoch 4761: train loss: 0.15485206571584592, test loss: 0.2601020788738464\n",
      "epoch 4762: train loss: 0.15484197654043533, test loss: 0.26009754413335673\n",
      "epoch 4763: train loss: 0.15483189042662443, test loss: 0.26009301186300066\n",
      "epoch 4764: train loss: 0.15482180737283366, test loss: 0.26008848206124663\n",
      "epoch 4765: train loss: 0.15481172737748475, test loss: 0.26008395472657264\n",
      "epoch 4766: train loss: 0.15480165043900038, test loss: 0.26007942985745675\n",
      "epoch 4767: train loss: 0.15479157655580458, test loss: 0.2600749074523741\n",
      "epoch 4768: train loss: 0.15478150572632246, test loss: 0.26007038750980466\n",
      "epoch 4769: train loss: 0.15477143794898024, test loss: 0.260065870028225\n",
      "epoch 4770: train loss: 0.15476137322220535, test loss: 0.26006135500612315\n",
      "epoch 4771: train loss: 0.1547513115444263, test loss: 0.26005684244197397\n",
      "epoch 4772: train loss: 0.1547412529140728, test loss: 0.2600523323342627\n",
      "epoch 4773: train loss: 0.15473119732957574, test loss: 0.2600478246814759\n",
      "epoch 4774: train loss: 0.1547211447893671, test loss: 0.2600433194820988\n",
      "epoch 4775: train loss: 0.15471109529187999, test loss: 0.2600388167346172\n",
      "epoch 4776: train loss: 0.15470104883554864, test loss: 0.2600343164375196\n",
      "epoch 4777: train loss: 0.15469100541880854, test loss: 0.2600298185892983\n",
      "epoch 4778: train loss: 0.15468096504009624, test loss: 0.2600253231884439\n",
      "epoch 4779: train loss: 0.1546709276978494, test loss: 0.26002083023344474\n",
      "epoch 4780: train loss: 0.1546608933905069, test loss: 0.2600163397227956\n",
      "epoch 4781: train loss: 0.15465086211650864, test loss: 0.2600118516549926\n",
      "epoch 4782: train loss: 0.15464083387429584, test loss: 0.26000736602852975\n",
      "epoch 4783: train loss: 0.1546308086623106, test loss: 0.26000288284190254\n",
      "epoch 4784: train loss: 0.1546207864789964, test loss: 0.2599984020936092\n",
      "epoch 4785: train loss: 0.15461076732279777, test loss: 0.2599939237821523\n",
      "epoch 4786: train loss: 0.15460075119216027, test loss: 0.25998944790602985\n",
      "epoch 4787: train loss: 0.15459073808553073, test loss: 0.25998497446373875\n",
      "epoch 4788: train loss: 0.15458072800135703, test loss: 0.2599805034537928\n",
      "epoch 4789: train loss: 0.15457072093808819, test loss: 0.2599760348746846\n",
      "epoch 4790: train loss: 0.1545607168941744, test loss: 0.2599715687249263\n",
      "epoch 4791: train loss: 0.1545507158680669, test loss: 0.2599671050030222\n",
      "epoch 4792: train loss: 0.15454071785821813, test loss: 0.25996264370747846\n",
      "epoch 4793: train loss: 0.15453072286308167, test loss: 0.2599581848368057\n",
      "epoch 4794: train loss: 0.15452073088111207, test loss: 0.2599537283895143\n",
      "epoch 4795: train loss: 0.1545107419107652, test loss: 0.25994927436411075\n",
      "epoch 4796: train loss: 0.15450075595049792, test loss: 0.2599448227591133\n",
      "epoch 4797: train loss: 0.15449077299876823, test loss: 0.25994037357303373\n",
      "epoch 4798: train loss: 0.1544807930540353, test loss: 0.25993592680438665\n",
      "epoch 4799: train loss: 0.15447081611475943, test loss: 0.2599314824516851\n",
      "epoch 4800: train loss: 0.15446084217940192, test loss: 0.25992704051344534\n",
      "epoch 4801: train loss: 0.15445087124642531, test loss: 0.25992260098819275\n",
      "epoch 4802: train loss: 0.15444090331429317, test loss: 0.25991816387444105\n",
      "epoch 4803: train loss: 0.1544309383814702, test loss: 0.2599137291707096\n",
      "epoch 4804: train loss: 0.15442097644642225, test loss: 0.25990929687552566\n",
      "epoch 4805: train loss: 0.15441101750761632, test loss: 0.2599048669874064\n",
      "epoch 4806: train loss: 0.1544010615635204, test loss: 0.2599004395048807\n",
      "epoch 4807: train loss: 0.15439110861260363, test loss: 0.25989601442646887\n",
      "epoch 4808: train loss: 0.15438115865333635, test loss: 0.2598915917507016\n",
      "epoch 4809: train loss: 0.1543712116841899, test loss: 0.2598871714761064\n",
      "epoch 4810: train loss: 0.15436126770363676, test loss: 0.25988275360120844\n",
      "epoch 4811: train loss: 0.15435132671015048, test loss: 0.25987833812453887\n",
      "epoch 4812: train loss: 0.15434138870220585, test loss: 0.2598739250446308\n",
      "epoch 4813: train loss: 0.15433145367827858, test loss: 0.2598695143600142\n",
      "epoch 4814: train loss: 0.1543215216368456, test loss: 0.2598651060692239\n",
      "epoch 4815: train loss: 0.15431159257638488, test loss: 0.2598607001707941\n",
      "epoch 4816: train loss: 0.1543016664953756, test loss: 0.25985629666326326\n",
      "epoch 4817: train loss: 0.15429174339229787, test loss: 0.2598518955451635\n",
      "epoch 4818: train loss: 0.154281823265633, test loss: 0.2598474968150365\n",
      "epoch 4819: train loss: 0.15427190611386343, test loss: 0.2598431004714204\n",
      "epoch 4820: train loss: 0.1542619919354726, test loss: 0.25983870651285523\n",
      "epoch 4821: train loss: 0.15425208072894508, test loss: 0.2598343149378802\n",
      "epoch 4822: train loss: 0.15424217249276664, test loss: 0.2598299257450417\n",
      "epoch 4823: train loss: 0.1542322672254239, test loss: 0.25982553893288307\n",
      "epoch 4824: train loss: 0.15422236492540486, test loss: 0.2598211544999503\n",
      "epoch 4825: train loss: 0.1542124655911984, test loss: 0.2598167724447846\n",
      "epoch 4826: train loss: 0.15420256922129455, test loss: 0.2598123927659393\n",
      "epoch 4827: train loss: 0.1541926758141845, test loss: 0.25980801546196064\n",
      "epoch 4828: train loss: 0.1541827853683604, test loss: 0.259803640531395\n",
      "epoch 4829: train loss: 0.15417289788231556, test loss: 0.25979926797279984\n",
      "epoch 4830: train loss: 0.1541630133545444, test loss: 0.2597948977847199\n",
      "epoch 4831: train loss: 0.1541531317835424, test loss: 0.2597905299657157\n",
      "epoch 4832: train loss: 0.15414325316780608, test loss: 0.25978616451433495\n",
      "epoch 4833: train loss: 0.15413337750583314, test loss: 0.2597818014291345\n",
      "epoch 4834: train loss: 0.1541235047961222, test loss: 0.25977744070867487\n",
      "epoch 4835: train loss: 0.15411363503717312, test loss: 0.25977308235150987\n",
      "epoch 4836: train loss: 0.15410376822748678, test loss: 0.25976872635619974\n",
      "epoch 4837: train loss: 0.15409390436556517, test loss: 0.2597643727213036\n",
      "epoch 4838: train loss: 0.15408404344991122, test loss: 0.2597600214453822\n",
      "epoch 4839: train loss: 0.15407418547902915, test loss: 0.2597556725269998\n",
      "epoch 4840: train loss: 0.15406433045142406, test loss: 0.2597513259647188\n",
      "epoch 4841: train loss: 0.15405447836560232, test loss: 0.25974698175710187\n",
      "epoch 4842: train loss: 0.15404462922007114, test loss: 0.2597426399027179\n",
      "epoch 4843: train loss: 0.154034783013339, test loss: 0.25973830040012813\n",
      "epoch 4844: train loss: 0.15402493974391537, test loss: 0.25973396324790804\n",
      "epoch 4845: train loss: 0.1540150994103108, test loss: 0.2597296284446235\n",
      "epoch 4846: train loss: 0.15400526201103687, test loss: 0.2597252959888412\n",
      "epoch 4847: train loss: 0.1539954275446063, test loss: 0.2597209658791379\n",
      "epoch 4848: train loss: 0.1539855960095328, test loss: 0.2597166381140822\n",
      "epoch 4849: train loss: 0.15397576740433125, test loss: 0.2597123126922496\n",
      "epoch 4850: train loss: 0.1539659417275175, test loss: 0.25970798961221403\n",
      "epoch 4851: train loss: 0.1539561189776085, test loss: 0.2597036688725516\n",
      "epoch 4852: train loss: 0.15394629915312225, test loss: 0.2596993504718387\n",
      "epoch 4853: train loss: 0.15393648225257783, test loss: 0.2596950344086537\n",
      "epoch 4854: train loss: 0.15392666827449536, test loss: 0.2596907206815773\n",
      "epoch 4855: train loss: 0.15391685721739606, test loss: 0.2596864092891877\n",
      "epoch 4856: train loss: 0.15390704907980216, test loss: 0.2596821002300656\n",
      "epoch 4857: train loss: 0.153897243860237, test loss: 0.25967779350279635\n",
      "epoch 4858: train loss: 0.15388744155722495, test loss: 0.25967348910596294\n",
      "epoch 4859: train loss: 0.1538776421692914, test loss: 0.25966918703814706\n",
      "epoch 4860: train loss: 0.15386784569496284, test loss: 0.2596648872979381\n",
      "epoch 4861: train loss: 0.1538580521327668, test loss: 0.25966058988392227\n",
      "epoch 4862: train loss: 0.15384826148123187, test loss: 0.2596562947946894\n",
      "epoch 4863: train loss: 0.15383847373888773, test loss: 0.25965200202882505\n",
      "epoch 4864: train loss: 0.15382868890426502, test loss: 0.25964771158491934\n",
      "epoch 4865: train loss: 0.1538189069758955, test loss: 0.2596434234615666\n",
      "epoch 4866: train loss: 0.15380912795231197, test loss: 0.2596391376573577\n",
      "epoch 4867: train loss: 0.15379935183204826, test loss: 0.25963485417089077\n",
      "epoch 4868: train loss: 0.15378957861363923, test loss: 0.2596305730007535\n",
      "epoch 4869: train loss: 0.15377980829562082, test loss: 0.25962629414554567\n",
      "epoch 4870: train loss: 0.15377004087653004, test loss: 0.2596220176038651\n",
      "epoch 4871: train loss: 0.15376027635490488, test loss: 0.2596177433743072\n",
      "epoch 4872: train loss: 0.15375051472928444, test loss: 0.2596134714554731\n",
      "epoch 4873: train loss: 0.1537407559982088, test loss: 0.2596092018459624\n",
      "epoch 4874: train loss: 0.15373100016021912, test loss: 0.25960493454437633\n",
      "epoch 4875: train loss: 0.15372124721385758, test loss: 0.25960066954932015\n",
      "epoch 4876: train loss: 0.1537114971576674, test loss: 0.25959640685939095\n",
      "epoch 4877: train loss: 0.1537017499901929, test loss: 0.259592146473202\n",
      "epoch 4878: train loss: 0.15369200570997932, test loss: 0.25958788838935276\n",
      "epoch 4879: train loss: 0.15368226431557303, test loss: 0.2595836326064511\n",
      "epoch 4880: train loss: 0.1536725258055214, test loss: 0.2595793791231083\n",
      "epoch 4881: train loss: 0.15366279017837287, test loss: 0.25957512793792964\n",
      "epoch 4882: train loss: 0.15365305743267682, test loss: 0.25957087904952697\n",
      "epoch 4883: train loss: 0.15364332756698382, test loss: 0.25956663245651157\n",
      "epoch 4884: train loss: 0.1536336005798453, test loss: 0.25956238815749544\n",
      "epoch 4885: train loss: 0.15362387646981385, test loss: 0.2595581461510891\n",
      "epoch 4886: train loss: 0.153614155235443, test loss: 0.25955390643591614\n",
      "epoch 4887: train loss: 0.1536044368752874, test loss: 0.2595496690105809\n",
      "epoch 4888: train loss: 0.15359472138790267, test loss: 0.2595454338737113\n",
      "epoch 4889: train loss: 0.15358500877184544, test loss: 0.25954120102391437\n",
      "epoch 4890: train loss: 0.15357529902567335, test loss: 0.2595369704598176\n",
      "epoch 4891: train loss: 0.1535655921479452, test loss: 0.25953274218003375\n",
      "epoch 4892: train loss: 0.15355588813722065, test loss: 0.2595285161831884\n",
      "epoch 4893: train loss: 0.1535461869920605, test loss: 0.25952429246790665\n",
      "epoch 4894: train loss: 0.15353648871102646, test loss: 0.2595200710328042\n",
      "epoch 4895: train loss: 0.15352679329268137, test loss: 0.2595158518765085\n",
      "epoch 4896: train loss: 0.15351710073558905, test loss: 0.25951163499764646\n",
      "epoch 4897: train loss: 0.15350741103831436, test loss: 0.25950742039484537\n",
      "epoch 4898: train loss: 0.1534977241994231, test loss: 0.2595032080667284\n",
      "epoch 4899: train loss: 0.15348804021748216, test loss: 0.2594989980119268\n",
      "epoch 4900: train loss: 0.15347835909105945, test loss: 0.2594947902290702\n",
      "epoch 4901: train loss: 0.15346868081872383, test loss: 0.2594905847167912\n",
      "epoch 4902: train loss: 0.15345900539904528, test loss: 0.25948638147371783\n",
      "epoch 4903: train loss: 0.15344933283059464, test loss: 0.25948218049848465\n",
      "epoch 4904: train loss: 0.15343966311194396, test loss: 0.259477981789728\n",
      "epoch 4905: train loss: 0.15342999624166612, test loss: 0.25947378534608284\n",
      "epoch 4906: train loss: 0.15342033221833515, test loss: 0.2594695911661774\n",
      "epoch 4907: train loss: 0.15341067104052597, test loss: 0.25946539924865675\n",
      "epoch 4908: train loss: 0.15340101270681458, test loss: 0.25946120959215785\n",
      "epoch 4909: train loss: 0.153391357215778, test loss: 0.2594570221953226\n",
      "epoch 4910: train loss: 0.15338170456599426, test loss: 0.259452837056783\n",
      "epoch 4911: train loss: 0.15337205475604226, test loss: 0.2594486541751867\n",
      "epoch 4912: train loss: 0.15336240778450208, test loss: 0.2594444735491752\n",
      "epoch 4913: train loss: 0.15335276364995473, test loss: 0.2594402951773925\n",
      "epoch 4914: train loss: 0.15334312235098227, test loss: 0.25943611905848146\n",
      "epoch 4915: train loss: 0.1533334838861677, test loss: 0.25943194519109\n",
      "epoch 4916: train loss: 0.15332384825409495, test loss: 0.259427773573865\n",
      "epoch 4917: train loss: 0.1533142154533492, test loss: 0.2594236042054508\n",
      "epoch 4918: train loss: 0.15330458548251635, test loss: 0.2594194370844997\n",
      "epoch 4919: train loss: 0.1532949583401835, test loss: 0.2594152722096586\n",
      "epoch 4920: train loss: 0.15328533402493866, test loss: 0.2594111095795813\n",
      "epoch 4921: train loss: 0.1532757125353708, test loss: 0.2594069491929207\n",
      "epoch 4922: train loss: 0.15326609387006995, test loss: 0.2594027910483285\n",
      "epoch 4923: train loss: 0.15325647802762718, test loss: 0.259398635144453\n",
      "epoch 4924: train loss: 0.15324686500663445, test loss: 0.25939448147996\n",
      "epoch 4925: train loss: 0.15323725480568476, test loss: 0.25939033005349627\n",
      "epoch 4926: train loss: 0.1532276474233721, test loss: 0.2593861808637262\n",
      "epoch 4927: train loss: 0.1532180428582914, test loss: 0.2593820339093057\n",
      "epoch 4928: train loss: 0.1532084411090387, test loss: 0.2593778891888896\n",
      "epoch 4929: train loss: 0.15319884217421095, test loss: 0.25937374670114516\n",
      "epoch 4930: train loss: 0.15318924605240608, test loss: 0.2593696064447319\n",
      "epoch 4931: train loss: 0.153179652742223, test loss: 0.25936546841830976\n",
      "epoch 4932: train loss: 0.1531700622422617, test loss: 0.2593613326205428\n",
      "epoch 4933: train loss: 0.15316047455112305, test loss: 0.25935719905010046\n",
      "epoch 4934: train loss: 0.15315088966740892, test loss: 0.25935306770564237\n",
      "epoch 4935: train loss: 0.15314130758972222, test loss: 0.25934893858583746\n",
      "epoch 4936: train loss: 0.15313172831666683, test loss: 0.25934481168935214\n",
      "epoch 4937: train loss: 0.15312215184684758, test loss: 0.25934068701485913\n",
      "epoch 4938: train loss: 0.15311257817887025, test loss: 0.2593365645610227\n",
      "epoch 4939: train loss: 0.1531030073113417, test loss: 0.25933244432651736\n",
      "epoch 4940: train loss: 0.1530934392428697, test loss: 0.2593283263100136\n",
      "epoch 4941: train loss: 0.15308387397206302, test loss: 0.2593242105101845\n",
      "epoch 4942: train loss: 0.15307431149753137, test loss: 0.25932009692570734\n",
      "epoch 4943: train loss: 0.15306475181788548, test loss: 0.2593159855552514\n",
      "epoch 4944: train loss: 0.15305519493173708, test loss: 0.2593118763974966\n",
      "epoch 4945: train loss: 0.15304564083769884, test loss: 0.2593077694511157\n",
      "epoch 4946: train loss: 0.15303608953438433, test loss: 0.25930366471479394\n",
      "epoch 4947: train loss: 0.15302654102040822, test loss: 0.25929956218720407\n",
      "epoch 4948: train loss: 0.15301699529438612, test loss: 0.25929546186702546\n",
      "epoch 4949: train loss: 0.15300745235493451, test loss: 0.2592913637529447\n",
      "epoch 4950: train loss: 0.15299791220067102, test loss: 0.2592872678436413\n",
      "epoch 4951: train loss: 0.1529883748302141, test loss: 0.2592831741377985\n",
      "epoch 4952: train loss: 0.15297884024218322, test loss: 0.25927908263409877\n",
      "epoch 4953: train loss: 0.15296930843519882, test loss: 0.25927499333123316\n",
      "epoch 4954: train loss: 0.1529597794078823, test loss: 0.25927090622787785\n",
      "epoch 4955: train loss: 0.15295025315885608, test loss: 0.2592668213227283\n",
      "epoch 4956: train loss: 0.1529407296867434, test loss: 0.25926273861446975\n",
      "epoch 4957: train loss: 0.15293120899016863, test loss: 0.25925865810179405\n",
      "epoch 4958: train loss: 0.152921691067757, test loss: 0.2592545797833878\n",
      "epoch 4959: train loss: 0.15291217591813477, test loss: 0.259250503657944\n",
      "epoch 4960: train loss: 0.15290266353992915, test loss: 0.25924642972415607\n",
      "epoch 4961: train loss: 0.1528931539317682, test loss: 0.25924235798071427\n",
      "epoch 4962: train loss: 0.15288364709228108, test loss: 0.2592382884263191\n",
      "epoch 4963: train loss: 0.15287414302009789, test loss: 0.25923422105965854\n",
      "epoch 4964: train loss: 0.15286464171384959, test loss: 0.25923015587943576\n",
      "epoch 4965: train loss: 0.15285514317216822, test loss: 0.25922609288434073\n",
      "epoch 4966: train loss: 0.1528456473936867, test loss: 0.25922203207307887\n",
      "epoch 4967: train loss: 0.15283615437703893, test loss: 0.2592179734443454\n",
      "epoch 4968: train loss: 0.1528266641208597, test loss: 0.2592139169968421\n",
      "epoch 4969: train loss: 0.1528171766237849, test loss: 0.25920986272926966\n",
      "epoch 4970: train loss: 0.15280769188445129, test loss: 0.2592058106403324\n",
      "epoch 4971: train loss: 0.15279820990149648, test loss: 0.25920176072873113\n",
      "epoch 4972: train loss: 0.15278873067355922, test loss: 0.2591977129931732\n",
      "epoch 4973: train loss: 0.15277925419927907, test loss: 0.25919366743236094\n",
      "epoch 4974: train loss: 0.15276978047729664, test loss: 0.2591896240450029\n",
      "epoch 4975: train loss: 0.15276030950625338, test loss: 0.2591855828298057\n",
      "epoch 4976: train loss: 0.1527508412847918, test loss: 0.2591815437854823\n",
      "epoch 4977: train loss: 0.15274137581155528, test loss: 0.2591775069107334\n",
      "epoch 4978: train loss: 0.15273191308518816, test loss: 0.25917347220427667\n",
      "epoch 4979: train loss: 0.15272245310433574, test loss: 0.25916943966482003\n",
      "epoch 4980: train loss: 0.15271299586764425, test loss: 0.259165409291078\n",
      "epoch 4981: train loss: 0.1527035413737609, test loss: 0.2591613810817608\n",
      "epoch 4982: train loss: 0.1526940896213338, test loss: 0.25915735503558834\n",
      "epoch 4983: train loss: 0.152684640609012, test loss: 0.25915333115127015\n",
      "epoch 4984: train loss: 0.1526751943354455, test loss: 0.2591493094275272\n",
      "epoch 4985: train loss: 0.15266575079928532, test loss: 0.25914528986307317\n",
      "epoch 4986: train loss: 0.15265630999918325, test loss: 0.2591412724566308\n",
      "epoch 4987: train loss: 0.15264687193379217, test loss: 0.2591372572069158\n",
      "epoch 4988: train loss: 0.15263743660176582, test loss: 0.25913324411264954\n",
      "epoch 4989: train loss: 0.15262800400175894, test loss: 0.2591292331725546\n",
      "epoch 4990: train loss: 0.1526185741324271, test loss: 0.25912522438535157\n",
      "epoch 4991: train loss: 0.1526091469924269, test loss: 0.25912121774976377\n",
      "epoch 4992: train loss: 0.15259972258041582, test loss: 0.25911721326451825\n",
      "epoch 4993: train loss: 0.15259030089505235, test loss: 0.2591132109283383\n",
      "epoch 4994: train loss: 0.1525808819349958, test loss: 0.25910921073995036\n",
      "epoch 4995: train loss: 0.15257146569890653, test loss: 0.25910521269808257\n",
      "epoch 4996: train loss: 0.15256205218544572, test loss: 0.2591012168014607\n",
      "epoch 4997: train loss: 0.1525526413932755, test loss: 0.2590972230488199\n",
      "epoch 4998: train loss: 0.15254323332105907, test loss: 0.2590932314388835\n",
      "epoch 4999: train loss: 0.15253382796746032, test loss: 0.2590892419703837\n",
      "epoch 5000: train loss: 0.15252442533114433, test loss: 0.2590852546420566\n",
      "epoch 5001: train loss: 0.15251502541077686, test loss: 0.2590812694526347\n",
      "epoch 5002: train loss: 0.1525056282050247, test loss: 0.25907728640085\n",
      "epoch 5003: train loss: 0.15249623371255563, test loss: 0.259073305485436\n",
      "epoch 5004: train loss: 0.15248684193203832, test loss: 0.2590693267051347\n",
      "epoch 5005: train loss: 0.15247745286214226, test loss: 0.2590653500586802\n",
      "epoch 5006: train loss: 0.15246806650153796, test loss: 0.25906137554480907\n",
      "epoch 5007: train loss: 0.15245868284889685, test loss: 0.25905740316226017\n",
      "epoch 5008: train loss: 0.1524493019028913, test loss: 0.2590534329097731\n",
      "epoch 5009: train loss: 0.15243992366219447, test loss: 0.25904946478609164\n",
      "epoch 5010: train loss: 0.1524305481254806, test loss: 0.2590454987899593\n",
      "epoch 5011: train loss: 0.15242117529142474, test loss: 0.2590415349201147\n",
      "epoch 5012: train loss: 0.15241180515870292, test loss: 0.25903757317530257\n",
      "epoch 5013: train loss: 0.15240243772599205, test loss: 0.2590336135542682\n",
      "epoch 5014: train loss: 0.15239307299196997, test loss: 0.25902965605575995\n",
      "epoch 5015: train loss: 0.15238371095531542, test loss: 0.25902570067852015\n",
      "epoch 5016: train loss: 0.1523743516147081, test loss: 0.25902174742129985\n",
      "epoch 5017: train loss: 0.15236499496882852, test loss: 0.2590177962828464\n",
      "epoch 5018: train loss: 0.15235564101635826, test loss: 0.25901384726190707\n",
      "epoch 5019: train loss: 0.15234628975597966, test loss: 0.25900990035723753\n",
      "epoch 5020: train loss: 0.15233694118637608, test loss: 0.2590059555675861\n",
      "epoch 5021: train loss: 0.1523275953062317, test loss: 0.2590020128917082\n",
      "epoch 5022: train loss: 0.15231825211423167, test loss: 0.2589980723283514\n",
      "epoch 5023: train loss: 0.15230891160906201, test loss: 0.258994133876278\n",
      "epoch 5024: train loss: 0.15229957378940967, test loss: 0.25899019753423586\n",
      "epoch 5025: train loss: 0.15229023865396252, test loss: 0.2589862633009853\n",
      "epoch 5026: train loss: 0.15228090620140933, test loss: 0.25898233117528696\n",
      "epoch 5027: train loss: 0.15227157643043973, test loss: 0.2589784011558928\n",
      "epoch 5028: train loss: 0.1522622493397443, test loss: 0.25897447324156336\n",
      "epoch 5029: train loss: 0.15225292492801457, test loss: 0.25897054743106135\n",
      "epoch 5030: train loss: 0.1522436031939428, test loss: 0.2589666237231466\n",
      "epoch 5031: train loss: 0.15223428413622236, test loss: 0.25896270211658007\n",
      "epoch 5032: train loss: 0.15222496775354735, test loss: 0.2589587826101276\n",
      "epoch 5033: train loss: 0.1522156540446129, test loss: 0.2589548652025527\n",
      "epoch 5034: train loss: 0.15220634300811497, test loss: 0.25895094989261663\n",
      "epoch 5035: train loss: 0.1521970346427505, test loss: 0.25894703667909014\n",
      "epoch 5036: train loss: 0.15218772894721713, test loss: 0.25894312556073756\n",
      "epoch 5037: train loss: 0.15217842592021358, test loss: 0.25893921653632423\n",
      "epoch 5038: train loss: 0.15216912556043943, test loss: 0.2589353096046211\n",
      "epoch 5039: train loss: 0.15215982786659515, test loss: 0.2589314047644012\n",
      "epoch 5040: train loss: 0.15215053283738206, test loss: 0.2589275020144349\n",
      "epoch 5041: train loss: 0.1521412404715024, test loss: 0.2589236013534857\n",
      "epoch 5042: train loss: 0.15213195076765934, test loss: 0.25891970278033194\n",
      "epoch 5043: train loss: 0.15212266372455693, test loss: 0.2589158062937487\n",
      "epoch 5044: train loss: 0.15211337934090002, test loss: 0.2589119118925053\n",
      "epoch 5045: train loss: 0.15210409761539448, test loss: 0.25890801957538373\n",
      "epoch 5046: train loss: 0.152094818546747, test loss: 0.258904129341153\n",
      "epoch 5047: train loss: 0.15208554213366512, test loss: 0.2589002411885971\n",
      "epoch 5048: train loss: 0.15207626837485738, test loss: 0.2588963551164869\n",
      "epoch 5049: train loss: 0.1520669972690331, test loss: 0.2588924711236084\n",
      "epoch 5050: train loss: 0.1520577288149026, test loss: 0.2588885892087371\n",
      "epoch 5051: train loss: 0.15204846301117692, test loss: 0.2588847093706545\n",
      "epoch 5052: train loss: 0.15203919985656814, test loss: 0.2588808316081458\n",
      "epoch 5053: train loss: 0.15202993934978917, test loss: 0.2588769559199904\n",
      "epoch 5054: train loss: 0.15202068148955378, test loss: 0.2588730823049729\n",
      "epoch 5055: train loss: 0.15201142627457664, test loss: 0.2588692107618757\n",
      "epoch 5056: train loss: 0.1520021737035733, test loss: 0.2588653412894912\n",
      "epoch 5057: train loss: 0.15199292377526016, test loss: 0.2588614738865983\n",
      "epoch 5058: train loss: 0.15198367648835462, test loss: 0.2588576085519879\n",
      "epoch 5059: train loss: 0.15197443184157478, test loss: 0.25885374528444494\n",
      "epoch 5060: train loss: 0.15196518983363977, test loss: 0.2588498840827662\n",
      "epoch 5061: train loss: 0.15195595046326954, test loss: 0.25884602494573483\n",
      "epoch 5062: train loss: 0.1519467137291848, test loss: 0.2588421678721481\n",
      "epoch 5063: train loss: 0.15193747963010742, test loss: 0.258838312860789\n",
      "epoch 5064: train loss: 0.15192824816475986, test loss: 0.25883445991045845\n",
      "epoch 5065: train loss: 0.15191901933186563, test loss: 0.25883060901994903\n",
      "epoch 5066: train loss: 0.15190979313014902, test loss: 0.2588267601880553\n",
      "epoch 5067: train loss: 0.1519005695583352, test loss: 0.25882291341357105\n",
      "epoch 5068: train loss: 0.15189134861515033, test loss: 0.25881906869529103\n",
      "epoch 5069: train loss: 0.1518821302993212, test loss: 0.2588152260320195\n",
      "epoch 5070: train loss: 0.15187291460957575, test loss: 0.25881138542254944\n",
      "epoch 5071: train loss: 0.15186370154464265, test loss: 0.25880754686568413\n",
      "epoch 5072: train loss: 0.1518544911032514, test loss: 0.2588037103602165\n",
      "epoch 5073: train loss: 0.1518452832841324, test loss: 0.25879987590495884\n",
      "epoch 5074: train loss: 0.15183607808601698, test loss: 0.25879604349870616\n",
      "epoch 5075: train loss: 0.1518268755076373, test loss: 0.25879221314026163\n",
      "epoch 5076: train loss: 0.15181767554772632, test loss: 0.258788384828432\n",
      "epoch 5077: train loss: 0.15180847820501797, test loss: 0.25878455856201926\n",
      "epoch 5078: train loss: 0.151799283478247, test loss: 0.25878073433983106\n",
      "epoch 5079: train loss: 0.15179009136614893, test loss: 0.2587769121606744\n",
      "epoch 5080: train loss: 0.15178090186746038, test loss: 0.2587730920233558\n",
      "epoch 5081: train loss: 0.1517717149809186, test loss: 0.2587692739266867\n",
      "epoch 5082: train loss: 0.15176253070526172, test loss: 0.25876545786947\n",
      "epoch 5083: train loss: 0.1517533490392289, test loss: 0.25876164385052175\n",
      "epoch 5084: train loss: 0.15174416998156004, test loss: 0.25875783186865114\n",
      "epoch 5085: train loss: 0.15173499353099587, test loss: 0.2587540219226714\n",
      "epoch 5086: train loss: 0.15172581968627805, test loss: 0.25875021401139564\n",
      "epoch 5087: train loss: 0.15171664844614907, test loss: 0.258746408133636\n",
      "epoch 5088: train loss: 0.15170747980935229, test loss: 0.25874260428821044\n",
      "epoch 5089: train loss: 0.15169831377463186, test loss: 0.2587388024739286\n",
      "epoch 5090: train loss: 0.1516891503407329, test loss: 0.2587350026896144\n",
      "epoch 5091: train loss: 0.15167998950640127, test loss: 0.258731204934081\n",
      "epoch 5092: train loss: 0.15167083127038378, test loss: 0.2587274092061504\n",
      "epoch 5093: train loss: 0.15166167563142804, test loss: 0.258723615504636\n",
      "epoch 5094: train loss: 0.1516525225882825, test loss: 0.2587198238283625\n",
      "epoch 5095: train loss: 0.15164337213969645, test loss: 0.25871603417615163\n",
      "epoch 5096: train loss: 0.1516342242844201, test loss: 0.25871224654682407\n",
      "epoch 5097: train loss: 0.15162507902120453, test loss: 0.2587084609392039\n",
      "epoch 5098: train loss: 0.1516159363488015, test loss: 0.25870467735211183\n",
      "epoch 5099: train loss: 0.15160679626596377, test loss: 0.2587008957843763\n",
      "epoch 5100: train loss: 0.151597658771445, test loss: 0.2586971162348193\n",
      "epoch 5101: train loss: 0.15158852386399943, test loss: 0.2586933387022714\n",
      "epoch 5102: train loss: 0.15157939154238242, test loss: 0.25868956318555425\n",
      "epoch 5103: train loss: 0.15157026180535008, test loss: 0.2586857896835034\n",
      "epoch 5104: train loss: 0.15156113465165932, test loss: 0.25868201819494346\n",
      "epoch 5105: train loss: 0.15155201008006797, test loss: 0.2586782487187039\n",
      "epoch 5106: train loss: 0.15154288808933467, test loss: 0.25867448125362164\n",
      "epoch 5107: train loss: 0.15153376867821888, test loss: 0.2586707157985183\n",
      "epoch 5108: train loss: 0.1515246518454809, test loss: 0.2586669523522365\n",
      "epoch 5109: train loss: 0.1515155375898819, test loss: 0.258663190913604\n",
      "epoch 5110: train loss: 0.15150642591018387, test loss: 0.2586594314814569\n",
      "epoch 5111: train loss: 0.1514973168051497, test loss: 0.25865567405463047\n",
      "epoch 5112: train loss: 0.15148821027354306, test loss: 0.25865191863196446\n",
      "epoch 5113: train loss: 0.1514791063141284, test loss: 0.258648165212292\n",
      "epoch 5114: train loss: 0.15147000492567111, test loss: 0.2586444137944509\n",
      "epoch 5115: train loss: 0.15146090610693735, test loss: 0.2586406643772824\n",
      "epoch 5116: train loss: 0.15145180985669426, test loss: 0.25863691695962215\n",
      "epoch 5117: train loss: 0.15144271617370958, test loss: 0.2586331715403163\n",
      "epoch 5118: train loss: 0.15143362505675204, test loss: 0.2586294281182032\n",
      "epoch 5119: train loss: 0.15142453650459115, test loss: 0.2586256866921247\n",
      "epoch 5120: train loss: 0.1514154505159973, test loss: 0.2586219472609262\n",
      "epoch 5121: train loss: 0.15140636708974167, test loss: 0.2586182098234536\n",
      "epoch 5122: train loss: 0.15139728622459628, test loss: 0.25861447437854473\n",
      "epoch 5123: train loss: 0.151388207919334, test loss: 0.2586107409250528\n",
      "epoch 5124: train loss: 0.15137913217272847, test loss: 0.2586070094618217\n",
      "epoch 5125: train loss: 0.1513700589835542, test loss: 0.2586032799876986\n",
      "epoch 5126: train loss: 0.15136098835058664, test loss: 0.25859955250153105\n",
      "epoch 5127: train loss: 0.1513519202726018, test loss: 0.2585958270021738\n",
      "epoch 5128: train loss: 0.15134285474837678, test loss: 0.2585921034884729\n",
      "epoch 5129: train loss: 0.15133379177668937, test loss: 0.2585883819592791\n",
      "epoch 5130: train loss: 0.15132473135631822, test loss: 0.2585846624134456\n",
      "epoch 5131: train loss: 0.15131567348604277, test loss: 0.2585809448498241\n",
      "epoch 5132: train loss: 0.15130661816464336, test loss: 0.2585772292672726\n",
      "epoch 5133: train loss: 0.1512975653909011, test loss: 0.2585735156646368\n",
      "epoch 5134: train loss: 0.15128851516359787, test loss: 0.25856980404078217\n",
      "epoch 5135: train loss: 0.15127946748151652, test loss: 0.25856609439456135\n",
      "epoch 5136: train loss: 0.15127042234344054, test loss: 0.2585623867248292\n",
      "epoch 5137: train loss: 0.15126137974815443, test loss: 0.2585586810304436\n",
      "epoch 5138: train loss: 0.15125233969444332, test loss: 0.2585549773102695\n",
      "epoch 5139: train loss: 0.1512433021810933, test loss: 0.25855127556316143\n",
      "epoch 5140: train loss: 0.15123426720689123, test loss: 0.2585475757879817\n",
      "epoch 5141: train loss: 0.15122523477062477, test loss: 0.2585438779835894\n",
      "epoch 5142: train loss: 0.1512162048710824, test loss: 0.25854018214885083\n",
      "epoch 5143: train loss: 0.15120717750705345, test loss: 0.258536488282626\n",
      "epoch 5144: train loss: 0.15119815267732803, test loss: 0.2585327963837791\n",
      "epoch 5145: train loss: 0.1511891303806971, test loss: 0.2585291064511797\n",
      "epoch 5146: train loss: 0.1511801106159524, test loss: 0.2585254184836886\n",
      "epoch 5147: train loss: 0.15117109338188647, test loss: 0.25852173248017074\n",
      "epoch 5148: train loss: 0.1511620786772927, test loss: 0.2585180484395016\n",
      "epoch 5149: train loss: 0.1511530665009653, test loss: 0.25851436636054537\n",
      "epoch 5150: train loss: 0.15114405685169927, test loss: 0.2585106862421674\n",
      "epoch 5151: train loss: 0.15113504972829037, test loss: 0.2585070080832418\n",
      "epoch 5152: train loss: 0.15112604512953529, test loss: 0.25850333188263885\n",
      "epoch 5153: train loss: 0.15111704305423135, test loss: 0.2584996576392346\n",
      "epoch 5154: train loss: 0.1511080435011769, test loss: 0.2584959853518906\n",
      "epoch 5155: train loss: 0.15109904646917094, test loss: 0.2584923150194932\n",
      "epoch 5156: train loss: 0.1510900519570133, test loss: 0.25848864664090454\n",
      "epoch 5157: train loss: 0.15108105996350468, test loss: 0.25848498021501315\n",
      "epoch 5158: train loss: 0.15107207048744647, test loss: 0.25848131574068356\n",
      "epoch 5159: train loss: 0.151063083527641, test loss: 0.25847765321679694\n",
      "epoch 5160: train loss: 0.15105409908289133, test loss: 0.25847399264222926\n",
      "epoch 5161: train loss: 0.1510451171520013, test loss: 0.258470334015864\n",
      "epoch 5162: train loss: 0.15103613773377558, test loss: 0.2584666773365763\n",
      "epoch 5163: train loss: 0.1510271608270197, test loss: 0.2584630226032459\n",
      "epoch 5164: train loss: 0.1510181864305399, test loss: 0.2584593698147561\n",
      "epoch 5165: train loss: 0.15100921454314326, test loss: 0.2584557189699906\n",
      "epoch 5166: train loss: 0.1510002451636376, test loss: 0.25845207006782794\n",
      "epoch 5167: train loss: 0.15099127829083173, test loss: 0.2584484231071526\n",
      "epoch 5168: train loss: 0.150982313923535, test loss: 0.25844477808684785\n",
      "epoch 5169: train loss: 0.15097335206055773, test loss: 0.2584411350058033\n",
      "epoch 5170: train loss: 0.150964392700711, test loss: 0.25843749386290327\n",
      "epoch 5171: train loss: 0.15095543584280666, test loss: 0.2584338546570322\n",
      "epoch 5172: train loss: 0.15094648148565734, test loss: 0.2584302173870834\n",
      "epoch 5173: train loss: 0.15093752962807652, test loss: 0.2584265820519382\n",
      "epoch 5174: train loss: 0.15092858026887848, test loss: 0.258422948650493\n",
      "epoch 5175: train loss: 0.15091963340687822, test loss: 0.25841931718163\n",
      "epoch 5176: train loss: 0.15091068904089155, test loss: 0.25841568764425016\n",
      "epoch 5177: train loss: 0.15090174716973517, test loss: 0.25841206003723877\n",
      "epoch 5178: train loss: 0.15089280779222647, test loss: 0.2584084343594889\n",
      "epoch 5179: train loss: 0.1508838709071836, test loss: 0.25840481060989895\n",
      "epoch 5180: train loss: 0.1508749365134256, test loss: 0.2584011887873583\n",
      "epoch 5181: train loss: 0.15086600460977231, test loss: 0.25839756889076443\n",
      "epoch 5182: train loss: 0.15085707519504418, test loss: 0.2583939509190094\n",
      "epoch 5183: train loss: 0.15084814826806267, test loss: 0.25839033487099694\n",
      "epoch 5184: train loss: 0.15083922382764992, test loss: 0.2583867207456187\n",
      "epoch 5185: train loss: 0.15083030187262883, test loss: 0.2583831085417795\n",
      "epoch 5186: train loss: 0.15082138240182313, test loss: 0.2583794982583745\n",
      "epoch 5187: train loss: 0.15081246541405735, test loss: 0.2583758898943039\n",
      "epoch 5188: train loss: 0.15080355090815678, test loss: 0.25837228344847046\n",
      "epoch 5189: train loss: 0.15079463888294747, test loss: 0.2583686789197732\n",
      "epoch 5190: train loss: 0.15078572933725629, test loss: 0.25836507630712036\n",
      "epoch 5191: train loss: 0.15077682226991088, test loss: 0.25836147560940914\n",
      "epoch 5192: train loss: 0.15076791767973965, test loss: 0.2583578768255482\n",
      "epoch 5193: train loss: 0.15075901556557184, test loss: 0.25835427995443927\n",
      "epoch 5194: train loss: 0.1507501159262374, test loss: 0.2583506849949917\n",
      "epoch 5195: train loss: 0.1507412187605671, test loss: 0.2583470919461117\n",
      "epoch 5196: train loss: 0.15073232406739245, test loss: 0.25834350080670604\n",
      "epoch 5197: train loss: 0.15072343184554582, test loss: 0.2583399115756829\n",
      "epoch 5198: train loss: 0.15071454209386026, test loss: 0.25833632425195086\n",
      "epoch 5199: train loss: 0.1507056548111697, test loss: 0.2583327388344199\n",
      "epoch 5200: train loss: 0.15069676999630877, test loss: 0.2583291553220037\n",
      "epoch 5201: train loss: 0.15068788764811283, test loss: 0.25832557371361026\n",
      "epoch 5202: train loss: 0.1506790077654182, test loss: 0.25832199400815653\n",
      "epoch 5203: train loss: 0.15067013034706175, test loss: 0.2583184162045526\n",
      "epoch 5204: train loss: 0.15066125539188127, test loss: 0.2583148403017121\n",
      "epoch 5205: train loss: 0.15065238289871524, test loss: 0.258311266298554\n",
      "epoch 5206: train loss: 0.15064351286640304, test loss: 0.2583076941939881\n",
      "epoch 5207: train loss: 0.15063464529378467, test loss: 0.25830412398693703\n",
      "epoch 5208: train loss: 0.15062578017970096, test loss: 0.25830055567631427\n",
      "epoch 5209: train loss: 0.15061691752299355, test loss: 0.25829698926103745\n",
      "epoch 5210: train loss: 0.1506080573225048, test loss: 0.2582934247400292\n",
      "epoch 5211: train loss: 0.15059919957707785, test loss: 0.25828986211220545\n",
      "epoch 5212: train loss: 0.15059034428555662, test loss: 0.25828630137648945\n",
      "epoch 5213: train loss: 0.15058149144678576, test loss: 0.25828274253180505\n",
      "epoch 5214: train loss: 0.15057264105961074, test loss: 0.25827918557706503\n",
      "epoch 5215: train loss: 0.15056379312287776, test loss: 0.25827563051120356\n",
      "epoch 5216: train loss: 0.1505549476354338, test loss: 0.25827207733313906\n",
      "epoch 5217: train loss: 0.15054610459612663, test loss: 0.258268526041794\n",
      "epoch 5218: train loss: 0.15053726400380474, test loss: 0.25826497663609854\n",
      "epoch 5219: train loss: 0.15052842585731738, test loss: 0.2582614291149766\n",
      "epoch 5220: train loss: 0.15051959015551458, test loss: 0.25825788347735534\n",
      "epoch 5221: train loss: 0.1505107568972472, test loss: 0.2582543397221621\n",
      "epoch 5222: train loss: 0.1505019260813667, test loss: 0.25825079784832394\n",
      "epoch 5223: train loss: 0.15049309770672545, test loss: 0.258247257854778\n",
      "epoch 5224: train loss: 0.15048427177217655, test loss: 0.2582437197404457\n",
      "epoch 5225: train loss: 0.15047544827657375, test loss: 0.25824018350426103\n",
      "epoch 5226: train loss: 0.15046662721877171, test loss: 0.25823664914515637\n",
      "epoch 5227: train loss: 0.15045780859762584, test loss: 0.2582331166620639\n",
      "epoch 5228: train loss: 0.15044899241199214, test loss: 0.25822958605391905\n",
      "epoch 5229: train loss: 0.15044017866072754, test loss: 0.2582260573196519\n",
      "epoch 5230: train loss: 0.15043136734268964, test loss: 0.2582225304582019\n",
      "epoch 5231: train loss: 0.15042255845673683, test loss: 0.25821900546850146\n",
      "epoch 5232: train loss: 0.15041375200172827, test loss: 0.25821548234948677\n",
      "epoch 5233: train loss: 0.1504049479765238, test loss: 0.25821196110009903\n",
      "epoch 5234: train loss: 0.1503961463799841, test loss: 0.25820844171927443\n",
      "epoch 5235: train loss: 0.15038734721097052, test loss: 0.2582049242059496\n",
      "epoch 5236: train loss: 0.15037855046834528, test loss: 0.2582014085590689\n",
      "epoch 5237: train loss: 0.15036975615097123, test loss: 0.2581978947775687\n",
      "epoch 5238: train loss: 0.15036096425771203, test loss: 0.2581943828603928\n",
      "epoch 5239: train loss: 0.1503521747874321, test loss: 0.2581908728064811\n",
      "epoch 5240: train loss: 0.15034338773899655, test loss: 0.25818736461478\n",
      "epoch 5241: train loss: 0.15033460311127134, test loss: 0.2581838582842324\n",
      "epoch 5242: train loss: 0.15032582090312302, test loss: 0.25818035381377624\n",
      "epoch 5243: train loss: 0.1503170411134191, test loss: 0.25817685120236605\n",
      "epoch 5244: train loss: 0.15030826374102765, test loss: 0.2581733504489423\n",
      "epoch 5245: train loss: 0.15029948878481758, test loss: 0.25816985155245153\n",
      "epoch 5246: train loss: 0.15029071624365856, test loss: 0.25816635451184644\n",
      "epoch 5247: train loss: 0.1502819461164209, test loss: 0.25816285932606964\n",
      "epoch 5248: train loss: 0.15027317840197582, test loss: 0.2581593659940751\n",
      "epoch 5249: train loss: 0.15026441309919508, test loss: 0.25815587451480954\n",
      "epoch 5250: train loss: 0.1502556502069514, test loss: 0.2581523848872234\n",
      "epoch 5251: train loss: 0.15024688972411807, test loss: 0.2581488971102709\n",
      "epoch 5252: train loss: 0.15023813164956917, test loss: 0.25814541118289974\n",
      "epoch 5253: train loss: 0.15022937598217959, test loss: 0.2581419271040698\n",
      "epoch 5254: train loss: 0.1502206227208249, test loss: 0.2581384448727275\n",
      "epoch 5255: train loss: 0.1502118718643814, test loss: 0.2581349644878342\n",
      "epoch 5256: train loss: 0.15020312341172617, test loss: 0.258131485948341\n",
      "epoch 5257: train loss: 0.15019437736173696, test loss: 0.25812800925320634\n",
      "epoch 5258: train loss: 0.1501856337132924, test loss: 0.2581245344013828\n",
      "epoch 5259: train loss: 0.15017689246527172, test loss: 0.25812106139183283\n",
      "epoch 5260: train loss: 0.15016815361655486, test loss: 0.25811759022351455\n",
      "epoch 5261: train loss: 0.15015941716602266, test loss: 0.25811412089538094\n",
      "epoch 5262: train loss: 0.1501506831125566, test loss: 0.2581106534064024\n",
      "epoch 5263: train loss: 0.15014195145503884, test loss: 0.2581071877555343\n",
      "epoch 5264: train loss: 0.15013322219235234, test loss: 0.2581037239417352\n",
      "epoch 5265: train loss: 0.15012449532338087, test loss: 0.2581002619639739\n",
      "epoch 5266: train loss: 0.15011577084700875, test loss: 0.2580968018212066\n",
      "epoch 5267: train loss: 0.15010704876212114, test loss: 0.25809334351240343\n",
      "epoch 5268: train loss: 0.15009832906760404, test loss: 0.2580898870365238\n",
      "epoch 5269: train loss: 0.1500896117623439, test loss: 0.2580864323925395\n",
      "epoch 5270: train loss: 0.15008089684522816, test loss: 0.25808297957940723\n",
      "epoch 5271: train loss: 0.1500721843151449, test loss: 0.2580795285961052\n",
      "epoch 5272: train loss: 0.15006347417098292, test loss: 0.2580760794415905\n",
      "epoch 5273: train loss: 0.15005476641163173, test loss: 0.25807263211484005\n",
      "epoch 5274: train loss: 0.15004606103598161, test loss: 0.2580691866148165\n",
      "epoch 5275: train loss: 0.15003735804292354, test loss: 0.25806574294049434\n",
      "epoch 5276: train loss: 0.15002865743134922, test loss: 0.2580623010908405\n",
      "epoch 5277: train loss: 0.15001995920015113, test loss: 0.25805886106483444\n",
      "epoch 5278: train loss: 0.1500112633482224, test loss: 0.2580554228614377\n",
      "epoch 5279: train loss: 0.15000256987445695, test loss: 0.25805198647962846\n",
      "epoch 5280: train loss: 0.14999387877774942, test loss: 0.25804855191838433\n",
      "epoch 5281: train loss: 0.1499851900569951, test loss: 0.25804511917666956\n",
      "epoch 5282: train loss: 0.14997650371109011, test loss: 0.2580416882534688\n",
      "epoch 5283: train loss: 0.1499678197389312, test loss: 0.25803825914775674\n",
      "epoch 5284: train loss: 0.14995913813941586, test loss: 0.2580348318585079\n",
      "epoch 5285: train loss: 0.14995045891144237, test loss: 0.25803140638469946\n",
      "epoch 5286: train loss: 0.14994178205390965, test loss: 0.2580279827253107\n",
      "epoch 5287: train loss: 0.14993310756571743, test loss: 0.2580245608793202\n",
      "epoch 5288: train loss: 0.14992443544576597, test loss: 0.2580211408457103\n",
      "epoch 5289: train loss: 0.14991576569295653, test loss: 0.2580177226234588\n",
      "epoch 5290: train loss: 0.14990709830619087, test loss: 0.25801430621155075\n",
      "epoch 5291: train loss: 0.14989843328437152, test loss: 0.2580108916089602\n",
      "epoch 5292: train loss: 0.14988977062640177, test loss: 0.25800747881467934\n",
      "epoch 5293: train loss: 0.1498811103311856, test loss: 0.25800406782768043\n",
      "epoch 5294: train loss: 0.14987245239762767, test loss: 0.25800065864696103\n",
      "epoch 5295: train loss: 0.1498637968246334, test loss: 0.25799725127149975\n",
      "epoch 5296: train loss: 0.149855143611109, test loss: 0.25799384570028067\n",
      "epoch 5297: train loss: 0.1498464927559612, test loss: 0.25799044193229365\n",
      "epoch 5298: train loss: 0.1498378442580976, test loss: 0.2579870399665251\n",
      "epoch 5299: train loss: 0.14982919811642648, test loss: 0.2579836398019629\n",
      "epoch 5300: train loss: 0.14982055432985678, test loss: 0.25798024143759657\n",
      "epoch 5301: train loss: 0.14981191289729817, test loss: 0.257976844872411\n",
      "epoch 5302: train loss: 0.14980327381766112, test loss: 0.25797345010540523\n",
      "epoch 5303: train loss: 0.14979463708985674, test loss: 0.2579700571355615\n",
      "epoch 5304: train loss: 0.14978600271279677, test loss: 0.25796666596187956\n",
      "epoch 5305: train loss: 0.1497773706853938, test loss: 0.2579632765833441\n",
      "epoch 5306: train loss: 0.14976874100656107, test loss: 0.25795988899895356\n",
      "epoch 5307: train loss: 0.1497601136752125, test loss: 0.25795650320770014\n",
      "epoch 5308: train loss: 0.14975148869026275, test loss: 0.2579531192085801\n",
      "epoch 5309: train loss: 0.1497428660506272, test loss: 0.2579497370005883\n",
      "epoch 5310: train loss: 0.14973424575522187, test loss: 0.2579463565827192\n",
      "epoch 5311: train loss: 0.14972562780296358, test loss: 0.25794297795397525\n",
      "epoch 5312: train loss: 0.14971701219276978, test loss: 0.2579396011133453\n",
      "epoch 5313: train loss: 0.14970839892355867, test loss: 0.25793622605983224\n",
      "epoch 5314: train loss: 0.1496997879942491, test loss: 0.25793285279243705\n",
      "epoch 5315: train loss: 0.1496911794037607, test loss: 0.2579294813101602\n",
      "epoch 5316: train loss: 0.14968257315101371, test loss: 0.2579261116119984\n",
      "epoch 5317: train loss: 0.14967396923492918, test loss: 0.2579227436969545\n",
      "epoch 5318: train loss: 0.14966536765442875, test loss: 0.2579193775640314\n",
      "epoch 5319: train loss: 0.1496567684084349, test loss: 0.2579160132122344\n",
      "epoch 5320: train loss: 0.14964817149587062, test loss: 0.25791265064056235\n",
      "epoch 5321: train loss: 0.14963957691565974, test loss: 0.25790928984801953\n",
      "epoch 5322: train loss: 0.14963098466672675, test loss: 0.25790593083361485\n",
      "epoch 5323: train loss: 0.1496223947479969, test loss: 0.25790257359635144\n",
      "epoch 5324: train loss: 0.149613807158396, test loss: 0.2578992181352389\n",
      "epoch 5325: train loss: 0.1496052218968507, test loss: 0.25789586444928314\n",
      "epoch 5326: train loss: 0.14959663896228823, test loss: 0.25789251253748646\n",
      "epoch 5327: train loss: 0.14958805835363656, test loss: 0.25788916239886484\n",
      "epoch 5328: train loss: 0.14957948006982444, test loss: 0.2578858140324248\n",
      "epoch 5329: train loss: 0.14957090410978116, test loss: 0.2578824674371774\n",
      "epoch 5330: train loss: 0.14956233047243683, test loss: 0.25787912261213164\n",
      "epoch 5331: train loss: 0.1495537591567222, test loss: 0.2578757795563027\n",
      "epoch 5332: train loss: 0.1495451901615687, test loss: 0.25787243826870115\n",
      "epoch 5333: train loss: 0.14953662348590854, test loss: 0.25786909874833597\n",
      "epoch 5334: train loss: 0.14952805912867445, test loss: 0.25786576099422637\n",
      "epoch 5335: train loss: 0.14951949708880005, test loss: 0.2578624250053863\n",
      "epoch 5336: train loss: 0.14951093736521948, test loss: 0.2578590907808264\n",
      "epoch 5337: train loss: 0.14950237995686766, test loss: 0.2578557583195697\n",
      "epoch 5338: train loss: 0.14949382486268026, test loss: 0.2578524276206285\n",
      "epoch 5339: train loss: 0.14948527208159348, test loss: 0.25784909868301903\n",
      "epoch 5340: train loss: 0.14947672161254436, test loss: 0.2578457715057592\n",
      "epoch 5341: train loss: 0.1494681734544705, test loss: 0.25784244608787416\n",
      "epoch 5342: train loss: 0.1494596276063103, test loss: 0.25783912242837476\n",
      "epoch 5343: train loss: 0.14945108406700275, test loss: 0.2578358005262903\n",
      "epoch 5344: train loss: 0.14944254283548764, test loss: 0.2578324803806329\n",
      "epoch 5345: train loss: 0.1494340039107053, test loss: 0.25782916199043093\n",
      "epoch 5346: train loss: 0.14942546729159686, test loss: 0.2578258453547038\n",
      "epoch 5347: train loss: 0.1494169329771041, test loss: 0.2578225304724745\n",
      "epoch 5348: train loss: 0.14940840096616945, test loss: 0.2578192173427671\n",
      "epoch 5349: train loss: 0.1493998712577361, test loss: 0.2578159059646069\n",
      "epoch 5350: train loss: 0.14939134385074784, test loss: 0.25781259633701975\n",
      "epoch 5351: train loss: 0.14938281874414922, test loss: 0.25780928845902606\n",
      "epoch 5352: train loss: 0.1493742959368854, test loss: 0.25780598232966123\n",
      "epoch 5353: train loss: 0.14936577542790222, test loss: 0.2578026779479471\n",
      "epoch 5354: train loss: 0.14935725721614626, test loss: 0.25779937531291436\n",
      "epoch 5355: train loss: 0.14934874130056477, test loss: 0.2577960744235921\n",
      "epoch 5356: train loss: 0.14934022768010563, test loss: 0.2577927752790036\n",
      "epoch 5357: train loss: 0.14933171635371745, test loss: 0.2577894778781871\n",
      "epoch 5358: train loss: 0.14932320732034954, test loss: 0.25778618222017013\n",
      "epoch 5359: train loss: 0.14931470057895171, test loss: 0.25778288830398277\n",
      "epoch 5360: train loss: 0.14930619612847473, test loss: 0.25777959612865947\n",
      "epoch 5361: train loss: 0.1492976939678698, test loss: 0.25777630569323495\n",
      "epoch 5362: train loss: 0.14928919409608896, test loss: 0.2577730169967407\n",
      "epoch 5363: train loss: 0.14928069651208473, test loss: 0.2577697300382104\n",
      "epoch 5364: train loss: 0.14927220121481058, test loss: 0.25776644481667965\n",
      "epoch 5365: train loss: 0.14926370820322046, test loss: 0.2577631613311897\n",
      "epoch 5366: train loss: 0.14925521747626905, test loss: 0.25775987958076524\n",
      "epoch 5367: train loss: 0.14924672903291167, test loss: 0.25775659956445446\n",
      "epoch 5368: train loss: 0.14923824287210433, test loss: 0.2577533212812925\n",
      "epoch 5369: train loss: 0.1492297589928037, test loss: 0.2577500447303184\n",
      "epoch 5370: train loss: 0.1492212773939672, test loss: 0.2577467699105668\n",
      "epoch 5371: train loss: 0.14921279807455284, test loss: 0.25774349682108233\n",
      "epoch 5372: train loss: 0.1492043210335193, test loss: 0.257740225460908\n",
      "epoch 5373: train loss: 0.14919584626982593, test loss: 0.2577369558290799\n",
      "epoch 5374: train loss: 0.14918737378243285, test loss: 0.25773368792464185\n",
      "epoch 5375: train loss: 0.1491789035703007, test loss: 0.25773042174663596\n",
      "epoch 5376: train loss: 0.14917043563239085, test loss: 0.25772715729410917\n",
      "epoch 5377: train loss: 0.14916196996766534, test loss: 0.2577238945661078\n",
      "epoch 5378: train loss: 0.14915350657508694, test loss: 0.257720633561668\n",
      "epoch 5379: train loss: 0.14914504545361895, test loss: 0.25771737427984165\n",
      "epoch 5380: train loss: 0.14913658660222545, test loss: 0.25771411671967026\n",
      "epoch 5381: train loss: 0.14912813001987113, test loss: 0.25771086088021117\n",
      "epoch 5382: train loss: 0.1491196757055214, test loss: 0.257707606760502\n",
      "epoch 5383: train loss: 0.14911122365814225, test loss: 0.25770435435959493\n",
      "epoch 5384: train loss: 0.1491027738767004, test loss: 0.257701103676538\n",
      "epoch 5385: train loss: 0.14909432636016315, test loss: 0.25769785471038165\n",
      "epoch 5386: train loss: 0.14908588110749865, test loss: 0.25769460746018025\n",
      "epoch 5387: train loss: 0.14907743811767543, test loss: 0.2576913619249772\n",
      "epoch 5388: train loss: 0.149068997389663, test loss: 0.2576881181038289\n",
      "epoch 5389: train loss: 0.14906055892243125, test loss: 0.2576848759957909\n",
      "epoch 5390: train loss: 0.14905212271495083, test loss: 0.257681635599909\n",
      "epoch 5391: train loss: 0.14904368876619314, test loss: 0.2576783969152428\n",
      "epoch 5392: train loss: 0.14903525707513016, test loss: 0.25767515994084905\n",
      "epoch 5393: train loss: 0.1490268276407345, test loss: 0.2576719246757763\n",
      "epoch 5394: train loss: 0.14901840046197948, test loss: 0.25766869111908464\n",
      "epoch 5395: train loss: 0.149009975537839, test loss: 0.2576654592698283\n",
      "epoch 5396: train loss: 0.14900155286728778, test loss: 0.25766222912706793\n",
      "epoch 5397: train loss: 0.148993132449301, test loss: 0.2576590006898615\n",
      "epoch 5398: train loss: 0.14898471428285467, test loss: 0.2576557739572634\n",
      "epoch 5399: train loss: 0.14897629836692527, test loss: 0.2576525489283407\n",
      "epoch 5400: train loss: 0.14896788470049013, test loss: 0.2576493256021473\n",
      "epoch 5401: train loss: 0.14895947328252712, test loss: 0.25764610397774296\n",
      "epoch 5402: train loss: 0.14895106411201475, test loss: 0.25764288405419444\n",
      "epoch 5403: train loss: 0.14894265718793226, test loss: 0.25763966583056086\n",
      "epoch 5404: train loss: 0.14893425250925943, test loss: 0.25763644930590374\n",
      "epoch 5405: train loss: 0.14892585007497686, test loss: 0.25763323447929215\n",
      "epoch 5406: train loss: 0.14891744988406563, test loss: 0.2576300213497863\n",
      "epoch 5407: train loss: 0.14890905193550755, test loss: 0.25762680991644993\n",
      "epoch 5408: train loss: 0.14890065622828513, test loss: 0.2576236001783487\n",
      "epoch 5409: train loss: 0.14889226276138143, test loss: 0.25762039213455484\n",
      "epoch 5410: train loss: 0.1488838715337802, test loss: 0.2576171857841248\n",
      "epoch 5411: train loss: 0.1488754825444658, test loss: 0.2576139811261381\n",
      "epoch 5412: train loss: 0.1488670957924234, test loss: 0.2576107781596527\n",
      "epoch 5413: train loss: 0.1488587112766386, test loss: 0.2576075768837455\n",
      "epoch 5414: train loss: 0.1488503289960978, test loss: 0.2576043772974794\n",
      "epoch 5415: train loss: 0.14884194894978794, test loss: 0.25760117939992966\n",
      "epoch 5416: train loss: 0.1488335711366967, test loss: 0.2575979831901671\n",
      "epoch 5417: train loss: 0.1488251955558123, test loss: 0.25759478866725877\n",
      "epoch 5418: train loss: 0.14881682220612374, test loss: 0.25759159583028235\n",
      "epoch 5419: train loss: 0.14880845108662058, test loss: 0.2575884046783106\n",
      "epoch 5420: train loss: 0.148800082196293, test loss: 0.25758521521040945\n",
      "epoch 5421: train loss: 0.1487917155341319, test loss: 0.25758202742566233\n",
      "epoch 5422: train loss: 0.1487833510991287, test loss: 0.257578841323142\n",
      "epoch 5423: train loss: 0.14877498889027566, test loss: 0.2575756569019212\n",
      "epoch 5424: train loss: 0.14876662890656545, test loss: 0.2575724741610799\n",
      "epoch 5425: train loss: 0.14875827114699158, test loss: 0.2575692930996901\n",
      "epoch 5426: train loss: 0.14874991561054812, test loss: 0.25756611371683796\n",
      "epoch 5427: train loss: 0.14874156229622973, test loss: 0.2575629360115931\n",
      "epoch 5428: train loss: 0.1487332112030318, test loss: 0.2575597599830371\n",
      "epoch 5429: train loss: 0.14872486232995022, test loss: 0.2575565856302538\n",
      "epoch 5430: train loss: 0.1487165156759817, test loss: 0.2575534129523157\n",
      "epoch 5431: train loss: 0.14870817124012356, test loss: 0.25755024194831116\n",
      "epoch 5432: train loss: 0.14869982902137355, test loss: 0.25754707261731785\n",
      "epoch 5433: train loss: 0.14869148901873028, test loss: 0.25754390495842544\n",
      "epoch 5434: train loss: 0.14868315123119294, test loss: 0.2575407389707052\n",
      "epoch 5435: train loss: 0.14867481565776136, test loss: 0.25753757465324606\n",
      "epoch 5436: train loss: 0.1486664822974359, test loss: 0.25753441200513455\n",
      "epoch 5437: train loss: 0.14865815114921768, test loss: 0.257531251025454\n",
      "epoch 5438: train loss: 0.14864982221210846, test loss: 0.25752809171329133\n",
      "epoch 5439: train loss: 0.14864149548511052, test loss: 0.25752493406772947\n",
      "epoch 5440: train loss: 0.14863317096722686, test loss: 0.25752177808785753\n",
      "epoch 5441: train loss: 0.1486248486574611, test loss: 0.2575186237727639\n",
      "epoch 5442: train loss: 0.1486165285548175, test loss: 0.2575154711215336\n",
      "epoch 5443: train loss: 0.1486082106583009, test loss: 0.25751232013325964\n",
      "epoch 5444: train loss: 0.1485998949669168, test loss: 0.2575091708070326\n",
      "epoch 5445: train loss: 0.1485915814796714, test loss: 0.2575060231419395\n",
      "epoch 5446: train loss: 0.1485832701955714, test loss: 0.2575028771370688\n",
      "epoch 5447: train loss: 0.14857496111362428, test loss: 0.25749973279151417\n",
      "epoch 5448: train loss: 0.148566654232838, test loss: 0.25749659010437204\n",
      "epoch 5449: train loss: 0.1485583495522212, test loss: 0.25749344907473165\n",
      "epoch 5450: train loss: 0.14855004707078323, test loss: 0.257490309701688\n",
      "epoch 5451: train loss: 0.14854174678753393, test loss: 0.2574871719843336\n",
      "epoch 5452: train loss: 0.1485334487014839, test loss: 0.2574840359217628\n",
      "epoch 5453: train loss: 0.1485251528116443, test loss: 0.2574809015130723\n",
      "epoch 5454: train loss: 0.14851685911702683, test loss: 0.2574777687573593\n",
      "epoch 5455: train loss: 0.14850856761664402, test loss: 0.25747463765371786\n",
      "epoch 5456: train loss: 0.14850027830950885, test loss: 0.25747150820124715\n",
      "epoch 5457: train loss: 0.14849199119463502, test loss: 0.2574683803990463\n",
      "epoch 5458: train loss: 0.1484837062710368, test loss: 0.257465254246213\n",
      "epoch 5459: train loss: 0.14847542353772902, test loss: 0.25746212974184574\n",
      "epoch 5460: train loss: 0.14846714299372737, test loss: 0.2574590068850466\n",
      "epoch 5461: train loss: 0.14845886463804794, test loss: 0.2574558856749162\n",
      "epoch 5462: train loss: 0.14845058846970746, test loss: 0.2574527661105529\n",
      "epoch 5463: train loss: 0.14844231448772338, test loss: 0.2574496481910588\n",
      "epoch 5464: train loss: 0.14843404269111374, test loss: 0.2574465319155411\n",
      "epoch 5465: train loss: 0.14842577307889712, test loss: 0.2574434172830989\n",
      "epoch 5466: train loss: 0.14841750565009282, test loss: 0.2574403042928379\n",
      "epoch 5467: train loss: 0.14840924040372072, test loss: 0.2574371929438623\n",
      "epoch 5468: train loss: 0.1484009773388013, test loss: 0.2574340832352746\n",
      "epoch 5469: train loss: 0.14839271645435573, test loss: 0.2574309751661858\n",
      "epoch 5470: train loss: 0.14838445774940567, test loss: 0.25742786873569934\n",
      "epoch 5471: train loss: 0.14837620122297351, test loss: 0.25742476394292163\n",
      "epoch 5472: train loss: 0.14836794687408222, test loss: 0.2574216607869653\n",
      "epoch 5473: train loss: 0.1483596947017554, test loss: 0.25741855926693097\n",
      "epoch 5474: train loss: 0.1483514447050172, test loss: 0.25741545938193233\n",
      "epoch 5475: train loss: 0.14834319688289252, test loss: 0.25741236113108207\n",
      "epoch 5476: train loss: 0.1483349512344067, test loss: 0.2574092645134818\n",
      "epoch 5477: train loss: 0.14832670775858586, test loss: 0.25740616952825396\n",
      "epoch 5478: train loss: 0.14831846645445662, test loss: 0.257403076174501\n",
      "epoch 5479: train loss: 0.14831022732104626, test loss: 0.2573999844513351\n",
      "epoch 5480: train loss: 0.14830199035738265, test loss: 0.2573968943578749\n",
      "epoch 5481: train loss: 0.14829375556249427, test loss: 0.2573938058932309\n",
      "epoch 5482: train loss: 0.14828552293541025, test loss: 0.2573907190565217\n",
      "epoch 5483: train loss: 0.1482772924751604, test loss: 0.2573876338468501\n",
      "epoch 5484: train loss: 0.1482690641807749, test loss: 0.2573845502633458\n",
      "epoch 5485: train loss: 0.1482608380512848, test loss: 0.25738146830511593\n",
      "epoch 5486: train loss: 0.14825261408572157, test loss: 0.2573783879712782\n",
      "epoch 5487: train loss: 0.14824439228311745, test loss: 0.2573753092609541\n",
      "epoch 5488: train loss: 0.1482361726425051, test loss: 0.25737223217325644\n",
      "epoch 5489: train loss: 0.148227955162918, test loss: 0.2573691567073069\n",
      "epoch 5490: train loss: 0.14821973984339012, test loss: 0.25736608286222923\n",
      "epoch 5491: train loss: 0.14821152668295604, test loss: 0.25736301063713113\n",
      "epoch 5492: train loss: 0.14820331568065087, test loss: 0.25735994003114393\n",
      "epoch 5493: train loss: 0.14819510683551057, test loss: 0.2573568710433842\n",
      "epoch 5494: train loss: 0.1481869001465714, test loss: 0.2573538036729751\n",
      "epoch 5495: train loss: 0.1481786956128705, test loss: 0.2573507379190351\n",
      "epoch 5496: train loss: 0.14817049323344542, test loss: 0.25734767378069245\n",
      "epoch 5497: train loss: 0.1481622930073344, test loss: 0.2573446112570718\n",
      "epoch 5498: train loss: 0.14815409493357623, test loss: 0.25734155034729084\n",
      "epoch 5499: train loss: 0.14814589901121042, test loss: 0.2573384910504785\n",
      "epoch 5500: train loss: 0.14813770523927694, test loss: 0.25733543336575765\n",
      "epoch 5501: train loss: 0.14812951361681645, test loss: 0.25733237729226016\n",
      "epoch 5502: train loss: 0.14812132414287016, test loss: 0.25732932282910675\n",
      "epoch 5503: train loss: 0.14811313681647995, test loss: 0.2573262699754283\n",
      "epoch 5504: train loss: 0.14810495163668824, test loss: 0.2573232187303519\n",
      "epoch 5505: train loss: 0.1480967686025381, test loss: 0.2573201690930056\n",
      "epoch 5506: train loss: 0.1480885877130731, test loss: 0.25731712106252125\n",
      "epoch 5507: train loss: 0.14808040896733754, test loss: 0.25731407463802647\n",
      "epoch 5508: train loss: 0.14807223236437622, test loss: 0.2573110298186476\n",
      "epoch 5509: train loss: 0.1480640579032346, test loss: 0.25730798660352383\n",
      "epoch 5510: train loss: 0.1480558855829587, test loss: 0.25730494499178125\n",
      "epoch 5511: train loss: 0.14804771540259515, test loss: 0.25730190498255623\n",
      "epoch 5512: train loss: 0.1480395473611912, test loss: 0.2572988665749794\n",
      "epoch 5513: train loss: 0.14803138145779468, test loss: 0.25729582976818216\n",
      "epoch 5514: train loss: 0.14802321769145396, test loss: 0.25729279456130455\n",
      "epoch 5515: train loss: 0.14801505606121812, test loss: 0.2572897609534759\n",
      "epoch 5516: train loss: 0.1480068965661367, test loss: 0.25728672894383353\n",
      "epoch 5517: train loss: 0.14799873920525994, test loss: 0.2572836985315136\n",
      "epoch 5518: train loss: 0.14799058397763865, test loss: 0.25728066971565633\n",
      "epoch 5519: train loss: 0.14798243088232424, test loss: 0.25727764249539287\n",
      "epoch 5520: train loss: 0.14797427991836865, test loss: 0.2572746168698631\n",
      "epoch 5521: train loss: 0.14796613108482448, test loss: 0.2572715928382044\n",
      "epoch 5522: train loss: 0.1479579843807449, test loss: 0.2572685703995628\n",
      "epoch 5523: train loss: 0.14794983980518364, test loss: 0.25726554955307174\n",
      "epoch 5524: train loss: 0.1479416973571951, test loss: 0.2572625302978728\n",
      "epoch 5525: train loss: 0.14793355703583416, test loss: 0.2572595126331085\n",
      "epoch 5526: train loss: 0.14792541884015642, test loss: 0.25725649655791477\n",
      "epoch 5527: train loss: 0.147917282769218, test loss: 0.25725348207144244\n",
      "epoch 5528: train loss: 0.14790914882207556, test loss: 0.25725046917283\n",
      "epoch 5529: train loss: 0.14790101699778646, test loss: 0.2572474578612177\n",
      "epoch 5530: train loss: 0.14789288729540853, test loss: 0.25724444813575387\n",
      "epoch 5531: train loss: 0.1478847597140003, test loss: 0.25724143999558363\n",
      "epoch 5532: train loss: 0.1478766342526208, test loss: 0.2572384334398529\n",
      "epoch 5533: train loss: 0.1478685109103297, test loss: 0.2572354284677012\n",
      "epoch 5534: train loss: 0.14786038968618723, test loss: 0.25723242507828187\n",
      "epoch 5535: train loss: 0.14785227057925418, test loss: 0.2572294232707391\n",
      "epoch 5536: train loss: 0.147844153588592, test loss: 0.2572264230442183\n",
      "epoch 5537: train loss: 0.14783603871326265, test loss: 0.2572234243978717\n",
      "epoch 5538: train loss: 0.14782792595232874, test loss: 0.2572204273308492\n",
      "epoch 5539: train loss: 0.14781981530485344, test loss: 0.25721743184229795\n",
      "epoch 5540: train loss: 0.1478117067699004, test loss: 0.25721443793136656\n",
      "epoch 5541: train loss: 0.14780360034653406, test loss: 0.25721144559720543\n",
      "epoch 5542: train loss: 0.1477954960338193, test loss: 0.2572084548389703\n",
      "epoch 5543: train loss: 0.14778739383082157, test loss: 0.2572054656558091\n",
      "epoch 5544: train loss: 0.147779293736607, test loss: 0.25720247804687957\n",
      "epoch 5545: train loss: 0.1477711957502422, test loss: 0.25719949201132847\n",
      "epoch 5546: train loss: 0.1477630998707944, test loss: 0.25719650754831014\n",
      "epoch 5547: train loss: 0.14775500609733144, test loss: 0.2571935246569852\n",
      "epoch 5548: train loss: 0.14774691442892174, test loss: 0.25719054333650126\n",
      "epoch 5549: train loss: 0.14773882486463422, test loss: 0.2571875635860168\n",
      "epoch 5550: train loss: 0.14773073740353845, test loss: 0.25718458540468975\n",
      "epoch 5551: train loss: 0.14772265204470458, test loss: 0.2571816087916756\n",
      "epoch 5552: train loss: 0.1477145687872033, test loss: 0.2571786337461268\n",
      "epoch 5553: train loss: 0.14770648763010588, test loss: 0.2571756602672123\n",
      "epoch 5554: train loss: 0.14769840857248423, test loss: 0.25717268835407775\n",
      "epoch 5555: train loss: 0.14769033161341075, test loss: 0.2571697180058951\n",
      "epoch 5556: train loss: 0.14768225675195845, test loss: 0.2571667492218133\n",
      "epoch 5557: train loss: 0.14767418398720097, test loss: 0.25716378200099943\n",
      "epoch 5558: train loss: 0.1476661133182124, test loss: 0.25716081634261295\n",
      "epoch 5559: train loss: 0.14765804474406752, test loss: 0.2571578522458095\n",
      "epoch 5560: train loss: 0.14764997826384169, test loss: 0.2571548897097591\n",
      "epoch 5561: train loss: 0.14764191387661071, test loss: 0.25715192873362086\n",
      "epoch 5562: train loss: 0.1476338515814511, test loss: 0.2571489693165601\n",
      "epoch 5563: train loss: 0.14762579137743986, test loss: 0.25714601145774035\n",
      "epoch 5564: train loss: 0.14761773326365465, test loss: 0.2571430551563229\n",
      "epoch 5565: train loss: 0.1476096772391736, test loss: 0.2571401004114754\n",
      "epoch 5566: train loss: 0.1476016233030754, test loss: 0.25713714722236425\n",
      "epoch 5567: train loss: 0.1475935714544395, test loss: 0.257134195588152\n",
      "epoch 5568: train loss: 0.1475855216923457, test loss: 0.2571312455080118\n",
      "epoch 5569: train loss: 0.1475774740158745, test loss: 0.25712829698110645\n",
      "epoch 5570: train loss: 0.14756942842410692, test loss: 0.2571253500066033\n",
      "epoch 5571: train loss: 0.14756138491612453, test loss: 0.2571224045836733\n",
      "epoch 5572: train loss: 0.14755334349100951, test loss: 0.25711946071148434\n",
      "epoch 5573: train loss: 0.1475453041478446, test loss: 0.25711651838920685\n",
      "epoch 5574: train loss: 0.14753726688571311, test loss: 0.257113577616011\n",
      "epoch 5575: train loss: 0.14752923170369894, test loss: 0.25711063839106996\n",
      "epoch 5576: train loss: 0.14752119860088644, test loss: 0.2571077007135537\n",
      "epoch 5577: train loss: 0.14751316757636068, test loss: 0.2571047645826333\n",
      "epoch 5578: train loss: 0.14750513862920722, test loss: 0.2571018299974781\n",
      "epoch 5579: train loss: 0.14749711175851218, test loss: 0.25709889695726995\n",
      "epoch 5580: train loss: 0.14748908696336224, test loss: 0.2570959654611798\n",
      "epoch 5581: train loss: 0.14748106424284468, test loss: 0.25709303550837836\n",
      "epoch 5582: train loss: 0.14747304359604738, test loss: 0.2570901070980392\n",
      "epoch 5583: train loss: 0.14746502502205863, test loss: 0.25708718022934407\n",
      "epoch 5584: train loss: 0.14745700851996746, test loss: 0.2570842549014701\n",
      "epoch 5585: train loss: 0.1474489940888633, test loss: 0.25708133111358916\n",
      "epoch 5586: train loss: 0.14744098172783635, test loss: 0.2570784088648798\n",
      "epoch 5587: train loss: 0.14743297143597714, test loss: 0.25707548815452297\n",
      "epoch 5588: train loss: 0.14742496321237694, test loss: 0.2570725689816918\n",
      "epoch 5589: train loss: 0.14741695705612745, test loss: 0.2570696513455692\n",
      "epoch 5590: train loss: 0.14740895296632106, test loss: 0.2570667352453357\n",
      "epoch 5591: train loss: 0.14740095094205063, test loss: 0.2570638206801697\n",
      "epoch 5592: train loss: 0.1473929509824096, test loss: 0.2570609076492495\n",
      "epoch 5593: train loss: 0.14738495308649194, test loss: 0.2570579961517635\n",
      "epoch 5594: train loss: 0.14737695725339223, test loss: 0.25705508618688794\n",
      "epoch 5595: train loss: 0.14736896348220557, test loss: 0.2570521777538056\n",
      "epoch 5596: train loss: 0.14736097177202767, test loss: 0.25704927085170504\n",
      "epoch 5597: train loss: 0.14735298212195475, test loss: 0.25704636547976223\n",
      "epoch 5598: train loss: 0.14734499453108357, test loss: 0.2570434616371694\n",
      "epoch 5599: train loss: 0.14733700899851152, test loss: 0.2570405593231062\n",
      "epoch 5600: train loss: 0.14732902552333646, test loss: 0.25703765853675975\n",
      "epoch 5601: train loss: 0.14732104410465685, test loss: 0.2570347592773137\n",
      "epoch 5602: train loss: 0.14731306474157174, test loss: 0.2570318615439585\n",
      "epoch 5603: train loss: 0.1473050874331807, test loss: 0.2570289653358829\n",
      "epoch 5604: train loss: 0.14729711217858374, test loss: 0.25702607065226696\n",
      "epoch 5605: train loss: 0.14728913897688165, test loss: 0.2570231774923057\n",
      "epoch 5606: train loss: 0.14728116782717562, test loss: 0.2570202858551879\n",
      "epoch 5607: train loss: 0.14727319872856745, test loss: 0.2570173957400981\n",
      "epoch 5608: train loss: 0.1472652316801594, test loss: 0.2570145071462324\n",
      "epoch 5609: train loss: 0.14725726668105443, test loss: 0.2570116200727771\n",
      "epoch 5610: train loss: 0.14724930373035597, test loss: 0.25700873451892364\n",
      "epoch 5611: train loss: 0.14724134282716794, test loss: 0.2570058504838698\n",
      "epoch 5612: train loss: 0.14723338397059493, test loss: 0.2570029679667989\n",
      "epoch 5613: train loss: 0.14722542715974204, test loss: 0.2570000869669083\n",
      "epoch 5614: train loss: 0.14721747239371485, test loss: 0.25699720748339006\n",
      "epoch 5615: train loss: 0.14720951967161963, test loss: 0.25699432951544077\n",
      "epoch 5616: train loss: 0.147201568992563, test loss: 0.2569914530622558\n",
      "epoch 5617: train loss: 0.14719362035565237, test loss: 0.2569885781230284\n",
      "epoch 5618: train loss: 0.14718567375999547, test loss: 0.25698570469694965\n",
      "epoch 5619: train loss: 0.1471777292047007, test loss: 0.256982832783223\n",
      "epoch 5620: train loss: 0.147169786688877, test loss: 0.2569799623810472\n",
      "epoch 5621: train loss: 0.14716184621163386, test loss: 0.25697709348960873\n",
      "epoch 5622: train loss: 0.1471539077720813, test loss: 0.2569742261081156\n",
      "epoch 5623: train loss: 0.14714597136932983, test loss: 0.2569713602357622\n",
      "epoch 5624: train loss: 0.1471380370024906, test loss: 0.256968495871748\n",
      "epoch 5625: train loss: 0.14713010467067522, test loss: 0.2569656330152754\n",
      "epoch 5626: train loss: 0.147122174372996, test loss: 0.25696277166554066\n",
      "epoch 5627: train loss: 0.14711424610856555, test loss: 0.2569599118217453\n",
      "epoch 5628: train loss: 0.14710631987649722, test loss: 0.25695705348309533\n",
      "epoch 5629: train loss: 0.14709839567590483, test loss: 0.2569541966487873\n",
      "epoch 5630: train loss: 0.14709047350590276, test loss: 0.25695134131802233\n",
      "epoch 5631: train loss: 0.14708255336560588, test loss: 0.2569484874900114\n",
      "epoch 5632: train loss: 0.1470746352541297, test loss: 0.2569456351639534\n",
      "epoch 5633: train loss: 0.14706671917059022, test loss: 0.2569427843390529\n",
      "epoch 5634: train loss: 0.14705880511410394, test loss: 0.25693993501450996\n",
      "epoch 5635: train loss: 0.14705089308378794, test loss: 0.256937087189538\n",
      "epoch 5636: train loss: 0.14704298307875985, test loss: 0.256934240863337\n",
      "epoch 5637: train loss: 0.14703507509813787, test loss: 0.25693139603511717\n",
      "epoch 5638: train loss: 0.14702716914104064, test loss: 0.2569285527040836\n",
      "epoch 5639: train loss: 0.14701926520658742, test loss: 0.25692571086944704\n",
      "epoch 5640: train loss: 0.14701136329389797, test loss: 0.2569228705304091\n",
      "epoch 5641: train loss: 0.14700346340209264, test loss: 0.25692003168618166\n",
      "epoch 5642: train loss: 0.14699556553029225, test loss: 0.2569171943359752\n",
      "epoch 5643: train loss: 0.14698766967761814, test loss: 0.2569143584790009\n",
      "epoch 5644: train loss: 0.14697977584319233, test loss: 0.2569115241144665\n",
      "epoch 5645: train loss: 0.1469718840261372, test loss: 0.2569086912415811\n",
      "epoch 5646: train loss: 0.14696399422557582, test loss: 0.2569058598595582\n",
      "epoch 5647: train loss: 0.1469561064406317, test loss: 0.2569030299676104\n",
      "epoch 5648: train loss: 0.1469482206704289, test loss: 0.2569002015649496\n",
      "epoch 5649: train loss: 0.14694033691409195, test loss: 0.2568973746507878\n",
      "epoch 5650: train loss: 0.14693245517074607, test loss: 0.25689454922433913\n",
      "epoch 5651: train loss: 0.14692457543951692, test loss: 0.256891725284822\n",
      "epoch 5652: train loss: 0.1469166977195307, test loss: 0.2568889028314458\n",
      "epoch 5653: train loss: 0.14690882200991412, test loss: 0.2568860818634268\n",
      "epoch 5654: train loss: 0.14690094830979447, test loss: 0.2568832623799788\n",
      "epoch 5655: train loss: 0.14689307661829953, test loss: 0.25688044438032553\n",
      "epoch 5656: train loss: 0.14688520693455764, test loss: 0.2568776278636773\n",
      "epoch 5657: train loss: 0.1468773392576977, test loss: 0.25687481282925195\n",
      "epoch 5658: train loss: 0.14686947358684904, test loss: 0.25687199927627014\n",
      "epoch 5659: train loss: 0.1468616099211416, test loss: 0.2568691872039489\n",
      "epoch 5660: train loss: 0.1468537482597059, test loss: 0.25686637661151096\n",
      "epoch 5661: train loss: 0.14684588860167286, test loss: 0.2568635674981691\n",
      "epoch 5662: train loss: 0.14683803094617395, test loss: 0.25686075986315\n",
      "epoch 5663: train loss: 0.14683017529234133, test loss: 0.2568579537056689\n",
      "epoch 5664: train loss: 0.14682232163930747, test loss: 0.25685514902495293\n",
      "epoch 5665: train loss: 0.1468144699862055, test loss: 0.25685234582021865\n",
      "epoch 5666: train loss: 0.14680662033216904, test loss: 0.2568495440906908\n",
      "epoch 5667: train loss: 0.14679877267633226, test loss: 0.2568467438355948\n",
      "epoch 5668: train loss: 0.14679092701782978, test loss: 0.25684394505414815\n",
      "epoch 5669: train loss: 0.14678308335579693, test loss: 0.25684114774557737\n",
      "epoch 5670: train loss: 0.14677524168936926, test loss: 0.25683835190911136\n",
      "epoch 5671: train loss: 0.14676740201768318, test loss: 0.25683555754397036\n",
      "epoch 5672: train loss: 0.14675956433987541, test loss: 0.2568327646493801\n",
      "epoch 5673: train loss: 0.14675172865508324, test loss: 0.25682997322456885\n",
      "epoch 5674: train loss: 0.14674389496244453, test loss: 0.25682718326876325\n",
      "epoch 5675: train loss: 0.14673606326109762, test loss: 0.256824394781187\n",
      "epoch 5676: train loss: 0.14672823355018136, test loss: 0.25682160776107116\n",
      "epoch 5677: train loss: 0.1467204058288352, test loss: 0.2568188222076455\n",
      "epoch 5678: train loss: 0.14671258009619903, test loss: 0.25681603812013604\n",
      "epoch 5679: train loss: 0.14670475635141325, test loss: 0.2568132554977754\n",
      "epoch 5680: train loss: 0.1466969345936189, test loss: 0.2568104743397876\n",
      "epoch 5681: train loss: 0.14668911482195746, test loss: 0.25680769464540826\n",
      "epoch 5682: train loss: 0.14668129703557087, test loss: 0.2568049164138653\n",
      "epoch 5683: train loss: 0.14667348123360174, test loss: 0.25680213964439447\n",
      "epoch 5684: train loss: 0.14666566741519305, test loss: 0.2567993643362256\n",
      "epoch 5685: train loss: 0.1466578555794884, test loss: 0.25679659048858744\n",
      "epoch 5686: train loss: 0.14665004572563187, test loss: 0.2567938181007191\n",
      "epoch 5687: train loss: 0.14664223785276806, test loss: 0.2567910471718517\n",
      "epoch 5688: train loss: 0.14663443196004206, test loss: 0.25678827770122026\n",
      "epoch 5689: train loss: 0.1466266280465996, test loss: 0.25678550968805797\n",
      "epoch 5690: train loss: 0.14661882611158675, test loss: 0.25678274313159877\n",
      "epoch 5691: train loss: 0.14661102615415023, test loss: 0.25677997803108404\n",
      "epoch 5692: train loss: 0.14660322817343718, test loss: 0.25677721438574735\n",
      "epoch 5693: train loss: 0.14659543216859539, test loss: 0.2567744521948223\n",
      "epoch 5694: train loss: 0.146587638138773, test loss: 0.2567716914575523\n",
      "epoch 5695: train loss: 0.14657984608311886, test loss: 0.2567689321731698\n",
      "epoch 5696: train loss: 0.1465720560007821, test loss: 0.2567661743409162\n",
      "epoch 5697: train loss: 0.1465642678909126, test loss: 0.2567634179600281\n",
      "epoch 5698: train loss: 0.14655648175266053, test loss: 0.25676066302974887\n",
      "epoch 5699: train loss: 0.14654869758517675, test loss: 0.2567579095493154\n",
      "epoch 5700: train loss: 0.1465409153876126, test loss: 0.25675515751797257\n",
      "epoch 5701: train loss: 0.14653313515911984, test loss: 0.25675240693495416\n",
      "epoch 5702: train loss: 0.14652535689885085, test loss: 0.2567496577995083\n",
      "epoch 5703: train loss: 0.14651758060595843, test loss: 0.2567469101108753\n",
      "epoch 5704: train loss: 0.14650980627959603, test loss: 0.2567441638682968\n",
      "epoch 5705: train loss: 0.14650203391891745, test loss: 0.2567414190710149\n",
      "epoch 5706: train loss: 0.1464942635230771, test loss: 0.2567386757182802\n",
      "epoch 5707: train loss: 0.1464864950912298, test loss: 0.2567359338093282\n",
      "epoch 5708: train loss: 0.14647872862253108, test loss: 0.25673319334340744\n",
      "epoch 5709: train loss: 0.14647096411613675, test loss: 0.2567304543197622\n",
      "epoch 5710: train loss: 0.14646320157120335, test loss: 0.2567277167376412\n",
      "epoch 5711: train loss: 0.14645544098688767, test loss: 0.25672498059629045\n",
      "epoch 5712: train loss: 0.1464476823623472, test loss: 0.2567222458949565\n",
      "epoch 5713: train loss: 0.14643992569673997, test loss: 0.25671951263288195\n",
      "epoch 5714: train loss: 0.14643217098922434, test loss: 0.2567167808093195\n",
      "epoch 5715: train loss: 0.1464244182389593, test loss: 0.2567140504235177\n",
      "epoch 5716: train loss: 0.14641666744510437, test loss: 0.25671132147472575\n",
      "epoch 5717: train loss: 0.14640891860681943, test loss: 0.25670859396218854\n",
      "epoch 5718: train loss: 0.14640117172326506, test loss: 0.2567058678851627\n",
      "epoch 5719: train loss: 0.1463934267936022, test loss: 0.25670314324289106\n",
      "epoch 5720: train loss: 0.1463856838169924, test loss: 0.25670042003463467\n",
      "epoch 5721: train loss: 0.14637794279259755, test loss: 0.2566976982596359\n",
      "epoch 5722: train loss: 0.14637020371958026, test loss: 0.2566949779171508\n",
      "epoch 5723: train loss: 0.1463624665971035, test loss: 0.2566922590064322\n",
      "epoch 5724: train loss: 0.1463547314243308, test loss: 0.256689541526736\n",
      "epoch 5725: train loss: 0.14634699820042613, test loss: 0.2566868254773098\n",
      "epoch 5726: train loss: 0.1463392669245541, test loss: 0.2566841108574134\n",
      "epoch 5727: train loss: 0.14633153759587963, test loss: 0.2566813976662975\n",
      "epoch 5728: train loss: 0.14632381021356827, test loss: 0.25667868590321397\n",
      "epoch 5729: train loss: 0.14631608477678612, test loss: 0.2566759755674303\n",
      "epoch 5730: train loss: 0.14630836128469962, test loss: 0.2566732666581926\n",
      "epoch 5731: train loss: 0.14630063973647586, test loss: 0.2566705591747629\n",
      "epoch 5732: train loss: 0.14629292013128237, test loss: 0.2566678531163921\n",
      "epoch 5733: train loss: 0.14628520246828713, test loss: 0.25666514848234434\n",
      "epoch 5734: train loss: 0.1462774867466587, test loss: 0.25666244527187365\n",
      "epoch 5735: train loss: 0.1462697729655661, test loss: 0.25665974348424386\n",
      "epoch 5736: train loss: 0.1462620611241789, test loss: 0.25665704311871435\n",
      "epoch 5737: train loss: 0.1462543512216671, test loss: 0.2566543441745356\n",
      "epoch 5738: train loss: 0.14624664325720116, test loss: 0.25665164665098045\n",
      "epoch 5739: train loss: 0.14623893722995224, test loss: 0.25664895054729836\n",
      "epoch 5740: train loss: 0.14623123313909173, test loss: 0.2566462558627568\n",
      "epoch 5741: train loss: 0.14622353098379176, test loss: 0.2566435625966197\n",
      "epoch 5742: train loss: 0.14621583076322478, test loss: 0.25664087074814534\n",
      "epoch 5743: train loss: 0.14620813247656383, test loss: 0.2566381803165976\n",
      "epoch 5744: train loss: 0.1462004361229824, test loss: 0.25663549130124225\n",
      "epoch 5745: train loss: 0.1461927417016545, test loss: 0.25663280370134217\n",
      "epoch 5746: train loss: 0.14618504921175465, test loss: 0.2566301175161543\n",
      "epoch 5747: train loss: 0.14617735865245782, test loss: 0.2566274327449567\n",
      "epoch 5748: train loss: 0.14616967002293946, test loss: 0.2566247493870062\n",
      "epoch 5749: train loss: 0.14616198332237565, test loss: 0.25662206744156774\n",
      "epoch 5750: train loss: 0.14615429854994283, test loss: 0.256619386907913\n",
      "epoch 5751: train loss: 0.14614661570481793, test loss: 0.25661670778530954\n",
      "epoch 5752: train loss: 0.14613893478617845, test loss: 0.25661403007302125\n",
      "epoch 5753: train loss: 0.14613125579320235, test loss: 0.25661135377031535\n",
      "epoch 5754: train loss: 0.14612357872506807, test loss: 0.25660867887646455\n",
      "epoch 5755: train loss: 0.14611590358095453, test loss: 0.25660600539073436\n",
      "epoch 5756: train loss: 0.1461082303600412, test loss: 0.2566033333123906\n",
      "epoch 5757: train loss: 0.14610055906150796, test loss: 0.25660066264071246\n",
      "epoch 5758: train loss: 0.14609288968453524, test loss: 0.25659799337496486\n",
      "epoch 5759: train loss: 0.14608522222830395, test loss: 0.2565953255144195\n",
      "epoch 5760: train loss: 0.14607755669199549, test loss: 0.25659265905834877\n",
      "epoch 5761: train loss: 0.14606989307479176, test loss: 0.25658999400602667\n",
      "epoch 5762: train loss: 0.14606223137587507, test loss: 0.2565873303567185\n",
      "epoch 5763: train loss: 0.1460545715944283, test loss: 0.2565846681097054\n",
      "epoch 5764: train loss: 0.1460469137296349, test loss: 0.25658200726425406\n",
      "epoch 5765: train loss: 0.14603925778067858, test loss: 0.2565793478196434\n",
      "epoch 5766: train loss: 0.1460316037467437, test loss: 0.25657668977515036\n",
      "epoch 5767: train loss: 0.1460239516270151, test loss: 0.25657403313003757\n",
      "epoch 5768: train loss: 0.14601630142067806, test loss: 0.2565713778835947\n",
      "epoch 5769: train loss: 0.14600865312691838, test loss: 0.25656872403509345\n",
      "epoch 5770: train loss: 0.14600100674492233, test loss: 0.2565660715838054\n",
      "epoch 5771: train loss: 0.1459933622738766, test loss: 0.25656342052901293\n",
      "epoch 5772: train loss: 0.1459857197129686, test loss: 0.25656077086999124\n",
      "epoch 5773: train loss: 0.14597807906138588, test loss: 0.25655812260601984\n",
      "epoch 5774: train loss: 0.14597044031831674, test loss: 0.2565554757363748\n",
      "epoch 5775: train loss: 0.14596280348294985, test loss: 0.2565528302603378\n",
      "epoch 5776: train loss: 0.14595516855447443, test loss: 0.25655018617718384\n",
      "epoch 5777: train loss: 0.1459475355320801, test loss: 0.2565475434862018\n",
      "epoch 5778: train loss: 0.14593990441495702, test loss: 0.25654490218666337\n",
      "epoch 5779: train loss: 0.14593227520229582, test loss: 0.2565422622778514\n",
      "epoch 5780: train loss: 0.14592464789328766, test loss: 0.2565396237590517\n",
      "epoch 5781: train loss: 0.14591702248712404, test loss: 0.25653698662954033\n",
      "epoch 5782: train loss: 0.14590939898299715, test loss: 0.25653435088860455\n",
      "epoch 5783: train loss: 0.14590177738009946, test loss: 0.2565317165355258\n",
      "epoch 5784: train loss: 0.145894157677624, test loss: 0.2565290835695876\n",
      "epoch 5785: train loss: 0.1458865398747644, test loss: 0.2565264519900716\n",
      "epoch 5786: train loss: 0.14587892397071456, test loss: 0.256523821796262\n",
      "epoch 5787: train loss: 0.14587130996466896, test loss: 0.25652119298745085\n",
      "epoch 5788: train loss: 0.1458636978558226, test loss: 0.2565185655629131\n",
      "epoch 5789: train loss: 0.14585608764337088, test loss: 0.25651593952194324\n",
      "epoch 5790: train loss: 0.14584847932650977, test loss: 0.2565133148638259\n",
      "epoch 5791: train loss: 0.14584087290443565, test loss: 0.25651069158784306\n",
      "epoch 5792: train loss: 0.14583326837634533, test loss: 0.25650806969328893\n",
      "epoch 5793: train loss: 0.1458256657414362, test loss: 0.25650544917944673\n",
      "epoch 5794: train loss: 0.14581806499890615, test loss: 0.2565028300456085\n",
      "epoch 5795: train loss: 0.1458104661479534, test loss: 0.2565002122910574\n",
      "epoch 5796: train loss: 0.14580286918777674, test loss: 0.2564975959150864\n",
      "epoch 5797: train loss: 0.14579527411757545, test loss: 0.25649498091698414\n",
      "epoch 5798: train loss: 0.14578768093654929, test loss: 0.25649236729604385\n",
      "epoch 5799: train loss: 0.14578008964389838, test loss: 0.25648975505155586\n",
      "epoch 5800: train loss: 0.1457725002388235, test loss: 0.25648714418280844\n",
      "epoch 5801: train loss: 0.14576491272052572, test loss: 0.25648453468909344\n",
      "epoch 5802: train loss: 0.14575732708820674, test loss: 0.25648192656970475\n",
      "epoch 5803: train loss: 0.14574974334106866, test loss: 0.25647931982393646\n",
      "epoch 5804: train loss: 0.145742161478314, test loss: 0.2564767144510781\n",
      "epoch 5805: train loss: 0.1457345814991459, test loss: 0.2564741104504251\n",
      "epoch 5806: train loss: 0.1457270034027678, test loss: 0.256471507821275\n",
      "epoch 5807: train loss: 0.1457194271883838, test loss: 0.25646890656291677\n",
      "epoch 5808: train loss: 0.14571185285519822, test loss: 0.2564663066746501\n",
      "epoch 5809: train loss: 0.14570428040241615, test loss: 0.2564637081557663\n",
      "epoch 5810: train loss: 0.1456967098292429, test loss: 0.2564611110055683\n",
      "epoch 5811: train loss: 0.1456891411348844, test loss: 0.2564585152233451\n",
      "epoch 5812: train loss: 0.14568157431854703, test loss: 0.25645592080839796\n",
      "epoch 5813: train loss: 0.14567400937943756, test loss: 0.256453327760025\n",
      "epoch 5814: train loss: 0.1456664463167633, test loss: 0.25645073607751995\n",
      "epoch 5815: train loss: 0.145658885129732, test loss: 0.2564481457601859\n",
      "epoch 5816: train loss: 0.1456513258175519, test loss: 0.25644555680732245\n",
      "epoch 5817: train loss: 0.1456437683794318, test loss: 0.2564429692182241\n",
      "epoch 5818: train loss: 0.1456362128145807, test loss: 0.25644038299219063\n",
      "epoch 5819: train loss: 0.14562865912220835, test loss: 0.2564377981285332\n",
      "epoch 5820: train loss: 0.14562110730152486, test loss: 0.25643521462653746\n",
      "epoch 5821: train loss: 0.14561355735174072, test loss: 0.25643263248551845\n",
      "epoch 5822: train loss: 0.14560600927206707, test loss: 0.25643005170476363\n",
      "epoch 5823: train loss: 0.14559846306171534, test loss: 0.25642747228359286\n",
      "epoch 5824: train loss: 0.14559091871989754, test loss: 0.2564248942212962\n",
      "epoch 5825: train loss: 0.1455833762458261, test loss: 0.25642231751717803\n",
      "epoch 5826: train loss: 0.14557583563871393, test loss: 0.2564197421705459\n",
      "epoch 5827: train loss: 0.1455682968977744, test loss: 0.2564171681807007\n",
      "epoch 5828: train loss: 0.14556076002222135, test loss: 0.25641459554695273\n",
      "epoch 5829: train loss: 0.14555322501126905, test loss: 0.25641202426860493\n",
      "epoch 5830: train loss: 0.14554569186413235, test loss: 0.2564094543449605\n",
      "epoch 5831: train loss: 0.14553816058002636, test loss: 0.2564068857753238\n",
      "epoch 5832: train loss: 0.14553063115816683, test loss: 0.2564043185590075\n",
      "epoch 5833: train loss: 0.1455231035977699, test loss: 0.25640175269531623\n",
      "epoch 5834: train loss: 0.14551557789805222, test loss: 0.25639918818355634\n",
      "epoch 5835: train loss: 0.14550805405823083, test loss: 0.25639662502303534\n",
      "epoch 5836: train loss: 0.14550053207752328, test loss: 0.2563940632130643\n",
      "epoch 5837: train loss: 0.1454930119551476, test loss: 0.25639150275294853\n",
      "epoch 5838: train loss: 0.14548549369032224, test loss: 0.25638894364200193\n",
      "epoch 5839: train loss: 0.14547797728226608, test loss: 0.2563863858795345\n",
      "epoch 5840: train loss: 0.1454704627301986, test loss: 0.2563838294648507\n",
      "epoch 5841: train loss: 0.14546295003333956, test loss: 0.2563812743972687\n",
      "epoch 5842: train loss: 0.14545543919090928, test loss: 0.25637872067609213\n",
      "epoch 5843: train loss: 0.14544793020212857, test loss: 0.25637616830064186\n",
      "epoch 5844: train loss: 0.14544042306621863, test loss: 0.25637361727022034\n",
      "epoch 5845: train loss: 0.1454329177824011, test loss: 0.2563710675841493\n",
      "epoch 5846: train loss: 0.1454254143498982, test loss: 0.2563685192417379\n",
      "epoch 5847: train loss: 0.14541791276793245, test loss: 0.25636597224229685\n",
      "epoch 5848: train loss: 0.14541041303572697, test loss: 0.25636342658514966\n",
      "epoch 5849: train loss: 0.14540291515250525, test loss: 0.25636088226960074\n",
      "epoch 5850: train loss: 0.1453954191174913, test loss: 0.2563583392949667\n",
      "epoch 5851: train loss: 0.1453879249299095, test loss: 0.2563557976605689\n",
      "epoch 5852: train loss: 0.14538043258898475, test loss: 0.2563532573657198\n",
      "epoch 5853: train loss: 0.1453729420939424, test loss: 0.2563507184097356\n",
      "epoch 5854: train loss: 0.14536545344400825, test loss: 0.25634818079193306\n",
      "epoch 5855: train loss: 0.14535796663840853, test loss: 0.2563456445116314\n",
      "epoch 5856: train loss: 0.14535048167636996, test loss: 0.2563431095681481\n",
      "epoch 5857: train loss: 0.1453429985571197, test loss: 0.2563405759607986\n",
      "epoch 5858: train loss: 0.1453355172798854, test loss: 0.25633804368890606\n",
      "epoch 5859: train loss: 0.14532803784389506, test loss: 0.25633551275178595\n",
      "epoch 5860: train loss: 0.1453205602483773, test loss: 0.2563329831487632\n",
      "epoch 5861: train loss: 0.14531308449256103, test loss: 0.2563304548791479\n",
      "epoch 5862: train loss: 0.14530561057567568, test loss: 0.2563279279422766\n",
      "epoch 5863: train loss: 0.14529813849695114, test loss: 0.25632540233745355\n",
      "epoch 5864: train loss: 0.14529066825561776, test loss: 0.25632287806400855\n",
      "epoch 5865: train loss: 0.14528319985090632, test loss: 0.25632035512126566\n",
      "epoch 5866: train loss: 0.1452757332820481, test loss: 0.25631783350854503\n",
      "epoch 5867: train loss: 0.14526826854827468, test loss: 0.2563153132251703\n",
      "epoch 5868: train loss: 0.14526080564881833, test loss: 0.2563127942704616\n",
      "epoch 5869: train loss: 0.14525334458291156, test loss: 0.25631027664374734\n",
      "epoch 5870: train loss: 0.14524588534978744, test loss: 0.2563077603443456\n",
      "epoch 5871: train loss: 0.14523842794867944, test loss: 0.2563052453715887\n",
      "epoch 5872: train loss: 0.14523097237882154, test loss: 0.25630273172479645\n",
      "epoch 5873: train loss: 0.14522351863944807, test loss: 0.2563002194032979\n",
      "epoch 5874: train loss: 0.14521606672979395, test loss: 0.2562977084064198\n",
      "epoch 5875: train loss: 0.1452086166490944, test loss: 0.25629519873348316\n",
      "epoch 5876: train loss: 0.14520116839658517, test loss: 0.2562926903838231\n",
      "epoch 5877: train loss: 0.14519372197150246, test loss: 0.2562901833567597\n",
      "epoch 5878: train loss: 0.1451862773730829, test loss: 0.25628767765162636\n",
      "epoch 5879: train loss: 0.14517883460056355, test loss: 0.25628517326774725\n",
      "epoch 5880: train loss: 0.14517139365318193, test loss: 0.2562826702044527\n",
      "epoch 5881: train loss: 0.14516395453017605, test loss: 0.2562801684610774\n",
      "epoch 5882: train loss: 0.14515651723078432, test loss: 0.25627766803694\n",
      "epoch 5883: train loss: 0.14514908175424557, test loss: 0.2562751689313817\n",
      "epoch 5884: train loss: 0.1451416480997991, test loss: 0.2562726711437314\n",
      "epoch 5885: train loss: 0.14513421626668474, test loss: 0.2562701746733132\n",
      "epoch 5886: train loss: 0.14512678625414258, test loss: 0.2562676795194633\n",
      "epoch 5887: train loss: 0.14511935806141335, test loss: 0.2562651856815168\n",
      "epoch 5888: train loss: 0.1451119316877381, test loss: 0.25626269315880246\n",
      "epoch 5889: train loss: 0.14510450713235834, test loss: 0.2562602019506508\n",
      "epoch 5890: train loss: 0.1450970843945161, test loss: 0.2562577120564019\n",
      "epoch 5891: train loss: 0.14508966347345376, test loss: 0.2562552234753841\n",
      "epoch 5892: train loss: 0.1450822443684142, test loss: 0.25625273620693556\n",
      "epoch 5893: train loss: 0.14507482707864067, test loss: 0.2562502502503926\n",
      "epoch 5894: train loss: 0.14506741160337697, test loss: 0.2562477656050821\n",
      "epoch 5895: train loss: 0.14505999794186727, test loss: 0.25624528227034693\n",
      "epoch 5896: train loss: 0.14505258609335617, test loss: 0.25624280024552304\n",
      "epoch 5897: train loss: 0.14504517605708878, test loss: 0.2562403195299419\n",
      "epoch 5898: train loss: 0.14503776783231057, test loss: 0.25623784012294615\n",
      "epoch 5899: train loss: 0.14503036141826753, test loss: 0.2562353620238713\n",
      "epoch 5900: train loss: 0.14502295681420604, test loss: 0.25623288523205523\n",
      "epoch 5901: train loss: 0.14501555401937288, test loss: 0.2562304097468349\n",
      "epoch 5902: train loss: 0.1450081530330154, test loss: 0.25622793556755225\n",
      "epoch 5903: train loss: 0.14500075385438124, test loss: 0.25622546269354307\n",
      "epoch 5904: train loss: 0.14499335648271855, test loss: 0.2562229911241474\n",
      "epoch 5905: train loss: 0.14498596091727595, test loss: 0.2562205208587097\n",
      "epoch 5906: train loss: 0.14497856715730242, test loss: 0.2562180518965652\n",
      "epoch 5907: train loss: 0.1449711752020475, test loss: 0.2562155842370642\n",
      "epoch 5908: train loss: 0.144963785050761, test loss: 0.2562131178795354\n",
      "epoch 5909: train loss: 0.14495639670269328, test loss: 0.25621065282332695\n",
      "epoch 5910: train loss: 0.1449490101570951, test loss: 0.2562081890677817\n",
      "epoch 5911: train loss: 0.1449416254132177, test loss: 0.25620572661224134\n",
      "epoch 5912: train loss: 0.14493424247031275, test loss: 0.2562032654560545\n",
      "epoch 5913: train loss: 0.14492686132763224, test loss: 0.2562008055985557\n",
      "epoch 5914: train loss: 0.14491948198442875, test loss: 0.25619834703909267\n",
      "epoch 5915: train loss: 0.14491210443995522, test loss: 0.25619588977701646\n",
      "epoch 5916: train loss: 0.14490472869346502, test loss: 0.25619343381166404\n",
      "epoch 5917: train loss: 0.14489735474421198, test loss: 0.2561909791423799\n",
      "epoch 5918: train loss: 0.14488998259145036, test loss: 0.2561885257685187\n",
      "epoch 5919: train loss: 0.14488261223443483, test loss: 0.2561860736894193\n",
      "epoch 5920: train loss: 0.1448752436724205, test loss: 0.25618362290443175\n",
      "epoch 5921: train loss: 0.14486787690466293, test loss: 0.25618117341289987\n",
      "epoch 5922: train loss: 0.14486051193041816, test loss: 0.25617872521417884\n",
      "epoch 5923: train loss: 0.1448531487489425, test loss: 0.2561762783076101\n",
      "epoch 5924: train loss: 0.14484578735949294, test loss: 0.25617383269254335\n",
      "epoch 5925: train loss: 0.14483842776132666, test loss: 0.2561713883683286\n",
      "epoch 5926: train loss: 0.1448310699537014, test loss: 0.2561689453343165\n",
      "epoch 5927: train loss: 0.14482371393587534, test loss: 0.25616650358985665\n",
      "epoch 5928: train loss: 0.14481635970710693, test loss: 0.25616406313429496\n",
      "epoch 5929: train loss: 0.14480900726665535, test loss: 0.25616162396698905\n",
      "epoch 5930: train loss: 0.14480165661377994, test loss: 0.2561591860872829\n",
      "epoch 5931: train loss: 0.1447943077477406, test loss: 0.25615674949453227\n",
      "epoch 5932: train loss: 0.1447869606677976, test loss: 0.2561543141880914\n",
      "epoch 5933: train loss: 0.14477961537321168, test loss: 0.25615188016731133\n",
      "epoch 5934: train loss: 0.14477227186324396, test loss: 0.25614944743154383\n",
      "epoch 5935: train loss: 0.1447649301371561, test loss: 0.2561470159801384\n",
      "epoch 5936: train loss: 0.14475759019421003, test loss: 0.2561445858124558\n",
      "epoch 5937: train loss: 0.14475025203366823, test loss: 0.2561421569278481\n",
      "epoch 5938: train loss: 0.14474291565479358, test loss: 0.2561397293256697\n",
      "epoch 5939: train loss: 0.1447355810568493, test loss: 0.2561373030052722\n",
      "epoch 5940: train loss: 0.14472824823909922, test loss: 0.25613487796602\n",
      "epoch 5941: train loss: 0.14472091720080738, test loss: 0.2561324542072602\n",
      "epoch 5942: train loss: 0.14471358794123843, test loss: 0.2561300317283513\n",
      "epoch 5943: train loss: 0.14470626045965734, test loss: 0.25612761052865496\n",
      "epoch 5944: train loss: 0.14469893475532955, test loss: 0.2561251906075189\n",
      "epoch 5945: train loss: 0.1446916108275209, test loss: 0.2561227719643128\n",
      "epoch 5946: train loss: 0.14468428867549768, test loss: 0.25612035459838434\n",
      "epoch 5947: train loss: 0.14467696829852655, test loss: 0.2561179385091011\n",
      "epoch 5948: train loss: 0.1446696496958747, test loss: 0.25611552369581764\n",
      "epoch 5949: train loss: 0.1446623328668096, test loss: 0.25611311015789345\n",
      "epoch 5950: train loss: 0.1446550178105993, test loss: 0.2561106978946823\n",
      "epoch 5951: train loss: 0.1446477045265121, test loss: 0.256108286905558\n",
      "epoch 5952: train loss: 0.14464039301381695, test loss: 0.25610587718986977\n",
      "epoch 5953: train loss: 0.14463308327178295, test loss: 0.25610346874698486\n",
      "epoch 5954: train loss: 0.1446257752996799, test loss: 0.25610106157626245\n",
      "epoch 5955: train loss: 0.1446184690967778, test loss: 0.25609865567706835\n",
      "epoch 5956: train loss: 0.14461116466234716, test loss: 0.2560962510487546\n",
      "epoch 5957: train loss: 0.14460386199565897, test loss: 0.2560938476906983\n",
      "epoch 5958: train loss: 0.1445965610959845, test loss: 0.25609144560225144\n",
      "epoch 5959: train loss: 0.14458926196259558, test loss: 0.2560890447827848\n",
      "epoch 5960: train loss: 0.1445819645947644, test loss: 0.2560866452316578\n",
      "epoch 5961: train loss: 0.14457466899176358, test loss: 0.25608424694823734\n",
      "epoch 5962: train loss: 0.1445673751528661, test loss: 0.2560818499318883\n",
      "epoch 5963: train loss: 0.14456008307734547, test loss: 0.2560794541819778\n",
      "epoch 5964: train loss: 0.14455279276447555, test loss: 0.256077059697868\n",
      "epoch 5965: train loss: 0.14454550421353057, test loss: 0.2560746664789275\n",
      "epoch 5966: train loss: 0.1445382174237853, test loss: 0.25607227452452364\n",
      "epoch 5967: train loss: 0.14453093239451492, test loss: 0.25606988383401824\n",
      "epoch 5968: train loss: 0.14452364912499488, test loss: 0.25606749440678633\n",
      "epoch 5969: train loss: 0.14451636761450115, test loss: 0.2560651062421935\n",
      "epoch 5970: train loss: 0.14450908786231018, test loss: 0.25606271933960906\n",
      "epoch 5971: train loss: 0.14450180986769873, test loss: 0.25606033369839454\n",
      "epoch 5972: train loss: 0.14449453362994402, test loss: 0.25605794931792863\n",
      "epoch 5973: train loss: 0.1444872591483236, test loss: 0.2560555661975791\n",
      "epoch 5974: train loss: 0.14447998642211565, test loss: 0.2560531843367113\n",
      "epoch 5975: train loss: 0.14447271545059862, test loss: 0.25605080373470146\n",
      "epoch 5976: train loss: 0.14446544623305133, test loss: 0.2560484243909124\n",
      "epoch 5977: train loss: 0.14445817876875305, test loss: 0.2560460463047277\n",
      "epoch 5978: train loss: 0.14445091305698357, test loss: 0.2560436694755087\n",
      "epoch 5979: train loss: 0.14444364909702295, test loss: 0.2560412939026292\n",
      "epoch 5980: train loss: 0.14443638688815177, test loss: 0.2560389195854662\n",
      "epoch 5981: train loss: 0.14442912642965097, test loss: 0.256036546523389\n",
      "epoch 5982: train loss: 0.14442186772080196, test loss: 0.25603417471577467\n",
      "epoch 5983: train loss: 0.14441461076088644, test loss: 0.2560318041619917\n",
      "epoch 5984: train loss: 0.14440735554918663, test loss: 0.25602943486141905\n",
      "epoch 5985: train loss: 0.14440010208498516, test loss: 0.2560270668134323\n",
      "epoch 5986: train loss: 0.14439285036756505, test loss: 0.2560247000174025\n",
      "epoch 5987: train loss: 0.1443856003962097, test loss: 0.2560223344727065\n",
      "epoch 5988: train loss: 0.14437835217020298, test loss: 0.25601997017871925\n",
      "epoch 5989: train loss: 0.14437110568882916, test loss: 0.25601760713482186\n",
      "epoch 5990: train loss: 0.14436386095137285, test loss: 0.25601524534038683\n",
      "epoch 5991: train loss: 0.14435661795711915, test loss: 0.25601288479479206\n",
      "epoch 5992: train loss: 0.14434937670535355, test loss: 0.25601052549741676\n",
      "epoch 5993: train loss: 0.14434213719536199, test loss: 0.2560081674476389\n",
      "epoch 5994: train loss: 0.1443348994264307, test loss: 0.25600581064483174\n",
      "epoch 5995: train loss: 0.14432766339784645, test loss: 0.2560034550883806\n",
      "epoch 5996: train loss: 0.1443204291088964, test loss: 0.25600110077766236\n",
      "epoch 5997: train loss: 0.14431319655886796, test loss: 0.2559987477120571\n",
      "epoch 5998: train loss: 0.1443059657470492, test loss: 0.2559963958909455\n",
      "epoch 5999: train loss: 0.1442987366727284, test loss: 0.2559940453137065\n",
      "epoch 6000: train loss: 0.14429150933519438, test loss: 0.25599169597971766\n",
      "epoch 6001: train loss: 0.14428428373373625, test loss: 0.25598934788836947\n",
      "epoch 6002: train loss: 0.14427705986764366, test loss: 0.2559870010390383\n",
      "epoch 6003: train loss: 0.1442698377362065, test loss: 0.25598465543110827\n",
      "epoch 6004: train loss: 0.14426261733871523, test loss: 0.25598231106395775\n",
      "epoch 6005: train loss: 0.1442553986744606, test loss: 0.2559799679369737\n",
      "epoch 6006: train loss: 0.1442481817427339, test loss: 0.25597762604953717\n",
      "epoch 6007: train loss: 0.14424096654282667, test loss: 0.25597528540103665\n",
      "epoch 6008: train loss: 0.1442337530740309, test loss: 0.25597294599084985\n",
      "epoch 6009: train loss: 0.14422654133563909, test loss: 0.25597060781836684\n",
      "epoch 6010: train loss: 0.14421933132694403, test loss: 0.25596827088296964\n",
      "epoch 6011: train loss: 0.14421212304723893, test loss: 0.2559659351840419\n",
      "epoch 6012: train loss: 0.14420491649581746, test loss: 0.2559636007209744\n",
      "epoch 6013: train loss: 0.14419771167197365, test loss: 0.2559612674931557\n",
      "epoch 6014: train loss: 0.14419050857500193, test loss: 0.2559589354999621\n",
      "epoch 6015: train loss: 0.14418330720419717, test loss: 0.25595660474078474\n",
      "epoch 6016: train loss: 0.14417610755885463, test loss: 0.2559542752150202\n",
      "epoch 6017: train loss: 0.14416890963826995, test loss: 0.2559519469220449\n",
      "epoch 6018: train loss: 0.14416171344173917, test loss: 0.2559496198612504\n",
      "epoch 6019: train loss: 0.14415451896855877, test loss: 0.25594729403202654\n",
      "epoch 6020: train loss: 0.14414732621802562, test loss: 0.25594496943376777\n",
      "epoch 6021: train loss: 0.14414013518943697, test loss: 0.255942646065851\n",
      "epoch 6022: train loss: 0.14413294588209047, test loss: 0.25594032392767657\n",
      "epoch 6023: train loss: 0.14412575829528426, test loss: 0.2559380030186364\n",
      "epoch 6024: train loss: 0.1441185724283167, test loss: 0.25593568333810973\n",
      "epoch 6025: train loss: 0.14411138828048675, test loss: 0.25593336488549867\n",
      "epoch 6026: train loss: 0.14410420585109363, test loss: 0.2559310476601901\n",
      "epoch 6027: train loss: 0.144097025139437, test loss: 0.25592873166157587\n",
      "epoch 6028: train loss: 0.14408984614481699, test loss: 0.25592641688904877\n",
      "epoch 6029: train loss: 0.14408266886653404, test loss: 0.2559241033420003\n",
      "epoch 6030: train loss: 0.144075493303889, test loss: 0.2559217910198285\n",
      "epoch 6031: train loss: 0.14406831945618315, test loss: 0.25591947992192354\n",
      "epoch 6032: train loss: 0.14406114732271816, test loss: 0.25591717004768244\n",
      "epoch 6033: train loss: 0.1440539769027961, test loss: 0.25591486139649183\n",
      "epoch 6034: train loss: 0.14404680819571936, test loss: 0.2559125539677569\n",
      "epoch 6035: train loss: 0.14403964120079094, test loss: 0.2559102477608625\n",
      "epoch 6036: train loss: 0.144032475917314, test loss: 0.255907942775211\n",
      "epoch 6037: train loss: 0.14402531234459218, test loss: 0.25590563901019725\n",
      "epoch 6038: train loss: 0.1440181504819296, test loss: 0.25590333646521307\n",
      "epoch 6039: train loss: 0.14401099032863066, test loss: 0.25590103513966705\n",
      "epoch 6040: train loss: 0.14400383188400023, test loss: 0.25589873503294447\n",
      "epoch 6041: train loss: 0.1439966751473435, test loss: 0.25589643614444774\n",
      "epoch 6042: train loss: 0.1439895201179662, test loss: 0.25589413847357223\n",
      "epoch 6043: train loss: 0.1439823667951743, test loss: 0.25589184201972187\n",
      "epoch 6044: train loss: 0.14397521517827416, test loss: 0.2558895467822894\n",
      "epoch 6045: train loss: 0.14396806526657274, test loss: 0.2558872527606758\n",
      "epoch 6046: train loss: 0.1439609170593772, test loss: 0.25588495995428445\n",
      "epoch 6047: train loss: 0.14395377055599512, test loss: 0.25588266836251267\n",
      "epoch 6048: train loss: 0.1439466257557345, test loss: 0.25588037798475893\n",
      "epoch 6049: train loss: 0.1439394826579038, test loss: 0.2558780888204229\n",
      "epoch 6050: train loss: 0.14393234126181176, test loss: 0.2558758008689126\n",
      "epoch 6051: train loss: 0.14392520156676758, test loss: 0.2558735141296227\n",
      "epoch 6052: train loss: 0.1439180635720808, test loss: 0.25587122860195943\n",
      "epoch 6053: train loss: 0.14391092727706148, test loss: 0.25586894428532586\n",
      "epoch 6054: train loss: 0.1439037926810199, test loss: 0.2558666611791206\n",
      "epoch 6055: train loss: 0.1438966597832669, test loss: 0.2558643792827511\n",
      "epoch 6056: train loss: 0.14388952858311352, test loss: 0.25586209859561376\n",
      "epoch 6057: train loss: 0.14388239907987133, test loss: 0.2558598191171229\n",
      "epoch 6058: train loss: 0.14387527127285227, test loss: 0.255857540846676\n",
      "epoch 6059: train loss: 0.1438681451613687, test loss: 0.25585526378367834\n",
      "epoch 6060: train loss: 0.1438610207447333, test loss: 0.25585298792753547\n",
      "epoch 6061: train loss: 0.14385389802225912, test loss: 0.255850713277657\n",
      "epoch 6062: train loss: 0.1438467769932597, test loss: 0.25584843983344163\n",
      "epoch 6063: train loss: 0.14383965765704892, test loss: 0.25584616759430245\n",
      "epoch 6064: train loss: 0.14383254001294105, test loss: 0.2558438965596417\n",
      "epoch 6065: train loss: 0.1438254240602507, test loss: 0.25584162672886634\n",
      "epoch 6066: train loss: 0.143818309798293, test loss: 0.2558393581013896\n",
      "epoch 6067: train loss: 0.14381119722638327, test loss: 0.2558370906766141\n",
      "epoch 6068: train loss: 0.14380408634383743, test loss: 0.25583482445394495\n",
      "epoch 6069: train loss: 0.14379697714997167, test loss: 0.2558325594328018\n",
      "epoch 6070: train loss: 0.14378986964410256, test loss: 0.25583029561258064\n",
      "epoch 6071: train loss: 0.14378276382554717, test loss: 0.25582803299270224\n",
      "epoch 6072: train loss: 0.14377565969362271, test loss: 0.25582577157256886\n",
      "epoch 6073: train loss: 0.1437685572476471, test loss: 0.2558235113515955\n",
      "epoch 6074: train loss: 0.14376145648693842, test loss: 0.25582125232918634\n",
      "epoch 6075: train loss: 0.1437543574108152, test loss: 0.2558189945047644\n",
      "epoch 6076: train loss: 0.14374726001859636, test loss: 0.2558167378777279\n",
      "epoch 6077: train loss: 0.14374016430960118, test loss: 0.25581448244749505\n",
      "epoch 6078: train loss: 0.1437330702831494, test loss: 0.25581222821347555\n",
      "epoch 6079: train loss: 0.1437259779385611, test loss: 0.2558099751750872\n",
      "epoch 6080: train loss: 0.1437188872751567, test loss: 0.25580772333173885\n",
      "epoch 6081: train loss: 0.143711798292257, test loss: 0.2558054726828442\n",
      "epoch 6082: train loss: 0.14370471098918336, test loss: 0.2558032232278141\n",
      "epoch 6083: train loss: 0.14369762536525726, test loss: 0.25580097496606946\n",
      "epoch 6084: train loss: 0.14369054141980075, test loss: 0.25579872789702024\n",
      "epoch 6085: train loss: 0.14368345915213626, test loss: 0.25579648202007815\n",
      "epoch 6086: train loss: 0.1436763785615865, test loss: 0.25579423733466855\n",
      "epoch 6087: train loss: 0.14366929964747455, test loss: 0.25579199384019763\n",
      "epoch 6088: train loss: 0.14366222240912405, test loss: 0.2557897515360837\n",
      "epoch 6089: train loss: 0.1436551468458589, test loss: 0.2557875104217505\n",
      "epoch 6090: train loss: 0.1436480729570033, test loss: 0.2557852704966032\n",
      "epoch 6091: train loss: 0.14364100074188196, test loss: 0.2557830317600656\n",
      "epoch 6092: train loss: 0.14363393019982001, test loss: 0.25578079421155053\n",
      "epoch 6093: train loss: 0.14362686133014282, test loss: 0.25577855785048387\n",
      "epoch 6094: train loss: 0.1436197941321762, test loss: 0.2557763226762784\n",
      "epoch 6095: train loss: 0.1436127286052464, test loss: 0.2557740886883553\n",
      "epoch 6096: train loss: 0.14360566474867992, test loss: 0.2557718558861307\n",
      "epoch 6097: train loss: 0.1435986025618038, test loss: 0.2557696242690267\n",
      "epoch 6098: train loss: 0.1435915420439453, test loss: 0.25576739383646047\n",
      "epoch 6099: train loss: 0.1435844831944322, test loss: 0.25576516458785536\n",
      "epoch 6100: train loss: 0.14357742601259257, test loss: 0.255762936522631\n",
      "epoch 6101: train loss: 0.1435703704977549, test loss: 0.25576070964020914\n",
      "epoch 6102: train loss: 0.143563316649248, test loss: 0.25575848394000916\n",
      "epoch 6103: train loss: 0.14355626446640116, test loss: 0.255756259421455\n",
      "epoch 6104: train loss: 0.14354921394854395, test loss: 0.25575403608396713\n",
      "epoch 6105: train loss: 0.14354216509500636, test loss: 0.25575181392696594\n",
      "epoch 6106: train loss: 0.1435351179051188, test loss: 0.2557495929498808\n",
      "epoch 6107: train loss: 0.14352807237821197, test loss: 0.25574737315213025\n",
      "epoch 6108: train loss: 0.143521028513617, test loss: 0.25574515453313845\n",
      "epoch 6109: train loss: 0.14351398631066542, test loss: 0.2557429370923311\n",
      "epoch 6110: train loss: 0.14350694576868903, test loss: 0.2557407208291312\n",
      "epoch 6111: train loss: 0.14349990688702013, test loss: 0.2557385057429641\n",
      "epoch 6112: train loss: 0.14349286966499136, test loss: 0.2557362918332545\n",
      "epoch 6113: train loss: 0.1434858341019357, test loss: 0.25573407909942736\n",
      "epoch 6114: train loss: 0.14347880019718648, test loss: 0.25573186754091054\n",
      "epoch 6115: train loss: 0.14347176795007752, test loss: 0.25572965715712775\n",
      "epoch 6116: train loss: 0.14346473735994292, test loss: 0.2557274479475081\n",
      "epoch 6117: train loss: 0.14345770842611716, test loss: 0.25572523991147966\n",
      "epoch 6118: train loss: 0.1434506811479352, test loss: 0.2557230330484666\n",
      "epoch 6119: train loss: 0.14344365552473218, test loss: 0.25572082735789675\n",
      "epoch 6120: train loss: 0.1434366315558438, test loss: 0.2557186228392044\n",
      "epoch 6121: train loss: 0.143429609240606, test loss: 0.25571641949181023\n",
      "epoch 6122: train loss: 0.14342258857835516, test loss: 0.2557142173151438\n",
      "epoch 6123: train loss: 0.14341556956842805, test loss: 0.2557120163086431\n",
      "epoch 6124: train loss: 0.14340855221016177, test loss: 0.25570981647172697\n",
      "epoch 6125: train loss: 0.14340153650289383, test loss: 0.25570761780383316\n",
      "epoch 6126: train loss: 0.14339452244596207, test loss: 0.2557054203043896\n",
      "epoch 6127: train loss: 0.14338751003870467, test loss: 0.25570322397282547\n",
      "epoch 6128: train loss: 0.14338049928046034, test loss: 0.2557010288085758\n",
      "epoch 6129: train loss: 0.14337349017056794, test loss: 0.25569883481106476\n",
      "epoch 6130: train loss: 0.14336648270836694, test loss: 0.25569664197973174\n",
      "epoch 6131: train loss: 0.14335947689319692, test loss: 0.25569445031400595\n",
      "epoch 6132: train loss: 0.14335247272439808, test loss: 0.25569225981332083\n",
      "epoch 6133: train loss: 0.14334547020131083, test loss: 0.2556900704771115\n",
      "epoch 6134: train loss: 0.14333846932327596, test loss: 0.25568788230480977\n",
      "epoch 6135: train loss: 0.14333147008963473, test loss: 0.25568569529584473\n",
      "epoch 6136: train loss: 0.14332447249972866, test loss: 0.25568350944965695\n",
      "epoch 6137: train loss: 0.14331747655289973, test loss: 0.25568132476568023\n",
      "epoch 6138: train loss: 0.1433104822484902, test loss: 0.25567914124334423\n",
      "epoch 6139: train loss: 0.1433034895858427, test loss: 0.2556769588820898\n",
      "epoch 6140: train loss: 0.1432964985643004, test loss: 0.2556747776813515\n",
      "epoch 6141: train loss: 0.1432895091832066, test loss: 0.25567259764056244\n",
      "epoch 6142: train loss: 0.14328252144190512, test loss: 0.2556704187591632\n",
      "epoch 6143: train loss: 0.14327553533974008, test loss: 0.25566824103658803\n",
      "epoch 6144: train loss: 0.14326855087605603, test loss: 0.25566606447227275\n",
      "epoch 6145: train loss: 0.14326156805019782, test loss: 0.2556638890656599\n",
      "epoch 6146: train loss: 0.14325458686151069, test loss: 0.25566171481617955\n",
      "epoch 6147: train loss: 0.14324760730934025, test loss: 0.25565954172327726\n",
      "epoch 6148: train loss: 0.1432406293930325, test loss: 0.2556573697863858\n",
      "epoch 6149: train loss: 0.14323365311193376, test loss: 0.25565519900495093\n",
      "epoch 6150: train loss: 0.14322667846539075, test loss: 0.2556530293784079\n",
      "epoch 6151: train loss: 0.14321970545275053, test loss: 0.2556508609061973\n",
      "epoch 6152: train loss: 0.14321273407336058, test loss: 0.2556486935877519\n",
      "epoch 6153: train loss: 0.14320576432656865, test loss: 0.25564652742252586\n",
      "epoch 6154: train loss: 0.14319879621172293, test loss: 0.2556443624099488\n",
      "epoch 6155: train loss: 0.143191829728172, test loss: 0.25564219854946785\n",
      "epoch 6156: train loss: 0.14318486487526463, test loss: 0.25564003584052114\n",
      "epoch 6157: train loss: 0.14317790165235025, test loss: 0.2556378742825536\n",
      "epoch 6158: train loss: 0.14317094005877837, test loss: 0.2556357138750063\n",
      "epoch 6159: train loss: 0.143163980093899, test loss: 0.2556335546173188\n",
      "epoch 6160: train loss: 0.1431570217570625, test loss: 0.2556313965089376\n",
      "epoch 6161: train loss: 0.1431500650476196, test loss: 0.2556292395493081\n",
      "epoch 6162: train loss: 0.14314310996492133, test loss: 0.2556270837378658\n",
      "epoch 6163: train loss: 0.14313615650831918, test loss: 0.25562492907406337\n",
      "epoch 6164: train loss: 0.1431292046771649, test loss: 0.2556227755573429\n",
      "epoch 6165: train loss: 0.14312225447081067, test loss: 0.2556206231871445\n",
      "epoch 6166: train loss: 0.14311530588860905, test loss: 0.2556184719629195\n",
      "epoch 6167: train loss: 0.1431083589299129, test loss: 0.25561632188411104\n",
      "epoch 6168: train loss: 0.14310141359407547, test loss: 0.25561417295016153\n",
      "epoch 6169: train loss: 0.14309446988045033, test loss: 0.25561202516052245\n",
      "epoch 6170: train loss: 0.14308752778839146, test loss: 0.2556098785146418\n",
      "epoch 6171: train loss: 0.14308058731725323, test loss: 0.2556077330119579\n",
      "epoch 6172: train loss: 0.14307364846639029, test loss: 0.2556055886519266\n",
      "epoch 6173: train loss: 0.14306671123515768, test loss: 0.25560344543399033\n",
      "epoch 6174: train loss: 0.14305977562291086, test loss: 0.25560130335760206\n",
      "epoch 6175: train loss: 0.14305284162900553, test loss: 0.2555991624222039\n",
      "epoch 6176: train loss: 0.14304590925279784, test loss: 0.25559702262725087\n",
      "epoch 6177: train loss: 0.14303897849364428, test loss: 0.2555948839721836\n",
      "epoch 6178: train loss: 0.14303204935090166, test loss: 0.2555927464564631\n",
      "epoch 6179: train loss: 0.1430251218239272, test loss: 0.2555906100795332\n",
      "epoch 6180: train loss: 0.14301819591207848, test loss: 0.2555884748408415\n",
      "epoch 6181: train loss: 0.1430112716147134, test loss: 0.25558634073983905\n",
      "epoch 6182: train loss: 0.14300434893119018, test loss: 0.25558420777598245\n",
      "epoch 6183: train loss: 0.14299742786086753, test loss: 0.25558207594872107\n",
      "epoch 6184: train loss: 0.14299050840310437, test loss: 0.25557994525750105\n",
      "epoch 6185: train loss: 0.14298359055726007, test loss: 0.2555778157017788\n",
      "epoch 6186: train loss: 0.14297667432269434, test loss: 0.25557568728100866\n",
      "epoch 6187: train loss: 0.1429697596987672, test loss: 0.25557355999463865\n",
      "epoch 6188: train loss: 0.14296284668483908, test loss: 0.25557143384212017\n",
      "epoch 6189: train loss: 0.14295593528027076, test loss: 0.25556930882291473\n",
      "epoch 6190: train loss: 0.14294902548442331, test loss: 0.2555671849364667\n",
      "epoch 6191: train loss: 0.1429421172966583, test loss: 0.2555650621822394\n",
      "epoch 6192: train loss: 0.14293521071633744, test loss: 0.255562940559681\n",
      "epoch 6193: train loss: 0.142928305742823, test loss: 0.25556082006825137\n",
      "epoch 6194: train loss: 0.14292140237547749, test loss: 0.25555870070740194\n",
      "epoch 6195: train loss: 0.14291450061366381, test loss: 0.25555658247658714\n",
      "epoch 6196: train loss: 0.14290760045674522, test loss: 0.2555544653752644\n",
      "epoch 6197: train loss: 0.14290070190408527, test loss: 0.2555523494028902\n",
      "epoch 6198: train loss: 0.14289380495504797, test loss: 0.25555023455891945\n",
      "epoch 6199: train loss: 0.1428869096089976, test loss: 0.25554812084281603\n",
      "epoch 6200: train loss: 0.1428800158652988, test loss: 0.25554600825402624\n",
      "epoch 6201: train loss: 0.14287312372331662, test loss: 0.25554389679201395\n",
      "epoch 6202: train loss: 0.14286623318241642, test loss: 0.25554178645623726\n",
      "epoch 6203: train loss: 0.14285934424196386, test loss: 0.2555396772461569\n",
      "epoch 6204: train loss: 0.14285245690132511, test loss: 0.25553756916122633\n",
      "epoch 6205: train loss: 0.14284557115986649, test loss: 0.2555354622009062\n",
      "epoch 6206: train loss: 0.14283868701695482, test loss: 0.2555333563646559\n",
      "epoch 6207: train loss: 0.1428318044719572, test loss: 0.2555312516519351\n",
      "epoch 6208: train loss: 0.14282492352424112, test loss: 0.2555291480622076\n",
      "epoch 6209: train loss: 0.1428180441731744, test loss: 0.255527045594928\n",
      "epoch 6210: train loss: 0.14281116641812516, test loss: 0.2555249442495604\n",
      "epoch 6211: train loss: 0.142804290258462, test loss: 0.255522844025563\n",
      "epoch 6212: train loss: 0.14279741569355375, test loss: 0.25552074492240356\n",
      "epoch 6213: train loss: 0.14279054272276967, test loss: 0.2555186469395378\n",
      "epoch 6214: train loss: 0.14278367134547926, test loss: 0.25551655007643503\n",
      "epoch 6215: train loss: 0.1427768015610525, test loss: 0.25551445433254505\n",
      "epoch 6216: train loss: 0.14276993336885963, test loss: 0.255512359707345\n",
      "epoch 6217: train loss: 0.14276306676827125, test loss: 0.2555102662002855\n",
      "epoch 6218: train loss: 0.14275620175865839, test loss: 0.25550817381084207\n",
      "epoch 6219: train loss: 0.1427493383393923, test loss: 0.25550608253847046\n",
      "epoch 6220: train loss: 0.14274247650984467, test loss: 0.2555039923826369\n",
      "epoch 6221: train loss: 0.14273561626938747, test loss: 0.2555019033428091\n",
      "epoch 6222: train loss: 0.14272875761739312, test loss: 0.25549981541844835\n",
      "epoch 6223: train loss: 0.14272190055323425, test loss: 0.2554977286090201\n",
      "epoch 6224: train loss: 0.142715045076284, test loss: 0.25549564291399096\n",
      "epoch 6225: train loss: 0.14270819118591566, test loss: 0.25549355833282505\n",
      "epoch 6226: train loss: 0.14270133888150302, test loss: 0.25549147486499535\n",
      "epoch 6227: train loss: 0.14269448816242017, test loss: 0.25548939250995684\n",
      "epoch 6228: train loss: 0.14268763902804155, test loss: 0.25548731126718816\n",
      "epoch 6229: train loss: 0.14268079147774196, test loss: 0.255485231136153\n",
      "epoch 6230: train loss: 0.14267394551089643, test loss: 0.25548315211631417\n",
      "epoch 6231: train loss: 0.1426671011268805, test loss: 0.2554810742071465\n",
      "epoch 6232: train loss: 0.14266025832506998, test loss: 0.2554789974081146\n",
      "epoch 6233: train loss: 0.142653417104841, test loss: 0.2554769217186911\n",
      "epoch 6234: train loss: 0.14264657746557013, test loss: 0.2554748471383414\n",
      "epoch 6235: train loss: 0.14263973940663413, test loss: 0.25547277366653376\n",
      "epoch 6236: train loss: 0.1426329029274102, test loss: 0.25547070130273786\n",
      "epoch 6237: train loss: 0.14262606802727598, test loss: 0.25546863004642695\n",
      "epoch 6238: train loss: 0.1426192347056092, test loss: 0.2554665598970742\n",
      "epoch 6239: train loss: 0.1426124029617882, test loss: 0.25546449085414286\n",
      "epoch 6240: train loss: 0.14260557279519143, test loss: 0.25546242291711097\n",
      "epoch 6241: train loss: 0.14259874420519786, test loss: 0.255460356085448\n",
      "epoch 6242: train loss: 0.14259191719118675, test loss: 0.25545829035861883\n",
      "epoch 6243: train loss: 0.14258509175253764, test loss: 0.2554562257361075\n",
      "epoch 6244: train loss: 0.1425782678886305, test loss: 0.25545416221737965\n",
      "epoch 6245: train loss: 0.14257144559884558, test loss: 0.25545209980190803\n",
      "epoch 6246: train loss: 0.1425646248825635, test loss: 0.2554500384891656\n",
      "epoch 6247: train loss: 0.14255780573916524, test loss: 0.25544797827862875\n",
      "epoch 6248: train loss: 0.14255098816803205, test loss: 0.2554459191697676\n",
      "epoch 6249: train loss: 0.14254417216854556, test loss: 0.2554438611620625\n",
      "epoch 6250: train loss: 0.14253735774008783, test loss: 0.2554418042549812\n",
      "epoch 6251: train loss: 0.14253054488204112, test loss: 0.2554397484480042\n",
      "epoch 6252: train loss: 0.14252373359378803, test loss: 0.25543769374060044\n",
      "epoch 6253: train loss: 0.14251692387471163, test loss: 0.2554356401322513\n",
      "epoch 6254: train loss: 0.14251011572419522, test loss: 0.25543358762242646\n",
      "epoch 6255: train loss: 0.14250330914162251, test loss: 0.2554315362106108\n",
      "epoch 6256: train loss: 0.1424965041263775, test loss: 0.2554294858962758\n",
      "epoch 6257: train loss: 0.14248970067784453, test loss: 0.2554274366788993\n",
      "epoch 6258: train loss: 0.14248289879540826, test loss: 0.255425388557957\n",
      "epoch 6259: train loss: 0.1424760984784538, test loss: 0.2554233415329248\n",
      "epoch 6260: train loss: 0.14246929972636643, test loss: 0.2554212956032858\n",
      "epoch 6261: train loss: 0.14246250253853188, test loss: 0.25541925076851657\n",
      "epoch 6262: train loss: 0.14245570691433623, test loss: 0.2554172070280951\n",
      "epoch 6263: train loss: 0.14244891285316583, test loss: 0.25541516438150164\n",
      "epoch 6264: train loss: 0.14244212035440737, test loss: 0.25541312282821166\n",
      "epoch 6265: train loss: 0.14243532941744794, test loss: 0.25541108236770627\n",
      "epoch 6266: train loss: 0.14242854004167488, test loss: 0.2554090429994668\n",
      "epoch 6267: train loss: 0.142421752226476, test loss: 0.25540700472297717\n",
      "epoch 6268: train loss: 0.14241496597123926, test loss: 0.2554049675377083\n",
      "epoch 6269: train loss: 0.14240818127535312, test loss: 0.25540293144314946\n",
      "epoch 6270: train loss: 0.14240139813820632, test loss: 0.25540089643878006\n",
      "epoch 6271: train loss: 0.1423946165591879, test loss: 0.2553988625240784\n",
      "epoch 6272: train loss: 0.14238783653768722, test loss: 0.25539682969853084\n",
      "epoch 6273: train loss: 0.1423810580730941, test loss: 0.2553947979616153\n",
      "epoch 6274: train loss: 0.14237428116479856, test loss: 0.2553927673128211\n",
      "epoch 6275: train loss: 0.14236750581219101, test loss: 0.2553907377516199\n",
      "epoch 6276: train loss: 0.14236073201466223, test loss: 0.25538870927750573\n",
      "epoch 6277: train loss: 0.14235395977160323, test loss: 0.25538668188995645\n",
      "epoch 6278: train loss: 0.14234718908240546, test loss: 0.2553846555884631\n",
      "epoch 6279: train loss: 0.14234041994646066, test loss: 0.25538263037250386\n",
      "epoch 6280: train loss: 0.14233365236316087, test loss: 0.2553806062415563\n",
      "epoch 6281: train loss: 0.14232688633189852, test loss: 0.25537858319512075\n",
      "epoch 6282: train loss: 0.14232012185206636, test loss: 0.255376561232673\n",
      "epoch 6283: train loss: 0.14231335892305744, test loss: 0.2553745403537003\n",
      "epoch 6284: train loss: 0.1423065975442652, test loss: 0.25537252055768966\n",
      "epoch 6285: train loss: 0.1422998377150833, test loss: 0.2553705018441235\n",
      "epoch 6286: train loss: 0.14229307943490588, test loss: 0.25536848421249586\n",
      "epoch 6287: train loss: 0.14228632270312733, test loss: 0.25536646766228477\n",
      "epoch 6288: train loss: 0.1422795675191424, test loss: 0.25536445219298565\n",
      "epoch 6289: train loss: 0.14227281388234606, test loss: 0.25536243780407925\n",
      "epoch 6290: train loss: 0.14226606179213377, test loss: 0.25536042449506124\n",
      "epoch 6291: train loss: 0.14225931124790125, test loss: 0.255358412265409\n",
      "epoch 6292: train loss: 0.14225256224904456, test loss: 0.25535640111461844\n",
      "epoch 6293: train loss: 0.14224581479496, test loss: 0.2553543910421814\n",
      "epoch 6294: train loss: 0.14223906888504445, test loss: 0.2553523820475796\n",
      "epoch 6295: train loss: 0.14223232451869477, test loss: 0.255350374130305\n",
      "epoch 6296: train loss: 0.1422255816953085, test loss: 0.25534836728984794\n",
      "epoch 6297: train loss: 0.1422188404142832, test loss: 0.25534636152570384\n",
      "epoch 6298: train loss: 0.14221210067501697, test loss: 0.25534435683735385\n",
      "epoch 6299: train loss: 0.14220536247690815, test loss: 0.2553423532242951\n",
      "epoch 6300: train loss: 0.14219862581935544, test loss: 0.2553403506860139\n",
      "epoch 6301: train loss: 0.14219189070175786, test loss: 0.2553383492220096\n",
      "epoch 6302: train loss: 0.14218515712351473, test loss: 0.255336348831764\n",
      "epoch 6303: train loss: 0.1421784250840257, test loss: 0.25533434951477907\n",
      "epoch 6304: train loss: 0.14217169458269086, test loss: 0.25533235127054077\n",
      "epoch 6305: train loss: 0.14216496561891045, test loss: 0.25533035409854304\n",
      "epoch 6306: train loss: 0.14215823819208512, test loss: 0.2553283579982802\n",
      "epoch 6307: train loss: 0.1421515123016159, test loss: 0.2553263629692484\n",
      "epoch 6308: train loss: 0.1421447879469041, test loss: 0.2553243690109309\n",
      "epoch 6309: train loss: 0.14213806512735133, test loss: 0.25532237612283487\n",
      "epoch 6310: train loss: 0.1421313438423595, test loss: 0.25532038430444925\n",
      "epoch 6311: train loss: 0.14212462409133098, test loss: 0.2553183935552637\n",
      "epoch 6312: train loss: 0.14211790587366835, test loss: 0.25531640387478305\n",
      "epoch 6313: train loss: 0.14211118918877452, test loss: 0.25531441526249327\n",
      "epoch 6314: train loss: 0.14210447403605278, test loss: 0.25531242771789847\n",
      "epoch 6315: train loss: 0.1420977604149067, test loss: 0.2553104412404881\n",
      "epoch 6316: train loss: 0.1420910483247402, test loss: 0.2553084558297597\n",
      "epoch 6317: train loss: 0.14208433776495752, test loss: 0.2553064714852142\n",
      "epoch 6318: train loss: 0.1420776287349632, test loss: 0.25530448820634205\n",
      "epoch 6319: train loss: 0.14207092123416215, test loss: 0.25530250599264953\n",
      "epoch 6320: train loss: 0.14206421526195956, test loss: 0.2553005248436249\n",
      "epoch 6321: train loss: 0.142057510817761, test loss: 0.2552985447587682\n",
      "epoch 6322: train loss: 0.14205080790097224, test loss: 0.2552965657375836\n",
      "epoch 6323: train loss: 0.14204410651099952, test loss: 0.25529458777956476\n",
      "epoch 6324: train loss: 0.14203740664724937, test loss: 0.2552926108842102\n",
      "epoch 6325: train loss: 0.14203070830912853, test loss: 0.2552906350510194\n",
      "epoch 6326: train loss: 0.1420240114960442, test loss: 0.2552886602794955\n",
      "epoch 6327: train loss: 0.14201731620740385, test loss: 0.2552866865691368\n",
      "epoch 6328: train loss: 0.14201062244261525, test loss: 0.25528471391943613\n",
      "epoch 6329: train loss: 0.14200393020108654, test loss: 0.25528274232990583\n",
      "epoch 6330: train loss: 0.14199723948222612, test loss: 0.2552807718000406\n",
      "epoch 6331: train loss: 0.14199055028544275, test loss: 0.25527880232934097\n",
      "epoch 6332: train loss: 0.14198386261014556, test loss: 0.25527683391730904\n",
      "epoch 6333: train loss: 0.14197717645574387, test loss: 0.2552748665634505\n",
      "epoch 6334: train loss: 0.14197049182164745, test loss: 0.25527290026726235\n",
      "epoch 6335: train loss: 0.1419638087072663, test loss: 0.255270935028248\n",
      "epoch 6336: train loss: 0.1419571271120108, test loss: 0.25526897084590755\n",
      "epoch 6337: train loss: 0.14195044703529164, test loss: 0.25526700771975436\n",
      "epoch 6338: train loss: 0.1419437684765198, test loss: 0.2552650456492775\n",
      "epoch 6339: train loss: 0.14193709143510663, test loss: 0.25526308463399505\n",
      "epoch 6340: train loss: 0.1419304159104637, test loss: 0.2552611246734005\n",
      "epoch 6341: train loss: 0.14192374190200305, test loss: 0.25525916576700053\n",
      "epoch 6342: train loss: 0.14191706940913687, test loss: 0.2552572079143045\n",
      "epoch 6343: train loss: 0.14191039843127787, test loss: 0.2552552511148107\n",
      "epoch 6344: train loss: 0.14190372896783882, test loss: 0.2552532953680259\n",
      "epoch 6345: train loss: 0.14189706101823307, test loss: 0.2552513406734569\n",
      "epoch 6346: train loss: 0.14189039458187408, test loss: 0.25524938703061595\n",
      "epoch 6347: train loss: 0.14188372965817578, test loss: 0.2552474344389983\n",
      "epoch 6348: train loss: 0.1418770662465523, test loss: 0.25524548289811394\n",
      "epoch 6349: train loss: 0.14187040434641818, test loss: 0.25524353240747355\n",
      "epoch 6350: train loss: 0.1418637439571883, test loss: 0.25524158296657923\n",
      "epoch 6351: train loss: 0.14185708507827766, test loss: 0.25523963457494325\n",
      "epoch 6352: train loss: 0.1418504277091018, test loss: 0.25523768723207235\n",
      "epoch 6353: train loss: 0.14184377184907646, test loss: 0.2552357409374681\n",
      "epoch 6354: train loss: 0.14183711749761774, test loss: 0.2552337956906473\n",
      "epoch 6355: train loss: 0.141830464654142, test loss: 0.2552318514911169\n",
      "epoch 6356: train loss: 0.14182381331806604, test loss: 0.25522990833838155\n",
      "epoch 6357: train loss: 0.1418171634888068, test loss: 0.255227966231955\n",
      "epoch 6358: train loss: 0.1418105151657817, test loss: 0.2552260251713471\n",
      "epoch 6359: train loss: 0.1418038683484083, test loss: 0.25522408515605993\n",
      "epoch 6360: train loss: 0.14179722303610473, test loss: 0.25522214618561456\n",
      "epoch 6361: train loss: 0.14179057922828917, test loss: 0.2552202082595184\n",
      "epoch 6362: train loss: 0.14178393692438024, test loss: 0.2552182713772772\n",
      "epoch 6363: train loss: 0.1417772961237969, test loss: 0.25521633553840517\n",
      "epoch 6364: train loss: 0.14177065682595835, test loss: 0.25521440074241675\n",
      "epoch 6365: train loss: 0.14176401903028416, test loss: 0.25521246698882005\n",
      "epoch 6366: train loss: 0.14175738273619415, test loss: 0.25521053427712875\n",
      "epoch 6367: train loss: 0.14175074794310855, test loss: 0.2552086026068529\n",
      "epoch 6368: train loss: 0.14174411465044784, test loss: 0.2552066719775125\n",
      "epoch 6369: train loss: 0.14173748285763277, test loss: 0.2552047423886122\n",
      "epoch 6370: train loss: 0.14173085256408452, test loss: 0.25520281383966703\n",
      "epoch 6371: train loss: 0.14172422376922447, test loss: 0.2552008863301942\n",
      "epoch 6372: train loss: 0.14171759647247442, test loss: 0.25519895985970487\n",
      "epoch 6373: train loss: 0.14171097067325636, test loss: 0.25519703442771635\n",
      "epoch 6374: train loss: 0.14170434637099263, test loss: 0.2551951100337403\n",
      "epoch 6375: train loss: 0.14169772356510602, test loss: 0.2551931866772872\n",
      "epoch 6376: train loss: 0.14169110225501938, test loss: 0.25519126435788286\n",
      "epoch 6377: train loss: 0.1416844824401561, test loss: 0.2551893430750373\n",
      "epoch 6378: train loss: 0.14167786411993974, test loss: 0.25518742282826296\n",
      "epoch 6379: train loss: 0.14167124729379424, test loss: 0.2551855036170801\n",
      "epoch 6380: train loss: 0.14166463196114384, test loss: 0.2551835854410072\n",
      "epoch 6381: train loss: 0.14165801812141304, test loss: 0.25518166829955485\n",
      "epoch 6382: train loss: 0.14165140577402674, test loss: 0.25517975219224437\n",
      "epoch 6383: train loss: 0.14164479491841006, test loss: 0.25517783711859016\n",
      "epoch 6384: train loss: 0.14163818555398847, test loss: 0.2551759230781176\n",
      "epoch 6385: train loss: 0.1416315776801878, test loss: 0.25517401007033524\n",
      "epoch 6386: train loss: 0.14162497129643406, test loss: 0.2551720980947633\n",
      "epoch 6387: train loss: 0.14161836640215372, test loss: 0.2551701871509221\n",
      "epoch 6388: train loss: 0.14161176299677344, test loss: 0.25516827723833296\n",
      "epoch 6389: train loss: 0.1416051610797202, test loss: 0.2551663683565122\n",
      "epoch 6390: train loss: 0.14159856065042145, test loss: 0.255164460504979\n",
      "epoch 6391: train loss: 0.14159196170830468, test loss: 0.25516255368325375\n",
      "epoch 6392: train loss: 0.14158536425279794, test loss: 0.25516064789085846\n",
      "epoch 6393: train loss: 0.14157876828332938, test loss: 0.2551587431273087\n",
      "epoch 6394: train loss: 0.1415721737993276, test loss: 0.2551568393921261\n",
      "epoch 6395: train loss: 0.1415655808002215, test loss: 0.25515493668484096\n",
      "epoch 6396: train loss: 0.1415589892854402, test loss: 0.2551530350049575\n",
      "epoch 6397: train loss: 0.14155239925441315, test loss: 0.25515113435201486\n",
      "epoch 6398: train loss: 0.1415458107065702, test loss: 0.25514923472552686\n",
      "epoch 6399: train loss: 0.1415392236413414, test loss: 0.25514733612501234\n",
      "epoch 6400: train loss: 0.1415326380581571, test loss: 0.2551454385499941\n",
      "epoch 6401: train loss: 0.14152605395644813, test loss: 0.25514354200000283\n",
      "epoch 6402: train loss: 0.14151947133564538, test loss: 0.25514164647456106\n",
      "epoch 6403: train loss: 0.1415128901951802, test loss: 0.25513975197318284\n",
      "epoch 6404: train loss: 0.1415063105344842, test loss: 0.25513785849539905\n",
      "epoch 6405: train loss: 0.14149973235298932, test loss: 0.2551359660407291\n",
      "epoch 6406: train loss: 0.14149315565012777, test loss: 0.25513407460870413\n",
      "epoch 6407: train loss: 0.1414865804253321, test loss: 0.2551321841988435\n",
      "epoch 6408: train loss: 0.14148000667803515, test loss: 0.2551302948106702\n",
      "epoch 6409: train loss: 0.14147343440767, test loss: 0.2551284064437152\n",
      "epoch 6410: train loss: 0.1414668636136702, test loss: 0.2551265190974996\n",
      "epoch 6411: train loss: 0.14146029429546939, test loss: 0.25512463277155417\n",
      "epoch 6412: train loss: 0.1414537264525017, test loss: 0.2551227474653974\n",
      "epoch 6413: train loss: 0.14144716008420144, test loss: 0.25512086317856303\n",
      "epoch 6414: train loss: 0.14144059519000335, test loss: 0.25511897991057403\n",
      "epoch 6415: train loss: 0.14143403176934227, test loss: 0.25511709766095836\n",
      "epoch 6416: train loss: 0.14142746982165355, test loss: 0.2551152164292451\n",
      "epoch 6417: train loss: 0.14142090934637275, test loss: 0.25511333621495824\n",
      "epoch 6418: train loss: 0.14141435034293573, test loss: 0.25511145701762666\n",
      "epoch 6419: train loss: 0.14140779281077867, test loss: 0.25510957883677815\n",
      "epoch 6420: train loss: 0.141401236749338, test loss: 0.25510770167194274\n",
      "epoch 6421: train loss: 0.1413946821580506, test loss: 0.2551058255226499\n",
      "epoch 6422: train loss: 0.1413881290363534, test loss: 0.2551039503884276\n",
      "epoch 6423: train loss: 0.1413815773836839, test loss: 0.25510207626880294\n",
      "epoch 6424: train loss: 0.14137502719947975, test loss: 0.2551002031633086\n",
      "epoch 6425: train loss: 0.14136847848317893, test loss: 0.2550983310714756\n",
      "epoch 6426: train loss: 0.1413619312342197, test loss: 0.2550964599928356\n",
      "epoch 6427: train loss: 0.14135538545204068, test loss: 0.2550945899269102\n",
      "epoch 6428: train loss: 0.1413488411360807, test loss: 0.25509272087323587\n",
      "epoch 6429: train loss: 0.14134229828577902, test loss: 0.25509085283134597\n",
      "epoch 6430: train loss: 0.14133575690057507, test loss: 0.25508898580076783\n",
      "epoch 6431: train loss: 0.14132921697990863, test loss: 0.2550871197810391\n",
      "epoch 6432: train loss: 0.1413226785232198, test loss: 0.25508525477168736\n",
      "epoch 6433: train loss: 0.14131614152994898, test loss: 0.25508339077223857\n",
      "epoch 6434: train loss: 0.14130960599953682, test loss: 0.25508152778224114\n",
      "epoch 6435: train loss: 0.14130307193142433, test loss: 0.2550796658012116\n",
      "epoch 6436: train loss: 0.14129653932505273, test loss: 0.2550778048286964\n",
      "epoch 6437: train loss: 0.1412900081798637, test loss: 0.255075944864219\n",
      "epoch 6438: train loss: 0.14128347849529901, test loss: 0.25507408590732256\n",
      "epoch 6439: train loss: 0.14127695027080087, test loss: 0.25507222795753387\n",
      "epoch 6440: train loss: 0.1412704235058118, test loss: 0.2550703710143853\n",
      "epoch 6441: train loss: 0.14126389819977447, test loss: 0.25506851507742245\n",
      "epoch 6442: train loss: 0.14125737435213206, test loss: 0.2550666601461675\n",
      "epoch 6443: train loss: 0.14125085196232784, test loss: 0.25506480622016614\n",
      "epoch 6444: train loss: 0.14124433102980558, test loss: 0.25506295329894785\n",
      "epoch 6445: train loss: 0.1412378115540091, test loss: 0.2550611013820507\n",
      "epoch 6446: train loss: 0.14123129353438277, test loss: 0.2550592504690101\n",
      "epoch 6447: train loss: 0.14122477697037109, test loss: 0.25505740055935755\n",
      "epoch 6448: train loss: 0.1412182618614189, test loss: 0.2550555516526399\n",
      "epoch 6449: train loss: 0.14121174820697135, test loss: 0.2550537037483924\n",
      "epoch 6450: train loss: 0.1412052360064739, test loss: 0.25505185684614395\n",
      "epoch 6451: train loss: 0.14119872525937224, test loss: 0.25505001094543955\n",
      "epoch 6452: train loss: 0.14119221596511244, test loss: 0.2550481660458141\n",
      "epoch 6453: train loss: 0.14118570812314085, test loss: 0.2550463221468064\n",
      "epoch 6454: train loss: 0.14117920173290405, test loss: 0.2550444792479538\n",
      "epoch 6455: train loss: 0.14117269679384895, test loss: 0.25504263734879873\n",
      "epoch 6456: train loss: 0.14116619330542274, test loss: 0.255040796448875\n",
      "epoch 6457: train loss: 0.14115969126707298, test loss: 0.2550389565477253\n",
      "epoch 6458: train loss: 0.14115319067824744, test loss: 0.25503711764489034\n",
      "epoch 6459: train loss: 0.1411466915383942, test loss: 0.25503527973990503\n",
      "epoch 6460: train loss: 0.14114019384696172, test loss: 0.2550334428323138\n",
      "epoch 6461: train loss: 0.14113369760339858, test loss: 0.2550316069216562\n",
      "epoch 6462: train loss: 0.1411272028071538, test loss: 0.2550297720074755\n",
      "epoch 6463: train loss: 0.14112070945767663, test loss: 0.25502793808930496\n",
      "epoch 6464: train loss: 0.1411142175544167, test loss: 0.2550261051666944\n",
      "epoch 6465: train loss: 0.14110772709682373, test loss: 0.2550242732391838\n",
      "epoch 6466: train loss: 0.14110123808434796, test loss: 0.25502244230630844\n",
      "epoch 6467: train loss: 0.14109475051643983, test loss: 0.2550206123676164\n",
      "epoch 6468: train loss: 0.14108826439255, test loss: 0.25501878342265044\n",
      "epoch 6469: train loss: 0.14108177971212957, test loss: 0.25501695547095404\n",
      "epoch 6470: train loss: 0.14107529647462982, test loss: 0.2550151285120632\n",
      "epoch 6471: train loss: 0.14106881467950236, test loss: 0.2550133025455312\n",
      "epoch 6472: train loss: 0.14106233432619905, test loss: 0.255011477570897\n",
      "epoch 6473: train loss: 0.14105585541417212, test loss: 0.25500965358769856\n",
      "epoch 6474: train loss: 0.141049377942874, test loss: 0.2550078305954901\n",
      "epoch 6475: train loss: 0.14104290191175756, test loss: 0.2550060085938133\n",
      "epoch 6476: train loss: 0.14103642732027577, test loss: 0.25500418758220594\n",
      "epoch 6477: train loss: 0.141029954167882, test loss: 0.25500236756022493\n",
      "epoch 6478: train loss: 0.14102348245402987, test loss: 0.25500054852740506\n",
      "epoch 6479: train loss: 0.14101701217817336, test loss: 0.25499873048329547\n",
      "epoch 6480: train loss: 0.14101054333976668, test loss: 0.254996913427443\n",
      "epoch 6481: train loss: 0.14100407593826428, test loss: 0.25499509735939485\n",
      "epoch 6482: train loss: 0.14099760997312102, test loss: 0.2549932822786921\n",
      "epoch 6483: train loss: 0.140991145443792, test loss: 0.2549914681848904\n",
      "epoch 6484: train loss: 0.14098468234973255, test loss: 0.2549896550775329\n",
      "epoch 6485: train loss: 0.14097822069039834, test loss: 0.25498784295616383\n",
      "epoch 6486: train loss: 0.14097176046524532, test loss: 0.25498603182032903\n",
      "epoch 6487: train loss: 0.14096530167372978, test loss: 0.25498422166958623\n",
      "epoch 6488: train loss: 0.14095884431530822, test loss: 0.2549824125034737\n",
      "epoch 6489: train loss: 0.14095238838943747, test loss: 0.25498060432154535\n",
      "epoch 6490: train loss: 0.1409459338955746, test loss: 0.25497879712334626\n",
      "epoch 6491: train loss: 0.14093948083317706, test loss: 0.2549769909084282\n",
      "epoch 6492: train loss: 0.14093302920170248, test loss: 0.25497518567634203\n",
      "epoch 6493: train loss: 0.14092657900060887, test loss: 0.25497338142662895\n",
      "epoch 6494: train loss: 0.14092013022935446, test loss: 0.25497157815884913\n",
      "epoch 6495: train loss: 0.1409136828873978, test loss: 0.2549697758725519\n",
      "epoch 6496: train loss: 0.14090723697419774, test loss: 0.2549679745672779\n",
      "epoch 6497: train loss: 0.14090079248921336, test loss: 0.25496617424258766\n",
      "epoch 6498: train loss: 0.14089434943190407, test loss: 0.2549643748980279\n",
      "epoch 6499: train loss: 0.14088790780172958, test loss: 0.25496257653315263\n",
      "epoch 6500: train loss: 0.14088146759814987, test loss: 0.25496077914750925\n",
      "epoch 6501: train loss: 0.14087502882062516, test loss: 0.25495898274064777\n",
      "epoch 6502: train loss: 0.14086859146861602, test loss: 0.25495718731212885\n",
      "epoch 6503: train loss: 0.1408621555415833, test loss: 0.25495539286149976\n",
      "epoch 6504: train loss: 0.14085572103898808, test loss: 0.25495359938831286\n",
      "epoch 6505: train loss: 0.14084928796029178, test loss: 0.2549518068921175\n",
      "epoch 6506: train loss: 0.14084285630495605, test loss: 0.25495001537247874\n",
      "epoch 6507: train loss: 0.14083642607244293, test loss: 0.25494822482893953\n",
      "epoch 6508: train loss: 0.1408299972622146, test loss: 0.25494643526105404\n",
      "epoch 6509: train loss: 0.14082356987373365, test loss: 0.25494464666838107\n",
      "epoch 6510: train loss: 0.14081714390646285, test loss: 0.25494285905046943\n",
      "epoch 6511: train loss: 0.1408107193598654, test loss: 0.25494107240687813\n",
      "epoch 6512: train loss: 0.14080429623340457, test loss: 0.254939286737161\n",
      "epoch 6513: train loss: 0.1407978745265441, test loss: 0.25493750204087817\n",
      "epoch 6514: train loss: 0.14079145423874795, test loss: 0.2549357183175731\n",
      "epoch 6515: train loss: 0.14078503536948034, test loss: 0.25493393556681027\n",
      "epoch 6516: train loss: 0.1407786179182058, test loss: 0.25493215378814166\n",
      "epoch 6517: train loss: 0.14077220188438913, test loss: 0.2549303729811287\n",
      "epoch 6518: train loss: 0.14076578726749542, test loss: 0.25492859314532235\n",
      "epoch 6519: train loss: 0.14075937406699002, test loss: 0.2549268142802839\n",
      "epoch 6520: train loss: 0.1407529622823386, test loss: 0.2549250363855664\n",
      "epoch 6521: train loss: 0.14074655191300708, test loss: 0.2549232594607294\n",
      "epoch 6522: train loss: 0.14074014295846168, test loss: 0.25492148350532795\n",
      "epoch 6523: train loss: 0.14073373541816891, test loss: 0.25491970851892565\n",
      "epoch 6524: train loss: 0.14072732929159554, test loss: 0.25491793450107786\n",
      "epoch 6525: train loss: 0.14072092457820856, test loss: 0.2549161614513377\n",
      "epoch 6526: train loss: 0.1407145212774754, test loss: 0.25491438936927163\n",
      "epoch 6527: train loss: 0.14070811938886366, test loss: 0.25491261825443634\n",
      "epoch 6528: train loss: 0.14070171891184122, test loss: 0.2549108481063868\n",
      "epoch 6529: train loss: 0.14069531984587627, test loss: 0.2549090789246874\n",
      "epoch 6530: train loss: 0.14068892219043724, test loss: 0.25490731070889733\n",
      "epoch 6531: train loss: 0.1406825259449929, test loss: 0.254905543458576\n",
      "epoch 6532: train loss: 0.14067613110901225, test loss: 0.2549037771732868\n",
      "epoch 6533: train loss: 0.1406697376819646, test loss: 0.25490201185258093\n",
      "epoch 6534: train loss: 0.14066334566331956, test loss: 0.2549002474960276\n",
      "epoch 6535: train loss: 0.1406569550525469, test loss: 0.25489848410318766\n",
      "epoch 6536: train loss: 0.14065056584911684, test loss: 0.2548967216736202\n",
      "epoch 6537: train loss: 0.1406441780524998, test loss: 0.2548949602068873\n",
      "epoch 6538: train loss: 0.1406377916621664, test loss: 0.25489319970254753\n",
      "epoch 6539: train loss: 0.14063140667758764, test loss: 0.2548914401601733\n",
      "epoch 6540: train loss: 0.1406250230982348, test loss: 0.2548896815793161\n",
      "epoch 6541: train loss: 0.14061864092357937, test loss: 0.2548879239595442\n",
      "epoch 6542: train loss: 0.14061226015309317, test loss: 0.2548861673004229\n",
      "epoch 6543: train loss: 0.14060588078624828, test loss: 0.2548844116015061\n",
      "epoch 6544: train loss: 0.1405995028225171, test loss: 0.25488265686236844\n",
      "epoch 6545: train loss: 0.14059312626137221, test loss: 0.2548809030825689\n",
      "epoch 6546: train loss: 0.14058675110228655, test loss: 0.2548791502616715\n",
      "epoch 6547: train loss: 0.14058037734473328, test loss: 0.25487739839923657\n",
      "epoch 6548: train loss: 0.14057400498818592, test loss: 0.25487564749483915\n",
      "epoch 6549: train loss: 0.14056763403211822, test loss: 0.25487389754803885\n",
      "epoch 6550: train loss: 0.14056126447600412, test loss: 0.25487214855839313\n",
      "epoch 6551: train loss: 0.140554896319318, test loss: 0.25487040052548005\n",
      "epoch 6552: train loss: 0.14054852956153438, test loss: 0.2548686534488583\n",
      "epoch 6553: train loss: 0.14054216420212814, test loss: 0.25486690732809647\n",
      "epoch 6554: train loss: 0.1405358002405744, test loss: 0.2548651621627597\n",
      "epoch 6555: train loss: 0.14052943767634854, test loss: 0.254863417952418\n",
      "epoch 6556: train loss: 0.1405230765089263, test loss: 0.25486167469663035\n",
      "epoch 6557: train loss: 0.1405167167377835, test loss: 0.25485993239496774\n",
      "epoch 6558: train loss: 0.14051035836239648, test loss: 0.2548581910470041\n",
      "epoch 6559: train loss: 0.1405040013822417, test loss: 0.2548564506522992\n",
      "epoch 6560: train loss: 0.14049764579679594, test loss: 0.2548547112104191\n",
      "epoch 6561: train loss: 0.14049129160553622, test loss: 0.2548529727209388\n",
      "epoch 6562: train loss: 0.14048493880793989, test loss: 0.2548512351834212\n",
      "epoch 6563: train loss: 0.14047858740348457, test loss: 0.25484949859744144\n",
      "epoch 6564: train loss: 0.14047223739164807, test loss: 0.2548477629625653\n",
      "epoch 6565: train loss: 0.14046588877190855, test loss: 0.2548460282783581\n",
      "epoch 6566: train loss: 0.14045954154374446, test loss: 0.25484429454439395\n",
      "epoch 6567: train loss: 0.14045319570663445, test loss: 0.2548425617602412\n",
      "epoch 6568: train loss: 0.1404468512600575, test loss: 0.2548408299254741\n",
      "epoch 6569: train loss: 0.14044050820349285, test loss: 0.25483909903965346\n",
      "epoch 6570: train loss: 0.14043416653642, test loss: 0.25483736910235455\n",
      "epoch 6571: train loss: 0.14042782625831873, test loss: 0.2548356401131533\n",
      "epoch 6572: train loss: 0.14042148736866908, test loss: 0.25483391207161704\n",
      "epoch 6573: train loss: 0.14041514986695136, test loss: 0.2548321849773119\n",
      "epoch 6574: train loss: 0.14040881375264624, test loss: 0.254830458829816\n",
      "epoch 6575: train loss: 0.14040247902523453, test loss: 0.2548287336286997\n",
      "epoch 6576: train loss: 0.14039614568419734, test loss: 0.25482700937353736\n",
      "epoch 6577: train loss: 0.14038981372901616, test loss: 0.25482528606389393\n",
      "epoch 6578: train loss: 0.14038348315917262, test loss: 0.25482356369935166\n",
      "epoch 6579: train loss: 0.14037715397414863, test loss: 0.2548218422794745\n",
      "epoch 6580: train loss: 0.14037082617342653, test loss: 0.25482012180384145\n",
      "epoch 6581: train loss: 0.1403644997564887, test loss: 0.2548184022720282\n",
      "epoch 6582: train loss: 0.140358174722818, test loss: 0.2548166836835991\n",
      "epoch 6583: train loss: 0.14035185107189738, test loss: 0.25481496603813675\n",
      "epoch 6584: train loss: 0.14034552880321016, test loss: 0.25481324933521327\n",
      "epoch 6585: train loss: 0.14033920791623997, test loss: 0.2548115335744016\n",
      "epoch 6586: train loss: 0.1403328884104706, test loss: 0.25480981875527803\n",
      "epoch 6587: train loss: 0.1403265702853862, test loss: 0.2548081048774151\n",
      "epoch 6588: train loss: 0.14032025354047106, test loss: 0.2548063919403921\n",
      "epoch 6589: train loss: 0.14031393817520996, test loss: 0.2548046799437811\n",
      "epoch 6590: train loss: 0.14030762418908777, test loss: 0.25480296888715853\n",
      "epoch 6591: train loss: 0.14030131158158965, test loss: 0.2548012587701028\n",
      "epoch 6592: train loss: 0.14029500035220108, test loss: 0.25479954959218604\n",
      "epoch 6593: train loss: 0.14028869050040776, test loss: 0.2547978413529913\n",
      "epoch 6594: train loss: 0.1402823820256957, test loss: 0.25479613405208723\n",
      "epoch 6595: train loss: 0.14027607492755123, test loss: 0.25479442768906035\n",
      "epoch 6596: train loss: 0.14026976920546078, test loss: 0.25479272226347666\n",
      "epoch 6597: train loss: 0.1402634648589112, test loss: 0.2547910177749239\n",
      "epoch 6598: train loss: 0.14025716188738951, test loss: 0.2547893142229796\n",
      "epoch 6599: train loss: 0.14025086029038308, test loss: 0.254787611607211\n",
      "epoch 6600: train loss: 0.14024456006737954, test loss: 0.25478590992720884\n",
      "epoch 6601: train loss: 0.1402382612178667, test loss: 0.254784209182546\n",
      "epoch 6602: train loss: 0.14023196374133273, test loss: 0.25478250937280644\n",
      "epoch 6603: train loss: 0.14022566763726596, test loss: 0.25478081049755996\n",
      "epoch 6604: train loss: 0.14021937290515513, test loss: 0.25477911255639313\n",
      "epoch 6605: train loss: 0.1402130795444892, test loss: 0.25477741554888583\n",
      "epoch 6606: train loss: 0.14020678755475724, test loss: 0.2547757194746129\n",
      "epoch 6607: train loss: 0.14020049693544884, test loss: 0.25477402433315943\n",
      "epoch 6608: train loss: 0.14019420768605367, test loss: 0.2547723301241045\n",
      "epoch 6609: train loss: 0.1401879198060617, test loss: 0.2547706368470245\n",
      "epoch 6610: train loss: 0.14018163329496328, test loss: 0.2547689445015099\n",
      "epoch 6611: train loss: 0.14017534815224886, test loss: 0.2547672530871351\n",
      "epoch 6612: train loss: 0.14016906437740925, test loss: 0.2547655626034812\n",
      "epoch 6613: train loss: 0.1401627819699355, test loss: 0.25476387305013665\n",
      "epoch 6614: train loss: 0.14015650092931894, test loss: 0.25476218442667764\n",
      "epoch 6615: train loss: 0.14015022125505117, test loss: 0.2547604967326846\n",
      "epoch 6616: train loss: 0.140143942946624, test loss: 0.25475880996774153\n",
      "epoch 6617: train loss: 0.1401376660035296, test loss: 0.25475712413144164\n",
      "epoch 6618: train loss: 0.14013139042526027, test loss: 0.254755439223352\n",
      "epoch 6619: train loss: 0.1401251162113087, test loss: 0.25475375524306254\n",
      "epoch 6620: train loss: 0.14011884336116778, test loss: 0.2547520721901603\n",
      "epoch 6621: train loss: 0.1401125718743307, test loss: 0.25475039006422073\n",
      "epoch 6622: train loss: 0.14010630175029085, test loss: 0.2547487088648386\n",
      "epoch 6623: train loss: 0.14010003298854196, test loss: 0.25474702859159337\n",
      "epoch 6624: train loss: 0.14009376558857797, test loss: 0.2547453492440663\n",
      "epoch 6625: train loss: 0.14008749954989308, test loss: 0.2547436708218482\n",
      "epoch 6626: train loss: 0.14008123487198185, test loss: 0.25474199332452147\n",
      "epoch 6627: train loss: 0.1400749715543389, test loss: 0.25474031675166603\n",
      "epoch 6628: train loss: 0.14006870959645934, test loss: 0.25473864110287736\n",
      "epoch 6629: train loss: 0.14006244899783837, test loss: 0.25473696637773\n",
      "epoch 6630: train loss: 0.1400561897579716, test loss: 0.2547352925758208\n",
      "epoch 6631: train loss: 0.1400499318763547, test loss: 0.2547336196967349\n",
      "epoch 6632: train loss: 0.14004367535248383, test loss: 0.2547319477400494\n",
      "epoch 6633: train loss: 0.14003742018585527, test loss: 0.25473027670536574\n",
      "epoch 6634: train loss: 0.14003116637596558, test loss: 0.2547286065922571\n",
      "epoch 6635: train loss: 0.14002491392231162, test loss: 0.25472693740031693\n",
      "epoch 6636: train loss: 0.14001866282439043, test loss: 0.2547252691291335\n",
      "epoch 6637: train loss: 0.14001241308169945, test loss: 0.25472360177829617\n",
      "epoch 6638: train loss: 0.14000616469373625, test loss: 0.25472193534738474\n",
      "epoch 6639: train loss: 0.13999991765999872, test loss: 0.2547202698359984\n",
      "epoch 6640: train loss: 0.13999367197998502, test loss: 0.25471860524371887\n",
      "epoch 6641: train loss: 0.1399874276531935, test loss: 0.2547169415701352\n",
      "epoch 6642: train loss: 0.13998118467912282, test loss: 0.2547152788148409\n",
      "epoch 6643: train loss: 0.1399749430572719, test loss: 0.2547136169774243\n",
      "epoch 6644: train loss: 0.13996870278713996, test loss: 0.2547119560574725\n",
      "epoch 6645: train loss: 0.13996246386822642, test loss: 0.25471029605457646\n",
      "epoch 6646: train loss: 0.13995622630003093, test loss: 0.2547086369683237\n",
      "epoch 6647: train loss: 0.1399499900820535, test loss: 0.254706978798311\n",
      "epoch 6648: train loss: 0.13994375521379435, test loss: 0.2547053215441182\n",
      "epoch 6649: train loss: 0.13993752169475385, test loss: 0.2547036652053498\n",
      "epoch 6650: train loss: 0.13993128952443282, test loss: 0.2547020097815883\n",
      "epoch 6651: train loss: 0.13992505870233224, test loss: 0.25470035527242885\n",
      "epoch 6652: train loss: 0.1399188292279533, test loss: 0.2546987016774601\n",
      "epoch 6653: train loss: 0.13991260110079756, test loss: 0.25469704899627243\n",
      "epoch 6654: train loss: 0.13990637432036673, test loss: 0.2546953972284631\n",
      "epoch 6655: train loss: 0.1399001488861629, test loss: 0.25469374637362074\n",
      "epoch 6656: train loss: 0.13989392479768828, test loss: 0.2546920964313441\n",
      "epoch 6657: train loss: 0.13988770205444542, test loss: 0.25469044740121427\n",
      "epoch 6658: train loss: 0.1398814806559371, test loss: 0.254688799282835\n",
      "epoch 6659: train loss: 0.1398752606016664, test loss: 0.25468715207579407\n",
      "epoch 6660: train loss: 0.13986904189113664, test loss: 0.25468550577968785\n",
      "epoch 6661: train loss: 0.13986282452385126, test loss: 0.25468386039411245\n",
      "epoch 6662: train loss: 0.1398566084993142, test loss: 0.2546822159186557\n",
      "epoch 6663: train loss: 0.13985039381702946, test loss: 0.25468057235291325\n",
      "epoch 6664: train loss: 0.1398441804765014, test loss: 0.25467892969648515\n",
      "epoch 6665: train loss: 0.1398379684772346, test loss: 0.2546772879489643\n",
      "epoch 6666: train loss: 0.1398317578187339, test loss: 0.2546756471099379\n",
      "epoch 6667: train loss: 0.13982554850050435, test loss: 0.25467400717900956\n",
      "epoch 6668: train loss: 0.13981934052205136, test loss: 0.25467236815577726\n",
      "epoch 6669: train loss: 0.1398131338828805, test loss: 0.2546707300398303\n",
      "epoch 6670: train loss: 0.1398069285824976, test loss: 0.25466909283076505\n",
      "epoch 6671: train loss: 0.13980072462040888, test loss: 0.2546674565281827\n",
      "epoch 6672: train loss: 0.13979452199612058, test loss: 0.2546658211316746\n",
      "epoch 6673: train loss: 0.1397883207091394, test loss: 0.2546641866408416\n",
      "epoch 6674: train loss: 0.13978212075897223, test loss: 0.2546625530552763\n",
      "epoch 6675: train loss: 0.13977592214512613, test loss: 0.25466092037457955\n",
      "epoch 6676: train loss: 0.13976972486710854, test loss: 0.2546592885983514\n",
      "epoch 6677: train loss: 0.13976352892442706, test loss: 0.254657657726183\n",
      "epoch 6678: train loss: 0.1397573343165896, test loss: 0.2546560277576742\n",
      "epoch 6679: train loss: 0.13975114104310432, test loss: 0.25465439869242557\n",
      "epoch 6680: train loss: 0.1397449491034796, test loss: 0.25465277053003793\n",
      "epoch 6681: train loss: 0.13973875849722409, test loss: 0.25465114327010674\n",
      "epoch 6682: train loss: 0.13973256922384666, test loss: 0.2546495169122288\n",
      "epoch 6683: train loss: 0.13972638128285655, test loss: 0.2546478914560067\n",
      "epoch 6684: train loss: 0.13972019467376315, test loss: 0.25464626690103814\n",
      "epoch 6685: train loss: 0.13971400939607606, test loss: 0.25464464324692465\n",
      "epoch 6686: train loss: 0.13970782544930527, test loss: 0.25464302049326754\n",
      "epoch 6687: train loss: 0.13970164283296085, test loss: 0.2546413986396627\n",
      "epoch 6688: train loss: 0.1396954615465533, test loss: 0.2546397776857098\n",
      "epoch 6689: train loss: 0.1396892815895933, test loss: 0.2546381576310177\n",
      "epoch 6690: train loss: 0.13968310296159167, test loss: 0.2546365384751791\n",
      "epoch 6691: train loss: 0.1396769256620597, test loss: 0.2546349202178015\n",
      "epoch 6692: train loss: 0.13967074969050874, test loss: 0.25463330285848335\n",
      "epoch 6693: train loss: 0.13966457504645047, test loss: 0.2546316863968213\n",
      "epoch 6694: train loss: 0.13965840172939684, test loss: 0.2546300708324256\n",
      "epoch 6695: train loss: 0.13965222973886002, test loss: 0.25462845616489543\n",
      "epoch 6696: train loss: 0.1396460590743524, test loss: 0.2546268423938281\n",
      "epoch 6697: train loss: 0.13963988973538677, test loss: 0.2546252295188371\n",
      "epoch 6698: train loss: 0.1396337217214759, test loss: 0.2546236175395149\n",
      "epoch 6699: train loss: 0.1396275550321331, test loss: 0.2546220064554656\n",
      "epoch 6700: train loss: 0.13962138966687168, test loss: 0.25462039626630256\n",
      "epoch 6701: train loss: 0.13961522562520542, test loss: 0.2546187869716205\n",
      "epoch 6702: train loss: 0.1396090629066482, test loss: 0.25461717857102373\n",
      "epoch 6703: train loss: 0.1396029015107142, test loss: 0.25461557106411914\n",
      "epoch 6704: train loss: 0.1395967414369178, test loss: 0.2546139644505099\n",
      "epoch 6705: train loss: 0.13959058268477378, test loss: 0.25461235872980637\n",
      "epoch 6706: train loss: 0.139584425253797, test loss: 0.25461075390160137\n",
      "epoch 6707: train loss: 0.1395782691435026, test loss: 0.2546091499655065\n",
      "epoch 6708: train loss: 0.13957211435340602, test loss: 0.254607546921123\n",
      "epoch 6709: train loss: 0.13956596088302298, test loss: 0.25460594476806436\n",
      "epoch 6710: train loss: 0.13955980873186935, test loss: 0.2546043435059313\n",
      "epoch 6711: train loss: 0.13955365789946128, test loss: 0.25460274313433545\n",
      "epoch 6712: train loss: 0.1395475083853152, test loss: 0.254601143652873\n",
      "epoch 6713: train loss: 0.1395413601889478, test loss: 0.25459954506116\n",
      "epoch 6714: train loss: 0.13953521330987592, test loss: 0.25459794735879365\n",
      "epoch 6715: train loss: 0.1395290677476168, test loss: 0.2545963505453879\n",
      "epoch 6716: train loss: 0.13952292350168777, test loss: 0.25459475462054804\n",
      "epoch 6717: train loss: 0.13951678057160652, test loss: 0.2545931595838816\n",
      "epoch 6718: train loss: 0.13951063895689095, test loss: 0.25459156543499795\n",
      "epoch 6719: train loss: 0.13950449865705916, test loss: 0.25458997217350227\n",
      "epoch 6720: train loss: 0.13949835967162957, test loss: 0.25458837979900645\n",
      "epoch 6721: train loss: 0.13949222200012085, test loss: 0.2545867883111124\n",
      "epoch 6722: train loss: 0.1394860856420518, test loss: 0.25458519770943133\n",
      "epoch 6723: train loss: 0.13947995059694163, test loss: 0.2545836079935768\n",
      "epoch 6724: train loss: 0.13947381686430962, test loss: 0.25458201916315104\n",
      "epoch 6725: train loss: 0.13946768444367552, test loss: 0.254580431217769\n",
      "epoch 6726: train loss: 0.13946155333455906, test loss: 0.2545788441570406\n",
      "epoch 6727: train loss: 0.13945542353648047, test loss: 0.2545772579805677\n",
      "epoch 6728: train loss: 0.13944929504896003, test loss: 0.25457567268796816\n",
      "epoch 6729: train loss: 0.13944316787151834, test loss: 0.2545740882788494\n",
      "epoch 6730: train loss: 0.13943704200367626, test loss: 0.2545725047528213\n",
      "epoch 6731: train loss: 0.13943091744495495, test loss: 0.2545709221094986\n",
      "epoch 6732: train loss: 0.1394247941948756, test loss: 0.25456934034848616\n",
      "epoch 6733: train loss: 0.13941867225295995, test loss: 0.2545677594694\n",
      "epoch 6734: train loss: 0.13941255161872973, test loss: 0.25456617947185056\n",
      "epoch 6735: train loss: 0.13940643229170702, test loss: 0.25456460035544737\n",
      "epoch 6736: train loss: 0.13940031427141417, test loss: 0.25456302211980353\n",
      "epoch 6737: train loss: 0.13939419755737367, test loss: 0.25456144476452836\n",
      "epoch 6738: train loss: 0.13938808214910836, test loss: 0.2545598682892389\n",
      "epoch 6739: train loss: 0.1393819680461413, test loss: 0.2545582926935535\n",
      "epoch 6740: train loss: 0.13937585524799576, test loss: 0.254556717977071\n",
      "epoch 6741: train loss: 0.13936974375419528, test loss: 0.2545551441394114\n",
      "epoch 6742: train loss: 0.1393636335642636, test loss: 0.2545535711801853\n",
      "epoch 6743: train loss: 0.13935752467772478, test loss: 0.25455199909901166\n",
      "epoch 6744: train loss: 0.13935141709410304, test loss: 0.2545504278955024\n",
      "epoch 6745: train loss: 0.1393453108129229, test loss: 0.25454885756927054\n",
      "epoch 6746: train loss: 0.13933920583370912, test loss: 0.25454728811992466\n",
      "epoch 6747: train loss: 0.13933310215598665, test loss: 0.25454571954709304\n",
      "epoch 6748: train loss: 0.13932699977928073, test loss: 0.2545441518503778\n",
      "epoch 6749: train loss: 0.13932089870311684, test loss: 0.25454258502939553\n",
      "epoch 6750: train loss: 0.13931479892702073, test loss: 0.2545410190837679\n",
      "epoch 6751: train loss: 0.1393087004505183, test loss: 0.25453945401310474\n",
      "epoch 6752: train loss: 0.13930260327313573, test loss: 0.2545378898170242\n",
      "epoch 6753: train loss: 0.1392965073943995, test loss: 0.2545363264951392\n",
      "epoch 6754: train loss: 0.13929041281383625, test loss: 0.2545347640470702\n",
      "epoch 6755: train loss: 0.13928431953097295, test loss: 0.2545332024724335\n",
      "epoch 6756: train loss: 0.13927822754533672, test loss: 0.2545316417708361\n",
      "epoch 6757: train loss: 0.13927213685645495, test loss: 0.2545300819419085\n",
      "epoch 6758: train loss: 0.13926604746385532, test loss: 0.2545285229852589\n",
      "epoch 6759: train loss: 0.13925995936706567, test loss: 0.25452696490050664\n",
      "epoch 6760: train loss: 0.13925387256561414, test loss: 0.25452540768727383\n",
      "epoch 6761: train loss: 0.1392477870590291, test loss: 0.2545238513451671\n",
      "epoch 6762: train loss: 0.13924170284683915, test loss: 0.25452229587381614\n",
      "epoch 6763: train loss: 0.13923561992857308, test loss: 0.2545207412728334\n",
      "epoch 6764: train loss: 0.13922953830376003, test loss: 0.2545191875418339\n",
      "epoch 6765: train loss: 0.1392234579719293, test loss: 0.2545176346804471\n",
      "epoch 6766: train loss: 0.13921737893261046, test loss: 0.25451608268827786\n",
      "epoch 6767: train loss: 0.1392113011853333, test loss: 0.2545145315649608\n",
      "epoch 6768: train loss: 0.1392052247296278, test loss: 0.25451298131009836\n",
      "epoch 6769: train loss: 0.13919914956502433, test loss: 0.2545114319233268\n",
      "epoch 6770: train loss: 0.13919307569105333, test loss: 0.2545098834042508\n",
      "epoch 6771: train loss: 0.13918700310724558, test loss: 0.25450833575250137\n",
      "epoch 6772: train loss: 0.1391809318131321, test loss: 0.25450678896769036\n",
      "epoch 6773: train loss: 0.13917486180824407, test loss: 0.2545052430494464\n",
      "epoch 6774: train loss: 0.13916879309211297, test loss: 0.25450369799738687\n",
      "epoch 6775: train loss: 0.13916272566427054, test loss: 0.25450215381112906\n",
      "epoch 6776: train loss: 0.13915665952424866, test loss: 0.254500610490298\n",
      "epoch 6777: train loss: 0.13915059467157953, test loss: 0.254499068034518\n",
      "epoch 6778: train loss: 0.1391445311057956, test loss: 0.2544975264434008\n",
      "epoch 6779: train loss: 0.1391384688264295, test loss: 0.2544959857165776\n",
      "epoch 6780: train loss: 0.13913240783301414, test loss: 0.2544944458536686\n",
      "epoch 6781: train loss: 0.1391263481250826, test loss: 0.25449290685429105\n",
      "epoch 6782: train loss: 0.1391202897021683, test loss: 0.2544913687180735\n",
      "epoch 6783: train loss: 0.1391142325638048, test loss: 0.25448983144463666\n",
      "epoch 6784: train loss: 0.13910817670952597, test loss: 0.25448829503359943\n",
      "epoch 6785: train loss: 0.13910212213886586, test loss: 0.25448675948459065\n",
      "epoch 6786: train loss: 0.13909606885135875, test loss: 0.25448522479723\n",
      "epoch 6787: train loss: 0.1390900168465393, test loss: 0.254483690971146\n",
      "epoch 6788: train loss: 0.13908396612394217, test loss: 0.25448215800595564\n",
      "epoch 6789: train loss: 0.13907791668310243, test loss: 0.2544806259012888\n",
      "epoch 6790: train loss: 0.13907186852355533, test loss: 0.254479094656766\n",
      "epoch 6791: train loss: 0.13906582164483633, test loss: 0.2544775642720119\n",
      "epoch 6792: train loss: 0.1390597760464812, test loss: 0.2544760347466575\n",
      "epoch 6793: train loss: 0.13905373172802588, test loss: 0.2544745060803188\n",
      "epoch 6794: train loss: 0.13904768868900658, test loss: 0.25447297827262405\n",
      "epoch 6795: train loss: 0.13904164692895965, test loss: 0.25447145132319854\n",
      "epoch 6796: train loss: 0.13903560644742183, test loss: 0.25446992523167394\n",
      "epoch 6797: train loss: 0.13902956724393, test loss: 0.25446839999767096\n",
      "epoch 6798: train loss: 0.1390235293180213, test loss: 0.2544668756208134\n",
      "epoch 6799: train loss: 0.1390174926692331, test loss: 0.25446535210073096\n",
      "epoch 6800: train loss: 0.13901145729710296, test loss: 0.25446382943704954\n",
      "epoch 6801: train loss: 0.13900542320116874, test loss: 0.2544623076293975\n",
      "epoch 6802: train loss: 0.1389993903809685, test loss: 0.25446078667739597\n",
      "epoch 6803: train loss: 0.13899335883604055, test loss: 0.2544592665806783\n",
      "epoch 6804: train loss: 0.13898732856592344, test loss: 0.2544577473388701\n",
      "epoch 6805: train loss: 0.13898129957015587, test loss: 0.2544562289515973\n",
      "epoch 6806: train loss: 0.1389752718482769, test loss: 0.2544547114184891\n",
      "epoch 6807: train loss: 0.13896924539982575, test loss: 0.25445319473917544\n",
      "epoch 6808: train loss: 0.1389632202243419, test loss: 0.2544516789132828\n",
      "epoch 6809: train loss: 0.13895719632136502, test loss: 0.2544501639404342\n",
      "epoch 6810: train loss: 0.13895117369043505, test loss: 0.2544486498202733\n",
      "epoch 6811: train loss: 0.13894515233109214, test loss: 0.25444713655241236\n",
      "epoch 6812: train loss: 0.1389391322428767, test loss: 0.254445624136494\n",
      "epoch 6813: train loss: 0.1389331134253294, test loss: 0.2544441125721367\n",
      "epoch 6814: train loss: 0.13892709587799104, test loss: 0.25444260185897444\n",
      "epoch 6815: train loss: 0.1389210796004027, test loss: 0.254441091996639\n",
      "epoch 6816: train loss: 0.13891506459210576, test loss: 0.2544395829847643\n",
      "epoch 6817: train loss: 0.1389090508526417, test loss: 0.254438074822971\n",
      "epoch 6818: train loss: 0.1389030383815524, test loss: 0.25443656751089366\n",
      "epoch 6819: train loss: 0.13889702717837982, test loss: 0.2544350610481649\n",
      "epoch 6820: train loss: 0.13889101724266617, test loss: 0.25443355543441354\n",
      "epoch 6821: train loss: 0.138885008573954, test loss: 0.25443205066926994\n",
      "epoch 6822: train loss: 0.13887900117178598, test loss: 0.25443054675236787\n",
      "epoch 6823: train loss: 0.13887299503570508, test loss: 0.2544290436833419\n",
      "epoch 6824: train loss: 0.13886699016525436, test loss: 0.25442754146181573\n",
      "epoch 6825: train loss: 0.1388609865599774, test loss: 0.25442604008742925\n",
      "epoch 6826: train loss: 0.1388549842194177, test loss: 0.2544245395598069\n",
      "epoch 6827: train loss: 0.1388489831431191, test loss: 0.25442303987858694\n",
      "epoch 6828: train loss: 0.1388429833306258, test loss: 0.2544215410434024\n",
      "epoch 6829: train loss: 0.13883698478148204, test loss: 0.25442004305388416\n",
      "epoch 6830: train loss: 0.13883098749523243, test loss: 0.2544185459096679\n",
      "epoch 6831: train loss: 0.13882499147142163, test loss: 0.25441704961037936\n",
      "epoch 6832: train loss: 0.13881899670959477, test loss: 0.25441555415565936\n",
      "epoch 6833: train loss: 0.13881300320929704, test loss: 0.2544140595451406\n",
      "epoch 6834: train loss: 0.1388070109700739, test loss: 0.25441256577845694\n",
      "epoch 6835: train loss: 0.13880101999147101, test loss: 0.25441107285524706\n",
      "epoch 6836: train loss: 0.13879503027303436, test loss: 0.254409580775134\n",
      "epoch 6837: train loss: 0.13878904181431007, test loss: 0.25440808953776406\n",
      "epoch 6838: train loss: 0.13878305461484447, test loss: 0.2544065991427599\n",
      "epoch 6839: train loss: 0.13877706867418424, test loss: 0.2544051095897679\n",
      "epoch 6840: train loss: 0.1387710839918762, test loss: 0.25440362087842017\n",
      "epoch 6841: train loss: 0.13876510056746735, test loss: 0.25440213300834974\n",
      "epoch 6842: train loss: 0.1387591184005051, test loss: 0.2544006459791952\n",
      "epoch 6843: train loss: 0.1387531374905368, test loss: 0.254399159790588\n",
      "epoch 6844: train loss: 0.13874715783711034, test loss: 0.25439767444217265\n",
      "epoch 6845: train loss: 0.13874117943977363, test loss: 0.2543961899335808\n",
      "epoch 6846: train loss: 0.1387352022980749, test loss: 0.2543947062644468\n",
      "epoch 6847: train loss: 0.13872922641156252, test loss: 0.25439322343441034\n",
      "epoch 6848: train loss: 0.1387232517797852, test loss: 0.25439174144310506\n",
      "epoch 6849: train loss: 0.13871727840229178, test loss: 0.25439026029017436\n",
      "epoch 6850: train loss: 0.13871130627863137, test loss: 0.2543887799752484\n",
      "epoch 6851: train loss: 0.13870533540835336, test loss: 0.2543873004979736\n",
      "epoch 6852: train loss: 0.13869936579100725, test loss: 0.25438582185797753\n",
      "epoch 6853: train loss: 0.13869339742614278, test loss: 0.2543843440549083\n",
      "epoch 6854: train loss: 0.13868743031331007, test loss: 0.2543828670884008\n",
      "epoch 6855: train loss: 0.13868146445205928, test loss: 0.2543813909580885\n",
      "epoch 6856: train loss: 0.1386754998419409, test loss: 0.2543799156636132\n",
      "epoch 6857: train loss: 0.1386695364825056, test loss: 0.2543784412046179\n",
      "epoch 6858: train loss: 0.13866357437330432, test loss: 0.25437696758074174\n",
      "epoch 6859: train loss: 0.1386576135138882, test loss: 0.25437549479161764\n",
      "epoch 6860: train loss: 0.13865165390380857, test loss: 0.2543740228368867\n",
      "epoch 6861: train loss: 0.138645695542617, test loss: 0.2543725517161916\n",
      "epoch 6862: train loss: 0.13863973842986538, test loss: 0.2543710814291785\n",
      "epoch 6863: train loss: 0.13863378256510564, test loss: 0.2543696119754743\n",
      "epoch 6864: train loss: 0.13862782794789014, test loss: 0.2543681433547252\n",
      "epoch 6865: train loss: 0.13862187457777134, test loss: 0.2543666755665803\n",
      "epoch 6866: train loss: 0.13861592245430188, test loss: 0.25436520861066814\n",
      "epoch 6867: train loss: 0.1386099715770348, test loss: 0.25436374248663013\n",
      "epoch 6868: train loss: 0.13860402194552318, test loss: 0.2543622771941188\n",
      "epoch 6869: train loss: 0.13859807355932044, test loss: 0.25436081273276856\n",
      "epoch 6870: train loss: 0.13859212641798016, test loss: 0.2543593491022226\n",
      "epoch 6871: train loss: 0.1385861805210562, test loss: 0.254357886302118\n",
      "epoch 6872: train loss: 0.13858023586810259, test loss: 0.25435642433209993\n",
      "epoch 6873: train loss: 0.1385742924586736, test loss: 0.25435496319181833\n",
      "epoch 6874: train loss: 0.13856835029232373, test loss: 0.2543535028809059\n",
      "epoch 6875: train loss: 0.13856240936860773, test loss: 0.2543520433990094\n",
      "epoch 6876: train loss: 0.13855646968708046, test loss: 0.25435058474577255\n",
      "epoch 6877: train loss: 0.1385505312472972, test loss: 0.2543491269208302\n",
      "epoch 6878: train loss: 0.13854459404881325, test loss: 0.2543476699238399\n",
      "epoch 6879: train loss: 0.1385386580911843, test loss: 0.2543462137544364\n",
      "epoch 6880: train loss: 0.13853272337396608, test loss: 0.25434475841226745\n",
      "epoch 6881: train loss: 0.13852678989671474, test loss: 0.25434330389697396\n",
      "epoch 6882: train loss: 0.13852085765898647, test loss: 0.25434185020819744\n",
      "epoch 6883: train loss: 0.13851492666033785, test loss: 0.2543403973455903\n",
      "epoch 6884: train loss: 0.13850899690032556, test loss: 0.2543389453087974\n",
      "epoch 6885: train loss: 0.13850306837850654, test loss: 0.2543374940974557\n",
      "epoch 6886: train loss: 0.13849714109443795, test loss: 0.2543360437112167\n",
      "epoch 6887: train loss: 0.13849121504767722, test loss: 0.2543345941497173\n",
      "epoch 6888: train loss: 0.1384852902377819, test loss: 0.2543331454126141\n",
      "epoch 6889: train loss: 0.1384793666643098, test loss: 0.25433169749954826\n",
      "epoch 6890: train loss: 0.13847344432681902, test loss: 0.2543302504101644\n",
      "epoch 6891: train loss: 0.1384675232248678, test loss: 0.25432880414410763\n",
      "epoch 6892: train loss: 0.1384616033580146, test loss: 0.2543273587010294\n",
      "epoch 6893: train loss: 0.13845568472581823, test loss: 0.25432591408057087\n",
      "epoch 6894: train loss: 0.1384497673278375, test loss: 0.25432447028238137\n",
      "epoch 6895: train loss: 0.13844385116363164, test loss: 0.25432302730610906\n",
      "epoch 6896: train loss: 0.13843793623275996, test loss: 0.25432158515140274\n",
      "epoch 6897: train loss: 0.13843202253478207, test loss: 0.2543201438178995\n",
      "epoch 6898: train loss: 0.13842611006925776, test loss: 0.25431870330526335\n",
      "epoch 6899: train loss: 0.1384201988357471, test loss: 0.25431726361312496\n",
      "epoch 6900: train loss: 0.1384142888338103, test loss: 0.25431582474114917\n",
      "epoch 6901: train loss: 0.13840838006300782, test loss: 0.25431438668897066\n",
      "epoch 6902: train loss: 0.13840247252290036, test loss: 0.25431294945624094\n",
      "epoch 6903: train loss: 0.13839656621304883, test loss: 0.25431151304261607\n",
      "epoch 6904: train loss: 0.13839066113301432, test loss: 0.25431007744773865\n",
      "epoch 6905: train loss: 0.13838475728235822, test loss: 0.2543086426712585\n",
      "epoch 6906: train loss: 0.13837885466064204, test loss: 0.25430720871282253\n",
      "epoch 6907: train loss: 0.13837295326742757, test loss: 0.2543057755720824\n",
      "epoch 6908: train loss: 0.13836705310227682, test loss: 0.2543043432486924\n",
      "epoch 6909: train loss: 0.13836115416475198, test loss: 0.25430291174229686\n",
      "epoch 6910: train loss: 0.13835525645441552, test loss: 0.2543014810525466\n",
      "epoch 6911: train loss: 0.13834935997083003, test loss: 0.2543000511790931\n",
      "epoch 6912: train loss: 0.13834346471355843, test loss: 0.25429862212158816\n",
      "epoch 6913: train loss: 0.13833757068216376, test loss: 0.2542971938796789\n",
      "epoch 6914: train loss: 0.13833167787620937, test loss: 0.25429576645302054\n",
      "epoch 6915: train loss: 0.13832578629525874, test loss: 0.2542943398412571\n",
      "epoch 6916: train loss: 0.13831989593887561, test loss: 0.2542929140440471\n",
      "epoch 6917: train loss: 0.13831400680662395, test loss: 0.2542914890610432\n",
      "epoch 6918: train loss: 0.13830811889806793, test loss: 0.25429006489188954\n",
      "epoch 6919: train loss: 0.1383022322127719, test loss: 0.2542886415362464\n",
      "epoch 6920: train loss: 0.13829634675030048, test loss: 0.25428721899375856\n",
      "epoch 6921: train loss: 0.1382904625102185, test loss: 0.25428579726408473\n",
      "epoch 6922: train loss: 0.138284579492091, test loss: 0.25428437634686946\n",
      "epoch 6923: train loss: 0.1382786976954832, test loss: 0.2542829562417756\n",
      "epoch 6924: train loss: 0.13827281711996062, test loss: 0.25428153694844924\n",
      "epoch 6925: train loss: 0.1382669377650889, test loss: 0.254280118466547\n",
      "epoch 6926: train loss: 0.13826105963043392, test loss: 0.25427870079571957\n",
      "epoch 6927: train loss: 0.13825518271556184, test loss: 0.25427728393561627\n",
      "epoch 6928: train loss: 0.13824930702003893, test loss: 0.2542758678859046\n",
      "epoch 6929: train loss: 0.13824343254343183, test loss: 0.254274452646225\n",
      "epoch 6930: train loss: 0.1382375592853072, test loss: 0.25427303821623776\n",
      "epoch 6931: train loss: 0.1382316872452321, test loss: 0.2542716245955989\n",
      "epoch 6932: train loss: 0.13822581642277362, test loss: 0.25427021178396086\n",
      "epoch 6933: train loss: 0.1382199468174993, test loss: 0.2542687997809761\n",
      "epoch 6934: train loss: 0.13821407842897662, test loss: 0.25426738858630016\n",
      "epoch 6935: train loss: 0.1382082112567735, test loss: 0.25426597819959545\n",
      "epoch 6936: train loss: 0.13820234530045794, test loss: 0.25426456862050356\n",
      "epoch 6937: train loss: 0.13819648055959824, test loss: 0.2542631598486944\n",
      "epoch 6938: train loss: 0.13819061703376284, test loss: 0.25426175188382044\n",
      "epoch 6939: train loss: 0.13818475472252045, test loss: 0.2542603447255301\n",
      "epoch 6940: train loss: 0.13817889362543997, test loss: 0.2542589383734824\n",
      "epoch 6941: train loss: 0.13817303374209053, test loss: 0.2542575328273402\n",
      "epoch 6942: train loss: 0.13816717507204143, test loss: 0.2542561280867585\n",
      "epoch 6943: train loss: 0.13816131761486225, test loss: 0.254254724151387\n",
      "epoch 6944: train loss: 0.1381554613701227, test loss: 0.25425332102088566\n",
      "epoch 6945: train loss: 0.1381496063373928, test loss: 0.25425191869491615\n",
      "epoch 6946: train loss: 0.1381437525162427, test loss: 0.2542505171731338\n",
      "epoch 6947: train loss: 0.13813789990624278, test loss: 0.2542491164551923\n",
      "epoch 6948: train loss: 0.13813204850696373, test loss: 0.25424771654075917\n",
      "epoch 6949: train loss: 0.1381261983179763, test loss: 0.25424631742947695\n",
      "epoch 6950: train loss: 0.1381203493388515, test loss: 0.25424491912102165\n",
      "epoch 6951: train loss: 0.13811450156916064, test loss: 0.2542435216150396\n",
      "epoch 6952: train loss: 0.13810865500847516, test loss: 0.254242124911192\n",
      "epoch 6953: train loss: 0.1381028096563667, test loss: 0.25424072900913763\n",
      "epoch 6954: train loss: 0.1380969655124072, test loss: 0.25423933390853853\n",
      "epoch 6955: train loss: 0.13809112257616865, test loss: 0.2542379396090475\n",
      "epoch 6956: train loss: 0.13808528084722346, test loss: 0.25423654611033264\n",
      "epoch 6957: train loss: 0.1380794403251441, test loss: 0.25423515341205105\n",
      "epoch 6958: train loss: 0.1380736010095033, test loss: 0.2542337615138539\n",
      "epoch 6959: train loss: 0.13806776289987402, test loss: 0.254232370415416\n",
      "epoch 6960: train loss: 0.13806192599582937, test loss: 0.25423098011638684\n",
      "epoch 6961: train loss: 0.13805609029694274, test loss: 0.254229590616424\n",
      "epoch 6962: train loss: 0.1380502558027877, test loss: 0.254228201915198\n",
      "epoch 6963: train loss: 0.13804442251293803, test loss: 0.2542268140123662\n",
      "epoch 6964: train loss: 0.13803859042696773, test loss: 0.2542254269075918\n",
      "epoch 6965: train loss: 0.13803275954445093, test loss: 0.2542240406005289\n",
      "epoch 6966: train loss: 0.13802692986496218, test loss: 0.25422265509084324\n",
      "epoch 6967: train loss: 0.13802110138807597, test loss: 0.25422127037819386\n",
      "epoch 6968: train loss: 0.13801527411336723, test loss: 0.254219886462251\n",
      "epoch 6969: train loss: 0.13800944804041093, test loss: 0.25421850334267077\n",
      "epoch 6970: train loss: 0.1380036231687824, test loss: 0.2542171210191111\n",
      "epoch 6971: train loss: 0.13799779949805702, test loss: 0.2542157394912409\n",
      "epoch 6972: train loss: 0.13799197702781052, test loss: 0.25421435875871695\n",
      "epoch 6973: train loss: 0.13798615575761874, test loss: 0.25421297882121197\n",
      "epoch 6974: train loss: 0.1379803356870578, test loss: 0.2542115996783763\n",
      "epoch 6975: train loss: 0.137974516815704, test loss: 0.25421022132988386\n",
      "epoch 6976: train loss: 0.13796869914313387, test loss: 0.25420884377538916\n",
      "epoch 6977: train loss: 0.13796288266892404, test loss: 0.2542074670145651\n",
      "epoch 6978: train loss: 0.1379570673926515, test loss: 0.2542060910470672\n",
      "epoch 6979: train loss: 0.1379512533138934, test loss: 0.25420471587256527\n",
      "epoch 6980: train loss: 0.13794544043222706, test loss: 0.2542033414907182\n",
      "epoch 6981: train loss: 0.13793962874723004, test loss: 0.254201967901197\n",
      "epoch 6982: train loss: 0.13793381825848006, test loss: 0.25420059510365944\n",
      "epoch 6983: train loss: 0.13792800896555513, test loss: 0.25419922309777365\n",
      "epoch 6984: train loss: 0.1379222008680334, test loss: 0.2541978518832055\n",
      "epoch 6985: train loss: 0.13791639396549324, test loss: 0.2541964814596189\n",
      "epoch 6986: train loss: 0.13791058825751332, test loss: 0.2541951118266783\n",
      "epoch 6987: train loss: 0.13790478374367232, test loss: 0.2541937429840529\n",
      "epoch 6988: train loss: 0.13789898042354934, test loss: 0.25419237493140423\n",
      "epoch 6989: train loss: 0.13789317829672354, test loss: 0.25419100766839653\n",
      "epoch 6990: train loss: 0.13788737736277437, test loss: 0.2541896411947011\n",
      "epoch 6991: train loss: 0.13788157762128145, test loss: 0.2541882755099806\n",
      "epoch 6992: train loss: 0.13787577907182455, test loss: 0.25418691061391024\n",
      "epoch 6993: train loss: 0.1378699817139838, test loss: 0.25418554650614256\n",
      "epoch 6994: train loss: 0.1378641855473394, test loss: 0.2541841831863529\n",
      "epoch 6995: train loss: 0.1378583905714718, test loss: 0.2541828206542114\n",
      "epoch 6996: train loss: 0.1378525967859617, test loss: 0.2541814589093748\n",
      "epoch 6997: train loss: 0.13784680419038994, test loss: 0.25418009795151475\n",
      "epoch 6998: train loss: 0.13784101278433755, test loss: 0.25417873778030675\n",
      "epoch 6999: train loss: 0.13783522256738587, test loss: 0.25417737839541515\n",
      "epoch 7000: train loss: 0.13782943353911634, test loss: 0.2541760197964998\n",
      "epoch 7001: train loss: 0.13782364569911065, test loss: 0.25417466198323924\n",
      "epoch 7002: train loss: 0.1378178590469507, test loss: 0.254173304955296\n",
      "epoch 7003: train loss: 0.13781207358221864, test loss: 0.2541719487123386\n",
      "epoch 7004: train loss: 0.1378062893044967, test loss: 0.25417059325403624\n",
      "epoch 7005: train loss: 0.13780050621336742, test loss: 0.2541692385800623\n",
      "epoch 7006: train loss: 0.1377947243084135, test loss: 0.2541678846900836\n",
      "epoch 7007: train loss: 0.13778894358921787, test loss: 0.25416653158376973\n",
      "epoch 7008: train loss: 0.13778316405536367, test loss: 0.2541651792607847\n",
      "epoch 7009: train loss: 0.13777738570643422, test loss: 0.2541638277208068\n",
      "epoch 7010: train loss: 0.137771608542013, test loss: 0.2541624769635015\n",
      "epoch 7011: train loss: 0.13776583256168384, test loss: 0.2541611269885409\n",
      "epoch 7012: train loss: 0.1377600577650306, test loss: 0.2541597777955905\n",
      "epoch 7013: train loss: 0.1377542841516375, test loss: 0.254158429384332\n",
      "epoch 7014: train loss: 0.13774851172108882, test loss: 0.2541570817544189\n",
      "epoch 7015: train loss: 0.13774274047296914, test loss: 0.25415573490553794\n",
      "epoch 7016: train loss: 0.13773697040686322, test loss: 0.25415438883735314\n",
      "epoch 7017: train loss: 0.13773120152235602, test loss: 0.2541530435495361\n",
      "epoch 7018: train loss: 0.1377254338190327, test loss: 0.2541516990417596\n",
      "epoch 7019: train loss: 0.13771966729647864, test loss: 0.2541503553136986\n",
      "epoch 7020: train loss: 0.13771390195427938, test loss: 0.2541490123650199\n",
      "epoch 7021: train loss: 0.13770813779202074, test loss: 0.254147670195393\n",
      "epoch 7022: train loss: 0.1377023748092886, test loss: 0.25414632880449795\n",
      "epoch 7023: train loss: 0.13769661300566927, test loss: 0.25414498819200054\n",
      "epoch 7024: train loss: 0.13769085238074905, test loss: 0.25414364835758235\n",
      "epoch 7025: train loss: 0.13768509293411454, test loss: 0.25414230930090215\n",
      "epoch 7026: train loss: 0.13767933466535254, test loss: 0.2541409710216492\n",
      "epoch 7027: train loss: 0.13767357757405002, test loss: 0.25413963351948105\n",
      "epoch 7028: train loss: 0.1376678216597942, test loss: 0.25413829679408456\n",
      "epoch 7029: train loss: 0.13766206692217242, test loss: 0.2541369608451251\n",
      "epoch 7030: train loss: 0.1376563133607723, test loss: 0.2541356256722797\n",
      "epoch 7031: train loss: 0.13765056097518166, test loss: 0.25413429127521936\n",
      "epoch 7032: train loss: 0.1376448097649885, test loss: 0.25413295765362226\n",
      "epoch 7033: train loss: 0.13763905972978097, test loss: 0.2541316248071571\n",
      "epoch 7034: train loss: 0.1376333108691475, test loss: 0.2541302927355057\n",
      "epoch 7035: train loss: 0.1376275631826767, test loss: 0.2541289614383405\n",
      "epoch 7036: train loss: 0.13762181666995738, test loss: 0.25412763091532975\n",
      "epoch 7037: train loss: 0.13761607133057852, test loss: 0.2541263011661601\n",
      "epoch 7038: train loss: 0.13761032716412935, test loss: 0.2541249721905016\n",
      "epoch 7039: train loss: 0.13760458417019925, test loss: 0.2541236439880207\n",
      "epoch 7040: train loss: 0.13759884234837785, test loss: 0.2541223165584086\n",
      "epoch 7041: train loss: 0.13759310169825492, test loss: 0.2541209899013282\n",
      "epoch 7042: train loss: 0.13758736221942053, test loss: 0.25411966401646324\n",
      "epoch 7043: train loss: 0.1375816239114648, test loss: 0.2541183389034911\n",
      "epoch 7044: train loss: 0.13757588677397828, test loss: 0.25411701456208197\n",
      "epoch 7045: train loss: 0.1375701508065514, test loss: 0.2541156909919138\n",
      "epoch 7046: train loss: 0.13756441600877511, test loss: 0.2541143681926687\n",
      "epoch 7047: train loss: 0.13755868238024033, test loss: 0.25411304616402125\n",
      "epoch 7048: train loss: 0.13755294992053832, test loss: 0.25411172490564404\n",
      "epoch 7049: train loss: 0.13754721862926045, test loss: 0.2541104044172187\n",
      "epoch 7050: train loss: 0.13754148850599834, test loss: 0.25410908469841703\n",
      "epoch 7051: train loss: 0.1375357595503438, test loss: 0.25410776574893057\n",
      "epoch 7052: train loss: 0.13753003176188888, test loss: 0.254106447568418\n",
      "epoch 7053: train loss: 0.13752430514022568, test loss: 0.2541051301565763\n",
      "epoch 7054: train loss: 0.13751857968494668, test loss: 0.2541038135130727\n",
      "epoch 7055: train loss: 0.13751285539564445, test loss: 0.2541024976375849\n",
      "epoch 7056: train loss: 0.1375071322719118, test loss: 0.2541011825297991\n",
      "epoch 7057: train loss: 0.13750141031334173, test loss: 0.25409986818938796\n",
      "epoch 7058: train loss: 0.13749568951952743, test loss: 0.254098554616025\n",
      "epoch 7059: train loss: 0.1374899698900623, test loss: 0.25409724180940413\n",
      "epoch 7060: train loss: 0.13748425142453993, test loss: 0.25409592976920015\n",
      "epoch 7061: train loss: 0.1374785341225541, test loss: 0.2540946184950827\n",
      "epoch 7062: train loss: 0.13747281798369881, test loss: 0.25409330798674357\n",
      "epoch 7063: train loss: 0.13746710300756826, test loss: 0.25409199824385664\n",
      "epoch 7064: train loss: 0.1374613891937568, test loss: 0.2540906892661014\n",
      "epoch 7065: train loss: 0.13745567654185906, test loss: 0.2540893810531613\n",
      "epoch 7066: train loss: 0.13744996505146975, test loss: 0.25408807360471625\n",
      "epoch 7067: train loss: 0.13744425472218388, test loss: 0.25408676692044535\n",
      "epoch 7068: train loss: 0.1374385455535967, test loss: 0.25408546100002843\n",
      "epoch 7069: train loss: 0.13743283754530344, test loss: 0.25408415584314986\n",
      "epoch 7070: train loss: 0.13742713069689974, test loss: 0.254082851449489\n",
      "epoch 7071: train loss: 0.13742142500798138, test loss: 0.254081547818734\n",
      "epoch 7072: train loss: 0.13741572047814432, test loss: 0.254080244950553\n",
      "epoch 7073: train loss: 0.13741001710698467, test loss: 0.254078942844635\n",
      "epoch 7074: train loss: 0.1374043148940988, test loss: 0.25407764150066514\n",
      "epoch 7075: train loss: 0.13739861383908328, test loss: 0.25407634091831804\n",
      "epoch 7076: train loss: 0.13739291394153486, test loss: 0.2540750410972851\n",
      "epoch 7077: train loss: 0.13738721520105046, test loss: 0.25407374203724153\n",
      "epoch 7078: train loss: 0.1373815176172272, test loss: 0.25407244373787335\n",
      "epoch 7079: train loss: 0.1373758211896625, test loss: 0.25407114619886373\n",
      "epoch 7080: train loss: 0.13737012591795375, test loss: 0.2540698494198923\n",
      "epoch 7081: train loss: 0.13736443180169877, test loss: 0.25406855340063983\n",
      "epoch 7082: train loss: 0.1373587388404955, test loss: 0.254067258140805\n",
      "epoch 7083: train loss: 0.137353047033942, test loss: 0.2540659636400527\n",
      "epoch 7084: train loss: 0.1373473563816366, test loss: 0.254064669898079\n",
      "epoch 7085: train loss: 0.13734166688317778, test loss: 0.2540633769145611\n",
      "epoch 7086: train loss: 0.13733597853816426, test loss: 0.25406208468919067\n",
      "epoch 7087: train loss: 0.13733029134619495, test loss: 0.25406079322164415\n",
      "epoch 7088: train loss: 0.13732460530686896, test loss: 0.2540595025116107\n",
      "epoch 7089: train loss: 0.1373189204197855, test loss: 0.25405821255877326\n",
      "epoch 7090: train loss: 0.13731323668454412, test loss: 0.2540569233628167\n",
      "epoch 7091: train loss: 0.13730755410074444, test loss: 0.25405563492342664\n",
      "epoch 7092: train loss: 0.13730187266798635, test loss: 0.2540543472402858\n",
      "epoch 7093: train loss: 0.13729619238586996, test loss: 0.2540530603130852\n",
      "epoch 7094: train loss: 0.1372905132539954, test loss: 0.2540517741415055\n",
      "epoch 7095: train loss: 0.13728483527196325, test loss: 0.2540504887252333\n",
      "epoch 7096: train loss: 0.1372791584393741, test loss: 0.2540492040639581\n",
      "epoch 7097: train loss: 0.13727348275582876, test loss: 0.25404792015736094\n",
      "epoch 7098: train loss: 0.13726780822092832, test loss: 0.2540466370051286\n",
      "epoch 7099: train loss: 0.13726213483427396, test loss: 0.25404535460695127\n",
      "epoch 7100: train loss: 0.1372564625954671, test loss: 0.25404407296251713\n",
      "epoch 7101: train loss: 0.13725079150410935, test loss: 0.25404279207150554\n",
      "epoch 7102: train loss: 0.1372451215598025, test loss: 0.25404151193361213\n",
      "epoch 7103: train loss: 0.1372394527621486, test loss: 0.25404023254851277\n",
      "epoch 7104: train loss: 0.1372337851107498, test loss: 0.25403895391590825\n",
      "epoch 7105: train loss: 0.13722811860520845, test loss: 0.25403767603547966\n",
      "epoch 7106: train loss: 0.1372224532451272, test loss: 0.2540363989069111\n",
      "epoch 7107: train loss: 0.1372167890301087, test loss: 0.25403512252989446\n",
      "epoch 7108: train loss: 0.13721112595975604, test loss: 0.2540338469041211\n",
      "epoch 7109: train loss: 0.1372054640336723, test loss: 0.254032572029271\n",
      "epoch 7110: train loss: 0.1371998032514608, test loss: 0.25403129790504403\n",
      "epoch 7111: train loss: 0.13719414361272514, test loss: 0.25403002453111756\n",
      "epoch 7112: train loss: 0.137188485117069, test loss: 0.25402875190718704\n",
      "epoch 7113: train loss: 0.1371828277640963, test loss: 0.25402748003293624\n",
      "epoch 7114: train loss: 0.13717717155341114, test loss: 0.25402620890805977\n",
      "epoch 7115: train loss: 0.13717151648461784, test loss: 0.25402493853224517\n",
      "epoch 7116: train loss: 0.13716586255732094, test loss: 0.2540236689051823\n",
      "epoch 7117: train loss: 0.13716020977112503, test loss: 0.25402240002656107\n",
      "epoch 7118: train loss: 0.137154558125635, test loss: 0.2540211318960653\n",
      "epoch 7119: train loss: 0.13714890762045598, test loss: 0.2540198645133976\n",
      "epoch 7120: train loss: 0.13714325825519316, test loss: 0.2540185978782399\n",
      "epoch 7121: train loss: 0.13713761002945202, test loss: 0.25401733199028176\n",
      "epoch 7122: train loss: 0.1371319629428382, test loss: 0.2540160668492107\n",
      "epoch 7123: train loss: 0.13712631699495748, test loss: 0.25401480245472874\n",
      "epoch 7124: train loss: 0.13712067218541596, test loss: 0.2540135388065202\n",
      "epoch 7125: train loss: 0.13711502851381976, test loss: 0.2540122759042767\n",
      "epoch 7126: train loss: 0.13710938597977534, test loss: 0.25401101374768653\n",
      "epoch 7127: train loss: 0.1371037445828893, test loss: 0.25400975233644557\n",
      "epoch 7128: train loss: 0.1370981043227683, test loss: 0.25400849167024697\n",
      "epoch 7129: train loss: 0.13709246519901946, test loss: 0.25400723174877593\n",
      "epoch 7130: train loss: 0.13708682721124987, test loss: 0.25400597257172797\n",
      "epoch 7131: train loss: 0.13708119035906685, test loss: 0.2540047141388011\n",
      "epoch 7132: train loss: 0.13707555464207796, test loss: 0.25400345644967376\n",
      "epoch 7133: train loss: 0.13706992005989094, test loss: 0.2540021995040523\n",
      "epoch 7134: train loss: 0.1370642866121137, test loss: 0.25400094330162754\n",
      "epoch 7135: train loss: 0.13705865429835432, test loss: 0.25399968784208193\n",
      "epoch 7136: train loss: 0.13705302311822112, test loss: 0.25399843312511367\n",
      "epoch 7137: train loss: 0.13704739307132258, test loss: 0.2539971791504255\n",
      "epoch 7138: train loss: 0.13704176415726735, test loss: 0.25399592591769826\n",
      "epoch 7139: train loss: 0.1370361363756643, test loss: 0.25399467342662824\n",
      "epoch 7140: train loss: 0.1370305097261225, test loss: 0.25399342167691485\n",
      "epoch 7141: train loss: 0.13702488420825112, test loss: 0.2539921706682506\n",
      "epoch 7142: train loss: 0.13701925982165963, test loss: 0.25399092040032584\n",
      "epoch 7143: train loss: 0.1370136365659577, test loss: 0.25398967087283514\n",
      "epoch 7144: train loss: 0.137008014440755, test loss: 0.2539884220854708\n",
      "epoch 7145: train loss: 0.13700239344566162, test loss: 0.2539871740379398\n",
      "epoch 7146: train loss: 0.13699677358028772, test loss: 0.2539859267299218\n",
      "epoch 7147: train loss: 0.13699115484424362, test loss: 0.2539846801611186\n",
      "epoch 7148: train loss: 0.13698553723713988, test loss: 0.2539834343312265\n",
      "epoch 7149: train loss: 0.13697992075858728, test loss: 0.25398218923993626\n",
      "epoch 7150: train loss: 0.13697430540819674, test loss: 0.25398094488695094\n",
      "epoch 7151: train loss: 0.1369686911855793, test loss: 0.25397970127195946\n",
      "epoch 7152: train loss: 0.13696307809034633, test loss: 0.2539784583946554\n",
      "epoch 7153: train loss: 0.13695746612210932, test loss: 0.25397721625474523\n",
      "epoch 7154: train loss: 0.13695185528047993, test loss: 0.25397597485191875\n",
      "epoch 7155: train loss: 0.13694624556506999, test loss: 0.2539747341858684\n",
      "epoch 7156: train loss: 0.13694063697549158, test loss: 0.2539734942563016\n",
      "epoch 7157: train loss: 0.13693502951135694, test loss: 0.25397225506289883\n",
      "epoch 7158: train loss: 0.13692942317227844, test loss: 0.2539710166053759\n",
      "epoch 7159: train loss: 0.13692381795786873, test loss: 0.25396977888341865\n",
      "epoch 7160: train loss: 0.1369182138677406, test loss: 0.25396854189672036\n",
      "epoch 7161: train loss: 0.13691261090150703, test loss: 0.2539673056449887\n",
      "epoch 7162: train loss: 0.13690700905878117, test loss: 0.2539660701279185\n",
      "epoch 7163: train loss: 0.13690140833917636, test loss: 0.2539648353452036\n",
      "epoch 7164: train loss: 0.13689580874230617, test loss: 0.2539636012965448\n",
      "epoch 7165: train loss: 0.13689021026778428, test loss: 0.253962367981642\n",
      "epoch 7166: train loss: 0.13688461291522463, test loss: 0.2539611354001877\n",
      "epoch 7167: train loss: 0.13687901668424132, test loss: 0.25395990355188525\n",
      "epoch 7168: train loss: 0.1368734215744486, test loss: 0.25395867243642933\n",
      "epoch 7169: train loss: 0.1368678275854609, test loss: 0.2539574420535242\n",
      "epoch 7170: train loss: 0.13686223471689296, test loss: 0.25395621240286453\n",
      "epoch 7171: train loss: 0.13685664296835953, test loss: 0.25395498348414774\n",
      "epoch 7172: train loss: 0.13685105233947567, test loss: 0.2539537552970822\n",
      "epoch 7173: train loss: 0.13684546282985657, test loss: 0.25395252784135686\n",
      "epoch 7174: train loss: 0.1368398744391176, test loss: 0.2539513011166785\n",
      "epoch 7175: train loss: 0.13683428716687435, test loss: 0.25395007512274026\n",
      "epoch 7176: train loss: 0.13682870101274258, test loss: 0.2539488498592506\n",
      "epoch 7177: train loss: 0.13682311597633823, test loss: 0.25394762532590265\n",
      "epoch 7178: train loss: 0.13681753205727737, test loss: 0.2539464015223939\n",
      "epoch 7179: train loss: 0.1368119492551764, test loss: 0.25394517844843373\n",
      "epoch 7180: train loss: 0.13680636756965175, test loss: 0.2539439561037197\n",
      "epoch 7181: train loss: 0.13680078700032008, test loss: 0.25394273448795196\n",
      "epoch 7182: train loss: 0.1367952075467983, test loss: 0.2539415136008341\n",
      "epoch 7183: train loss: 0.13678962920870344, test loss: 0.2539402934420598\n",
      "epoch 7184: train loss: 0.1367840519856527, test loss: 0.25393907401133914\n",
      "epoch 7185: train loss: 0.13677847587726347, test loss: 0.2539378553083697\n",
      "epoch 7186: train loss: 0.13677290088315341, test loss: 0.2539366373328536\n",
      "epoch 7187: train loss: 0.13676732700294025, test loss: 0.25393542008449144\n",
      "epoch 7188: train loss: 0.13676175423624196, test loss: 0.25393420356298757\n",
      "epoch 7189: train loss: 0.13675618258267666, test loss: 0.2539329877680387\n",
      "epoch 7190: train loss: 0.1367506120418627, test loss: 0.2539317726993529\n",
      "epoch 7191: train loss: 0.13674504261341858, test loss: 0.25393055835662887\n",
      "epoch 7192: train loss: 0.136739474296963, test loss: 0.25392934473957635\n",
      "epoch 7193: train loss: 0.1367339070921148, test loss: 0.25392813184788765\n",
      "epoch 7194: train loss: 0.13672834099849307, test loss: 0.25392691968127523\n",
      "epoch 7195: train loss: 0.136722776015717, test loss: 0.25392570823943456\n",
      "epoch 7196: train loss: 0.13671721214340604, test loss: 0.2539244975220769\n",
      "epoch 7197: train loss: 0.1367116493811798, test loss: 0.2539232875288998\n",
      "epoch 7198: train loss: 0.13670608772865803, test loss: 0.2539220782596089\n",
      "epoch 7199: train loss: 0.1367005271854607, test loss: 0.25392086971390726\n",
      "epoch 7200: train loss: 0.13669496775120799, test loss: 0.25391966189149917\n",
      "epoch 7201: train loss: 0.13668940942552016, test loss: 0.2539184547920926\n",
      "epoch 7202: train loss: 0.1366838522080178, test loss: 0.2539172484153841\n",
      "epoch 7203: train loss: 0.1366782960983215, test loss: 0.2539160427610871\n",
      "epoch 7204: train loss: 0.13667274109605224, test loss: 0.2539148378288987\n",
      "epoch 7205: train loss: 0.136667187200831, test loss: 0.2539136336185264\n",
      "epoch 7206: train loss: 0.13666163441227902, test loss: 0.25391243012968046\n",
      "epoch 7207: train loss: 0.13665608273001772, test loss: 0.25391122736205113\n",
      "epoch 7208: train loss: 0.1366505321536687, test loss: 0.2539100253153631\n",
      "epoch 7209: train loss: 0.13664498268285374, test loss: 0.2539088239893095\n",
      "epoch 7210: train loss: 0.13663943431719477, test loss: 0.25390762338359757\n",
      "epoch 7211: train loss: 0.13663388705631394, test loss: 0.25390642349793946\n",
      "epoch 7212: train loss: 0.13662834089983356, test loss: 0.2539052243320363\n",
      "epoch 7213: train loss: 0.13662279584737613, test loss: 0.25390402588559335\n",
      "epoch 7214: train loss: 0.13661725189856436, test loss: 0.2539028281583179\n",
      "epoch 7215: train loss: 0.13661170905302103, test loss: 0.2539016311499123\n",
      "epoch 7216: train loss: 0.13660616731036923, test loss: 0.2539004348600931\n",
      "epoch 7217: train loss: 0.13660062667023215, test loss: 0.2538992392885604\n",
      "epoch 7218: train loss: 0.1365950871322332, test loss: 0.25389804443502223\n",
      "epoch 7219: train loss: 0.13658954869599596, test loss: 0.2538968502991887\n",
      "epoch 7220: train loss: 0.13658401136114418, test loss: 0.2538956568807591\n",
      "epoch 7221: train loss: 0.13657847512730178, test loss: 0.2538944641794514\n",
      "epoch 7222: train loss: 0.1365729399940929, test loss: 0.2538932721949636\n",
      "epoch 7223: train loss: 0.1365674059611418, test loss: 0.25389208092700655\n",
      "epoch 7224: train loss: 0.13656187302807293, test loss: 0.2538908903752972\n",
      "epoch 7225: train loss: 0.136556341194511, test loss: 0.25388970053952975\n",
      "epoch 7226: train loss: 0.13655081046008077, test loss: 0.2538885114194276\n",
      "epoch 7227: train loss: 0.1365452808244073, test loss: 0.25388732301468375\n",
      "epoch 7228: train loss: 0.1365397522871158, test loss: 0.25388613532501486\n",
      "epoch 7229: train loss: 0.13653422484783156, test loss: 0.2538849483501287\n",
      "epoch 7230: train loss: 0.13652869850618016, test loss: 0.253883762089737\n",
      "epoch 7231: train loss: 0.1365231732617873, test loss: 0.2538825765435429\n",
      "epoch 7232: train loss: 0.13651764911427894, test loss: 0.25388139171126395\n",
      "epoch 7233: train loss: 0.13651212606328106, test loss: 0.25388020759259744\n",
      "epoch 7234: train loss: 0.13650660410841997, test loss: 0.25387902418726715\n",
      "epoch 7235: train loss: 0.13650108324932209, test loss: 0.25387784149497394\n",
      "epoch 7236: train loss: 0.136495563485614, test loss: 0.25387665951542865\n",
      "epoch 7237: train loss: 0.13649004481692256, test loss: 0.2538754782483451\n",
      "epoch 7238: train loss: 0.1364845272428747, test loss: 0.2538742976934296\n",
      "epoch 7239: train loss: 0.1364790107630975, test loss: 0.2538731178503899\n",
      "epoch 7240: train loss: 0.1364734953772184, test loss: 0.2538719387189472\n",
      "epoch 7241: train loss: 0.13646798108486477, test loss: 0.25387076029880573\n",
      "epoch 7242: train loss: 0.13646246788566438, test loss: 0.25386958258967396\n",
      "epoch 7243: train loss: 0.136456955779245, test loss: 0.2538684055912666\n",
      "epoch 7244: train loss: 0.13645144476523474, test loss: 0.25386722930329375\n",
      "epoch 7245: train loss: 0.13644593484326173, test loss: 0.2538660537254688\n",
      "epoch 7246: train loss: 0.13644042601295436, test loss: 0.2538648788574992\n",
      "epoch 7247: train loss: 0.13643491827394125, test loss: 0.2538637046990971\n",
      "epoch 7248: train loss: 0.13642941162585107, test loss: 0.2538625312499802\n",
      "epoch 7249: train loss: 0.13642390606831273, test loss: 0.25386135850986113\n",
      "epoch 7250: train loss: 0.13641840160095534, test loss: 0.2538601864784371\n",
      "epoch 7251: train loss: 0.13641289822340816, test loss: 0.25385901515543735\n",
      "epoch 7252: train loss: 0.1364073959353006, test loss: 0.2538578445405696\n",
      "epoch 7253: train loss: 0.1364018947362623, test loss: 0.25385667463354705\n",
      "epoch 7254: train loss: 0.13639639462592304, test loss: 0.25385550543407376\n",
      "epoch 7255: train loss: 0.1363908956039128, test loss: 0.2538543369418739\n",
      "epoch 7256: train loss: 0.13638539766986166, test loss: 0.2538531691566592\n",
      "epoch 7257: train loss: 0.1363799008234, test loss: 0.2538520020781354\n",
      "epoch 7258: train loss: 0.1363744050641583, test loss: 0.25385083570602446\n",
      "epoch 7259: train loss: 0.1363689103917672, test loss: 0.2538496700400359\n",
      "epoch 7260: train loss: 0.13636341680585753, test loss: 0.2538485050798848\n",
      "epoch 7261: train loss: 0.13635792430606034, test loss: 0.2538473408252869\n",
      "epoch 7262: train loss: 0.1363524328920068, test loss: 0.25384617727595116\n",
      "epoch 7263: train loss: 0.13634694256332827, test loss: 0.2538450144315958\n",
      "epoch 7264: train loss: 0.13634145331965633, test loss: 0.2538438522919315\n",
      "epoch 7265: train loss: 0.13633596516062266, test loss: 0.2538426908566817\n",
      "epoch 7266: train loss: 0.13633047808585913, test loss: 0.253841530125553\n",
      "epoch 7267: train loss: 0.13632499209499785, test loss: 0.2538403700982614\n",
      "epoch 7268: train loss: 0.13631950718767105, test loss: 0.25383921077452204\n",
      "epoch 7269: train loss: 0.13631402336351106, test loss: 0.25383805215405936\n",
      "epoch 7270: train loss: 0.1363085406221506, test loss: 0.2538368942365717\n",
      "epoch 7271: train loss: 0.13630305896322226, test loss: 0.2538357370217874\n",
      "epoch 7272: train loss: 0.13629757838635914, test loss: 0.25383458050942254\n",
      "epoch 7273: train loss: 0.13629209889119426, test loss: 0.253833424699188\n",
      "epoch 7274: train loss: 0.13628662047736093, test loss: 0.25383226959080085\n",
      "epoch 7275: train loss: 0.13628114314449255, test loss: 0.2538311151839756\n",
      "epoch 7276: train loss: 0.1362756668922228, test loss: 0.2538299614784286\n",
      "epoch 7277: train loss: 0.1362701917201855, test loss: 0.2538288084738813\n",
      "epoch 7278: train loss: 0.13626471762801454, test loss: 0.2538276561700508\n",
      "epoch 7279: train loss: 0.13625924461534414, test loss: 0.2538265045666493\n",
      "epoch 7280: train loss: 0.1362537726818086, test loss: 0.25382535366338976\n",
      "epoch 7281: train loss: 0.1362483018270424, test loss: 0.2538242034599996\n",
      "epoch 7282: train loss: 0.1362428320506802, test loss: 0.2538230539561958\n",
      "epoch 7283: train loss: 0.13623736335235684, test loss: 0.25382190515168435\n",
      "epoch 7284: train loss: 0.1362318957317073, test loss: 0.2538207570461951\n",
      "epoch 7285: train loss: 0.13622642918836686, test loss: 0.25381960963943945\n",
      "epoch 7286: train loss: 0.13622096372197082, test loss: 0.25381846293113325\n",
      "epoch 7287: train loss: 0.13621549933215465, test loss: 0.2538173169210047\n",
      "epoch 7288: train loss: 0.1362100360185541, test loss: 0.2538161716087661\n",
      "epoch 7289: train loss: 0.13620457378080508, test loss: 0.25381502699413083\n",
      "epoch 7290: train loss: 0.1361991126185436, test loss: 0.2538138830768228\n",
      "epoch 7291: train loss: 0.13619365253140583, test loss: 0.2538127398565598\n",
      "epoch 7292: train loss: 0.1361881935190282, test loss: 0.2538115973330626\n",
      "epoch 7293: train loss: 0.13618273558104727, test loss: 0.2538104555060514\n",
      "epoch 7294: train loss: 0.13617727871709973, test loss: 0.25380931437524057\n",
      "epoch 7295: train loss: 0.13617182292682253, test loss: 0.2538081739403524\n",
      "epoch 7296: train loss: 0.13616636820985273, test loss: 0.2538070342011018\n",
      "epoch 7297: train loss: 0.13616091456582755, test loss: 0.2538058951572151\n",
      "epoch 7298: train loss: 0.13615546199438444, test loss: 0.2538047568084145\n",
      "epoch 7299: train loss: 0.13615001049516093, test loss: 0.25380361915440885\n",
      "epoch 7300: train loss: 0.13614456006779485, test loss: 0.2538024821949263\n",
      "epoch 7301: train loss: 0.13613911071192406, test loss: 0.2538013459296861\n",
      "epoch 7302: train loss: 0.1361336624271867, test loss: 0.2538002103584084\n",
      "epoch 7303: train loss: 0.136128215213221, test loss: 0.2537990754808166\n",
      "epoch 7304: train loss: 0.13612276906966542, test loss: 0.2537979412966231\n",
      "epoch 7305: train loss: 0.13611732399615858, test loss: 0.2537968078055543\n",
      "epoch 7306: train loss: 0.13611187999233926, test loss: 0.25379567500733613\n",
      "epoch 7307: train loss: 0.13610643705784636, test loss: 0.25379454290167647\n",
      "epoch 7308: train loss: 0.13610099519231908, test loss: 0.25379341148831314\n",
      "epoch 7309: train loss: 0.13609555439539667, test loss: 0.25379228076695654\n",
      "epoch 7310: train loss: 0.13609011466671853, test loss: 0.25379115073733494\n",
      "epoch 7311: train loss: 0.13608467600592436, test loss: 0.25379002139916595\n",
      "epoch 7312: train loss: 0.13607923841265396, test loss: 0.2537888927521704\n",
      "epoch 7313: train loss: 0.13607380188654725, test loss: 0.25378776479607285\n",
      "epoch 7314: train loss: 0.13606836642724443, test loss: 0.2537866375305993\n",
      "epoch 7315: train loss: 0.13606293203438571, test loss: 0.25378551095546414\n",
      "epoch 7316: train loss: 0.13605749870761166, test loss: 0.25378438507039836\n",
      "epoch 7317: train loss: 0.13605206644656292, test loss: 0.25378325987511813\n",
      "epoch 7318: train loss: 0.13604663525088026, test loss: 0.2537821353693454\n",
      "epoch 7319: train loss: 0.13604120512020465, test loss: 0.25378101155281363\n",
      "epoch 7320: train loss: 0.13603577605417727, test loss: 0.25377988842523697\n",
      "epoch 7321: train loss: 0.13603034805243944, test loss: 0.25377876598634136\n",
      "epoch 7322: train loss: 0.13602492111463266, test loss: 0.2537776442358497\n",
      "epoch 7323: train loss: 0.13601949524039855, test loss: 0.2537765231734823\n",
      "epoch 7324: train loss: 0.136014070429379, test loss: 0.25377540279897665\n",
      "epoch 7325: train loss: 0.13600864668121596, test loss: 0.25377428311204187\n",
      "epoch 7326: train loss: 0.13600322399555156, test loss: 0.25377316411240536\n",
      "epoch 7327: train loss: 0.13599780237202816, test loss: 0.2537720457997961\n",
      "epoch 7328: train loss: 0.13599238181028828, test loss: 0.25377092817393915\n",
      "epoch 7329: train loss: 0.13598696230997456, test loss: 0.253769811234552\n",
      "epoch 7330: train loss: 0.13598154387072986, test loss: 0.2537686949813672\n",
      "epoch 7331: train loss: 0.13597612649219715, test loss: 0.2537675794141011\n",
      "epoch 7332: train loss: 0.13597071017401963, test loss: 0.2537664645324849\n",
      "epoch 7333: train loss: 0.13596529491584058, test loss: 0.25376535033624353\n",
      "epoch 7334: train loss: 0.1359598807173036, test loss: 0.2537642368251018\n",
      "epoch 7335: train loss: 0.13595446757805227, test loss: 0.2537631239987894\n",
      "epoch 7336: train loss: 0.13594905549773045, test loss: 0.2537620118570235\n",
      "epoch 7337: train loss: 0.13594364447598217, test loss: 0.2537609003995389\n",
      "epoch 7338: train loss: 0.13593823451245157, test loss: 0.25375978962605256\n",
      "epoch 7339: train loss: 0.13593282560678305, test loss: 0.25375867953629533\n",
      "epoch 7340: train loss: 0.13592741775862105, test loss: 0.25375757012999256\n",
      "epoch 7341: train loss: 0.13592201096761028, test loss: 0.2537564614068726\n",
      "epoch 7342: train loss: 0.13591660523339552, test loss: 0.2537553533666663\n",
      "epoch 7343: train loss: 0.13591120055562186, test loss: 0.25375424600908697\n",
      "epoch 7344: train loss: 0.13590579693393445, test loss: 0.2537531393338732\n",
      "epoch 7345: train loss: 0.1359003943679786, test loss: 0.25375203334074753\n",
      "epoch 7346: train loss: 0.13589499285739978, test loss: 0.2537509280294383\n",
      "epoch 7347: train loss: 0.13588959240184376, test loss: 0.25374982339967483\n",
      "epoch 7348: train loss: 0.1358841930009563, test loss: 0.2537487194511822\n",
      "epoch 7349: train loss: 0.1358787946543834, test loss: 0.25374761618369024\n",
      "epoch 7350: train loss: 0.13587339736177131, test loss: 0.2537465135969231\n",
      "epoch 7351: train loss: 0.13586800112276629, test loss: 0.2537454116906089\n",
      "epoch 7352: train loss: 0.13586260593701482, test loss: 0.2537443104644842\n",
      "epoch 7353: train loss: 0.13585721180416363, test loss: 0.2537432099182629\n",
      "epoch 7354: train loss: 0.1358518187238595, test loss: 0.25374211005168645\n",
      "epoch 7355: train loss: 0.1358464266957495, test loss: 0.2537410108644764\n",
      "epoch 7356: train loss: 0.1358410357194807, test loss: 0.2537399123563636\n",
      "epoch 7357: train loss: 0.13583564579470045, test loss: 0.25373881452707375\n",
      "epoch 7358: train loss: 0.1358302569210563, test loss: 0.2537377173763445\n",
      "epoch 7359: train loss: 0.13582486909819583, test loss: 0.2537366209038997\n",
      "epoch 7360: train loss: 0.13581948232576685, test loss: 0.25373552510946623\n",
      "epoch 7361: train loss: 0.13581409660341742, test loss: 0.25373442999277357\n",
      "epoch 7362: train loss: 0.13580871193079566, test loss: 0.25373333555355776\n",
      "epoch 7363: train loss: 0.13580332830754988, test loss: 0.2537322417915428\n",
      "epoch 7364: train loss: 0.13579794573332854, test loss: 0.25373114870645674\n",
      "epoch 7365: train loss: 0.13579256420778033, test loss: 0.2537300562980406\n",
      "epoch 7366: train loss: 0.135787183730554, test loss: 0.25372896456601174\n",
      "epoch 7367: train loss: 0.13578180430129855, test loss: 0.25372787351010917\n",
      "epoch 7368: train loss: 0.1357764259196631, test loss: 0.2537267831300595\n",
      "epoch 7369: train loss: 0.13577104858529698, test loss: 0.25372569342559503\n",
      "epoch 7370: train loss: 0.13576567229784964, test loss: 0.25372460439643796\n",
      "epoch 7371: train loss: 0.13576029705697068, test loss: 0.25372351604233934\n",
      "epoch 7372: train loss: 0.1357549228623099, test loss: 0.2537224283630129\n",
      "epoch 7373: train loss: 0.13574954971351727, test loss: 0.25372134135819197\n",
      "epoch 7374: train loss: 0.1357441776102429, test loss: 0.2537202550276097\n",
      "epoch 7375: train loss: 0.13573880655213708, test loss: 0.2537191693710054\n",
      "epoch 7376: train loss: 0.13573343653885023, test loss: 0.25371808438810484\n",
      "epoch 7377: train loss: 0.13572806757003295, test loss: 0.253717000078635\n",
      "epoch 7378: train loss: 0.13572269964533606, test loss: 0.2537159164423349\n",
      "epoch 7379: train loss: 0.13571733276441045, test loss: 0.2537148334789343\n",
      "epoch 7380: train loss: 0.13571196692690723, test loss: 0.25371375118816253\n",
      "epoch 7381: train loss: 0.13570660213247765, test loss: 0.2537126695697552\n",
      "epoch 7382: train loss: 0.13570123838077314, test loss: 0.2537115886234462\n",
      "epoch 7383: train loss: 0.13569587567144528, test loss: 0.25371050834897346\n",
      "epoch 7384: train loss: 0.13569051400414578, test loss: 0.2537094287460516\n",
      "epoch 7385: train loss: 0.1356851533785266, test loss: 0.2537083498144346\n",
      "epoch 7386: train loss: 0.13567979379423978, test loss: 0.2537072715538433\n",
      "epoch 7387: train loss: 0.13567443525093753, test loss: 0.25370619396401045\n",
      "epoch 7388: train loss: 0.13566907774827233, test loss: 0.2537051170446785\n",
      "epoch 7389: train loss: 0.13566372128589665, test loss: 0.253704040795573\n",
      "epoch 7390: train loss: 0.13565836586346325, test loss: 0.2537029652164312\n",
      "epoch 7391: train loss: 0.135653011480625, test loss: 0.2537018903069875\n",
      "epoch 7392: train loss: 0.13564765813703494, test loss: 0.2537008160669687\n",
      "epoch 7393: train loss: 0.13564230583234627, test loss: 0.2536997424961253\n",
      "epoch 7394: train loss: 0.13563695456621233, test loss: 0.25369866959417464\n",
      "epoch 7395: train loss: 0.1356316043382867, test loss: 0.25369759736086084\n",
      "epoch 7396: train loss: 0.13562625514822302, test loss: 0.2536965257959099\n",
      "epoch 7397: train loss: 0.13562090699567517, test loss: 0.253695454899073\n",
      "epoch 7398: train loss: 0.13561555988029714, test loss: 0.253694384670068\n",
      "epoch 7399: train loss: 0.1356102138017431, test loss: 0.25369331510863774\n",
      "epoch 7400: train loss: 0.13560486875966737, test loss: 0.2536922462145145\n",
      "epoch 7401: train loss: 0.13559952475372447, test loss: 0.2536911779874355\n",
      "epoch 7402: train loss: 0.13559418178356905, test loss: 0.2536901104271421\n",
      "epoch 7403: train loss: 0.13558883984885592, test loss: 0.2536890435333564\n",
      "epoch 7404: train loss: 0.13558349894924002, test loss: 0.25368797730582676\n",
      "epoch 7405: train loss: 0.1355781590843765, test loss: 0.2536869117442873\n",
      "epoch 7406: train loss: 0.13557282025392067, test loss: 0.2536858468484673\n",
      "epoch 7407: train loss: 0.135567482457528, test loss: 0.25368478261810784\n",
      "epoch 7408: train loss: 0.13556214569485406, test loss: 0.2536837190529438\n",
      "epoch 7409: train loss: 0.13555680996555466, test loss: 0.25368265615271146\n",
      "epoch 7410: train loss: 0.1355514752692857, test loss: 0.2536815939171547\n",
      "epoch 7411: train loss: 0.1355461416057033, test loss: 0.2536805323459936\n",
      "epoch 7412: train loss: 0.1355408089744637, test loss: 0.25367947143898156\n",
      "epoch 7413: train loss: 0.13553547737522334, test loss: 0.2536784111958558\n",
      "epoch 7414: train loss: 0.13553014680763875, test loss: 0.25367735161633775\n",
      "epoch 7415: train loss: 0.1355248172713667, test loss: 0.25367629270017966\n",
      "epoch 7416: train loss: 0.13551948876606404, test loss: 0.25367523444711315\n",
      "epoch 7417: train loss: 0.13551416129138785, test loss: 0.2536741768568773\n",
      "epoch 7418: train loss: 0.13550883484699536, test loss: 0.2536731199292098\n",
      "epoch 7419: train loss: 0.1355035094325439, test loss: 0.2536720636638496\n",
      "epoch 7420: train loss: 0.13549818504769104, test loss: 0.2536710080605329\n",
      "epoch 7421: train loss: 0.13549286169209443, test loss: 0.2536699531189959\n",
      "epoch 7422: train loss: 0.1354875393654119, test loss: 0.25366889883898336\n",
      "epoch 7423: train loss: 0.13548221806730154, test loss: 0.2536678452202288\n",
      "epoch 7424: train loss: 0.1354768977974214, test loss: 0.25366679226247013\n",
      "epoch 7425: train loss: 0.13547157855542985, test loss: 0.2536657399654543\n",
      "epoch 7426: train loss: 0.1354662603409854, test loss: 0.2536646883289084\n",
      "epoch 7427: train loss: 0.13546094315374665, test loss: 0.2536636373525806\n",
      "epoch 7428: train loss: 0.13545562699337244, test loss: 0.253662587036211\n",
      "epoch 7429: train loss: 0.13545031185952167, test loss: 0.25366153737953\n",
      "epoch 7430: train loss: 0.13544499775185345, test loss: 0.2536604883822836\n",
      "epoch 7431: train loss: 0.13543968467002712, test loss: 0.25365944004421204\n",
      "epoch 7432: train loss: 0.13543437261370206, test loss: 0.25365839236505155\n",
      "epoch 7433: train loss: 0.1354290615825378, test loss: 0.2536573453445491\n",
      "epoch 7434: train loss: 0.1354237515761942, test loss: 0.25365629898243297\n",
      "epoch 7435: train loss: 0.13541844259433108, test loss: 0.25365525327845845\n",
      "epoch 7436: train loss: 0.13541313463660856, test loss: 0.2536542082323499\n",
      "epoch 7437: train loss: 0.13540782770268678, test loss: 0.25365316384385944\n",
      "epoch 7438: train loss: 0.13540252179222617, test loss: 0.2536521201127259\n",
      "epoch 7439: train loss: 0.13539721690488724, test loss: 0.2536510770386855\n",
      "epoch 7440: train loss: 0.13539191304033069, test loss: 0.2536500346214826\n",
      "epoch 7441: train loss: 0.13538661019821732, test loss: 0.25364899286085946\n",
      "epoch 7442: train loss: 0.1353813083782082, test loss: 0.2536479517565566\n",
      "epoch 7443: train loss: 0.13537600757996443, test loss: 0.2536469113083111\n",
      "epoch 7444: train loss: 0.13537070780314736, test loss: 0.2536458715158732\n",
      "epoch 7445: train loss: 0.13536540904741842, test loss: 0.25364483237897173\n",
      "epoch 7446: train loss: 0.1353601113124393, test loss: 0.2536437938973595\n",
      "epoch 7447: train loss: 0.13535481459787174, test loss: 0.25364275607078063\n",
      "epoch 7448: train loss: 0.13534951890337768, test loss: 0.2536417188989659\n",
      "epoch 7449: train loss: 0.13534422422861925, test loss: 0.253640682381662\n",
      "epoch 7450: train loss: 0.13533893057325866, test loss: 0.2536396465186194\n",
      "epoch 7451: train loss: 0.13533363793695835, test loss: 0.25363861130956644\n",
      "epoch 7452: train loss: 0.13532834631938087, test loss: 0.2536375767542619\n",
      "epoch 7453: train loss: 0.13532305572018896, test loss: 0.25363654285243215\n",
      "epoch 7454: train loss: 0.13531776613904545, test loss: 0.253635509603833\n",
      "epoch 7455: train loss: 0.13531247757561343, test loss: 0.2536344770081998\n",
      "epoch 7456: train loss: 0.13530719002955607, test loss: 0.2536334450652783\n",
      "epoch 7457: train loss: 0.1353019035005367, test loss: 0.2536324137748141\n",
      "epoch 7458: train loss: 0.1352966179882188, test loss: 0.2536313831365509\n",
      "epoch 7459: train loss: 0.13529133349226607, test loss: 0.25363035315022986\n",
      "epoch 7460: train loss: 0.13528605001234228, test loss: 0.2536293238155943\n",
      "epoch 7461: train loss: 0.13528076754811144, test loss: 0.2536282951323864\n",
      "epoch 7462: train loss: 0.13527548609923762, test loss: 0.25362726710035544\n",
      "epoch 7463: train loss: 0.1352702056653851, test loss: 0.2536262397192457\n",
      "epoch 7464: train loss: 0.1352649262462184, test loss: 0.25362521298879337\n",
      "epoch 7465: train loss: 0.13525964784140196, test loss: 0.25362418690874944\n",
      "epoch 7466: train loss: 0.1352543704506006, test loss: 0.25362316147886077\n",
      "epoch 7467: train loss: 0.1352490940734792, test loss: 0.25362213669886585\n",
      "epoch 7468: train loss: 0.1352438187097028, test loss: 0.25362111256851433\n",
      "epoch 7469: train loss: 0.13523854435893662, test loss: 0.2536200890875464\n",
      "epoch 7470: train loss: 0.13523327102084604, test loss: 0.2536190662557161\n",
      "epoch 7471: train loss: 0.1352279986950965, test loss: 0.25361804407276217\n",
      "epoch 7472: train loss: 0.1352227273813537, test loss: 0.25361702253842944\n",
      "epoch 7473: train loss: 0.1352174570792835, test loss: 0.25361600165246473\n",
      "epoch 7474: train loss: 0.1352121877885518, test loss: 0.25361498141461536\n",
      "epoch 7475: train loss: 0.13520691950882477, test loss: 0.2536139618246325\n",
      "epoch 7476: train loss: 0.13520165223976868, test loss: 0.2536129428822464\n",
      "epoch 7477: train loss: 0.13519638598104997, test loss: 0.25361192458721343\n",
      "epoch 7478: train loss: 0.1351911207323352, test loss: 0.25361090693928035\n",
      "epoch 7479: train loss: 0.13518585649329115, test loss: 0.2536098899381948\n",
      "epoch 7480: train loss: 0.13518059326358473, test loss: 0.25360887358369877\n",
      "epoch 7481: train loss: 0.1351753310428829, test loss: 0.2536078578755432\n",
      "epoch 7482: train loss: 0.13517006983085295, test loss: 0.2536068428134679\n",
      "epoch 7483: train loss: 0.13516480962716218, test loss: 0.2536058283972313\n",
      "epoch 7484: train loss: 0.13515955043147812, test loss: 0.2536048146265692\n",
      "epoch 7485: train loss: 0.13515429224346845, test loss: 0.2536038015012376\n",
      "epoch 7486: train loss: 0.13514903506280096, test loss: 0.2536027890209778\n",
      "epoch 7487: train loss: 0.1351437788891436, test loss: 0.2536017771855358\n",
      "epoch 7488: train loss: 0.1351385237221645, test loss: 0.2536007659946717\n",
      "epoch 7489: train loss: 0.13513326956153196, test loss: 0.2535997554481125\n",
      "epoch 7490: train loss: 0.13512801640691435, test loss: 0.25359874554563094\n",
      "epoch 7491: train loss: 0.1351227642579803, test loss: 0.2535977362869533\n",
      "epoch 7492: train loss: 0.1351175131143985, test loss: 0.2535967276718411\n",
      "epoch 7493: train loss: 0.13511226297583787, test loss: 0.25359571970003947\n",
      "epoch 7494: train loss: 0.13510701384196735, test loss: 0.25359471237129\n",
      "epoch 7495: train loss: 0.13510176571245625, test loss: 0.25359370568535733\n",
      "epoch 7496: train loss: 0.1350965185869738, test loss: 0.2535926996419693\n",
      "epoch 7497: train loss: 0.13509127246518957, test loss: 0.25359169424089173\n",
      "epoch 7498: train loss: 0.1350860273467732, test loss: 0.2535906894818637\n",
      "epoch 7499: train loss: 0.13508078323139439, test loss: 0.2535896853646453\n",
      "epoch 7500: train loss: 0.13507554011872314, test loss: 0.2535886818889712\n",
      "epoch 7501: train loss: 0.13507029800842957, test loss: 0.2535876790546039\n",
      "epoch 7502: train loss: 0.1350650569001839, test loss: 0.25358667686128167\n",
      "epoch 7503: train loss: 0.1350598167936565, test loss: 0.253585675308768\n",
      "epoch 7504: train loss: 0.13505457768851795, test loss: 0.25358467439679744\n",
      "epoch 7505: train loss: 0.13504933958443896, test loss: 0.2535836741251323\n",
      "epoch 7506: train loss: 0.13504410248109036, test loss: 0.2535826744935178\n",
      "epoch 7507: train loss: 0.1350388663781432, test loss: 0.2535816755016991\n",
      "epoch 7508: train loss: 0.13503363127526855, test loss: 0.25358067714943805\n",
      "epoch 7509: train loss: 0.13502839717213777, test loss: 0.253579679436473\n",
      "epoch 7510: train loss: 0.1350231640684223, test loss: 0.25357868236256037\n",
      "epoch 7511: train loss: 0.13501793196379375, test loss: 0.2535776859274598\n",
      "epoch 7512: train loss: 0.13501270085792386, test loss: 0.253576690130905\n",
      "epoch 7513: train loss: 0.13500747075048453, test loss: 0.2535756949726558\n",
      "epoch 7514: train loss: 0.13500224164114788, test loss: 0.25357470045246794\n",
      "epoch 7515: train loss: 0.13499701352958604, test loss: 0.25357370657008343\n",
      "epoch 7516: train loss: 0.13499178641547138, test loss: 0.25357271332526443\n",
      "epoch 7517: train loss: 0.13498656029847644, test loss: 0.2535717207177478\n",
      "epoch 7518: train loss: 0.13498133517827388, test loss: 0.2535707287472948\n",
      "epoch 7519: train loss: 0.13497611105453644, test loss: 0.2535697374136643\n",
      "epoch 7520: train loss: 0.13497088792693718, test loss: 0.25356874671659585\n",
      "epoch 7521: train loss: 0.1349656657951491, test loss: 0.2535677566558455\n",
      "epoch 7522: train loss: 0.13496044465884552, test loss: 0.25356676723116633\n",
      "epoch 7523: train loss: 0.1349552245176998, test loss: 0.253565778442308\n",
      "epoch 7524: train loss: 0.13495000537138557, test loss: 0.25356479028902906\n",
      "epoch 7525: train loss: 0.13494478721957648, test loss: 0.25356380277107554\n",
      "epoch 7526: train loss: 0.1349395700619464, test loss: 0.25356281588820534\n",
      "epoch 7527: train loss: 0.1349343538981693, test loss: 0.25356182964017004\n",
      "epoch 7528: train loss: 0.1349291387279194, test loss: 0.2535608440267207\n",
      "epoch 7529: train loss: 0.13492392455087088, test loss: 0.2535598590476194\n",
      "epoch 7530: train loss: 0.13491871136669834, test loss: 0.2535588747026042\n",
      "epoch 7531: train loss: 0.13491349917507625, test loss: 0.25355789099143455\n",
      "epoch 7532: train loss: 0.13490828797567944, test loss: 0.2535569079138741\n",
      "epoch 7533: train loss: 0.1349030777681828, test loss: 0.2535559254696601\n",
      "epoch 7534: train loss: 0.13489786855226132, test loss: 0.25355494365855913\n",
      "epoch 7535: train loss: 0.13489266032759026, test loss: 0.25355396248032064\n",
      "epoch 7536: train loss: 0.13488745309384487, test loss: 0.25355298193469467\n",
      "epoch 7537: train loss: 0.13488224685070074, test loss: 0.2535520020214508\n",
      "epoch 7538: train loss: 0.13487704159783345, test loss: 0.25355102274032154\n",
      "epoch 7539: train loss: 0.1348718373349188, test loss: 0.2535500440910781\n",
      "epoch 7540: train loss: 0.13486663406163277, test loss: 0.2535490660734673\n",
      "epoch 7541: train loss: 0.13486143177765136, test loss: 0.2535480886872451\n",
      "epoch 7542: train loss: 0.13485623048265086, test loss: 0.25354711193216883\n",
      "epoch 7543: train loss: 0.1348510301763076, test loss: 0.25354613580799146\n",
      "epoch 7544: train loss: 0.13484583085829815, test loss: 0.2535451603144713\n",
      "epoch 7545: train loss: 0.13484063252829911, test loss: 0.2535441854513552\n",
      "epoch 7546: train loss: 0.13483543518598745, test loss: 0.25354321121840917\n",
      "epoch 7547: train loss: 0.13483023883103998, test loss: 0.2535422376153856\n",
      "epoch 7548: train loss: 0.1348250434631339, test loss: 0.25354126464203935\n",
      "epoch 7549: train loss: 0.13481984908194644, test loss: 0.2535402922981217\n",
      "epoch 7550: train loss: 0.13481465568715506, test loss: 0.2535393205833935\n",
      "epoch 7551: train loss: 0.13480946327843724, test loss: 0.25353834949761045\n",
      "epoch 7552: train loss: 0.13480427185547075, test loss: 0.25353737904052814\n",
      "epoch 7553: train loss: 0.1347990814179334, test loss: 0.2535364092119017\n",
      "epoch 7554: train loss: 0.1347938919655032, test loss: 0.2535354400114988\n",
      "epoch 7555: train loss: 0.13478870349785832, test loss: 0.2535344714390552\n",
      "epoch 7556: train loss: 0.134783516014677, test loss: 0.2535335034943472\n",
      "epoch 7557: train loss: 0.13477832951563776, test loss: 0.25353253617711924\n",
      "epoch 7558: train loss: 0.1347731440004191, test loss: 0.2535315694871315\n",
      "epoch 7559: train loss: 0.13476795946869977, test loss: 0.2535306034241436\n",
      "epoch 7560: train loss: 0.13476277592015867, test loss: 0.25352963798790873\n",
      "epoch 7561: train loss: 0.13475759335447482, test loss: 0.25352867317819316\n",
      "epoch 7562: train loss: 0.13475241177132738, test loss: 0.2535277089947483\n",
      "epoch 7563: train loss: 0.1347472311703957, test loss: 0.253526745437329\n",
      "epoch 7564: train loss: 0.13474205155135915, test loss: 0.25352578250569263\n",
      "epoch 7565: train loss: 0.13473687291389747, test loss: 0.2535248201996054\n",
      "epoch 7566: train loss: 0.1347316952576903, test loss: 0.2535238585188199\n",
      "epoch 7567: train loss: 0.1347265185824176, test loss: 0.2535228974630915\n",
      "epoch 7568: train loss: 0.1347213428877594, test loss: 0.25352193703218723\n",
      "epoch 7569: train loss: 0.13471616817339588, test loss: 0.25352097722585915\n",
      "epoch 7570: train loss: 0.1347109944390074, test loss: 0.25352001804386104\n",
      "epoch 7571: train loss: 0.1347058216842744, test loss: 0.2535190594859631\n",
      "epoch 7572: train loss: 0.13470064990887756, test loss: 0.2535181015519157\n",
      "epoch 7573: train loss: 0.13469547911249763, test loss: 0.25351714424148414\n",
      "epoch 7574: train loss: 0.13469030929481549, test loss: 0.2535161875544265\n",
      "epoch 7575: train loss: 0.13468514045551225, test loss: 0.253515231490493\n",
      "epoch 7576: train loss: 0.13467997259426912, test loss: 0.25351427604945376\n",
      "epoch 7577: train loss: 0.13467480571076743, test loss: 0.2535133212310628\n",
      "epoch 7578: train loss: 0.13466963980468863, test loss: 0.25351236703508634\n",
      "epoch 7579: train loss: 0.13466447487571448, test loss: 0.25351141346127115\n",
      "epoch 7580: train loss: 0.13465931092352665, test loss: 0.2535104605093906\n",
      "epoch 7581: train loss: 0.13465414794780714, test loss: 0.2535095081791981\n",
      "epoch 7582: train loss: 0.13464898594823801, test loss: 0.2535085564704504\n",
      "epoch 7583: train loss: 0.13464382492450147, test loss: 0.25350760538291833\n",
      "epoch 7584: train loss: 0.13463866487627987, test loss: 0.25350665491635394\n",
      "epoch 7585: train loss: 0.13463350580325575, test loss: 0.25350570507051573\n",
      "epoch 7586: train loss: 0.13462834770511178, test loss: 0.2535047558451773\n",
      "epoch 7587: train loss: 0.1346231905815307, test loss: 0.2535038072400855\n",
      "epoch 7588: train loss: 0.13461803443219547, test loss: 0.25350285925500254\n",
      "epoch 7589: train loss: 0.13461287925678916, test loss: 0.2535019118896984\n",
      "epoch 7590: train loss: 0.13460772505499505, test loss: 0.253500965143931\n",
      "epoch 7591: train loss: 0.13460257182649646, test loss: 0.25350001901745606\n",
      "epoch 7592: train loss: 0.13459741957097696, test loss: 0.2534990735100422\n",
      "epoch 7593: train loss: 0.13459226828812013, test loss: 0.25349812862144394\n",
      "epoch 7594: train loss: 0.13458711797760986, test loss: 0.2534971843514267\n",
      "epoch 7595: train loss: 0.13458196863913002, test loss: 0.25349624069975396\n",
      "epoch 7596: train loss: 0.13457682027236473, test loss: 0.2534952976661921\n",
      "epoch 7597: train loss: 0.13457167287699826, test loss: 0.25349435525048747\n",
      "epoch 7598: train loss: 0.13456652645271489, test loss: 0.253493413452415\n",
      "epoch 7599: train loss: 0.13456138099919923, test loss: 0.253492472271733\n",
      "epoch 7600: train loss: 0.13455623651613588, test loss: 0.25349153170821126\n",
      "epoch 7601: train loss: 0.13455109300320972, test loss: 0.2534905917615995\n",
      "epoch 7602: train loss: 0.13454595046010565, test loss: 0.2534896524316646\n",
      "epoch 7603: train loss: 0.1345408088865087, test loss: 0.25348871371817494\n",
      "epoch 7604: train loss: 0.13453566828210423, test loss: 0.2534877756208899\n",
      "epoch 7605: train loss: 0.1345305286465775, test loss: 0.25348683813957607\n",
      "epoch 7606: train loss: 0.13452538997961408, test loss: 0.2534859012739848\n",
      "epoch 7607: train loss: 0.13452025228089964, test loss: 0.2534849650238954\n",
      "epoch 7608: train loss: 0.13451511555011994, test loss: 0.2534840293890624\n",
      "epoch 7609: train loss: 0.13450997978696097, test loss: 0.2534830943692487\n",
      "epoch 7610: train loss: 0.13450484499110882, test loss: 0.25348215996422213\n",
      "epoch 7611: train loss: 0.13449971116224965, test loss: 0.253481226173744\n",
      "epoch 7612: train loss: 0.1344945783000699, test loss: 0.25348029299757696\n",
      "epoch 7613: train loss: 0.13448944640425606, test loss: 0.25347936043549035\n",
      "epoch 7614: train loss: 0.13448431547449474, test loss: 0.25347842848724306\n",
      "epoch 7615: train loss: 0.13447918551047283, test loss: 0.2534774971526\n",
      "epoch 7616: train loss: 0.1344740565118772, test loss: 0.2534765664313297\n",
      "epoch 7617: train loss: 0.13446892847839492, test loss: 0.2534756363231951\n",
      "epoch 7618: train loss: 0.13446380140971326, test loss: 0.25347470682795875\n",
      "epoch 7619: train loss: 0.13445867530551955, test loss: 0.25347377794538123\n",
      "epoch 7620: train loss: 0.1344535501655013, test loss: 0.2534728496752397\n",
      "epoch 7621: train loss: 0.13444842598934617, test loss: 0.25347192201729285\n",
      "epoch 7622: train loss: 0.13444330277674188, test loss: 0.2534709949713033\n",
      "epoch 7623: train loss: 0.13443818052737647, test loss: 0.2534700685370385\n",
      "epoch 7624: train loss: 0.1344330592409379, test loss: 0.25346914271426146\n",
      "epoch 7625: train loss: 0.13442793891711444, test loss: 0.25346821750274345\n",
      "epoch 7626: train loss: 0.13442281955559443, test loss: 0.2534672929022468\n",
      "epoch 7627: train loss: 0.13441770115606633, test loss: 0.25346636891253305\n",
      "epoch 7628: train loss: 0.1344125837182188, test loss: 0.25346544553337824\n",
      "epoch 7629: train loss: 0.1344074672417406, test loss: 0.2534645227645413\n",
      "epoch 7630: train loss: 0.13440235172632067, test loss: 0.25346360060579354\n",
      "epoch 7631: train loss: 0.134397237171648, test loss: 0.25346267905689357\n",
      "epoch 7632: train loss: 0.13439212357741184, test loss: 0.2534617581176132\n",
      "epoch 7633: train loss: 0.13438701094330152, test loss: 0.25346083778771694\n",
      "epoch 7634: train loss: 0.13438189926900648, test loss: 0.25345991806697576\n",
      "epoch 7635: train loss: 0.13437678855421634, test loss: 0.2534589989551498\n",
      "epoch 7636: train loss: 0.13437167879862089, test loss: 0.2534580804520072\n",
      "epoch 7637: train loss: 0.13436657000190994, test loss: 0.25345716255732414\n",
      "epoch 7638: train loss: 0.13436146216377362, test loss: 0.2534562452708568\n",
      "epoch 7639: train loss: 0.13435635528390205, test loss: 0.2534553285923802\n",
      "epoch 7640: train loss: 0.13435124936198556, test loss: 0.2534544125216548\n",
      "epoch 7641: train loss: 0.13434614439771458, test loss: 0.25345349705844916\n",
      "epoch 7642: train loss: 0.13434104039077974, test loss: 0.2534525822025397\n",
      "epoch 7643: train loss: 0.13433593734087174, test loss: 0.2534516679536846\n",
      "epoch 7644: train loss: 0.13433083524768144, test loss: 0.2534507543116559\n",
      "epoch 7645: train loss: 0.13432573411089987, test loss: 0.2534498412762196\n",
      "epoch 7646: train loss: 0.1343206339302182, test loss: 0.25344892884714926\n",
      "epoch 7647: train loss: 0.13431553470532764, test loss: 0.2534480170242096\n",
      "epoch 7648: train loss: 0.13431043643591972, test loss: 0.2534471058071678\n",
      "epoch 7649: train loss: 0.1343053391216859, test loss: 0.2534461951957913\n",
      "epoch 7650: train loss: 0.13430024276231803, test loss: 0.2534452851898461\n",
      "epoch 7651: train loss: 0.13429514735750778, test loss: 0.2534443757891121\n",
      "epoch 7652: train loss: 0.13429005290694726, test loss: 0.2534434669933534\n",
      "epoch 7653: train loss: 0.13428495941032853, test loss: 0.2534425588023368\n",
      "epoch 7654: train loss: 0.13427986686734386, test loss: 0.2534416512158282\n",
      "epoch 7655: train loss: 0.13427477527768567, test loss: 0.2534407442336004\n",
      "epoch 7656: train loss: 0.13426968464104647, test loss: 0.2534398378554305\n",
      "epoch 7657: train loss: 0.13426459495711893, test loss: 0.25343893208107515\n",
      "epoch 7658: train loss: 0.1342595062255959, test loss: 0.2534380269103116\n",
      "epoch 7659: train loss: 0.13425441844617025, test loss: 0.25343712234290544\n",
      "epoch 7660: train loss: 0.1342493316185352, test loss: 0.2534362183786367\n",
      "epoch 7661: train loss: 0.13424424574238386, test loss: 0.2534353150172568\n",
      "epoch 7662: train loss: 0.13423916081740966, test loss: 0.25343441225855523\n",
      "epoch 7663: train loss: 0.13423407684330604, test loss: 0.25343351010228693\n",
      "epoch 7664: train loss: 0.13422899381976675, test loss: 0.2534326085482352\n",
      "epoch 7665: train loss: 0.13422391174648546, test loss: 0.253431707596159\n",
      "epoch 7666: train loss: 0.13421883062315612, test loss: 0.25343080724583605\n",
      "epoch 7667: train loss: 0.13421375044947279, test loss: 0.25342990749704\n",
      "epoch 7668: train loss: 0.13420867122512967, test loss: 0.2534290083495369\n",
      "epoch 7669: train loss: 0.13420359294982107, test loss: 0.2534281098030944\n",
      "epoch 7670: train loss: 0.13419851562324145, test loss: 0.25342721185748673\n",
      "epoch 7671: train loss: 0.13419343924508542, test loss: 0.25342631451248915\n",
      "epoch 7672: train loss: 0.13418836381504776, test loss: 0.2534254177678704\n",
      "epoch 7673: train loss: 0.13418328933282325, test loss: 0.2534245216233975\n",
      "epoch 7674: train loss: 0.13417821579810701, test loss: 0.2534236260788477\n",
      "epoch 7675: train loss: 0.13417314321059415, test loss: 0.25342273113399066\n",
      "epoch 7676: train loss: 0.13416807156997992, test loss: 0.25342183678859875\n",
      "epoch 7677: train loss: 0.1341630008759598, test loss: 0.2534209430424422\n",
      "epoch 7678: train loss: 0.13415793112822932, test loss: 0.2534200498952915\n",
      "epoch 7679: train loss: 0.13415286232648418, test loss: 0.25341915734692866\n",
      "epoch 7680: train loss: 0.13414779447042022, test loss: 0.25341826539711276\n",
      "epoch 7681: train loss: 0.13414272755973344, test loss: 0.25341737404562914\n",
      "epoch 7682: train loss: 0.13413766159411986, test loss: 0.25341648329223765\n",
      "epoch 7683: train loss: 0.13413259657327578, test loss: 0.25341559313672285\n",
      "epoch 7684: train loss: 0.13412753249689763, test loss: 0.25341470357885143\n",
      "epoch 7685: train loss: 0.1341224693646818, test loss: 0.25341381461839385\n",
      "epoch 7686: train loss: 0.1341174071763251, test loss: 0.25341292625512835\n",
      "epoch 7687: train loss: 0.13411234593152416, test loss: 0.2534120384888236\n",
      "epoch 7688: train loss: 0.13410728562997595, test loss: 0.25341115131925496\n",
      "epoch 7689: train loss: 0.1341022262713776, test loss: 0.25341026474619915\n",
      "epoch 7690: train loss: 0.13409716785542625, test loss: 0.2534093787694241\n",
      "epoch 7691: train loss: 0.13409211038181923, test loss: 0.2534084933887102\n",
      "epoch 7692: train loss: 0.134087053850254, test loss: 0.2534076086038223\n",
      "epoch 7693: train loss: 0.1340819982604282, test loss: 0.25340672441454004\n",
      "epoch 7694: train loss: 0.13407694361203948, test loss: 0.2534058408206358\n",
      "epoch 7695: train loss: 0.13407188990478583, test loss: 0.2534049578218837\n",
      "epoch 7696: train loss: 0.13406683713836517, test loss: 0.2534040754180622\n",
      "epoch 7697: train loss: 0.13406178531247565, test loss: 0.2534031936089363\n",
      "epoch 7698: train loss: 0.1340567344268156, test loss: 0.253402312394291\n",
      "epoch 7699: train loss: 0.13405168448108337, test loss: 0.2534014317738956\n",
      "epoch 7700: train loss: 0.13404663547497755, test loss: 0.2534005517475246\n",
      "epoch 7701: train loss: 0.13404158740819677, test loss: 0.25339967231495153\n",
      "epoch 7702: train loss: 0.13403654028043996, test loss: 0.2533987934759611\n",
      "epoch 7703: train loss: 0.1340314940914059, test loss: 0.25339791523031446\n",
      "epoch 7704: train loss: 0.13402644884079384, test loss: 0.25339703757779347\n",
      "epoch 7705: train loss: 0.1340214045283029, test loss: 0.2533961605181735\n",
      "epoch 7706: train loss: 0.13401636115363247, test loss: 0.253395284051227\n",
      "epoch 7707: train loss: 0.13401131871648206, test loss: 0.25339440817673486\n",
      "epoch 7708: train loss: 0.13400627721655123, test loss: 0.2533935328944718\n",
      "epoch 7709: train loss: 0.1340012366535398, test loss: 0.25339265820420886\n",
      "epoch 7710: train loss: 0.13399619702714763, test loss: 0.2533917841057293\n",
      "epoch 7711: train loss: 0.13399115833707478, test loss: 0.2533909105987941\n",
      "epoch 7712: train loss: 0.13398612058302137, test loss: 0.2533900376832002\n",
      "epoch 7713: train loss: 0.1339810837646877, test loss: 0.2533891653587074\n",
      "epoch 7714: train loss: 0.13397604788177422, test loss: 0.2533882936251012\n",
      "epoch 7715: train loss: 0.13397101293398148, test loss: 0.2533874224821577\n",
      "epoch 7716: train loss: 0.13396597892101017, test loss: 0.253386551929651\n",
      "epoch 7717: train loss: 0.13396094584256113, test loss: 0.2533856819673521\n",
      "epoch 7718: train loss: 0.1339559136983353, test loss: 0.2533848125950497\n",
      "epoch 7719: train loss: 0.1339508824880338, test loss: 0.25338394381250806\n",
      "epoch 7720: train loss: 0.13394585221135788, test loss: 0.2533830756195153\n",
      "epoch 7721: train loss: 0.13394082286800885, test loss: 0.25338220801584127\n",
      "epoch 7722: train loss: 0.13393579445768825, test loss: 0.2533813410012684\n",
      "epoch 7723: train loss: 0.13393076698009765, test loss: 0.25338047457557517\n",
      "epoch 7724: train loss: 0.13392574043493888, test loss: 0.2533796087385327\n",
      "epoch 7725: train loss: 0.1339207148219138, test loss: 0.2533787434899197\n",
      "epoch 7726: train loss: 0.13391569014072446, test loss: 0.2533778788295209\n",
      "epoch 7727: train loss: 0.13391066639107296, test loss: 0.2533770147571055\n",
      "epoch 7728: train loss: 0.13390564357266171, test loss: 0.25337615127245416\n",
      "epoch 7729: train loss: 0.13390062168519304, test loss: 0.2533752883753502\n",
      "epoch 7730: train loss: 0.13389560072836953, test loss: 0.25337442606556565\n",
      "epoch 7731: train loss: 0.13389058070189389, test loss: 0.2533735643428805\n",
      "epoch 7732: train loss: 0.1338855616054689, test loss: 0.2533727032070779\n",
      "epoch 7733: train loss: 0.1338805434387976, test loss: 0.25337184265792834\n",
      "epoch 7734: train loss: 0.13387552620158302, test loss: 0.2533709826952116\n",
      "epoch 7735: train loss: 0.13387050989352833, test loss: 0.253370123318716\n",
      "epoch 7736: train loss: 0.13386549451433702, test loss: 0.2533692645282097\n",
      "epoch 7737: train loss: 0.13386048006371248, test loss: 0.2533684063234789\n",
      "epoch 7738: train loss: 0.13385546654135833, test loss: 0.2533675487042986\n",
      "epoch 7739: train loss: 0.1338504539469783, test loss: 0.25336669167044706\n",
      "epoch 7740: train loss: 0.13384544228027637, test loss: 0.2533658352217075\n",
      "epoch 7741: train loss: 0.13384043154095646, test loss: 0.2533649793578618\n",
      "epoch 7742: train loss: 0.13383542172872276, test loss: 0.25336412407868586\n",
      "epoch 7743: train loss: 0.13383041284327948, test loss: 0.2533632693839569\n",
      "epoch 7744: train loss: 0.13382540488433112, test loss: 0.25336241527345443\n",
      "epoch 7745: train loss: 0.13382039785158217, test loss: 0.2533615617469644\n",
      "epoch 7746: train loss: 0.13381539174473733, test loss: 0.25336070880426403\n",
      "epoch 7747: train loss: 0.13381038656350136, test loss: 0.2533598564451274\n",
      "epoch 7748: train loss: 0.1338053823075792, test loss: 0.2533590046693465\n",
      "epoch 7749: train loss: 0.13380037897667596, test loss: 0.25335815347669405\n",
      "epoch 7750: train loss: 0.1337953765704968, test loss: 0.25335730286695596\n",
      "epoch 7751: train loss: 0.13379037508874705, test loss: 0.2533564528399065\n",
      "epoch 7752: train loss: 0.13378537453113218, test loss: 0.2533556033953278\n",
      "epoch 7753: train loss: 0.13378037489735775, test loss: 0.25335475453300693\n",
      "epoch 7754: train loss: 0.1337753761871295, test loss: 0.25335390625271875\n",
      "epoch 7755: train loss: 0.1337703784001533, test loss: 0.2533530585542461\n",
      "epoch 7756: train loss: 0.1337653815361351, test loss: 0.25335221143736564\n",
      "epoch 7757: train loss: 0.133760385594781, test loss: 0.2533513649018662\n",
      "epoch 7758: train loss: 0.1337553905757973, test loss: 0.2533505189475305\n",
      "epoch 7759: train loss: 0.1337503964788903, test loss: 0.25334967357413135\n",
      "epoch 7760: train loss: 0.13374540330376658, test loss: 0.2533488287814529\n",
      "epoch 7761: train loss: 0.13374041105013273, test loss: 0.25334798456928487\n",
      "epoch 7762: train loss: 0.13373541971769548, test loss: 0.2533471409373951\n",
      "epoch 7763: train loss: 0.13373042930616177, test loss: 0.2533462978855804\n",
      "epoch 7764: train loss: 0.13372543981523863, test loss: 0.2533454554136189\n",
      "epoch 7765: train loss: 0.1337204512446332, test loss: 0.2533446135212903\n",
      "epoch 7766: train loss: 0.13371546359405276, test loss: 0.2533437722083681\n",
      "epoch 7767: train loss: 0.1337104768632047, test loss: 0.25334293147465237\n",
      "epoch 7768: train loss: 0.1337054910517966, test loss: 0.2533420913199156\n",
      "epoch 7769: train loss: 0.13370050615953613, test loss: 0.25334125174393807\n",
      "epoch 7770: train loss: 0.1336955221861311, test loss: 0.253340412746511\n",
      "epoch 7771: train loss: 0.13369053913128937, test loss: 0.2533395743274097\n",
      "epoch 7772: train loss: 0.13368555699471912, test loss: 0.25333873648642224\n",
      "epoch 7773: train loss: 0.13368057577612846, test loss: 0.25333789922332933\n",
      "epoch 7774: train loss: 0.1336755954752257, test loss: 0.2533370625379157\n",
      "epoch 7775: train loss: 0.13367061609171935, test loss: 0.2533362264299667\n",
      "epoch 7776: train loss: 0.13366563762531794, test loss: 0.2533353908992597\n",
      "epoch 7777: train loss: 0.13366066007573021, test loss: 0.2533345559455851\n",
      "epoch 7778: train loss: 0.13365568344266499, test loss: 0.2533337215687198\n",
      "epoch 7779: train loss: 0.1336507077258312, test loss: 0.25333288776845087\n",
      "epoch 7780: train loss: 0.133645732924938, test loss: 0.25333205454456625\n",
      "epoch 7781: train loss: 0.1336407590396946, test loss: 0.2533312218968423\n",
      "epoch 7782: train loss: 0.13363578606981033, test loss: 0.25333038982507133\n",
      "epoch 7783: train loss: 0.1336308140149947, test loss: 0.25332955832903026\n",
      "epoch 7784: train loss: 0.13362584287495724, test loss: 0.25332872740850787\n",
      "epoch 7785: train loss: 0.13362087264940783, test loss: 0.2533278970632885\n",
      "epoch 7786: train loss: 0.1336159033380562, test loss: 0.253327067293158\n",
      "epoch 7787: train loss: 0.1336109349406124, test loss: 0.2533262380978965\n",
      "epoch 7788: train loss: 0.1336059674567866, test loss: 0.2533254094772919\n",
      "epoch 7789: train loss: 0.13360100088628898, test loss: 0.2533245814311305\n",
      "epoch 7790: train loss: 0.13359603522882996, test loss: 0.2533237539591912\n",
      "epoch 7791: train loss: 0.13359107048412, test loss: 0.25332292706126586\n",
      "epoch 7792: train loss: 0.1335861066518698, test loss: 0.25332210073713746\n",
      "epoch 7793: train loss: 0.1335811437317901, test loss: 0.25332127498659235\n",
      "epoch 7794: train loss: 0.13357618172359176, test loss: 0.2533204498094132\n",
      "epoch 7795: train loss: 0.13357122062698581, test loss: 0.25331962520539086\n",
      "epoch 7796: train loss: 0.1335662604416835, test loss: 0.25331880117430705\n",
      "epoch 7797: train loss: 0.13356130116739592, test loss: 0.25331797771595305\n",
      "epoch 7798: train loss: 0.13355634280383463, test loss: 0.25331715483009926\n",
      "epoch 7799: train loss: 0.13355138535071107, test loss: 0.2533163325165495\n",
      "epoch 7800: train loss: 0.13354642880773693, test loss: 0.2533155107750831\n",
      "epoch 7801: train loss: 0.13354147317462403, test loss: 0.25331468960549247\n",
      "epoch 7802: train loss: 0.1335365184510842, test loss: 0.253313869007551\n",
      "epoch 7803: train loss: 0.13353156463682958, test loss: 0.25331304898105467\n",
      "epoch 7804: train loss: 0.13352661173157224, test loss: 0.2533122295257859\n",
      "epoch 7805: train loss: 0.1335216597350245, test loss: 0.25331141064153756\n",
      "epoch 7806: train loss: 0.13351670864689885, test loss: 0.2533105923280876\n",
      "epoch 7807: train loss: 0.13351175846690777, test loss: 0.25330977458523174\n",
      "epoch 7808: train loss: 0.13350680919476393, test loss: 0.25330895741275067\n",
      "epoch 7809: train loss: 0.13350186083018017, test loss: 0.2533081408104344\n",
      "epoch 7810: train loss: 0.13349691337286937, test loss: 0.2533073247780684\n",
      "epoch 7811: train loss: 0.13349196682254463, test loss: 0.25330650931543786\n",
      "epoch 7812: train loss: 0.1334870211789191, test loss: 0.2533056944223416\n",
      "epoch 7813: train loss: 0.13348207644170612, test loss: 0.25330488009856045\n",
      "epoch 7814: train loss: 0.1334771326106191, test loss: 0.2533040663438792\n",
      "epoch 7815: train loss: 0.13347218968537164, test loss: 0.2533032531580856\n",
      "epoch 7816: train loss: 0.13346724766567736, test loss: 0.2533024405409718\n",
      "epoch 7817: train loss: 0.13346230655125013, test loss: 0.25330162849232224\n",
      "epoch 7818: train loss: 0.13345736634180383, test loss: 0.25330081701192625\n",
      "epoch 7819: train loss: 0.13345242703705257, test loss: 0.2533000060995724\n",
      "epoch 7820: train loss: 0.13344748863671055, test loss: 0.2532991957550553\n",
      "epoch 7821: train loss: 0.13344255114049206, test loss: 0.253298385978154\n",
      "epoch 7822: train loss: 0.13343761454811154, test loss: 0.25329757676866194\n",
      "epoch 7823: train loss: 0.13343267885928362, test loss: 0.25329676812636204\n",
      "epoch 7824: train loss: 0.1334277440737229, test loss: 0.25329596005105603\n",
      "epoch 7825: train loss: 0.13342281019114427, test loss: 0.25329515254251556\n",
      "epoch 7826: train loss: 0.13341787721126266, test loss: 0.25329434560054764\n",
      "epoch 7827: train loss: 0.13341294513379315, test loss: 0.2532935392249248\n",
      "epoch 7828: train loss: 0.13340801395845087, test loss: 0.25329273341544883\n",
      "epoch 7829: train loss: 0.13340308368495124, test loss: 0.2532919281719036\n",
      "epoch 7830: train loss: 0.1333981543130096, test loss: 0.2532911234940773\n",
      "epoch 7831: train loss: 0.13339322584234167, test loss: 0.25329031938176055\n",
      "epoch 7832: train loss: 0.133388298272663, test loss: 0.2532895158347497\n",
      "epoch 7833: train loss: 0.13338337160368952, test loss: 0.25328871285282584\n",
      "epoch 7834: train loss: 0.13337844583513708, test loss: 0.2532879104357807\n",
      "epoch 7835: train loss: 0.13337352096672184, test loss: 0.2532871085834092\n",
      "epoch 7836: train loss: 0.13336859699815998, test loss: 0.2532863072954921\n",
      "epoch 7837: train loss: 0.1333636739291678, test loss: 0.25328550657183385\n",
      "epoch 7838: train loss: 0.1333587517594617, test loss: 0.2532847064122108\n",
      "epoch 7839: train loss: 0.13335383048875837, test loss: 0.25328390681642077\n",
      "epoch 7840: train loss: 0.1333489101167744, test loss: 0.25328310778425217\n",
      "epoch 7841: train loss: 0.13334399064322672, test loss: 0.2532823093154983\n",
      "epoch 7842: train loss: 0.13333907206783213, test loss: 0.25328151140994726\n",
      "epoch 7843: train loss: 0.13333415439030782, test loss: 0.25328071406738895\n",
      "epoch 7844: train loss: 0.13332923761037094, test loss: 0.2532799172876189\n",
      "epoch 7845: train loss: 0.1333243217277388, test loss: 0.2532791210704202\n",
      "epoch 7846: train loss: 0.1333194067421289, test loss: 0.2532783254155952\n",
      "epoch 7847: train loss: 0.13331449265325873, test loss: 0.25327753032292494\n",
      "epoch 7848: train loss: 0.133309579460846, test loss: 0.253276735792212\n",
      "epoch 7849: train loss: 0.13330466716460856, test loss: 0.25327594182323954\n",
      "epoch 7850: train loss: 0.13329975576426434, test loss: 0.25327514841579246\n",
      "epoch 7851: train loss: 0.13329484525953134, test loss: 0.25327435556967925\n",
      "epoch 7852: train loss: 0.13328993565012784, test loss: 0.25327356328468\n",
      "epoch 7853: train loss: 0.13328502693577207, test loss: 0.2532727715605924\n",
      "epoch 7854: train loss: 0.1332801191161825, test loss: 0.25327198039720566\n",
      "epoch 7855: train loss: 0.13327521219107774, test loss: 0.253271189794315\n",
      "epoch 7856: train loss: 0.13327030616017638, test loss: 0.2532703997517065\n",
      "epoch 7857: train loss: 0.13326540102319723, test loss: 0.2532696102691766\n",
      "epoch 7858: train loss: 0.1332604967798593, test loss: 0.2532688213465195\n",
      "epoch 7859: train loss: 0.13325559342988158, test loss: 0.2532680329835229\n",
      "epoch 7860: train loss: 0.13325069097298323, test loss: 0.25326724517998955\n",
      "epoch 7861: train loss: 0.13324578940888357, test loss: 0.25326645793569824\n",
      "epoch 7862: train loss: 0.13324088873730203, test loss: 0.2532656712504503\n",
      "epoch 7863: train loss: 0.13323598895795816, test loss: 0.25326488512404205\n",
      "epoch 7864: train loss: 0.1332310900705716, test loss: 0.2532640995562589\n",
      "epoch 7865: train loss: 0.13322619207486214, test loss: 0.25326331454689754\n",
      "epoch 7866: train loss: 0.13322129497054966, test loss: 0.2532625300957508\n",
      "epoch 7867: train loss: 0.13321639875735428, test loss: 0.2532617462026169\n",
      "epoch 7868: train loss: 0.1332115034349961, test loss: 0.2532609628672805\n",
      "epoch 7869: train loss: 0.13320660900319542, test loss: 0.2532601800895406\n",
      "epoch 7870: train loss: 0.1332017154616726, test loss: 0.25325939786919166\n",
      "epoch 7871: train loss: 0.13319682281014822, test loss: 0.2532586162060263\n",
      "epoch 7872: train loss: 0.1331919310483429, test loss: 0.2532578350998418\n",
      "epoch 7873: train loss: 0.13318704017597738, test loss: 0.2532570545504258\n",
      "epoch 7874: train loss: 0.13318215019277263, test loss: 0.2532562745575753\n",
      "epoch 7875: train loss: 0.13317726109844957, test loss: 0.2532554951210851\n",
      "epoch 7876: train loss: 0.13317237289272943, test loss: 0.25325471624074886\n",
      "epoch 7877: train loss: 0.13316748557533342, test loss: 0.25325393791635814\n",
      "epoch 7878: train loss: 0.13316259914598289, test loss: 0.25325316014772037\n",
      "epoch 7879: train loss: 0.13315771360439935, test loss: 0.2532523829346111\n",
      "epoch 7880: train loss: 0.1331528289503045, test loss: 0.2532516062768419\n",
      "epoch 7881: train loss: 0.13314794518342, test loss: 0.2532508301742002\n",
      "epoch 7882: train loss: 0.13314306230346773, test loss: 0.25325005462648015\n",
      "epoch 7883: train loss: 0.13313818031016972, test loss: 0.2532492796334844\n",
      "epoch 7884: train loss: 0.133133299203248, test loss: 0.2532485051949974\n",
      "epoch 7885: train loss: 0.13312841898242492, test loss: 0.2532477313108238\n",
      "epoch 7886: train loss: 0.13312353964742274, test loss: 0.2532469579807509\n",
      "epoch 7887: train loss: 0.13311866119796398, test loss: 0.25324618520457975\n",
      "epoch 7888: train loss: 0.1331137836337712, test loss: 0.2532454129821066\n",
      "epoch 7889: train loss: 0.13310890695456712, test loss: 0.2532446413131251\n",
      "epoch 7890: train loss: 0.1331040311600746, test loss: 0.25324387019742334\n",
      "epoch 7891: train loss: 0.13309915625001661, test loss: 0.25324309963481817\n",
      "epoch 7892: train loss: 0.13309428222411618, test loss: 0.2532423296250853\n",
      "epoch 7893: train loss: 0.13308940908209657, test loss: 0.2532415601680281\n",
      "epoch 7894: train loss: 0.13308453682368104, test loss: 0.2532407912634455\n",
      "epoch 7895: train loss: 0.1330796654485931, test loss: 0.2532400229111305\n",
      "epoch 7896: train loss: 0.13307479495655625, test loss: 0.25323925511088213\n",
      "epoch 7897: train loss: 0.13306992534729423, test loss: 0.25323848786250147\n",
      "epoch 7898: train loss: 0.13306505662053078, test loss: 0.25323772116577364\n",
      "epoch 7899: train loss: 0.13306018877598988, test loss: 0.25323695502050264\n",
      "epoch 7900: train loss: 0.13305532181339555, test loss: 0.2532361894264804\n",
      "epoch 7901: train loss: 0.13305045573247198, test loss: 0.25323542438351443\n",
      "epoch 7902: train loss: 0.1330455905329434, test loss: 0.25323465989139066\n",
      "epoch 7903: train loss: 0.13304072621453428, test loss: 0.25323389594991313\n",
      "epoch 7904: train loss: 0.1330358627769691, test loss: 0.25323313255887464\n",
      "epoch 7905: train loss: 0.13303100021997255, test loss: 0.2532323697180774\n",
      "epoch 7906: train loss: 0.1330261385432694, test loss: 0.25323160742732237\n",
      "epoch 7907: train loss: 0.13302127774658445, test loss: 0.253230845686391\n",
      "epoch 7908: train loss: 0.1330164178296428, test loss: 0.25323008449509693\n",
      "epoch 7909: train loss: 0.13301155879216955, test loss: 0.25322932385323427\n",
      "epoch 7910: train loss: 0.13300670063388992, test loss: 0.25322856376059233\n",
      "epoch 7911: train loss: 0.1330018433545293, test loss: 0.25322780421698365\n",
      "epoch 7912: train loss: 0.13299698695381315, test loss: 0.2532270452221953\n",
      "epoch 7913: train loss: 0.13299213143146715, test loss: 0.25322628677603626\n",
      "epoch 7914: train loss: 0.13298727678721695, test loss: 0.2532255288782947\n",
      "epoch 7915: train loss: 0.13298242302078842, test loss: 0.2532247715287679\n",
      "epoch 7916: train loss: 0.1329775701319075, test loss: 0.2532240147272653\n",
      "epoch 7917: train loss: 0.1329727181203003, test loss: 0.25322325847357113\n",
      "epoch 7918: train loss: 0.132967866985693, test loss: 0.2532225027674958\n",
      "epoch 7919: train loss: 0.13296301672781197, test loss: 0.253221747608841\n",
      "epoch 7920: train loss: 0.13295816734638358, test loss: 0.2532209929973936\n",
      "epoch 7921: train loss: 0.13295331884113445, test loss: 0.2532202389329587\n",
      "epoch 7922: train loss: 0.13294847121179124, test loss: 0.2532194854153378\n",
      "epoch 7923: train loss: 0.13294362445808072, test loss: 0.25321873244432574\n",
      "epoch 7924: train loss: 0.13293877857972983, test loss: 0.25321798001972673\n",
      "epoch 7925: train loss: 0.1329339335764656, test loss: 0.25321722814133985\n",
      "epoch 7926: train loss: 0.1329290894480152, test loss: 0.2532164768089605\n",
      "epoch 7927: train loss: 0.13292424619410592, test loss: 0.2532157260223842\n",
      "epoch 7928: train loss: 0.13291940381446507, test loss: 0.2532149757814233\n",
      "epoch 7929: train loss: 0.13291456230882023, test loss: 0.25321422608587574\n",
      "epoch 7930: train loss: 0.13290972167689902, test loss: 0.2532134769355321\n",
      "epoch 7931: train loss: 0.13290488191842914, test loss: 0.2532127283301982\n",
      "epoch 7932: train loss: 0.13290004303313854, test loss: 0.25321198026967656\n",
      "epoch 7933: train loss: 0.13289520502075514, test loss: 0.2532112327537641\n",
      "epoch 7934: train loss: 0.13289036788100703, test loss: 0.2532104857822566\n",
      "epoch 7935: train loss: 0.13288553161362246, test loss: 0.25320973935496427\n",
      "epoch 7936: train loss: 0.13288069621832974, test loss: 0.25320899347168424\n",
      "epoch 7937: train loss: 0.13287586169485738, test loss: 0.2532082481322127\n",
      "epoch 7938: train loss: 0.13287102804293394, test loss: 0.2532075033363639\n",
      "epoch 7939: train loss: 0.13286619526228804, test loss: 0.2532067590839193\n",
      "epoch 7940: train loss: 0.13286136335264856, test loss: 0.25320601537469123\n",
      "epoch 7941: train loss: 0.1328565323137444, test loss: 0.25320527220848527\n",
      "epoch 7942: train loss: 0.13285170214530465, test loss: 0.2532045295850917\n",
      "epoch 7943: train loss: 0.1328468728470584, test loss: 0.25320378750432143\n",
      "epoch 7944: train loss: 0.13284204441873496, test loss: 0.25320304596596677\n",
      "epoch 7945: train loss: 0.13283721686006372, test loss: 0.2532023049698416\n",
      "epoch 7946: train loss: 0.13283239017077425, test loss: 0.2532015645157356\n",
      "epoch 7947: train loss: 0.13282756435059612, test loss: 0.2532008246034494\n",
      "epoch 7948: train loss: 0.13282273939925904, test loss: 0.2532000852327975\n",
      "epoch 7949: train loss: 0.13281791531649298, test loss: 0.25319934640357294\n",
      "epoch 7950: train loss: 0.13281309210202788, test loss: 0.253198608115578\n",
      "epoch 7951: train loss: 0.13280826975559382, test loss: 0.25319787036862207\n",
      "epoch 7952: train loss: 0.13280344827692103, test loss: 0.253197133162497\n",
      "epoch 7953: train loss: 0.13279862766573985, test loss: 0.25319639649701153\n",
      "epoch 7954: train loss: 0.13279380792178075, test loss: 0.25319566037196667\n",
      "epoch 7955: train loss: 0.13278898904477426, test loss: 0.253194924787164\n",
      "epoch 7956: train loss: 0.1327841710344511, test loss: 0.2531941897424134\n",
      "epoch 7957: train loss: 0.13277935389054202, test loss: 0.2531934552375024\n",
      "epoch 7958: train loss: 0.13277453761277797, test loss: 0.25319272127224723\n",
      "epoch 7959: train loss: 0.13276972220089, test loss: 0.2531919878464447\n",
      "epoch 7960: train loss: 0.13276490765460927, test loss: 0.25319125495990147\n",
      "epoch 7961: train loss: 0.13276009397366703, test loss: 0.25319052261242203\n",
      "epoch 7962: train loss: 0.1327552811577946, test loss: 0.25318979080380294\n",
      "epoch 7963: train loss: 0.13275046920672362, test loss: 0.25318905953385235\n",
      "epoch 7964: train loss: 0.13274565812018557, test loss: 0.25318832880237\n",
      "epoch 7965: train loss: 0.13274084789791227, test loss: 0.25318759860916573\n",
      "epoch 7966: train loss: 0.13273603853963553, test loss: 0.2531868689540324\n",
      "epoch 7967: train loss: 0.1327312300450873, test loss: 0.25318613983678856\n",
      "epoch 7968: train loss: 0.1327264224139997, test loss: 0.2531854112572286\n",
      "epoch 7969: train loss: 0.13272161564610496, test loss: 0.25318468321515825\n",
      "epoch 7970: train loss: 0.13271680974113528, test loss: 0.2531839557103823\n",
      "epoch 7971: train loss: 0.1327120046988232, test loss: 0.2531832287427027\n",
      "epoch 7972: train loss: 0.13270720051890117, test loss: 0.2531825023119252\n",
      "epoch 7973: train loss: 0.13270239720110194, test loss: 0.2531817764178516\n",
      "epoch 7974: train loss: 0.13269759474515821, test loss: 0.2531810510602973\n",
      "epoch 7975: train loss: 0.13269279315080293, test loss: 0.25318032623905246\n",
      "epoch 7976: train loss: 0.13268799241776907, test loss: 0.2531796019539291\n",
      "epoch 7977: train loss: 0.13268319254578975, test loss: 0.2531788782047303\n",
      "epoch 7978: train loss: 0.13267839353459826, test loss: 0.25317815499126206\n",
      "epoch 7979: train loss: 0.1326735953839279, test loss: 0.2531774323133243\n",
      "epoch 7980: train loss: 0.13266879809351212, test loss: 0.2531767101707276\n",
      "epoch 7981: train loss: 0.13266400166308456, test loss: 0.2531759885632838\n",
      "epoch 7982: train loss: 0.13265920609237888, test loss: 0.25317526749078423\n",
      "epoch 7983: train loss: 0.1326544113811289, test loss: 0.2531745469530431\n",
      "epoch 7984: train loss: 0.1326496175290686, test loss: 0.25317382694985974\n",
      "epoch 7985: train loss: 0.13264482453593193, test loss: 0.2531731074810425\n",
      "epoch 7986: train loss: 0.13264003240145306, test loss: 0.25317238854639745\n",
      "epoch 7987: train loss: 0.13263524112536637, test loss: 0.253171670145734\n",
      "epoch 7988: train loss: 0.1326304507074061, test loss: 0.25317095227884817\n",
      "epoch 7989: train loss: 0.1326256611473069, test loss: 0.25317023494556196\n",
      "epoch 7990: train loss: 0.13262087244480325, test loss: 0.25316951814566685\n",
      "epoch 7991: train loss: 0.13261608459962998, test loss: 0.2531688018789676\n",
      "epoch 7992: train loss: 0.13261129761152188, test loss: 0.25316808614528286\n",
      "epoch 7993: train loss: 0.1326065114802139, test loss: 0.25316737094441655\n",
      "epoch 7994: train loss: 0.1326017262054412, test loss: 0.25316665627616247\n",
      "epoch 7995: train loss: 0.13259694178693887, test loss: 0.25316594214034416\n",
      "epoch 7996: train loss: 0.13259215822444226, test loss: 0.25316522853675155\n",
      "epoch 7997: train loss: 0.13258737551768676, test loss: 0.25316451546520363\n",
      "epoch 7998: train loss: 0.13258259366640793, test loss: 0.2531638029255058\n",
      "epoch 7999: train loss: 0.13257781267034144, test loss: 0.2531630909174582\n",
      "epoch 8000: train loss: 0.13257303252922295, test loss: 0.2531623794408761\n",
      "epoch 8001: train loss: 0.13256825324278845, test loss: 0.25316166849556\n",
      "epoch 8002: train loss: 0.13256347481077385, test loss: 0.2531609580813182\n",
      "epoch 8003: train loss: 0.13255869723291527, test loss: 0.25316024819796823\n",
      "epoch 8004: train loss: 0.13255392050894893, test loss: 0.25315953884529785\n",
      "epoch 8005: train loss: 0.13254914463861117, test loss: 0.2531588300231349\n",
      "epoch 8006: train loss: 0.13254436962163837, test loss: 0.25315812173127183\n",
      "epoch 8007: train loss: 0.13253959545776717, test loss: 0.2531574139695273\n",
      "epoch 8008: train loss: 0.13253482214673418, test loss: 0.25315670673770013\n",
      "epoch 8009: train loss: 0.13253004968827617, test loss: 0.2531560000356\n",
      "epoch 8010: train loss: 0.1325252780821301, test loss: 0.25315529386304214\n",
      "epoch 8011: train loss: 0.13252050732803294, test loss: 0.2531545882198267\n",
      "epoch 8012: train loss: 0.1325157374257218, test loss: 0.2531538831057655\n",
      "epoch 8013: train loss: 0.13251096837493392, test loss: 0.25315317852066227\n",
      "epoch 8014: train loss: 0.13250620017540665, test loss: 0.25315247446433253\n",
      "epoch 8015: train loss: 0.13250143282687749, test loss: 0.25315177093658475\n",
      "epoch 8016: train loss: 0.13249666632908394, test loss: 0.25315106793721326\n",
      "epoch 8017: train loss: 0.13249190068176372, test loss: 0.25315036546605024\n",
      "epoch 8018: train loss: 0.13248713588465463, test loss: 0.2531496635228846\n",
      "epoch 8019: train loss: 0.1324823719374946, test loss: 0.2531489621075357\n",
      "epoch 8020: train loss: 0.13247760884002163, test loss: 0.25314826121980766\n",
      "epoch 8021: train loss: 0.13247284659197386, test loss: 0.25314756085950774\n",
      "epoch 8022: train loss: 0.13246808519308956, test loss: 0.25314686102645106\n",
      "epoch 8023: train loss: 0.13246332464310706, test loss: 0.25314616172044374\n",
      "epoch 8024: train loss: 0.13245856494176486, test loss: 0.2531454629412988\n",
      "epoch 8025: train loss: 0.13245380608880153, test loss: 0.25314476468881625\n",
      "epoch 8026: train loss: 0.1324490480839558, test loss: 0.2531440669628154\n",
      "epoch 8027: train loss: 0.1324442909269664, test loss: 0.25314336976310614\n",
      "epoch 8028: train loss: 0.13243953461757238, test loss: 0.2531426730894891\n",
      "epoch 8029: train loss: 0.1324347791555127, test loss: 0.25314197694178053\n",
      "epoch 8030: train loss: 0.13243002454052646, test loss: 0.25314128131978486\n",
      "epoch 8031: train loss: 0.13242527077235303, test loss: 0.2531405862233223\n",
      "epoch 8032: train loss: 0.1324205178507317, test loss: 0.25313989165219675\n",
      "epoch 8033: train loss: 0.132415765775402, test loss: 0.2531391976062152\n",
      "epoch 8034: train loss: 0.1324110145461035, test loss: 0.25313850408519656\n",
      "epoch 8035: train loss: 0.13240626416257592, test loss: 0.25313781108894146\n",
      "epoch 8036: train loss: 0.132401514624559, test loss: 0.25313711861726307\n",
      "epoch 8037: train loss: 0.13239676593179286, test loss: 0.253136426669975\n",
      "epoch 8038: train loss: 0.13239201808401735, test loss: 0.25313573524689315\n",
      "epoch 8039: train loss: 0.13238727108097273, test loss: 0.25313504434781725\n",
      "epoch 8040: train loss: 0.13238252492239921, test loss: 0.2531343539725621\n",
      "epoch 8041: train loss: 0.13237777960803723, test loss: 0.25313366412093985\n",
      "epoch 8042: train loss: 0.13237303513762722, test loss: 0.25313297479276214\n",
      "epoch 8043: train loss: 0.13236829151090979, test loss: 0.25313228598784177\n",
      "epoch 8044: train loss: 0.13236354872762568, test loss: 0.2531315977059835\n",
      "epoch 8045: train loss: 0.13235880678751566, test loss: 0.2531309099470034\n",
      "epoch 8046: train loss: 0.13235406569032074, test loss: 0.2531302227107113\n",
      "epoch 8047: train loss: 0.13234932543578187, test loss: 0.25312953599692006\n",
      "epoch 8048: train loss: 0.1323445860236403, test loss: 0.253128849805438\n",
      "epoch 8049: train loss: 0.1323398474536372, test loss: 0.2531281641360861\n",
      "epoch 8050: train loss: 0.13233510972551402, test loss: 0.25312747898866017\n",
      "epoch 8051: train loss: 0.13233037283901222, test loss: 0.2531267943629908\n",
      "epoch 8052: train loss: 0.13232563679387338, test loss: 0.25312611025887394\n",
      "epoch 8053: train loss: 0.13232090158983925, test loss: 0.2531254266761335\n",
      "epoch 8054: train loss: 0.13231616722665163, test loss: 0.25312474361457027\n",
      "epoch 8055: train loss: 0.13231143370405243, test loss: 0.2531240610740052\n",
      "epoch 8056: train loss: 0.1323067010217837, test loss: 0.2531233790542535\n",
      "epoch 8057: train loss: 0.13230196917958764, test loss: 0.25312269755511513\n",
      "epoch 8058: train loss: 0.13229723817720646, test loss: 0.2531220165764194\n",
      "epoch 8059: train loss: 0.13229250801438253, test loss: 0.2531213361179643\n",
      "epoch 8060: train loss: 0.1322877786908583, test loss: 0.25312065617956564\n",
      "epoch 8061: train loss: 0.13228305020637646, test loss: 0.25311997676104003\n",
      "epoch 8062: train loss: 0.13227832256067962, test loss: 0.25311929786219955\n",
      "epoch 8063: train loss: 0.13227359575351064, test loss: 0.2531186194828519\n",
      "epoch 8064: train loss: 0.13226886978461241, test loss: 0.25311794162282303\n",
      "epoch 8065: train loss: 0.13226414465372802, test loss: 0.25311726428191456\n",
      "epoch 8066: train loss: 0.13225942036060057, test loss: 0.25311658745994126\n",
      "epoch 8067: train loss: 0.13225469690497327, test loss: 0.25311591115671633\n",
      "epoch 8068: train loss: 0.13224997428658958, test loss: 0.2531152353720614\n",
      "epoch 8069: train loss: 0.1322452525051929, test loss: 0.2531145601057797\n",
      "epoch 8070: train loss: 0.13224053156052684, test loss: 0.2531138853576923\n",
      "epoch 8071: train loss: 0.13223581145233504, test loss: 0.25311321112761254\n",
      "epoch 8072: train loss: 0.13223109218036136, test loss: 0.2531125374153447\n",
      "epoch 8073: train loss: 0.1322263737443497, test loss: 0.25311186422071313\n",
      "epoch 8074: train loss: 0.13222165614404402, test loss: 0.25311119154352935\n",
      "epoch 8075: train loss: 0.13221693937918855, test loss: 0.2531105193836025\n",
      "epoch 8076: train loss: 0.13221222344952743, test loss: 0.2531098477407552\n",
      "epoch 8077: train loss: 0.13220750835480508, test loss: 0.25310917661479837\n",
      "epoch 8078: train loss: 0.1322027940947659, test loss: 0.2531085060055458\n",
      "epoch 8079: train loss: 0.1321980806691545, test loss: 0.2531078359128084\n",
      "epoch 8080: train loss: 0.1321933680777155, test loss: 0.2531071663364032\n",
      "epoch 8081: train loss: 0.13218865632019372, test loss: 0.25310649727615014\n",
      "epoch 8082: train loss: 0.13218394539633407, test loss: 0.2531058287318569\n",
      "epoch 8083: train loss: 0.1321792353058815, test loss: 0.25310516070334255\n",
      "epoch 8084: train loss: 0.13217452604858118, test loss: 0.2531044931904238\n",
      "epoch 8085: train loss: 0.13216981762417826, test loss: 0.25310382619290583\n",
      "epoch 8086: train loss: 0.13216511003241813, test loss: 0.2531031597106154\n",
      "epoch 8087: train loss: 0.13216040327304618, test loss: 0.2531024937433657\n",
      "epoch 8088: train loss: 0.13215569734580795, test loss: 0.2531018282909644\n",
      "epoch 8089: train loss: 0.13215099225044913, test loss: 0.2531011633532335\n",
      "epoch 8090: train loss: 0.13214628798671546, test loss: 0.2531004989299914\n",
      "epoch 8091: train loss: 0.13214158455435282, test loss: 0.2530998350210437\n",
      "epoch 8092: train loss: 0.13213688195310716, test loss: 0.2530991716262132\n",
      "epoch 8093: train loss: 0.1321321801827246, test loss: 0.25309850874531487\n",
      "epoch 8094: train loss: 0.13212747924295132, test loss: 0.25309784637816163\n",
      "epoch 8095: train loss: 0.13212277913353365, test loss: 0.2530971845245781\n",
      "epoch 8096: train loss: 0.13211807985421792, test loss: 0.2530965231843661\n",
      "epoch 8097: train loss: 0.13211338140475073, test loss: 0.25309586235735676\n",
      "epoch 8098: train loss: 0.1321086837848787, test loss: 0.253095202043357\n",
      "epoch 8099: train loss: 0.13210398699434847, test loss: 0.2530945422421807\n",
      "epoch 8100: train loss: 0.13209929103290702, test loss: 0.25309388295366003\n",
      "epoch 8101: train loss: 0.1320945959003012, test loss: 0.2530932241775928\n",
      "epoch 8102: train loss: 0.13208990159627812, test loss: 0.25309256591380536\n",
      "epoch 8103: train loss: 0.1320852081205849, test loss: 0.25309190816211113\n",
      "epoch 8104: train loss: 0.13208051547296887, test loss: 0.25309125092233126\n",
      "epoch 8105: train loss: 0.13207582365317738, test loss: 0.25309059419427593\n",
      "epoch 8106: train loss: 0.13207113266095788, test loss: 0.2530899379777731\n",
      "epoch 8107: train loss: 0.132066442496058, test loss: 0.25308928227262983\n",
      "epoch 8108: train loss: 0.1320617531582255, test loss: 0.2530886270786612\n",
      "epoch 8109: train loss: 0.1320570646472081, test loss: 0.253087972395691\n",
      "epoch 8110: train loss: 0.13205237696275376, test loss: 0.2530873182235382\n",
      "epoch 8111: train loss: 0.13204769010461048, test loss: 0.25308666456201684\n",
      "epoch 8112: train loss: 0.13204300407252642, test loss: 0.25308601141094456\n",
      "epoch 8113: train loss: 0.13203831886624984, test loss: 0.2530853587701379\n",
      "epoch 8114: train loss: 0.13203363448552902, test loss: 0.25308470663942373\n",
      "epoch 8115: train loss: 0.13202895093011244, test loss: 0.25308405501860537\n",
      "epoch 8116: train loss: 0.13202426819974866, test loss: 0.2530834039075081\n",
      "epoch 8117: train loss: 0.13201958629418636, test loss: 0.25308275330594443\n",
      "epoch 8118: train loss: 0.13201490521317433, test loss: 0.2530821032137427\n",
      "epoch 8119: train loss: 0.13201022495646142, test loss: 0.2530814536307187\n",
      "epoch 8120: train loss: 0.13200554552379662, test loss: 0.2530808045566857\n",
      "epoch 8121: train loss: 0.132000866914929, test loss: 0.25308015599146294\n",
      "epoch 8122: train loss: 0.1319961891296078, test loss: 0.2530795079348629\n",
      "epoch 8123: train loss: 0.13199151216758231, test loss: 0.25307886038671934\n",
      "epoch 8124: train loss: 0.13198683602860195, test loss: 0.25307821334684183\n",
      "epoch 8125: train loss: 0.13198216071241625, test loss: 0.25307756681505017\n",
      "epoch 8126: train loss: 0.1319774862187748, test loss: 0.2530769207911647\n",
      "epoch 8127: train loss: 0.1319728125474274, test loss: 0.25307627527499893\n",
      "epoch 8128: train loss: 0.1319681396981238, test loss: 0.25307563026637675\n",
      "epoch 8129: train loss: 0.13196346767061398, test loss: 0.25307498576511167\n",
      "epoch 8130: train loss: 0.131958796464648, test loss: 0.2530743417710346\n",
      "epoch 8131: train loss: 0.13195412607997603, test loss: 0.25307369828395676\n",
      "epoch 8132: train loss: 0.1319494565163483, test loss: 0.25307305530369795\n",
      "epoch 8133: train loss: 0.13194478777351518, test loss: 0.25307241283007337\n",
      "epoch 8134: train loss: 0.13194011985122717, test loss: 0.2530717708629069\n",
      "epoch 8135: train loss: 0.13193545274923482, test loss: 0.25307112940201754\n",
      "epoch 8136: train loss: 0.1319307864672889, test loss: 0.25307048844723284\n",
      "epoch 8137: train loss: 0.1319261210051401, test loss: 0.2530698479983607\n",
      "epoch 8138: train loss: 0.13192145636253932, test loss: 0.2530692080552257\n",
      "epoch 8139: train loss: 0.1319167925392376, test loss: 0.253068568617648\n",
      "epoch 8140: train loss: 0.13191212953498607, test loss: 0.2530679296854521\n",
      "epoch 8141: train loss: 0.1319074673495359, test loss: 0.25306729125844607\n",
      "epoch 8142: train loss: 0.13190280598263843, test loss: 0.25306665333646294\n",
      "epoch 8143: train loss: 0.1318981454340451, test loss: 0.2530660159193151\n",
      "epoch 8144: train loss: 0.1318934857035074, test loss: 0.25306537900682763\n",
      "epoch 8145: train loss: 0.13188882679077701, test loss: 0.25306474259881395\n",
      "epoch 8146: train loss: 0.13188416869560562, test loss: 0.2530641066951066\n",
      "epoch 8147: train loss: 0.1318795114177451, test loss: 0.25306347129551116\n",
      "epoch 8148: train loss: 0.1318748549569474, test loss: 0.25306283639986515\n",
      "epoch 8149: train loss: 0.1318701993129646, test loss: 0.2530622020079736\n",
      "epoch 8150: train loss: 0.13186554448554882, test loss: 0.2530615681196743\n",
      "epoch 8151: train loss: 0.13186089047445237, test loss: 0.2530609347347708\n",
      "epoch 8152: train loss: 0.13185623727942758, test loss: 0.2530603018530918\n",
      "epoch 8153: train loss: 0.13185158490022694, test loss: 0.2530596694744594\n",
      "epoch 8154: train loss: 0.131846933336603, test loss: 0.2530590375986991\n",
      "epoch 8155: train loss: 0.1318422825883085, test loss: 0.2530584062256224\n",
      "epoch 8156: train loss: 0.1318376326550962, test loss: 0.2530577753550604\n",
      "epoch 8157: train loss: 0.131832983536719, test loss: 0.25305714498682713\n",
      "epoch 8158: train loss: 0.1318283352329299, test loss: 0.2530565151207458\n",
      "epoch 8159: train loss: 0.13182368774348197, test loss: 0.2530558857566416\n",
      "epoch 8160: train loss: 0.13181904106812847, test loss: 0.25305525689433594\n",
      "epoch 8161: train loss: 0.13181439520662266, test loss: 0.2530546285336465\n",
      "epoch 8162: train loss: 0.131809750158718, test loss: 0.2530540006744049\n",
      "epoch 8163: train loss: 0.13180510592416797, test loss: 0.25305337331641725\n",
      "epoch 8164: train loss: 0.13180046250272623, test loss: 0.2530527464595198\n",
      "epoch 8165: train loss: 0.1317958198941465, test loss: 0.25305212010352507\n",
      "epoch 8166: train loss: 0.13179117809818255, test loss: 0.25305149424826295\n",
      "epoch 8167: train loss: 0.13178653711458843, test loss: 0.25305086889355816\n",
      "epoch 8168: train loss: 0.1317818969431181, test loss: 0.25305024403922277\n",
      "epoch 8169: train loss: 0.13177725758352574, test loss: 0.25304961968508255\n",
      "epoch 8170: train loss: 0.13177261903556553, test loss: 0.2530489958309639\n",
      "epoch 8171: train loss: 0.13176798129899192, test loss: 0.2530483724766885\n",
      "epoch 8172: train loss: 0.1317633443735593, test loss: 0.25304774962208293\n",
      "epoch 8173: train loss: 0.13175870825902225, test loss: 0.2530471272669647\n",
      "epoch 8174: train loss: 0.13175407295513544, test loss: 0.25304650541115487\n",
      "epoch 8175: train loss: 0.13174943846165363, test loss: 0.25304588405448397\n",
      "epoch 8176: train loss: 0.13174480477833167, test loss: 0.2530452631967689\n",
      "epoch 8177: train loss: 0.13174017190492454, test loss: 0.2530446428378361\n",
      "epoch 8178: train loss: 0.13173553984118733, test loss: 0.2530440229775058\n",
      "epoch 8179: train loss: 0.13173090858687522, test loss: 0.2530434036156119\n",
      "epoch 8180: train loss: 0.1317262781417435, test loss: 0.2530427847519619\n",
      "epoch 8181: train loss: 0.13172164850554752, test loss: 0.253042166386386\n",
      "epoch 8182: train loss: 0.1317170196780428, test loss: 0.2530415485187179\n",
      "epoch 8183: train loss: 0.1317123916589849, test loss: 0.2530409311487655\n",
      "epoch 8184: train loss: 0.1317077644481296, test loss: 0.2530403142763603\n",
      "epoch 8185: train loss: 0.1317031380452326, test loss: 0.25303969790132896\n",
      "epoch 8186: train loss: 0.13169851245004985, test loss: 0.2530390820234937\n",
      "epoch 8187: train loss: 0.13169388766233733, test loss: 0.2530384666426749\n",
      "epoch 8188: train loss: 0.13168926368185116, test loss: 0.2530378517587082\n",
      "epoch 8189: train loss: 0.13168464050834758, test loss: 0.2530372373713989\n",
      "epoch 8190: train loss: 0.13168001814158284, test loss: 0.25303662348058475\n",
      "epoch 8191: train loss: 0.1316753965813134, test loss: 0.25303601008609056\n",
      "epoch 8192: train loss: 0.13167077582729578, test loss: 0.25303539718773516\n",
      "epoch 8193: train loss: 0.13166615587928657, test loss: 0.2530347847853481\n",
      "epoch 8194: train loss: 0.1316615367370425, test loss: 0.25303417287875507\n",
      "epoch 8195: train loss: 0.13165691840032043, test loss: 0.2530335614677707\n",
      "epoch 8196: train loss: 0.13165230086887725, test loss: 0.2530329505522318\n",
      "epoch 8197: train loss: 0.13164768414247, test loss: 0.2530323401319566\n",
      "epoch 8198: train loss: 0.1316430682208558, test loss: 0.2530317302067761\n",
      "epoch 8199: train loss: 0.13163845310379194, test loss: 0.2530311207765116\n",
      "epoch 8200: train loss: 0.13163383879103568, test loss: 0.2530305118409873\n",
      "epoch 8201: train loss: 0.1316292252823445, test loss: 0.25302990340003195\n",
      "epoch 8202: train loss: 0.13162461257747593, test loss: 0.25302929545346325\n",
      "epoch 8203: train loss: 0.13162000067618762, test loss: 0.2530286880011174\n",
      "epoch 8204: train loss: 0.13161538957823735, test loss: 0.25302808104281155\n",
      "epoch 8205: train loss: 0.13161077928338288, test loss: 0.25302747457837826\n",
      "epoch 8206: train loss: 0.13160616979138223, test loss: 0.2530268686076426\n",
      "epoch 8207: train loss: 0.13160156110199342, test loss: 0.2530262631304241\n",
      "epoch 8208: train loss: 0.13159695321497458, test loss: 0.2530256581465514\n",
      "epoch 8209: train loss: 0.13159234613008405, test loss: 0.25302505365585787\n",
      "epoch 8210: train loss: 0.13158773984708008, test loss: 0.2530244496581545\n",
      "epoch 8211: train loss: 0.1315831343657212, test loss: 0.25302384615328477\n",
      "epoch 8212: train loss: 0.13157852968576594, test loss: 0.2530232431410653\n",
      "epoch 8213: train loss: 0.13157392580697294, test loss: 0.2530226406213225\n",
      "epoch 8214: train loss: 0.13156932272910102, test loss: 0.25302203859388755\n",
      "epoch 8215: train loss: 0.13156472045190898, test loss: 0.25302143705857705\n",
      "epoch 8216: train loss: 0.1315601189751558, test loss: 0.25302083601523134\n",
      "epoch 8217: train loss: 0.13155551829860057, test loss: 0.2530202354636687\n",
      "epoch 8218: train loss: 0.13155091842200242, test loss: 0.2530196354037189\n",
      "epoch 8219: train loss: 0.13154631934512065, test loss: 0.2530190358352096\n",
      "epoch 8220: train loss: 0.13154172106771467, test loss: 0.2530184367579571\n",
      "epoch 8221: train loss: 0.1315371235895438, test loss: 0.2530178381718004\n",
      "epoch 8222: train loss: 0.13153252691036774, test loss: 0.25301724007657056\n",
      "epoch 8223: train loss: 0.13152793102994612, test loss: 0.2530166424720789\n",
      "epoch 8224: train loss: 0.13152333594803875, test loss: 0.25301604535816286\n",
      "epoch 8225: train loss: 0.13151874166440541, test loss: 0.25301544873465676\n",
      "epoch 8226: train loss: 0.1315141481788062, test loss: 0.253014852601376\n",
      "epoch 8227: train loss: 0.13150955549100107, test loss: 0.25301425695815083\n",
      "epoch 8228: train loss: 0.13150496360075029, test loss: 0.25301366180481066\n",
      "epoch 8229: train loss: 0.1315003725078141, test loss: 0.25301306714117905\n",
      "epoch 8230: train loss: 0.13149578221195285, test loss: 0.2530124729670926\n",
      "epoch 8231: train loss: 0.13149119271292706, test loss: 0.25301187928237445\n",
      "epoch 8232: train loss: 0.13148660401049725, test loss: 0.25301128608684975\n",
      "epoch 8233: train loss: 0.13148201610442417, test loss: 0.2530106933803509\n",
      "epoch 8234: train loss: 0.13147742899446857, test loss: 0.2530101011627026\n",
      "epoch 8235: train loss: 0.13147284268039128, test loss: 0.25300950943374206\n",
      "epoch 8236: train loss: 0.13146825716195334, test loss: 0.2530089181932824\n",
      "epoch 8237: train loss: 0.1314636724389158, test loss: 0.25300832744116675\n",
      "epoch 8238: train loss: 0.13145908851103982, test loss: 0.25300773717720754\n",
      "epoch 8239: train loss: 0.13145450537808673, test loss: 0.25300714740125374\n",
      "epoch 8240: train loss: 0.13144992303981787, test loss: 0.2530065581131127\n",
      "epoch 8241: train loss: 0.1314453414959947, test loss: 0.2530059693126337\n",
      "epoch 8242: train loss: 0.13144076074637887, test loss: 0.25300538099963327\n",
      "epoch 8243: train loss: 0.131436180790732, test loss: 0.2530047931739379\n",
      "epoch 8244: train loss: 0.13143160162881584, test loss: 0.2530042058353852\n",
      "epoch 8245: train loss: 0.13142702326039235, test loss: 0.25300361898380175\n",
      "epoch 8246: train loss: 0.13142244568522346, test loss: 0.2530030326190144\n",
      "epoch 8247: train loss: 0.13141786890307122, test loss: 0.2530024467408567\n",
      "epoch 8248: train loss: 0.13141329291369785, test loss: 0.25300186134914776\n",
      "epoch 8249: train loss: 0.1314087177168656, test loss: 0.25300127644372855\n",
      "epoch 8250: train loss: 0.13140414331233688, test loss: 0.2530006920244232\n",
      "epoch 8251: train loss: 0.13139956969987412, test loss: 0.2530001080910572\n",
      "epoch 8252: train loss: 0.13139499687923994, test loss: 0.2529995246434734\n",
      "epoch 8253: train loss: 0.13139042485019697, test loss: 0.2529989416814915\n",
      "epoch 8254: train loss: 0.131385853612508, test loss: 0.2529983592049439\n",
      "epoch 8255: train loss: 0.1313812831659359, test loss: 0.2529977772136551\n",
      "epoch 8256: train loss: 0.13137671351024363, test loss: 0.2529971957074632\n",
      "epoch 8257: train loss: 0.1313721446451943, test loss: 0.2529966146861963\n",
      "epoch 8258: train loss: 0.13136757657055104, test loss: 0.2529960341496816\n",
      "epoch 8259: train loss: 0.1313630092860771, test loss: 0.2529954540977514\n",
      "epoch 8260: train loss: 0.1313584427915359, test loss: 0.25299487453023384\n",
      "epoch 8261: train loss: 0.13135387708669086, test loss: 0.2529942954469628\n",
      "epoch 8262: train loss: 0.13134931217130558, test loss: 0.25299371684777255\n",
      "epoch 8263: train loss: 0.13134474804514373, test loss: 0.2529931387324826\n",
      "epoch 8264: train loss: 0.131340184707969, test loss: 0.2529925611009286\n",
      "epoch 8265: train loss: 0.1313356221595453, test loss: 0.2529919839529445\n",
      "epoch 8266: train loss: 0.13133106039963663, test loss: 0.252991407288358\n",
      "epoch 8267: train loss: 0.13132649942800698, test loss: 0.25299083110699994\n",
      "epoch 8268: train loss: 0.1313219392444205, test loss: 0.25299025540870335\n",
      "epoch 8269: train loss: 0.13131737984864153, test loss: 0.25298968019330137\n",
      "epoch 8270: train loss: 0.1313128212404343, test loss: 0.25298910546061615\n",
      "epoch 8271: train loss: 0.1313082634195634, test loss: 0.25298853121048687\n",
      "epoch 8272: train loss: 0.13130370638579328, test loss: 0.25298795744274827\n",
      "epoch 8273: train loss: 0.1312991501388886, test loss: 0.25298738415721195\n",
      "epoch 8274: train loss: 0.13129459467861412, test loss: 0.2529868113537376\n",
      "epoch 8275: train loss: 0.1312900400047347, test loss: 0.2529862390321402\n",
      "epoch 8276: train loss: 0.13128548611701527, test loss: 0.25298566719225307\n",
      "epoch 8277: train loss: 0.13128093301522084, test loss: 0.2529850958339006\n",
      "epoch 8278: train loss: 0.13127638069911662, test loss: 0.25298452495693624\n",
      "epoch 8279: train loss: 0.13127182916846777, test loss: 0.25298395456116624\n",
      "epoch 8280: train loss: 0.13126727842303965, test loss: 0.25298338464644293\n",
      "epoch 8281: train loss: 0.1312627284625977, test loss: 0.2529828152125869\n",
      "epoch 8282: train loss: 0.13125817928690747, test loss: 0.25298224625943705\n",
      "epoch 8283: train loss: 0.13125363089573455, test loss: 0.25298167778682046\n",
      "epoch 8284: train loss: 0.13124908328884466, test loss: 0.2529811097945766\n",
      "epoch 8285: train loss: 0.13124453646600365, test loss: 0.25298054228252626\n",
      "epoch 8286: train loss: 0.13123999042697743, test loss: 0.252979975250506\n",
      "epoch 8287: train loss: 0.13123544517153202, test loss: 0.2529794086983565\n",
      "epoch 8288: train loss: 0.13123090069943352, test loss: 0.25297884262590403\n",
      "epoch 8289: train loss: 0.13122635701044816, test loss: 0.25297827703298004\n",
      "epoch 8290: train loss: 0.13122181410434228, test loss: 0.25297771191941576\n",
      "epoch 8291: train loss: 0.13121727198088215, test loss: 0.25297714728505727\n",
      "epoch 8292: train loss: 0.13121273063983446, test loss: 0.2529765831297206\n",
      "epoch 8293: train loss: 0.1312081900809657, test loss: 0.2529760194532424\n",
      "epoch 8294: train loss: 0.1312036503040426, test loss: 0.2529754562554674\n",
      "epoch 8295: train loss: 0.1311991113088319, test loss: 0.25297489353621666\n",
      "epoch 8296: train loss: 0.13119457309510063, test loss: 0.2529743312953288\n",
      "epoch 8297: train loss: 0.13119003566261564, test loss: 0.25297376953263595\n",
      "epoch 8298: train loss: 0.13118549901114407, test loss: 0.25297320824797187\n",
      "epoch 8299: train loss: 0.13118096314045313, test loss: 0.25297264744116565\n",
      "epoch 8300: train loss: 0.13117642805031005, test loss: 0.25297208711206226\n",
      "epoch 8301: train loss: 0.13117189374048221, test loss: 0.2529715272604841\n",
      "epoch 8302: train loss: 0.1311673602107371, test loss: 0.2529709678862671\n",
      "epoch 8303: train loss: 0.13116282746084232, test loss: 0.25297040898924955\n",
      "epoch 8304: train loss: 0.13115829549056549, test loss: 0.25296985056925686\n",
      "epoch 8305: train loss: 0.1311537642996744, test loss: 0.25296929262613627\n",
      "epoch 8306: train loss: 0.13114923388793687, test loss: 0.2529687351597121\n",
      "epoch 8307: train loss: 0.13114470425512093, test loss: 0.25296817816982187\n",
      "epoch 8308: train loss: 0.13114017540099457, test loss: 0.2529676216562976\n",
      "epoch 8309: train loss: 0.13113564732532598, test loss: 0.2529670656189688\n",
      "epoch 8310: train loss: 0.13113112002788338, test loss: 0.2529665100576831\n",
      "epoch 8311: train loss: 0.1311265935084351, test loss: 0.2529659549722728\n",
      "epoch 8312: train loss: 0.1311220677667496, test loss: 0.2529654003625614\n",
      "epoch 8313: train loss: 0.13111754280259544, test loss: 0.2529648462283854\n",
      "epoch 8314: train loss: 0.13111301861574118, test loss: 0.25296429256959263\n",
      "epoch 8315: train loss: 0.1311084952059556, test loss: 0.2529637393860022\n",
      "epoch 8316: train loss: 0.13110397257300752, test loss: 0.25296318667746276\n",
      "epoch 8317: train loss: 0.13109945071666584, test loss: 0.252962634443797\n",
      "epoch 8318: train loss: 0.13109492963669958, test loss: 0.25296208268484577\n",
      "epoch 8319: train loss: 0.13109040933287783, test loss: 0.2529615314004477\n",
      "epoch 8320: train loss: 0.13108588980496982, test loss: 0.2529609805904258\n",
      "epoch 8321: train loss: 0.1310813710527449, test loss: 0.2529604302546321\n",
      "epoch 8322: train loss: 0.13107685307597236, test loss: 0.2529598803928905\n",
      "epoch 8323: train loss: 0.13107233587442177, test loss: 0.25295933100504\n",
      "epoch 8324: train loss: 0.1310678194478627, test loss: 0.2529587820909138\n",
      "epoch 8325: train loss: 0.13106330379606482, test loss: 0.2529582336503507\n",
      "epoch 8326: train loss: 0.1310587889187979, test loss: 0.2529576856831868\n",
      "epoch 8327: train loss: 0.13105427481583187, test loss: 0.25295713818924875\n",
      "epoch 8328: train loss: 0.13104976148693667, test loss: 0.25295659116838676\n",
      "epoch 8329: train loss: 0.13104524893188238, test loss: 0.252956044620428\n",
      "epoch 8330: train loss: 0.1310407371504391, test loss: 0.2529554985452095\n",
      "epoch 8331: train loss: 0.13103622614237712, test loss: 0.2529549529425724\n",
      "epoch 8332: train loss: 0.13103171590746684, test loss: 0.25295440781234285\n",
      "epoch 8333: train loss: 0.13102720644547863, test loss: 0.25295386315436635\n",
      "epoch 8334: train loss: 0.1310226977561831, test loss: 0.2529533189684733\n",
      "epoch 8335: train loss: 0.13101818983935087, test loss: 0.2529527752545049\n",
      "epoch 8336: train loss: 0.1310136826947526, test loss: 0.2529522320122939\n",
      "epoch 8337: train loss: 0.13100917632215922, test loss: 0.2529516892416782\n",
      "epoch 8338: train loss: 0.1310046707213416, test loss: 0.25295114694249315\n",
      "epoch 8339: train loss: 0.13100016589207072, test loss: 0.2529506051145758\n",
      "epoch 8340: train loss: 0.1309956618341178, test loss: 0.2529500637577617\n",
      "epoch 8341: train loss: 0.13099115854725393, test loss: 0.25294952287189554\n",
      "epoch 8342: train loss: 0.13098665603125048, test loss: 0.2529489824568001\n",
      "epoch 8343: train loss: 0.1309821542858788, test loss: 0.2529484425123255\n",
      "epoch 8344: train loss: 0.13097765331091046, test loss: 0.2529479030383026\n",
      "epoch 8345: train loss: 0.13097315310611693, test loss: 0.25294736403457574\n",
      "epoch 8346: train loss: 0.13096865367126997, test loss: 0.2529468255009749\n",
      "epoch 8347: train loss: 0.13096415500614136, test loss: 0.2529462874373373\n",
      "epoch 8348: train loss: 0.1309596571105029, test loss: 0.2529457498434985\n",
      "epoch 8349: train loss: 0.1309551599841266, test loss: 0.25294521271930104\n",
      "epoch 8350: train loss: 0.1309506636267845, test loss: 0.2529446760645843\n",
      "epoch 8351: train loss: 0.13094616803824877, test loss: 0.25294413987918174\n",
      "epoch 8352: train loss: 0.13094167321829164, test loss: 0.2529436041629316\n",
      "epoch 8353: train loss: 0.13093717916668546, test loss: 0.2529430689156688\n",
      "epoch 8354: train loss: 0.13093268588320264, test loss: 0.2529425341372385\n",
      "epoch 8355: train loss: 0.13092819336761574, test loss: 0.25294199982746923\n",
      "epoch 8356: train loss: 0.13092370161969735, test loss: 0.25294146598620904\n",
      "epoch 8357: train loss: 0.13091921063922018, test loss: 0.2529409326132901\n",
      "epoch 8358: train loss: 0.1309147204259571, test loss: 0.2529403997085539\n",
      "epoch 8359: train loss: 0.13091023097968094, test loss: 0.2529398672718296\n",
      "epoch 8360: train loss: 0.13090574230016475, test loss: 0.2529393353029731\n",
      "epoch 8361: train loss: 0.1309012543871816, test loss: 0.25293880380180345\n",
      "epoch 8362: train loss: 0.1308967672405046, test loss: 0.2529382727681763\n",
      "epoch 8363: train loss: 0.13089228085990715, test loss: 0.25293774220191795\n",
      "epoch 8364: train loss: 0.13088779524516256, test loss: 0.25293721210287\n",
      "epoch 8365: train loss: 0.1308833103960443, test loss: 0.2529366824708727\n",
      "epoch 8366: train loss: 0.13087882631232595, test loss: 0.25293615330576336\n",
      "epoch 8367: train loss: 0.13087434299378112, test loss: 0.25293562460738644\n",
      "epoch 8368: train loss: 0.13086986044018356, test loss: 0.25293509637557304\n",
      "epoch 8369: train loss: 0.13086537865130715, test loss: 0.25293456861016644\n",
      "epoch 8370: train loss: 0.1308608976269258, test loss: 0.25293404131100006\n",
      "epoch 8371: train loss: 0.13085641736681353, test loss: 0.25293351447792317\n",
      "epoch 8372: train loss: 0.13085193787074448, test loss: 0.25293298811076725\n",
      "epoch 8373: train loss: 0.13084745913849283, test loss: 0.2529324622093786\n",
      "epoch 8374: train loss: 0.13084298116983287, test loss: 0.2529319367735851\n",
      "epoch 8375: train loss: 0.13083850396453903, test loss: 0.2529314118032436\n",
      "epoch 8376: train loss: 0.13083402752238582, test loss: 0.2529308872981785\n",
      "epoch 8377: train loss: 0.13082955184314776, test loss: 0.2529303632582304\n",
      "epoch 8378: train loss: 0.1308250769265996, test loss: 0.25292983968324684\n",
      "epoch 8379: train loss: 0.13082060277251606, test loss: 0.25292931657306217\n",
      "epoch 8380: train loss: 0.130816129380672, test loss: 0.2529287939275224\n",
      "epoch 8381: train loss: 0.1308116567508424, test loss: 0.25292827174645294\n",
      "epoch 8382: train loss: 0.1308071848828023, test loss: 0.25292775002971774\n",
      "epoch 8383: train loss: 0.1308027137763268, test loss: 0.25292722877713053\n",
      "epoch 8384: train loss: 0.1307982434311912, test loss: 0.25292670798855243\n",
      "epoch 8385: train loss: 0.1307937738471708, test loss: 0.2529261876638133\n",
      "epoch 8386: train loss: 0.130789305024041, test loss: 0.25292566780275416\n",
      "epoch 8387: train loss: 0.13078483696157733, test loss: 0.2529251484052166\n",
      "epoch 8388: train loss: 0.13078036965955536, test loss: 0.25292462947104094\n",
      "epoch 8389: train loss: 0.13077590311775084, test loss: 0.25292411100007095\n",
      "epoch 8390: train loss: 0.13077143733593954, test loss: 0.25292359299214373\n",
      "epoch 8391: train loss: 0.13076697231389725, test loss: 0.2529230754470973\n",
      "epoch 8392: train loss: 0.13076250805140008, test loss: 0.2529225583647788\n",
      "epoch 8393: train loss: 0.13075804454822404, test loss: 0.2529220417450226\n",
      "epoch 8394: train loss: 0.13075358180414523, test loss: 0.25292152558768016\n",
      "epoch 8395: train loss: 0.13074911981894, test loss: 0.2529210098925829\n",
      "epoch 8396: train loss: 0.13074465859238457, test loss: 0.25292049465957384\n",
      "epoch 8397: train loss: 0.1307401981242555, test loss: 0.25291997988849724\n",
      "epoch 8398: train loss: 0.13073573841432923, test loss: 0.2529194655791854\n",
      "epoch 8399: train loss: 0.1307312794623824, test loss: 0.25291895173149187\n",
      "epoch 8400: train loss: 0.13072682126819168, test loss: 0.2529184383452505\n",
      "epoch 8401: train loss: 0.13072236383153396, test loss: 0.2529179254203082\n",
      "epoch 8402: train loss: 0.13071790715218604, test loss: 0.2529174129565012\n",
      "epoch 8403: train loss: 0.13071345122992495, test loss: 0.25291690095367064\n",
      "epoch 8404: train loss: 0.13070899606452774, test loss: 0.2529163894116632\n",
      "epoch 8405: train loss: 0.13070454165577158, test loss: 0.2529158783303142\n",
      "epoch 8406: train loss: 0.13070008800343377, test loss: 0.2529153677094764\n",
      "epoch 8407: train loss: 0.1306956351072916, test loss: 0.25291485754897797\n",
      "epoch 8408: train loss: 0.13069118296712257, test loss: 0.2529143478486739\n",
      "epoch 8409: train loss: 0.13068673158270414, test loss: 0.25291383860839767\n",
      "epoch 8410: train loss: 0.130682280953814, test loss: 0.25291332982799014\n",
      "epoch 8411: train loss: 0.13067783108022982, test loss: 0.25291282150730127\n",
      "epoch 8412: train loss: 0.13067338196172945, test loss: 0.2529123136461669\n",
      "epoch 8413: train loss: 0.13066893359809073, test loss: 0.2529118062444369\n",
      "epoch 8414: train loss: 0.1306644859890917, test loss: 0.25291129930194606\n",
      "epoch 8415: train loss: 0.13066003913451038, test loss: 0.252910792818532\n",
      "epoch 8416: train loss: 0.130655593034125, test loss: 0.25291028679405403\n",
      "epoch 8417: train loss: 0.13065114768771383, test loss: 0.25290978122834157\n",
      "epoch 8418: train loss: 0.13064670309505516, test loss: 0.2529092761212417\n",
      "epoch 8419: train loss: 0.13064225925592748, test loss: 0.2529087714726012\n",
      "epoch 8420: train loss: 0.1306378161701093, test loss: 0.2529082672822559\n",
      "epoch 8421: train loss: 0.13063337383737925, test loss: 0.25290776355005046\n",
      "epoch 8422: train loss: 0.13062893225751604, test loss: 0.25290726027583205\n",
      "epoch 8423: train loss: 0.13062449143029853, test loss: 0.25290675745944247\n",
      "epoch 8424: train loss: 0.13062005135550553, test loss: 0.25290625510071846\n",
      "epoch 8425: train loss: 0.13061561203291608, test loss: 0.2529057531995059\n",
      "epoch 8426: train loss: 0.13061117346230927, test loss: 0.2529052517556549\n",
      "epoch 8427: train loss: 0.13060673564346423, test loss: 0.25290475076900487\n",
      "epoch 8428: train loss: 0.13060229857616026, test loss: 0.2529042502394027\n",
      "epoch 8429: train loss: 0.13059786226017667, test loss: 0.2529037501666818\n",
      "epoch 8430: train loss: 0.13059342669529295, test loss: 0.2529032505506948\n",
      "epoch 8431: train loss: 0.13058899188128853, test loss: 0.25290275139128293\n",
      "epoch 8432: train loss: 0.13058455781794318, test loss: 0.25290225268829275\n",
      "epoch 8433: train loss: 0.13058012450503648, test loss: 0.2529017544415616\n",
      "epoch 8434: train loss: 0.1305756919423483, test loss: 0.252901256650936\n",
      "epoch 8435: train loss: 0.1305712601296585, test loss: 0.25290075931626443\n",
      "epoch 8436: train loss: 0.1305668290667471, test loss: 0.2529002624373837\n",
      "epoch 8437: train loss: 0.13056239875339415, test loss: 0.25289976601414804\n",
      "epoch 8438: train loss: 0.1305579691893798, test loss: 0.2528992700463927\n",
      "epoch 8439: train loss: 0.1305535403744843, test loss: 0.25289877453396065\n",
      "epoch 8440: train loss: 0.130549112308488, test loss: 0.25289827947670723\n",
      "epoch 8441: train loss: 0.1305446849911714, test loss: 0.2528977848744642\n",
      "epoch 8442: train loss: 0.1305402584223149, test loss: 0.2528972907270884\n",
      "epoch 8443: train loss: 0.13053583260169915, test loss: 0.25289679703441625\n",
      "epoch 8444: train loss: 0.13053140752910491, test loss: 0.25289630379629185\n",
      "epoch 8445: train loss: 0.13052698320431289, test loss: 0.2528958110125633\n",
      "epoch 8446: train loss: 0.13052255962710405, test loss: 0.2528953186830774\n",
      "epoch 8447: train loss: 0.1305181367972593, test loss: 0.252894826807674\n",
      "epoch 8448: train loss: 0.13051371471455972, test loss: 0.25289433538620043\n",
      "epoch 8449: train loss: 0.13050929337878647, test loss: 0.2528938444185044\n",
      "epoch 8450: train loss: 0.13050487278972075, test loss: 0.2528933539044182\n",
      "epoch 8451: train loss: 0.13050045294714394, test loss: 0.2528928638438091\n",
      "epoch 8452: train loss: 0.13049603385083738, test loss: 0.25289237423650723\n",
      "epoch 8453: train loss: 0.13049161550058266, test loss: 0.2528918850823607\n",
      "epoch 8454: train loss: 0.13048719789616134, test loss: 0.25289139638121516\n",
      "epoch 8455: train loss: 0.1304827810373551, test loss: 0.2528909081329119\n",
      "epoch 8456: train loss: 0.13047836492394574, test loss: 0.25289042033730613\n",
      "epoch 8457: train loss: 0.1304739495557151, test loss: 0.25288993299423596\n",
      "epoch 8458: train loss: 0.1304695349324451, test loss: 0.25288944610355185\n",
      "epoch 8459: train loss: 0.13046512105391783, test loss: 0.25288895966509434\n",
      "epoch 8460: train loss: 0.13046070791991543, test loss: 0.2528884736787158\n",
      "epoch 8461: train loss: 0.1304562955302201, test loss: 0.2528879881442535\n",
      "epoch 8462: train loss: 0.13045188388461412, test loss: 0.25288750306156804\n",
      "epoch 8463: train loss: 0.13044747298287995, test loss: 0.25288701843048267\n",
      "epoch 8464: train loss: 0.1304430628248, test loss: 0.25288653425086094\n",
      "epoch 8465: train loss: 0.1304386534101569, test loss: 0.25288605052255125\n",
      "epoch 8466: train loss: 0.13043424473873327, test loss: 0.2528855672453939\n",
      "epoch 8467: train loss: 0.13042983681031192, test loss: 0.2528850844192257\n",
      "epoch 8468: train loss: 0.13042542962467563, test loss: 0.2528846020439083\n",
      "epoch 8469: train loss: 0.13042102318160737, test loss: 0.25288412011928246\n",
      "epoch 8470: train loss: 0.13041661748089012, test loss: 0.2528836386451958\n",
      "epoch 8471: train loss: 0.13041221252230703, test loss: 0.2528831576214829\n",
      "epoch 8472: train loss: 0.1304078083056413, test loss: 0.25288267704801576\n",
      "epoch 8473: train loss: 0.13040340483067614, test loss: 0.2528821969246191\n",
      "epoch 8474: train loss: 0.130399002097195, test loss: 0.25288171725114994\n",
      "epoch 8475: train loss: 0.13039460010498127, test loss: 0.2528812380274483\n",
      "epoch 8476: train loss: 0.13039019885381858, test loss: 0.2528807592533719\n",
      "epoch 8477: train loss: 0.1303857983434905, test loss: 0.25288028092875203\n",
      "epoch 8478: train loss: 0.13038139857378078, test loss: 0.252879803053455\n",
      "epoch 8479: train loss: 0.1303769995444732, test loss: 0.2528793256273195\n",
      "epoch 8480: train loss: 0.13037260125535172, test loss: 0.25287884865018684\n",
      "epoch 8481: train loss: 0.13036820370620028, test loss: 0.2528783721219073\n",
      "epoch 8482: train loss: 0.130363806896803, test loss: 0.25287789604233185\n",
      "epoch 8483: train loss: 0.130359410826944, test loss: 0.25287742041130784\n",
      "epoch 8484: train loss: 0.13035501549640752, test loss: 0.2528769452286766\n",
      "epoch 8485: train loss: 0.13035062090497795, test loss: 0.25287647049429884\n",
      "epoch 8486: train loss: 0.1303462270524397, test loss: 0.25287599620801304\n",
      "epoch 8487: train loss: 0.13034183393857726, test loss: 0.25287552236966276\n",
      "epoch 8488: train loss: 0.13033744156317525, test loss: 0.2528750489791046\n",
      "epoch 8489: train loss: 0.1303330499260184, test loss: 0.25287457603618085\n",
      "epoch 8490: train loss: 0.13032865902689145, test loss: 0.25287410354074824\n",
      "epoch 8491: train loss: 0.13032426886557924, test loss: 0.25287363149264147\n",
      "epoch 8492: train loss: 0.1303198794418668, test loss: 0.2528731598917156\n",
      "epoch 8493: train loss: 0.13031549075553905, test loss: 0.2528726887378225\n",
      "epoch 8494: train loss: 0.13031110280638125, test loss: 0.2528722180308076\n",
      "epoch 8495: train loss: 0.1303067155941785, test loss: 0.2528717477705197\n",
      "epoch 8496: train loss: 0.13030232911871623, test loss: 0.2528712779568015\n",
      "epoch 8497: train loss: 0.1302979433797797, test loss: 0.2528708085895142\n",
      "epoch 8498: train loss: 0.13029355837715445, test loss: 0.2528703396684922\n",
      "epoch 8499: train loss: 0.13028917411062607, test loss: 0.25286987119359183\n",
      "epoch 8500: train loss: 0.1302847905799802, test loss: 0.2528694031646596\n",
      "epoch 8501: train loss: 0.13028040778500255, test loss: 0.2528689355815409\n",
      "epoch 8502: train loss: 0.13027602572547894, test loss: 0.25286846844409505\n",
      "epoch 8503: train loss: 0.13027164440119532, test loss: 0.25286800175215995\n",
      "epoch 8504: train loss: 0.13026726381193765, test loss: 0.2528675355056002\n",
      "epoch 8505: train loss: 0.13026288395749203, test loss: 0.2528670697042472\n",
      "epoch 8506: train loss: 0.13025850483764465, test loss: 0.25286660434795605\n",
      "epoch 8507: train loss: 0.1302541264521818, test loss: 0.25286613943657477\n",
      "epoch 8508: train loss: 0.13024974880088974, test loss: 0.2528656749699622\n",
      "epoch 8509: train loss: 0.13024537188355498, test loss: 0.25286521094794695\n",
      "epoch 8510: train loss: 0.130240995699964, test loss: 0.2528647473704047\n",
      "epoch 8511: train loss: 0.13023662024990343, test loss: 0.25286428423717083\n",
      "epoch 8512: train loss: 0.13023224553315996, test loss: 0.25286382154809445\n",
      "epoch 8513: train loss: 0.13022787154952037, test loss: 0.2528633593030271\n",
      "epoch 8514: train loss: 0.1302234982987715, test loss: 0.252862897501822\n",
      "epoch 8515: train loss: 0.13021912578070036, test loss: 0.2528624361443142\n",
      "epoch 8516: train loss: 0.13021475399509394, test loss: 0.2528619752303742\n",
      "epoch 8517: train loss: 0.13021038294173937, test loss: 0.2528615147598417\n",
      "epoch 8518: train loss: 0.13020601262042392, test loss: 0.2528610547325675\n",
      "epoch 8519: train loss: 0.13020164303093482, test loss: 0.252860595148404\n",
      "epoch 8520: train loss: 0.13019727417305949, test loss: 0.2528601360071953\n",
      "epoch 8521: train loss: 0.13019290604658537, test loss: 0.2528596773087958\n",
      "epoch 8522: train loss: 0.13018853865130006, test loss: 0.2528592190530556\n",
      "epoch 8523: train loss: 0.1301841719869912, test loss: 0.2528587612398244\n",
      "epoch 8524: train loss: 0.13017980605344648, test loss: 0.25285830386894864\n",
      "epoch 8525: train loss: 0.13017544085045374, test loss: 0.25285784694029667\n",
      "epoch 8526: train loss: 0.13017107637780087, test loss: 0.25285739045369715\n",
      "epoch 8527: train loss: 0.13016671263527588, test loss: 0.2528569344090087\n",
      "epoch 8528: train loss: 0.13016234962266682, test loss: 0.2528564788060808\n",
      "epoch 8529: train loss: 0.13015798733976192, test loss: 0.2528560236447721\n",
      "epoch 8530: train loss: 0.1301536257863493, test loss: 0.25285556892492544\n",
      "epoch 8531: train loss: 0.13014926496221738, test loss: 0.25285511464639243\n",
      "epoch 8532: train loss: 0.13014490486715455, test loss: 0.25285466080902397\n",
      "epoch 8533: train loss: 0.13014054550094933, test loss: 0.25285420741267395\n",
      "epoch 8534: train loss: 0.13013618686339032, test loss: 0.25285375445718733\n",
      "epoch 8535: train loss: 0.13013182895426614, test loss: 0.25285330194243044\n",
      "epoch 8536: train loss: 0.1301274717733656, test loss: 0.2528528498682347\n",
      "epoch 8537: train loss: 0.1301231153204775, test loss: 0.2528523982344643\n",
      "epoch 8538: train loss: 0.1301187595953908, test loss: 0.2528519470409708\n",
      "epoch 8539: train loss: 0.1301144045978945, test loss: 0.252851496287601\n",
      "epoch 8540: train loss: 0.13011005032777773, test loss: 0.25285104597420077\n",
      "epoch 8541: train loss: 0.13010569678482964, test loss: 0.25285059610063615\n",
      "epoch 8542: train loss: 0.13010134396883952, test loss: 0.2528501466667485\n",
      "epoch 8543: train loss: 0.13009699187959672, test loss: 0.2528496976723897\n",
      "epoch 8544: train loss: 0.13009264051689068, test loss: 0.25284924911741924\n",
      "epoch 8545: train loss: 0.130088289880511, test loss: 0.25284880100168605\n",
      "epoch 8546: train loss: 0.13008393997024714, test loss: 0.2528483533250292\n",
      "epoch 8547: train loss: 0.13007959078588893, test loss: 0.25284790608731783\n",
      "epoch 8548: train loss: 0.1300752423272261, test loss: 0.25284745928840185\n",
      "epoch 8549: train loss: 0.1300708945940485, test loss: 0.2528470129281228\n",
      "epoch 8550: train loss: 0.1300665475861461, test loss: 0.2528465670063451\n",
      "epoch 8551: train loss: 0.13006220130330898, test loss: 0.2528461215229136\n",
      "epoch 8552: train loss: 0.1300578557453272, test loss: 0.25284567647767536\n",
      "epoch 8553: train loss: 0.130053510911991, test loss: 0.25284523187049346\n",
      "epoch 8554: train loss: 0.13004916680309062, test loss: 0.2528447877012251\n",
      "epoch 8555: train loss: 0.1300448234184165, test loss: 0.25284434396970296\n",
      "epoch 8556: train loss: 0.1300404807577591, test loss: 0.252843900675797\n",
      "epoch 8557: train loss: 0.13003613882090892, test loss: 0.25284345781935846\n",
      "epoch 8558: train loss: 0.13003179760765662, test loss: 0.25284301540023063\n",
      "epoch 8559: train loss: 0.1300274571177929, test loss: 0.2528425734182735\n",
      "epoch 8560: train loss: 0.13002311735110855, test loss: 0.2528421318733315\n",
      "epoch 8561: train loss: 0.1300187783073945, test loss: 0.25284169076527474\n",
      "epoch 8562: train loss: 0.13001443998644166, test loss: 0.2528412500939451\n",
      "epoch 8563: train loss: 0.1300101023880411, test loss: 0.2528408098591916\n",
      "epoch 8564: train loss: 0.130005765511984, test loss: 0.2528403700608734\n",
      "epoch 8565: train loss: 0.1300014293580615, test loss: 0.252839930698838\n",
      "epoch 8566: train loss: 0.12999709392606498, test loss: 0.25283949177294945\n",
      "epoch 8567: train loss: 0.1299927592157858, test loss: 0.25283905328305956\n",
      "epoch 8568: train loss: 0.1299884252270154, test loss: 0.2528386152290093\n",
      "epoch 8569: train loss: 0.1299840919595454, test loss: 0.2528381776106635\n",
      "epoch 8570: train loss: 0.1299797594131674, test loss: 0.2528377404278738\n",
      "epoch 8571: train loss: 0.12997542758767314, test loss: 0.2528373036804886\n",
      "epoch 8572: train loss: 0.12997109648285438, test loss: 0.2528368673683665\n",
      "epoch 8573: train loss: 0.1299667660985031, test loss: 0.2528364314913617\n",
      "epoch 8574: train loss: 0.12996243643441122, test loss: 0.25283599604932855\n",
      "epoch 8575: train loss: 0.12995810749037082, test loss: 0.2528355610421196\n",
      "epoch 8576: train loss: 0.12995377926617405, test loss: 0.25283512646958584\n",
      "epoch 8577: train loss: 0.1299494517616131, test loss: 0.252834692331585\n",
      "epoch 8578: train loss: 0.1299451249764803, test loss: 0.2528342586279738\n",
      "epoch 8579: train loss: 0.1299407989105681, test loss: 0.25283382535859705\n",
      "epoch 8580: train loss: 0.12993647356366889, test loss: 0.2528333925233179\n",
      "epoch 8581: train loss: 0.12993214893557528, test loss: 0.2528329601219886\n",
      "epoch 8582: train loss: 0.12992782502607994, test loss: 0.2528325281544628\n",
      "epoch 8583: train loss: 0.12992350183497556, test loss: 0.2528320966205959\n",
      "epoch 8584: train loss: 0.129919179362055, test loss: 0.2528316655202411\n",
      "epoch 8585: train loss: 0.12991485760711105, test loss: 0.252831234853255\n",
      "epoch 8586: train loss: 0.12991053656993679, test loss: 0.25283080461948415\n",
      "epoch 8587: train loss: 0.12990621625032528, test loss: 0.2528303748187913\n",
      "epoch 8588: train loss: 0.12990189664806961, test loss: 0.2528299454510304\n",
      "epoch 8589: train loss: 0.12989757776296307, test loss: 0.25282951651606544\n",
      "epoch 8590: train loss: 0.12989325959479892, test loss: 0.2528290880137282\n",
      "epoch 8591: train loss: 0.1298889421433706, test loss: 0.2528286599439013\n",
      "epoch 8592: train loss: 0.12988462540847157, test loss: 0.2528282323064176\n",
      "epoch 8593: train loss: 0.1298803093898954, test loss: 0.25282780510114305\n",
      "epoch 8594: train loss: 0.1298759940874357, test loss: 0.2528273783279266\n",
      "epoch 8595: train loss: 0.12987167950088624, test loss: 0.2528269519866256\n",
      "epoch 8596: train loss: 0.12986736563004078, test loss: 0.25282652607709977\n",
      "epoch 8597: train loss: 0.12986305247469332, test loss: 0.2528261005992042\n",
      "epoch 8598: train loss: 0.1298587400346377, test loss: 0.2528256755527939\n",
      "epoch 8599: train loss: 0.12985442830966812, test loss: 0.25282525093771874\n",
      "epoch 8600: train loss: 0.12985011729957857, test loss: 0.2528248267538403\n",
      "epoch 8601: train loss: 0.12984580700416343, test loss: 0.252824403001007\n",
      "epoch 8602: train loss: 0.12984149742321688, test loss: 0.25282397967909126\n",
      "epoch 8603: train loss: 0.12983718855653337, test loss: 0.2528235567879218\n",
      "epoch 8604: train loss: 0.1298328804039074, test loss: 0.2528231343273832\n",
      "epoch 8605: train loss: 0.12982857296513345, test loss: 0.2528227122973165\n",
      "epoch 8606: train loss: 0.12982426624000623, test loss: 0.2528222906975746\n",
      "epoch 8607: train loss: 0.12981996022832043, test loss: 0.2528218695280229\n",
      "epoch 8608: train loss: 0.12981565492987082, test loss: 0.25282144878851404\n",
      "epoch 8609: train loss: 0.12981135034445235, test loss: 0.2528210284789019\n",
      "epoch 8610: train loss: 0.12980704647185992, test loss: 0.2528206085990451\n",
      "epoch 8611: train loss: 0.12980274331188865, test loss: 0.2528201891487948\n",
      "epoch 8612: train loss: 0.12979844086433365, test loss: 0.2528197701280184\n",
      "epoch 8613: train loss: 0.1297941391289901, test loss: 0.25281935153656043\n",
      "epoch 8614: train loss: 0.1297898381056533, test loss: 0.2528189333742872\n",
      "epoch 8615: train loss: 0.12978553779411867, test loss: 0.2528185156410454\n",
      "epoch 8616: train loss: 0.12978123819418166, test loss: 0.25281809833670094\n",
      "epoch 8617: train loss: 0.1297769393056378, test loss: 0.2528176814611171\n",
      "epoch 8618: train loss: 0.12977264112828268, test loss: 0.25281726501412566\n",
      "epoch 8619: train loss: 0.1297683436619121, test loss: 0.2528168489956081\n",
      "epoch 8620: train loss: 0.12976404690632173, test loss: 0.2528164334054027\n",
      "epoch 8621: train loss: 0.12975975086130753, test loss: 0.25281601824338695\n",
      "epoch 8622: train loss: 0.12975545552666543, test loss: 0.2528156035094017\n",
      "epoch 8623: train loss: 0.12975116090219146, test loss: 0.2528151892033089\n",
      "epoch 8624: train loss: 0.1297468669876817, test loss: 0.25281477532496627\n",
      "epoch 8625: train loss: 0.1297425737829324, test loss: 0.2528143618742279\n",
      "epoch 8626: train loss: 0.12973828128773984, test loss: 0.252813948850961\n",
      "epoch 8627: train loss: 0.12973398950190035, test loss: 0.2528135362550078\n",
      "epoch 8628: train loss: 0.1297296984252104, test loss: 0.25281312408623846\n",
      "epoch 8629: train loss: 0.12972540805746646, test loss: 0.2528127123445119\n",
      "epoch 8630: train loss: 0.1297211183984652, test loss: 0.2528123010296682\n",
      "epoch 8631: train loss: 0.12971682944800328, test loss: 0.25281189014158384\n",
      "epoch 8632: train loss: 0.12971254120587747, test loss: 0.2528114796801123\n",
      "epoch 8633: train loss: 0.1297082536718846, test loss: 0.25281106964510075\n",
      "epoch 8634: train loss: 0.12970396684582164, test loss: 0.25281066003641817\n",
      "epoch 8635: train loss: 0.12969968072748558, test loss: 0.25281025085392217\n",
      "epoch 8636: train loss: 0.12969539531667348, test loss: 0.25280984209746393\n",
      "epoch 8637: train loss: 0.12969111061318256, test loss: 0.2528094337669122\n",
      "epoch 8638: train loss: 0.12968682661681008, test loss: 0.2528090258621082\n",
      "epoch 8639: train loss: 0.12968254332735335, test loss: 0.25280861838293117\n",
      "epoch 8640: train loss: 0.1296782607446098, test loss: 0.2528082113292213\n",
      "epoch 8641: train loss: 0.12967397886837695, test loss: 0.2528078047008452\n",
      "epoch 8642: train loss: 0.12966969769845235, test loss: 0.25280739849766576\n",
      "epoch 8643: train loss: 0.12966541723463365, test loss: 0.25280699271953333\n",
      "epoch 8644: train loss: 0.12966113747671862, test loss: 0.2528065873663074\n",
      "epoch 8645: train loss: 0.1296568584245051, test loss: 0.2528061824378471\n",
      "epoch 8646: train loss: 0.12965258007779093, test loss: 0.2528057779340187\n",
      "epoch 8647: train loss: 0.12964830243637415, test loss: 0.25280537385466273\n",
      "epoch 8648: train loss: 0.12964402550005283, test loss: 0.25280497019966147\n",
      "epoch 8649: train loss: 0.12963974926862507, test loss: 0.2528045669688592\n",
      "epoch 8650: train loss: 0.1296354737418891, test loss: 0.25280416416211193\n",
      "epoch 8651: train loss: 0.12963119891964325, test loss: 0.25280376177929437\n",
      "epoch 8652: train loss: 0.12962692480168594, test loss: 0.2528033598202504\n",
      "epoch 8653: train loss: 0.12962265138781556, test loss: 0.2528029582848439\n",
      "epoch 8654: train loss: 0.12961837867783074, test loss: 0.2528025571729326\n",
      "epoch 8655: train loss: 0.12961410667153003, test loss: 0.2528021564843831\n",
      "epoch 8656: train loss: 0.1296098353687122, test loss: 0.25280175621904294\n",
      "epoch 8657: train loss: 0.12960556476917606, test loss: 0.2528013563767853\n",
      "epoch 8658: train loss: 0.12960129487272037, test loss: 0.2528009569574622\n",
      "epoch 8659: train loss: 0.1295970256791442, test loss: 0.2528005579609242\n",
      "epoch 8660: train loss: 0.1295927571882465, test loss: 0.2528001593870492\n",
      "epoch 8661: train loss: 0.12958848939982645, test loss: 0.2527997612356834\n",
      "epoch 8662: train loss: 0.12958422231368316, test loss: 0.2527993635066912\n",
      "epoch 8663: train loss: 0.12957995592961596, test loss: 0.25279896619993397\n",
      "epoch 8664: train loss: 0.12957569024742419, test loss: 0.25279856931527106\n",
      "epoch 8665: train loss: 0.12957142526690726, test loss: 0.25279817285255407\n",
      "epoch 8666: train loss: 0.12956716098786472, test loss: 0.25279777681165155\n",
      "epoch 8667: train loss: 0.12956289741009613, test loss: 0.25279738119242373\n",
      "epoch 8668: train loss: 0.1295586345334012, test loss: 0.2527969859947292\n",
      "epoch 8669: train loss: 0.1295543723575796, test loss: 0.25279659121842873\n",
      "epoch 8670: train loss: 0.12955011088243126, test loss: 0.25279619686338045\n",
      "epoch 8671: train loss: 0.12954585010775602, test loss: 0.25279580292944204\n",
      "epoch 8672: train loss: 0.1295415900333539, test loss: 0.2527954094164792\n",
      "epoch 8673: train loss: 0.12953733065902495, test loss: 0.2527950163243501\n",
      "epoch 8674: train loss: 0.12953307198456934, test loss: 0.2527946236529169\n",
      "epoch 8675: train loss: 0.12952881400978733, test loss: 0.25279423140203394\n",
      "epoch 8676: train loss: 0.12952455673447916, test loss: 0.2527938395715741\n",
      "epoch 8677: train loss: 0.12952030015844523, test loss: 0.25279344816138216\n",
      "epoch 8678: train loss: 0.1295160442814861, test loss: 0.2527930571713378\n",
      "epoch 8679: train loss: 0.1295117891034022, test loss: 0.25279266660128213\n",
      "epoch 8680: train loss: 0.1295075346239942, test loss: 0.25279227645109237\n",
      "epoch 8681: train loss: 0.1295032808430628, test loss: 0.25279188672061914\n",
      "epoch 8682: train loss: 0.12949902776040884, test loss: 0.2527914974097261\n",
      "epoch 8683: train loss: 0.12949477537583312, test loss: 0.2527911085182759\n",
      "epoch 8684: train loss: 0.12949052368913663, test loss: 0.25279072004612785\n",
      "epoch 8685: train loss: 0.12948627270012036, test loss: 0.2527903319931447\n",
      "epoch 8686: train loss: 0.12948202240858542, test loss: 0.25278994435918234\n",
      "epoch 8687: train loss: 0.12947777281433298, test loss: 0.25278955714411105\n",
      "epoch 8688: train loss: 0.12947352391716435, test loss: 0.25278917034778714\n",
      "epoch 8689: train loss: 0.12946927571688083, test loss: 0.252788783970071\n",
      "epoch 8690: train loss: 0.12946502821328382, test loss: 0.2527883980108314\n",
      "epoch 8691: train loss: 0.12946078140617487, test loss: 0.25278801246991606\n",
      "epoch 8692: train loss: 0.1294565352953555, test loss: 0.2527876273472017\n",
      "epoch 8693: train loss: 0.12945228988062746, test loss: 0.25278724264253805\n",
      "epoch 8694: train loss: 0.12944804516179237, test loss: 0.25278685835578946\n",
      "epoch 8695: train loss: 0.12944380113865214, test loss: 0.2527864744868264\n",
      "epoch 8696: train loss: 0.12943955781100858, test loss: 0.2527860910355052\n",
      "epoch 8697: train loss: 0.12943531517866372, test loss: 0.25278570800168443\n",
      "epoch 8698: train loss: 0.1294310732414196, test loss: 0.25278532538522297\n",
      "epoch 8699: train loss: 0.1294268319990783, test loss: 0.25278494318599387\n",
      "epoch 8700: train loss: 0.1294225914514421, test loss: 0.2527845614038606\n",
      "epoch 8701: train loss: 0.12941835159831327, test loss: 0.25278418003867\n",
      "epoch 8702: train loss: 0.12941411243949413, test loss: 0.25278379909029325\n",
      "epoch 8703: train loss: 0.12940987397478718, test loss: 0.2527834185585963\n",
      "epoch 8704: train loss: 0.1294056362039949, test loss: 0.25278303844343936\n",
      "epoch 8705: train loss: 0.12940139912691986, test loss: 0.25278265874467765\n",
      "epoch 8706: train loss: 0.12939716274336482, test loss: 0.2527822794621819\n",
      "epoch 8707: train loss: 0.1293929270531325, test loss: 0.25278190059581856\n",
      "epoch 8708: train loss: 0.12938869205602577, test loss: 0.25278152214543326\n",
      "epoch 8709: train loss: 0.12938445775184743, test loss: 0.2527811441109041\n",
      "epoch 8710: train loss: 0.12938022414040057, test loss: 0.25278076649209136\n",
      "epoch 8711: train loss: 0.12937599122148824, test loss: 0.2527803892888499\n",
      "epoch 8712: train loss: 0.1293717589949136, test loss: 0.25278001250104976\n",
      "epoch 8713: train loss: 0.12936752746047986, test loss: 0.25277963612855536\n",
      "epoch 8714: train loss: 0.12936329661799031, test loss: 0.2527792601712223\n",
      "epoch 8715: train loss: 0.12935906646724835, test loss: 0.2527788846289213\n",
      "epoch 8716: train loss: 0.12935483700805744, test loss: 0.25277850950150493\n",
      "epoch 8717: train loss: 0.1293506082402211, test loss: 0.2527781347888523\n",
      "epoch 8718: train loss: 0.129346380163543, test loss: 0.2527777604908181\n",
      "epoch 8719: train loss: 0.12934215277782674, test loss: 0.2527773866072643\n",
      "epoch 8720: train loss: 0.12933792608287617, test loss: 0.2527770131380542\n",
      "epoch 8721: train loss: 0.12933370007849512, test loss: 0.25277664008304734\n",
      "epoch 8722: train loss: 0.1293294747644875, test loss: 0.2527762674421235\n",
      "epoch 8723: train loss: 0.12932525014065735, test loss: 0.2527758952151258\n",
      "epoch 8724: train loss: 0.12932102620680871, test loss: 0.2527755234019343\n",
      "epoch 8725: train loss: 0.12931680296274578, test loss: 0.25277515200239437\n",
      "epoch 8726: train loss: 0.12931258040827276, test loss: 0.25277478101639017\n",
      "epoch 8727: train loss: 0.12930835854319397, test loss: 0.2527744104437726\n",
      "epoch 8728: train loss: 0.12930413736731383, test loss: 0.2527740402844142\n",
      "epoch 8729: train loss: 0.1292999168804368, test loss: 0.25277367053816546\n",
      "epoch 8730: train loss: 0.12929569708236743, test loss: 0.2527733012049105\n",
      "epoch 8731: train loss: 0.12929147797291032, test loss: 0.25277293228449244\n",
      "epoch 8732: train loss: 0.1292872595518702, test loss: 0.25277256377679297\n",
      "epoch 8733: train loss: 0.12928304181905187, test loss: 0.2527721956816536\n",
      "epoch 8734: train loss: 0.12927882477426011, test loss: 0.2527718279989682\n",
      "epoch 8735: train loss: 0.12927460841729993, test loss: 0.25277146072857914\n",
      "epoch 8736: train loss: 0.1292703927479763, test loss: 0.2527710938703613\n",
      "epoch 8737: train loss: 0.12926617776609434, test loss: 0.2527707274241684\n",
      "epoch 8738: train loss: 0.12926196347145918, test loss: 0.2527703613898755\n",
      "epoch 8739: train loss: 0.1292577498638761, test loss: 0.25276999576734804\n",
      "epoch 8740: train loss: 0.12925353694315037, test loss: 0.25276963055643836\n",
      "epoch 8741: train loss: 0.12924932470908745, test loss: 0.25276926575702247\n",
      "epoch 8742: train loss: 0.1292451131614928, test loss: 0.25276890136896263\n",
      "epoch 8743: train loss: 0.12924090230017188, test loss: 0.252768537392122\n",
      "epoch 8744: train loss: 0.12923669212493047, test loss: 0.2527681738263671\n",
      "epoch 8745: train loss: 0.12923248263557416, test loss: 0.2527678106715597\n",
      "epoch 8746: train loss: 0.12922827383190877, test loss: 0.25276744792756317\n",
      "epoch 8747: train loss: 0.12922406571374015, test loss: 0.25276708559425165\n",
      "epoch 8748: train loss: 0.12921985828087426, test loss: 0.2527667236714861\n",
      "epoch 8749: train loss: 0.12921565153311707, test loss: 0.25276636215912474\n",
      "epoch 8750: train loss: 0.12921144547027472, test loss: 0.25276600105704455\n",
      "epoch 8751: train loss: 0.1292072400921533, test loss: 0.2527656403651004\n",
      "epoch 8752: train loss: 0.12920303539855912, test loss: 0.2527652800831652\n",
      "epoch 8753: train loss: 0.12919883138929852, test loss: 0.2527649202111006\n",
      "epoch 8754: train loss: 0.1291946280641778, test loss: 0.2527645607487692\n",
      "epoch 8755: train loss: 0.1291904254230035, test loss: 0.2527642016960374\n",
      "epoch 8756: train loss: 0.12918622346558212, test loss: 0.25276384305277827\n",
      "epoch 8757: train loss: 0.12918202219172037, test loss: 0.2527634848188552\n",
      "epoch 8758: train loss: 0.12917782160122482, test loss: 0.25276312699412956\n",
      "epoch 8759: train loss: 0.12917362169390237, test loss: 0.2527627695784608\n",
      "epoch 8760: train loss: 0.1291694224695598, test loss: 0.25276241257173476\n",
      "epoch 8761: train loss: 0.12916522392800409, test loss: 0.2527620559737953\n",
      "epoch 8762: train loss: 0.1291610260690422, test loss: 0.25276169978452395\n",
      "epoch 8763: train loss: 0.12915682889248123, test loss: 0.2527613440037833\n",
      "epoch 8764: train loss: 0.1291526323981283, test loss: 0.25276098863143015\n",
      "epoch 8765: train loss: 0.12914843658579073, test loss: 0.25276063366734575\n",
      "epoch 8766: train loss: 0.12914424145527575, test loss: 0.25276027911138255\n",
      "epoch 8767: train loss: 0.12914004700639078, test loss: 0.25275992496341515\n",
      "epoch 8768: train loss: 0.12913585323894328, test loss: 0.25275957122330694\n",
      "epoch 8769: train loss: 0.1291316601527408, test loss: 0.25275921789092587\n",
      "epoch 8770: train loss: 0.12912746774759093, test loss: 0.2527588649661306\n",
      "epoch 8771: train loss: 0.12912327602330134, test loss: 0.2527585124488055\n",
      "epoch 8772: train loss: 0.12911908497967983, test loss: 0.2527581603387944\n",
      "epoch 8773: train loss: 0.12911489461653425, test loss: 0.2527578086359843\n",
      "epoch 8774: train loss: 0.12911070493367252, test loss: 0.2527574573402355\n",
      "epoch 8775: train loss: 0.12910651593090258, test loss: 0.2527571064514011\n",
      "epoch 8776: train loss: 0.12910232760803253, test loss: 0.25275675596936403\n",
      "epoch 8777: train loss: 0.12909813996487052, test loss: 0.25275640589398785\n",
      "epoch 8778: train loss: 0.12909395300122473, test loss: 0.25275605622513125\n",
      "epoch 8779: train loss: 0.12908976671690353, test loss: 0.25275570696267663\n",
      "epoch 8780: train loss: 0.12908558111171523, test loss: 0.25275535810647726\n",
      "epoch 8781: train loss: 0.1290813961854683, test loss: 0.25275500965640585\n",
      "epoch 8782: train loss: 0.12907721193797125, test loss: 0.2527546616123308\n",
      "epoch 8783: train loss: 0.12907302836903267, test loss: 0.2527543139741171\n",
      "epoch 8784: train loss: 0.12906884547846126, test loss: 0.25275396674162937\n",
      "epoch 8785: train loss: 0.12906466326606572, test loss: 0.25275361991473694\n",
      "epoch 8786: train loss: 0.12906048173165494, test loss: 0.2527532734933159\n",
      "epoch 8787: train loss: 0.12905630087503778, test loss: 0.25275292747721795\n",
      "epoch 8788: train loss: 0.1290521206960232, test loss: 0.2527525818663214\n",
      "epoch 8789: train loss: 0.12904794119442026, test loss: 0.25275223666049695\n",
      "epoch 8790: train loss: 0.1290437623700381, test loss: 0.2527518918595946\n",
      "epoch 8791: train loss: 0.12903958422268588, test loss: 0.25275154746350303\n",
      "epoch 8792: train loss: 0.1290354067521729, test loss: 0.2527512034720841\n",
      "epoch 8793: train loss: 0.1290312299583085, test loss: 0.25275085988519636\n",
      "epoch 8794: train loss: 0.12902705384090216, test loss: 0.25275051670270876\n",
      "epoch 8795: train loss: 0.12902287839976323, test loss: 0.25275017392449944\n",
      "epoch 8796: train loss: 0.12901870363470144, test loss: 0.2527498315504338\n",
      "epoch 8797: train loss: 0.1290145295455264, test loss: 0.25274948958037113\n",
      "epoch 8798: train loss: 0.12901035613204775, test loss: 0.2527491480141926\n",
      "epoch 8799: train loss: 0.1290061833940754, test loss: 0.2527488068517582\n",
      "epoch 8800: train loss: 0.12900201133141914, test loss: 0.25274846609293283\n",
      "epoch 8801: train loss: 0.12899783994388897, test loss: 0.2527481257375872\n",
      "epoch 8802: train loss: 0.12899366923129488, test loss: 0.2527477857856001\n",
      "epoch 8803: train loss: 0.128989499193447, test loss: 0.2527474462368305\n",
      "epoch 8804: train loss: 0.12898532983015543, test loss: 0.2527471070911411\n",
      "epoch 8805: train loss: 0.12898116114123048, test loss: 0.2527467683484108\n",
      "epoch 8806: train loss: 0.12897699312648248, test loss: 0.25274643000850583\n",
      "epoch 8807: train loss: 0.12897282578572178, test loss: 0.2527460920712977\n",
      "epoch 8808: train loss: 0.12896865911875885, test loss: 0.25274575453664255\n",
      "epoch 8809: train loss: 0.12896449312540426, test loss: 0.25274541740442624\n",
      "epoch 8810: train loss: 0.12896032780546865, test loss: 0.2527450806745014\n",
      "epoch 8811: train loss: 0.12895616315876265, test loss: 0.25274474434675037\n",
      "epoch 8812: train loss: 0.12895199918509706, test loss: 0.252744408421037\n",
      "epoch 8813: train loss: 0.1289478358842827, test loss: 0.25274407289722556\n",
      "epoch 8814: train loss: 0.12894367325613054, test loss: 0.25274373777519116\n",
      "epoch 8815: train loss: 0.12893951130045153, test loss: 0.2527434030547979\n",
      "epoch 8816: train loss: 0.12893535001705672, test loss: 0.2527430687359287\n",
      "epoch 8817: train loss: 0.1289311894057573, test loss: 0.25274273481843357\n",
      "epoch 8818: train loss: 0.1289270294663644, test loss: 0.25274240130219083\n",
      "epoch 8819: train loss: 0.12892287019868934, test loss: 0.2527420681870702\n",
      "epoch 8820: train loss: 0.12891871160254353, test loss: 0.2527417354729417\n",
      "epoch 8821: train loss: 0.12891455367773838, test loss: 0.2527414031596724\n",
      "epoch 8822: train loss: 0.12891039642408533, test loss: 0.25274107124713313\n",
      "epoch 8823: train loss: 0.12890623984139604, test loss: 0.2527407397351968\n",
      "epoch 8824: train loss: 0.12890208392948213, test loss: 0.25274040862372865\n",
      "epoch 8825: train loss: 0.12889792868815533, test loss: 0.25274007791259295\n",
      "epoch 8826: train loss: 0.12889377411722747, test loss: 0.25273974760167023\n",
      "epoch 8827: train loss: 0.12888962021651038, test loss: 0.2527394176908257\n",
      "epoch 8828: train loss: 0.12888546698581604, test loss: 0.25273908817992524\n",
      "epoch 8829: train loss: 0.1288813144249565, test loss: 0.2527387590688482\n",
      "epoch 8830: train loss: 0.12887716253374382, test loss: 0.2527384303574575\n",
      "epoch 8831: train loss: 0.1288730113119902, test loss: 0.25273810204562563\n",
      "epoch 8832: train loss: 0.12886886075950782, test loss: 0.25273777413322224\n",
      "epoch 8833: train loss: 0.12886471087610907, test loss: 0.252737446620117\n",
      "epoch 8834: train loss: 0.1288605616616063, test loss: 0.2527371195061791\n",
      "epoch 8835: train loss: 0.128856413115812, test loss: 0.2527367927912827\n",
      "epoch 8836: train loss: 0.1288522652385387, test loss: 0.25273646647529324\n",
      "epoch 8837: train loss: 0.12884811802959903, test loss: 0.25273614055809224\n",
      "epoch 8838: train loss: 0.12884397148880564, test loss: 0.2527358150395342\n",
      "epoch 8839: train loss: 0.12883982561597132, test loss: 0.2527354899194976\n",
      "epoch 8840: train loss: 0.12883568041090887, test loss: 0.2527351651978477\n",
      "epoch 8841: train loss: 0.1288315358734312, test loss: 0.2527348408744674\n",
      "epoch 8842: train loss: 0.12882739200335133, test loss: 0.25273451694921845\n",
      "epoch 8843: train loss: 0.12882324880048224, test loss: 0.2527341934219725\n",
      "epoch 8844: train loss: 0.12881910626463713, test loss: 0.25273387029259536\n",
      "epoch 8845: train loss: 0.12881496439562917, test loss: 0.25273354756097227\n",
      "epoch 8846: train loss: 0.12881082319327164, test loss: 0.25273322522696007\n",
      "epoch 8847: train loss: 0.12880668265737782, test loss: 0.25273290329043985\n",
      "epoch 8848: train loss: 0.12880254278776124, test loss: 0.2527325817512682\n",
      "epoch 8849: train loss: 0.12879840358423525, test loss: 0.2527322606093383\n",
      "epoch 8850: train loss: 0.12879426504661354, test loss: 0.2527319398644991\n",
      "epoch 8851: train loss: 0.12879012717470972, test loss: 0.2527316195166365\n",
      "epoch 8852: train loss: 0.1287859899683374, test loss: 0.25273129956561424\n",
      "epoch 8853: train loss: 0.12878185342731047, test loss: 0.2527309800113112\n",
      "epoch 8854: train loss: 0.12877771755144277, test loss: 0.2527306608535839\n",
      "epoch 8855: train loss: 0.12877358234054817, test loss: 0.25273034209232603\n",
      "epoch 8856: train loss: 0.12876944779444072, test loss: 0.25273002372738745\n",
      "epoch 8857: train loss: 0.12876531391293447, test loss: 0.25272970575865444\n",
      "epoch 8858: train loss: 0.12876118069584358, test loss: 0.25272938818599106\n",
      "epoch 8859: train loss: 0.12875704814298225, test loss: 0.2527290710092695\n",
      "epoch 8860: train loss: 0.12875291625416477, test loss: 0.252728754228369\n",
      "epoch 8861: train loss: 0.12874878502920553, test loss: 0.2527284378431507\n",
      "epoch 8862: train loss: 0.1287446544679189, test loss: 0.2527281218534901\n",
      "epoch 8863: train loss: 0.12874052457011947, test loss: 0.2527278062592596\n",
      "epoch 8864: train loss: 0.12873639533562178, test loss: 0.2527274910603362\n",
      "epoch 8865: train loss: 0.1287322667642405, test loss: 0.2527271762565873\n",
      "epoch 8866: train loss: 0.1287281388557903, test loss: 0.2527268618478819\n",
      "epoch 8867: train loss: 0.128724011610086, test loss: 0.25272654783410053\n",
      "epoch 8868: train loss: 0.12871988502694254, test loss: 0.25272623421510504\n",
      "epoch 8869: train loss: 0.12871575910617475, test loss: 0.25272592099078217\n",
      "epoch 8870: train loss: 0.1287116338475977, test loss: 0.2527256081609825\n",
      "epoch 8871: train loss: 0.1287075092510265, test loss: 0.25272529572559477\n",
      "epoch 8872: train loss: 0.12870338531627626, test loss: 0.25272498368449847\n",
      "epoch 8873: train loss: 0.12869926204316223, test loss: 0.2527246720375443\n",
      "epoch 8874: train loss: 0.12869513943149968, test loss: 0.2527243607846143\n",
      "epoch 8875: train loss: 0.12869101748110406, test loss: 0.2527240499255886\n",
      "epoch 8876: train loss: 0.12868689619179072, test loss: 0.252723739460329\n",
      "epoch 8877: train loss: 0.1286827755633753, test loss: 0.2527234293887124\n",
      "epoch 8878: train loss: 0.12867865559567324, test loss: 0.2527231197106208\n",
      "epoch 8879: train loss: 0.12867453628850029, test loss: 0.252722810425909\n",
      "epoch 8880: train loss: 0.1286704176416722, test loss: 0.252722501534461\n",
      "epoch 8881: train loss: 0.12866629965500467, test loss: 0.25272219303615223\n",
      "epoch 8882: train loss: 0.12866218232831372, test loss: 0.2527218849308443\n",
      "epoch 8883: train loss: 0.1286580656614152, test loss: 0.25272157721842575\n",
      "epoch 8884: train loss: 0.12865394965412516, test loss: 0.25272126989875304\n",
      "epoch 8885: train loss: 0.12864983430625967, test loss: 0.25272096297171165\n",
      "epoch 8886: train loss: 0.12864571961763493, test loss: 0.25272065643716823\n",
      "epoch 8887: train loss: 0.12864160558806717, test loss: 0.2527203502949999\n",
      "epoch 8888: train loss: 0.12863749221737264, test loss: 0.25272004454508185\n",
      "epoch 8889: train loss: 0.12863337950536777, test loss: 0.2527197391872813\n",
      "epoch 8890: train loss: 0.12862926745186903, test loss: 0.25271943422147297\n",
      "epoch 8891: train loss: 0.12862515605669292, test loss: 0.25271912964753457\n",
      "epoch 8892: train loss: 0.12862104531965599, test loss: 0.2527188254653382\n",
      "epoch 8893: train loss: 0.1286169352405749, test loss: 0.2527185216747543\n",
      "epoch 8894: train loss: 0.12861282581926645, test loss: 0.2527182182756581\n",
      "epoch 8895: train loss: 0.12860871705554744, test loss: 0.2527179152679285\n",
      "epoch 8896: train loss: 0.12860460894923467, test loss: 0.25271761265142856\n",
      "epoch 8897: train loss: 0.12860050150014518, test loss: 0.25271731042603496\n",
      "epoch 8898: train loss: 0.12859639470809595, test loss: 0.2527170085916298\n",
      "epoch 8899: train loss: 0.12859228857290403, test loss: 0.2527167071480806\n",
      "epoch 8900: train loss: 0.12858818309438663, test loss: 0.2527164060952673\n",
      "epoch 8901: train loss: 0.12858407827236099, test loss: 0.2527161054330541\n",
      "epoch 8902: train loss: 0.12857997410664443, test loss: 0.25271580516132436\n",
      "epoch 8903: train loss: 0.12857587059705422, test loss: 0.2527155052799462\n",
      "epoch 8904: train loss: 0.12857176774340792, test loss: 0.2527152057887949\n",
      "epoch 8905: train loss: 0.128567665545523, test loss: 0.2527149066877429\n",
      "epoch 8906: train loss: 0.12856356400321706, test loss: 0.2527146079766746\n",
      "epoch 8907: train loss: 0.12855946311630773, test loss: 0.2527143096554504\n",
      "epoch 8908: train loss: 0.1285553628846128, test loss: 0.25271401172395425\n",
      "epoch 8909: train loss: 0.12855126330795, test loss: 0.2527137141820618\n",
      "epoch 8910: train loss: 0.12854716438613722, test loss: 0.252713417029639\n",
      "epoch 8911: train loss: 0.12854306611899244, test loss: 0.2527131202665662\n",
      "epoch 8912: train loss: 0.12853896850633362, test loss: 0.25271282389271915\n",
      "epoch 8913: train loss: 0.1285348715479789, test loss: 0.2527125279079669\n",
      "epoch 8914: train loss: 0.1285307752437464, test loss: 0.25271223231218715\n",
      "epoch 8915: train loss: 0.12852667959345432, test loss: 0.2527119371052563\n",
      "epoch 8916: train loss: 0.12852258459692098, test loss: 0.2527116422870516\n",
      "epoch 8917: train loss: 0.12851849025396475, test loss: 0.2527113478574414\n",
      "epoch 8918: train loss: 0.12851439656440408, test loss: 0.2527110538163072\n",
      "epoch 8919: train loss: 0.12851030352805742, test loss: 0.25271076016351884\n",
      "epoch 8920: train loss: 0.12850621114474342, test loss: 0.25271046689895366\n",
      "epoch 8921: train loss: 0.1285021194142807, test loss: 0.25271017402248874\n",
      "epoch 8922: train loss: 0.1284980283364879, test loss: 0.25270988153399065\n",
      "epoch 8923: train loss: 0.12849393791118394, test loss: 0.2527095894333473\n",
      "epoch 8924: train loss: 0.1284898481381876, test loss: 0.2527092977204258\n",
      "epoch 8925: train loss: 0.1284857590173178, test loss: 0.2527090063951077\n",
      "epoch 8926: train loss: 0.12848167054839357, test loss: 0.2527087154572581\n",
      "epoch 8927: train loss: 0.12847758273123397, test loss: 0.2527084249067644\n",
      "epoch 8928: train loss: 0.12847349556565812, test loss: 0.25270813474349585\n",
      "epoch 8929: train loss: 0.12846940905148524, test loss: 0.25270784496732596\n",
      "epoch 8930: train loss: 0.12846532318853465, test loss: 0.25270755557813146\n",
      "epoch 8931: train loss: 0.12846123797662565, test loss: 0.25270726657579623\n",
      "epoch 8932: train loss: 0.12845715341557765, test loss: 0.2527069779601869\n",
      "epoch 8933: train loss: 0.12845306950521018, test loss: 0.2527066897311796\n",
      "epoch 8934: train loss: 0.12844898624534273, test loss: 0.252706401888648\n",
      "epoch 8935: train loss: 0.12844490363579503, test loss: 0.25270611443248264\n",
      "epoch 8936: train loss: 0.1284408216763867, test loss: 0.25270582736254654\n",
      "epoch 8937: train loss: 0.12843674036693753, test loss: 0.2527055406787148\n",
      "epoch 8938: train loss: 0.12843265970726736, test loss: 0.2527052543808701\n",
      "epoch 8939: train loss: 0.1284285796971961, test loss: 0.2527049684688874\n",
      "epoch 8940: train loss: 0.1284245003365437, test loss: 0.2527046829426418\n",
      "epoch 8941: train loss: 0.12842042162513023, test loss: 0.2527043978020134\n",
      "epoch 8942: train loss: 0.12841634356277584, test loss: 0.25270411304686424\n",
      "epoch 8943: train loss: 0.12841226614930068, test loss: 0.25270382867708674\n",
      "epoch 8944: train loss: 0.128408189384525, test loss: 0.25270354469254863\n",
      "epoch 8945: train loss: 0.12840411326826912, test loss: 0.2527032610931324\n",
      "epoch 8946: train loss: 0.12840003780035347, test loss: 0.252702977878706\n",
      "epoch 8947: train loss: 0.12839596298059847, test loss: 0.252702695049158\n",
      "epoch 8948: train loss: 0.1283918888088247, test loss: 0.2527024126043555\n",
      "epoch 8949: train loss: 0.1283878152848527, test loss: 0.25270213054418267\n",
      "epoch 8950: train loss: 0.12838374240850325, test loss: 0.2527018488685075\n",
      "epoch 8951: train loss: 0.12837967017959698, test loss: 0.25270156757721396\n",
      "epoch 8952: train loss: 0.12837559859795472, test loss: 0.25270128667016717\n",
      "epoch 8953: train loss: 0.12837152766339743, test loss: 0.252701006147262\n",
      "epoch 8954: train loss: 0.12836745737574595, test loss: 0.25270072600835974\n",
      "epoch 8955: train loss: 0.12836338773482142, test loss: 0.25270044625335236\n",
      "epoch 8956: train loss: 0.1283593187404448, test loss: 0.2527001668821009\n",
      "epoch 8957: train loss: 0.12835525039243734, test loss: 0.2526998878944935\n",
      "epoch 8958: train loss: 0.1283511826906202, test loss: 0.25269960929040264\n",
      "epoch 8959: train loss: 0.12834711563481474, test loss: 0.25269933106970993\n",
      "epoch 8960: train loss: 0.1283430492248423, test loss: 0.25269905323228486\n",
      "epoch 8961: train loss: 0.1283389834605243, test loss: 0.2526987757780178\n",
      "epoch 8962: train loss: 0.12833491834168226, test loss: 0.25269849870676525\n",
      "epoch 8963: train loss: 0.1283308538681377, test loss: 0.25269822201843306\n",
      "epoch 8964: train loss: 0.12832679003971234, test loss: 0.25269794571286885\n",
      "epoch 8965: train loss: 0.12832272685622784, test loss: 0.2526976697899723\n",
      "epoch 8966: train loss: 0.12831866431750594, test loss: 0.2526973942496108\n",
      "epoch 8967: train loss: 0.1283146024233686, test loss: 0.2526971190916674\n",
      "epoch 8968: train loss: 0.12831054117363763, test loss: 0.2526968443160101\n",
      "epoch 8969: train loss: 0.12830648056813504, test loss: 0.252696569922527\n",
      "epoch 8970: train loss: 0.12830242060668293, test loss: 0.2526962959110922\n",
      "epoch 8971: train loss: 0.12829836128910338, test loss: 0.2526960222815877\n",
      "epoch 8972: train loss: 0.12829430261521854, test loss: 0.25269574903387954\n",
      "epoch 8973: train loss: 0.12829024458485075, test loss: 0.2526954761678649\n",
      "epoch 8974: train loss: 0.12828618719782234, test loss: 0.2526952036834026\n",
      "epoch 8975: train loss: 0.1282821304539556, test loss: 0.2526949315803753\n",
      "epoch 8976: train loss: 0.1282780743530731, test loss: 0.25269465985867184\n",
      "epoch 8977: train loss: 0.1282740188949973, test loss: 0.2526943885181565\n",
      "epoch 8978: train loss: 0.12826996407955082, test loss: 0.25269411755871846\n",
      "epoch 8979: train loss: 0.1282659099065564, test loss: 0.2526938469802321\n",
      "epoch 8980: train loss: 0.12826185637583667, test loss: 0.25269357678257665\n",
      "epoch 8981: train loss: 0.12825780348721452, test loss: 0.25269330696562603\n",
      "epoch 8982: train loss: 0.12825375124051278, test loss: 0.2526930375292638\n",
      "epoch 8983: train loss: 0.12824969963555438, test loss: 0.25269276847336486\n",
      "epoch 8984: train loss: 0.1282456486721624, test loss: 0.2526924997978118\n",
      "epoch 8985: train loss: 0.12824159835015983, test loss: 0.2526922315024831\n",
      "epoch 8986: train loss: 0.12823754866936984, test loss: 0.2526919635872511\n",
      "epoch 8987: train loss: 0.12823349962961572, test loss: 0.2526916960520025\n",
      "epoch 8988: train loss: 0.12822945123072066, test loss: 0.2526914288966118\n",
      "epoch 8989: train loss: 0.12822540347250805, test loss: 0.25269116212095627\n",
      "epoch 8990: train loss: 0.1282213563548013, test loss: 0.2526908957249209\n",
      "epoch 8991: train loss: 0.12821730987742394, test loss: 0.2526906297083752\n",
      "epoch 8992: train loss: 0.12821326404019948, test loss: 0.2526903640712064\n",
      "epoch 8993: train loss: 0.1282092188429515, test loss: 0.2526900988132937\n",
      "epoch 8994: train loss: 0.1282051742855038, test loss: 0.2526898339345095\n",
      "epoch 8995: train loss: 0.12820113036768002, test loss: 0.25268956943474036\n",
      "epoch 8996: train loss: 0.1281970870893041, test loss: 0.2526893053138567\n",
      "epoch 8997: train loss: 0.12819304445019988, test loss: 0.2526890415717522\n",
      "epoch 8998: train loss: 0.12818900245019132, test loss: 0.25268877820829233\n",
      "epoch 8999: train loss: 0.12818496108910243, test loss: 0.252688515223357\n",
      "epoch 9000: train loss: 0.12818092036675735, test loss: 0.25268825261683747\n",
      "epoch 9001: train loss: 0.12817688028298022, test loss: 0.25268799038860823\n",
      "epoch 9002: train loss: 0.12817284083759528, test loss: 0.2526877285385362\n",
      "epoch 9003: train loss: 0.12816880203042683, test loss: 0.2526874670665186\n",
      "epoch 9004: train loss: 0.1281647638612992, test loss: 0.25268720597242017\n",
      "epoch 9005: train loss: 0.12816072633003692, test loss: 0.25268694525613616\n",
      "epoch 9006: train loss: 0.1281566894364644, test loss: 0.2526866849175342\n",
      "epoch 9007: train loss: 0.12815265318040628, test loss: 0.25268642495650057\n",
      "epoch 9008: train loss: 0.12814861756168713, test loss: 0.2526861653729097\n",
      "epoch 9009: train loss: 0.12814458258013167, test loss: 0.2526859061666383\n",
      "epoch 9010: train loss: 0.12814054823556476, test loss: 0.2526856473375832\n",
      "epoch 9011: train loss: 0.12813651452781108, test loss: 0.2526853888856098\n",
      "epoch 9012: train loss: 0.12813248145669567, test loss: 0.2526851308106076\n",
      "epoch 9013: train loss: 0.12812844902204346, test loss: 0.2526848731124427\n",
      "epoch 9014: train loss: 0.1281244172236795, test loss: 0.2526846157910032\n",
      "epoch 9015: train loss: 0.1281203860614289, test loss: 0.25268435884617163\n",
      "epoch 9016: train loss: 0.12811635553511677, test loss: 0.25268410227782895\n",
      "epoch 9017: train loss: 0.12811232564456845, test loss: 0.2526838460858574\n",
      "epoch 9018: train loss: 0.1281082963896092, test loss: 0.25268359027012705\n",
      "epoch 9019: train loss: 0.12810426777006442, test loss: 0.2526833348305233\n",
      "epoch 9020: train loss: 0.12810023978575952, test loss: 0.2526830797669286\n",
      "epoch 9021: train loss: 0.12809621243652003, test loss: 0.2526828250792206\n",
      "epoch 9022: train loss: 0.12809218572217154, test loss: 0.25268257076728884\n",
      "epoch 9023: train loss: 0.12808815964253967, test loss: 0.2526823168310052\n",
      "epoch 9024: train loss: 0.12808413419745013, test loss: 0.2526820632702472\n",
      "epoch 9025: train loss: 0.12808010938672876, test loss: 0.25268181008490465\n",
      "epoch 9026: train loss: 0.1280760852102013, test loss: 0.25268155727484914\n",
      "epoch 9027: train loss: 0.12807206166769378, test loss: 0.25268130483997403\n",
      "epoch 9028: train loss: 0.12806803875903208, test loss: 0.2526810527801476\n",
      "epoch 9029: train loss: 0.12806401648404228, test loss: 0.2526808010952512\n",
      "epoch 9030: train loss: 0.1280599948425505, test loss: 0.2526805497851804\n",
      "epoch 9031: train loss: 0.12805597383438294, test loss: 0.2526802988498017\n",
      "epoch 9032: train loss: 0.12805195345936582, test loss: 0.252680048289002\n",
      "epoch 9033: train loss: 0.12804793371732542, test loss: 0.2526797981026613\n",
      "epoch 9034: train loss: 0.1280439146080882, test loss: 0.2526795482906536\n",
      "epoch 9035: train loss: 0.12803989613148054, test loss: 0.2526792988528746\n",
      "epoch 9036: train loss: 0.12803587828732893, test loss: 0.25267904978919986\n",
      "epoch 9037: train loss: 0.12803186107546005, test loss: 0.2526788010995088\n",
      "epoch 9038: train loss: 0.12802784449570045, test loss: 0.25267855278368373\n",
      "epoch 9039: train loss: 0.12802382854787683, test loss: 0.2526783048416065\n",
      "epoch 9040: train loss: 0.1280198132318161, test loss: 0.25267805727315495\n",
      "epoch 9041: train loss: 0.12801579854734496, test loss: 0.25267781007820544\n",
      "epoch 9042: train loss: 0.12801178449429038, test loss: 0.2526775632566616\n",
      "epoch 9043: train loss: 0.12800777107247932, test loss: 0.2526773168083801\n",
      "epoch 9044: train loss: 0.12800375828173885, test loss: 0.2526770707332631\n",
      "epoch 9045: train loss: 0.12799974612189605, test loss: 0.25267682503118477\n",
      "epoch 9046: train loss: 0.1279957345927781, test loss: 0.2526765797020202\n",
      "epoch 9047: train loss: 0.12799172369421224, test loss: 0.2526763347456532\n",
      "epoch 9048: train loss: 0.12798771342602583, test loss: 0.25267609016197345\n",
      "epoch 9049: train loss: 0.12798370378804616, test loss: 0.25267584595085246\n",
      "epoch 9050: train loss: 0.1279796947801007, test loss: 0.25267560211218265\n",
      "epoch 9051: train loss: 0.12797568640201698, test loss: 0.25267535864584106\n",
      "epoch 9052: train loss: 0.12797167865362258, test loss: 0.2526751155517083\n",
      "epoch 9053: train loss: 0.12796767153474506, test loss: 0.2526748728296685\n",
      "epoch 9054: train loss: 0.12796366504521217, test loss: 0.2526746304796072\n",
      "epoch 9055: train loss: 0.1279596591848517, test loss: 0.2526743885014044\n",
      "epoch 9056: train loss: 0.12795565395349148, test loss: 0.25267414689494183\n",
      "epoch 9057: train loss: 0.1279516493509594, test loss: 0.2526739056600962\n",
      "epoch 9058: train loss: 0.1279476453770834, test loss: 0.25267366479675757\n",
      "epoch 9059: train loss: 0.12794364203169153, test loss: 0.25267342430481354\n",
      "epoch 9060: train loss: 0.1279396393146119, test loss: 0.2526731841841302\n",
      "epoch 9061: train loss: 0.12793563722567267, test loss: 0.25267294443460586\n",
      "epoch 9062: train loss: 0.12793163576470204, test loss: 0.2526727050561084\n",
      "epoch 9063: train loss: 0.12792763493152837, test loss: 0.25267246604853644\n",
      "epoch 9064: train loss: 0.12792363472597995, test loss: 0.2526722274117561\n",
      "epoch 9065: train loss: 0.12791963514788526, test loss: 0.25267198914566735\n",
      "epoch 9066: train loss: 0.12791563619707277, test loss: 0.25267175125014496\n",
      "epoch 9067: train loss: 0.12791163787337104, test loss: 0.25267151372506674\n",
      "epoch 9068: train loss: 0.12790764017660863, test loss: 0.2526712765703232\n",
      "epoch 9069: train loss: 0.12790364310661434, test loss: 0.2526710397857914\n",
      "epoch 9070: train loss: 0.12789964666321688, test loss: 0.252670803371358\n",
      "epoch 9071: train loss: 0.127895650846245, test loss: 0.2526705673269176\n",
      "epoch 9072: train loss: 0.12789165565552768, test loss: 0.2526703316523236\n",
      "epoch 9073: train loss: 0.1278876610908938, test loss: 0.252670096347491\n",
      "epoch 9074: train loss: 0.12788366715217242, test loss: 0.25266986141228215\n",
      "epoch 9075: train loss: 0.1278796738391926, test loss: 0.25266962684659133\n",
      "epoch 9076: train loss: 0.1278756811517835, test loss: 0.25266939265029703\n",
      "epoch 9077: train loss: 0.1278716890897743, test loss: 0.25266915882327495\n",
      "epoch 9078: train loss: 0.1278676976529943, test loss: 0.252668925365427\n",
      "epoch 9079: train loss: 0.12786370684127282, test loss: 0.2526686922766242\n",
      "epoch 9080: train loss: 0.1278597166544393, test loss: 0.2526684595567572\n",
      "epoch 9081: train loss: 0.12785572709232318, test loss: 0.2526682272056932\n",
      "epoch 9082: train loss: 0.127851738154754, test loss: 0.2526679952233389\n",
      "epoch 9083: train loss: 0.12784774984156136, test loss: 0.2526677636095628\n",
      "epoch 9084: train loss: 0.12784376215257492, test loss: 0.252667532364255\n",
      "epoch 9085: train loss: 0.12783977508762445, test loss: 0.25266730148729943\n",
      "epoch 9086: train loss: 0.1278357886465397, test loss: 0.2526670709785749\n",
      "epoch 9087: train loss: 0.12783180282915055, test loss: 0.25266684083796515\n",
      "epoch 9088: train loss: 0.12782781763528692, test loss: 0.25266661106535465\n",
      "epoch 9089: train loss: 0.1278238330647788, test loss: 0.25266638166063354\n",
      "epoch 9090: train loss: 0.12781984911745625, test loss: 0.2526661526236906\n",
      "epoch 9091: train loss: 0.12781586579314935, test loss: 0.25266592395438514\n",
      "epoch 9092: train loss: 0.12781188309168834, test loss: 0.2526656956526334\n",
      "epoch 9093: train loss: 0.12780790101290343, test loss: 0.2526654677182947\n",
      "epoch 9094: train loss: 0.12780391955662496, test loss: 0.25266524015126257\n",
      "epoch 9095: train loss: 0.1277999387226833, test loss: 0.25266501295142413\n",
      "epoch 9096: train loss: 0.12779595851090889, test loss: 0.252664786118663\n",
      "epoch 9097: train loss: 0.12779197892113223, test loss: 0.252664559652853\n",
      "epoch 9098: train loss: 0.12778799995318388, test loss: 0.2526643335538934\n",
      "epoch 9099: train loss: 0.1277840216068945, test loss: 0.2526641078216646\n",
      "epoch 9100: train loss: 0.1277800438820948, test loss: 0.2526638824560441\n",
      "epoch 9101: train loss: 0.12777606677861553, test loss: 0.2526636574569273\n",
      "epoch 9102: train loss: 0.12777209029628747, test loss: 0.25266343282418435\n",
      "epoch 9103: train loss: 0.1277681144349416, test loss: 0.2526632085577149\n",
      "epoch 9104: train loss: 0.12776413919440882, test loss: 0.25266298465739606\n",
      "epoch 9105: train loss: 0.12776016457452016, test loss: 0.25266276112311326\n",
      "epoch 9106: train loss: 0.12775619057510673, test loss: 0.25266253795474714\n",
      "epoch 9107: train loss: 0.1277522171959997, test loss: 0.25266231515219845\n",
      "epoch 9108: train loss: 0.1277482444370302, test loss: 0.25266209271532947\n",
      "epoch 9109: train loss: 0.12774427229802957, test loss: 0.25266187064404344\n",
      "epoch 9110: train loss: 0.1277403007788291, test loss: 0.25266164893822013\n",
      "epoch 9111: train loss: 0.1277363298792603, test loss: 0.25266142759774346\n",
      "epoch 9112: train loss: 0.12773235959915458, test loss: 0.25266120662249547\n",
      "epoch 9113: train loss: 0.12772838993834346, test loss: 0.25266098601236164\n",
      "epoch 9114: train loss: 0.12772442089665853, test loss: 0.25266076576723107\n",
      "epoch 9115: train loss: 0.12772045247393152, test loss: 0.252660545886995\n",
      "epoch 9116: train loss: 0.12771648466999408, test loss: 0.25266032637152497\n",
      "epoch 9117: train loss: 0.12771251748467805, test loss: 0.2526601072207131\n",
      "epoch 9118: train loss: 0.12770855091781524, test loss: 0.25265988843444814\n",
      "epoch 9119: train loss: 0.12770458496923764, test loss: 0.2526596700126126\n",
      "epoch 9120: train loss: 0.12770061963877719, test loss: 0.25265945195508865\n",
      "epoch 9121: train loss: 0.1276966549262659, test loss: 0.25265923426177006\n",
      "epoch 9122: train loss: 0.12769269083153595, test loss: 0.25265901693253556\n",
      "epoch 9123: train loss: 0.12768872735441947, test loss: 0.25265879996726803\n",
      "epoch 9124: train loss: 0.12768476449474872, test loss: 0.25265858336586056\n",
      "epoch 9125: train loss: 0.12768080225235598, test loss: 0.25265836712820056\n",
      "epoch 9126: train loss: 0.1276768406270736, test loss: 0.2526581512541623\n",
      "epoch 9127: train loss: 0.12767287961873408, test loss: 0.2526579357436451\n",
      "epoch 9128: train loss: 0.12766891922716983, test loss: 0.2526577205965349\n",
      "epoch 9129: train loss: 0.12766495945221346, test loss: 0.2526575058126951\n",
      "epoch 9130: train loss: 0.12766100029369756, test loss: 0.2526572913920407\n",
      "epoch 9131: train loss: 0.12765704175145484, test loss: 0.2526570773344379\n",
      "epoch 9132: train loss: 0.12765308382531798, test loss: 0.2526568636397872\n",
      "epoch 9133: train loss: 0.1276491265151199, test loss: 0.25265665030796663\n",
      "epoch 9134: train loss: 0.12764516982069338, test loss: 0.25265643733885557\n",
      "epoch 9135: train loss: 0.1276412137418714, test loss: 0.25265622473236166\n",
      "epoch 9136: train loss: 0.12763725827848696, test loss: 0.2526560124883524\n",
      "epoch 9137: train loss: 0.12763330343037307, test loss: 0.25265580060671505\n",
      "epoch 9138: train loss: 0.1276293491973629, test loss: 0.25265558908734037\n",
      "epoch 9139: train loss: 0.12762539557928967, test loss: 0.25265537793012577\n",
      "epoch 9140: train loss: 0.12762144257598657, test loss: 0.2526551671349334\n",
      "epoch 9141: train loss: 0.12761749018728696, test loss: 0.25265495670167576\n",
      "epoch 9142: train loss: 0.1276135384130242, test loss: 0.2526547466302252\n",
      "epoch 9143: train loss: 0.1276095872530317, test loss: 0.25265453692046863\n",
      "epoch 9144: train loss: 0.127605636707143, test loss: 0.2526543275722944\n",
      "epoch 9145: train loss: 0.1276016867751917, test loss: 0.25265411858558684\n",
      "epoch 9146: train loss: 0.1275977374570114, test loss: 0.25265390996023823\n",
      "epoch 9147: train loss: 0.1275937887524358, test loss: 0.2526537016961364\n",
      "epoch 9148: train loss: 0.12758984066129858, test loss: 0.252653493793158\n",
      "epoch 9149: train loss: 0.1275858931834337, test loss: 0.25265328625119976\n",
      "epoch 9150: train loss: 0.12758194631867495, test loss: 0.25265307907014944\n",
      "epoch 9151: train loss: 0.1275780000668563, test loss: 0.25265287224988436\n",
      "epoch 9152: train loss: 0.12757405442781175, test loss: 0.2526526657903003\n",
      "epoch 9153: train loss: 0.12757010940137536, test loss: 0.2526524596912828\n",
      "epoch 9154: train loss: 0.1275661649873813, test loss: 0.2526522539527122\n",
      "epoch 9155: train loss: 0.12756222118566377, test loss: 0.25265204857448603\n",
      "epoch 9156: train loss: 0.12755827799605698, test loss: 0.2526518435564882\n",
      "epoch 9157: train loss: 0.1275543354183953, test loss: 0.25265163889860864\n",
      "epoch 9158: train loss: 0.1275503934525131, test loss: 0.2526514346007196\n",
      "epoch 9159: train loss: 0.12754645209824478, test loss: 0.25265123066272743\n",
      "epoch 9160: train loss: 0.12754251135542494, test loss: 0.2526510270845124\n",
      "epoch 9161: train loss: 0.1275385712238881, test loss: 0.25265082386595694\n",
      "epoch 9162: train loss: 0.12753463170346888, test loss: 0.25265062100695757\n",
      "epoch 9163: train loss: 0.127530692794002, test loss: 0.25265041850740017\n",
      "epoch 9164: train loss: 0.12752675449532222, test loss: 0.25265021636717383\n",
      "epoch 9165: train loss: 0.12752281680726438, test loss: 0.2526500145861488\n",
      "epoch 9166: train loss: 0.12751887972966333, test loss: 0.25264981316423835\n",
      "epoch 9167: train loss: 0.12751494326235405, test loss: 0.2526496121013129\n",
      "epoch 9168: train loss: 0.12751100740517152, test loss: 0.2526494113972698\n",
      "epoch 9169: train loss: 0.12750707215795085, test loss: 0.2526492110519957\n",
      "epoch 9170: train loss: 0.1275031375205271, test loss: 0.25264901106536763\n",
      "epoch 9171: train loss: 0.12749920349273558, test loss: 0.2526488114372885\n",
      "epoch 9172: train loss: 0.12749527007441147, test loss: 0.25264861216763196\n",
      "epoch 9173: train loss: 0.1274913372653901, test loss: 0.2526484132563094\n",
      "epoch 9174: train loss: 0.12748740506550685, test loss: 0.2526482147031776\n",
      "epoch 9175: train loss: 0.1274834734745972, test loss: 0.2526480165081517\n",
      "epoch 9176: train loss: 0.12747954249249666, test loss: 0.2526478186711032\n",
      "epoch 9177: train loss: 0.1274756121190407, test loss: 0.2526476211919286\n",
      "epoch 9178: train loss: 0.12747168235406509, test loss: 0.2526474240705122\n",
      "epoch 9179: train loss: 0.12746775319740544, test loss: 0.2526472273067489\n",
      "epoch 9180: train loss: 0.12746382464889752, test loss: 0.2526470309005206\n",
      "epoch 9181: train loss: 0.12745989670837718, test loss: 0.25264683485172207\n",
      "epoch 9182: train loss: 0.12745596937568024, test loss: 0.25264663916022867\n",
      "epoch 9183: train loss: 0.1274520426506427, test loss: 0.2526464438259444\n",
      "epoch 9184: train loss: 0.12744811653310054, test loss: 0.25264624884874687\n",
      "epoch 9185: train loss: 0.12744419102288979, test loss: 0.2526460542285356\n",
      "epoch 9186: train loss: 0.12744026611984666, test loss: 0.2526458599651943\n",
      "epoch 9187: train loss: 0.12743634182380728, test loss: 0.2526456660586025\n",
      "epoch 9188: train loss: 0.12743241813460787, test loss: 0.252645472508663\n",
      "epoch 9189: train loss: 0.12742849505208484, test loss: 0.2526452793152478\n",
      "epoch 9190: train loss: 0.12742457257607448, test loss: 0.2526450864782681\n",
      "epoch 9191: train loss: 0.12742065070641329, test loss: 0.25264489399759965\n",
      "epoch 9192: train loss: 0.1274167294429377, test loss: 0.2526447018731308\n",
      "epoch 9193: train loss: 0.12741280878548428, test loss: 0.2526445101047605\n",
      "epoch 9194: train loss: 0.12740888873388967, test loss: 0.2526443186923609\n",
      "epoch 9195: train loss: 0.12740496928799058, test loss: 0.2526441276358323\n",
      "epoch 9196: train loss: 0.12740105044762373, test loss: 0.25264393693506265\n",
      "epoch 9197: train loss: 0.1273971322126259, test loss: 0.2526437465899511\n",
      "epoch 9198: train loss: 0.127393214582834, test loss: 0.2526435566003629\n",
      "epoch 9199: train loss: 0.12738929755808492, test loss: 0.25264336696620737\n",
      "epoch 9200: train loss: 0.12738538113821568, test loss: 0.25264317768737454\n",
      "epoch 9201: train loss: 0.12738146532306335, test loss: 0.25264298876373736\n",
      "epoch 9202: train loss: 0.12737755011246496, test loss: 0.2526428001951971\n",
      "epoch 9203: train loss: 0.12737363550625777, test loss: 0.25264261198164234\n",
      "epoch 9204: train loss: 0.12736972150427892, test loss: 0.25264242412296156\n",
      "epoch 9205: train loss: 0.12736580810636583, test loss: 0.2526422366190511\n",
      "epoch 9206: train loss: 0.12736189531235576, test loss: 0.2526420494697839\n",
      "epoch 9207: train loss: 0.12735798312208613, test loss: 0.2526418626750736\n",
      "epoch 9208: train loss: 0.1273540715353945, test loss: 0.2526416762347797\n",
      "epoch 9209: train loss: 0.12735016055211834, test loss: 0.25264149014882453\n",
      "epoch 9210: train loss: 0.12734625017209525, test loss: 0.25264130441707233\n",
      "epoch 9211: train loss: 0.12734234039516296, test loss: 0.2526411190394249\n",
      "epoch 9212: train loss: 0.1273384312211591, test loss: 0.25264093401577137\n",
      "epoch 9213: train loss: 0.12733452264992154, test loss: 0.25264074934599284\n",
      "epoch 9214: train loss: 0.12733061468128806, test loss: 0.25264056502999876\n",
      "epoch 9215: train loss: 0.12732670731509665, test loss: 0.25264038106765807\n",
      "epoch 9216: train loss: 0.1273228005511852, test loss: 0.25264019745887756\n",
      "epoch 9217: train loss: 0.12731889438939173, test loss: 0.25264001420353815\n",
      "epoch 9218: train loss: 0.1273149888295544, test loss: 0.2526398313015293\n",
      "epoch 9219: train loss: 0.1273110838715113, test loss: 0.252639648752745\n",
      "epoch 9220: train loss: 0.12730717951510068, test loss: 0.2526394665570795\n",
      "epoch 9221: train loss: 0.1273032757601608, test loss: 0.2526392847144101\n",
      "epoch 9222: train loss: 0.12729937260653, test loss: 0.2526391032246397\n",
      "epoch 9223: train loss: 0.12729547005404662, test loss: 0.2526389220876569\n",
      "epoch 9224: train loss: 0.12729156810254919, test loss: 0.2526387413033459\n",
      "epoch 9225: train loss: 0.1272876667518762, test loss: 0.2526385608716065\n",
      "epoch 9226: train loss: 0.12728376600186622, test loss: 0.25263838079231526\n",
      "epoch 9227: train loss: 0.12727986585235784, test loss: 0.2526382010653855\n",
      "epoch 9228: train loss: 0.12727596630318985, test loss: 0.25263802169067956\n",
      "epoch 9229: train loss: 0.12727206735420094, test loss: 0.25263784266811584\n",
      "epoch 9230: train loss: 0.12726816900522994, test loss: 0.25263766399756155\n",
      "epoch 9231: train loss: 0.12726427125611575, test loss: 0.2526374856789224\n",
      "epoch 9232: train loss: 0.1272603741066973, test loss: 0.25263730771208665\n",
      "epoch 9233: train loss: 0.12725647755681357, test loss: 0.25263713009694405\n",
      "epoch 9234: train loss: 0.12725258160630365, test loss: 0.2526369528333821\n",
      "epoch 9235: train loss: 0.1272486862550066, test loss: 0.25263677592129136\n",
      "epoch 9236: train loss: 0.12724479150276166, test loss: 0.2526365993605743\n",
      "epoch 9237: train loss: 0.12724089734940808, test loss: 0.2526364231511067\n",
      "epoch 9238: train loss: 0.1272370037947851, test loss: 0.25263624729278855\n",
      "epoch 9239: train loss: 0.12723311083873218, test loss: 0.25263607178552155\n",
      "epoch 9240: train loss: 0.12722921848108862, test loss: 0.2526358966291704\n",
      "epoch 9241: train loss: 0.12722532672169395, test loss: 0.25263572182364963\n",
      "epoch 9242: train loss: 0.12722143556038773, test loss: 0.2526355473688356\n",
      "epoch 9243: train loss: 0.12721754499700957, test loss: 0.2526353732646303\n",
      "epoch 9244: train loss: 0.12721365503139911, test loss: 0.25263519951092717\n",
      "epoch 9245: train loss: 0.1272097656633961, test loss: 0.2526350261076011\n",
      "epoch 9246: train loss: 0.12720587689284027, test loss: 0.2526348530545552\n",
      "epoch 9247: train loss: 0.12720198871957153, test loss: 0.25263468035168657\n",
      "epoch 9248: train loss: 0.1271981011434297, test loss: 0.2526345079988746\n",
      "epoch 9249: train loss: 0.1271942141642548, test loss: 0.25263433599602\n",
      "epoch 9250: train loss: 0.12719032778188682, test loss: 0.25263416434301367\n",
      "epoch 9251: train loss: 0.12718644199616588, test loss: 0.2526339930397398\n",
      "epoch 9252: train loss: 0.1271825568069321, test loss: 0.2526338220860999\n",
      "epoch 9253: train loss: 0.12717867221402568, test loss: 0.252633651481977\n",
      "epoch 9254: train loss: 0.12717478821728692, test loss: 0.25263348122726803\n",
      "epoch 9255: train loss: 0.12717090481655607, test loss: 0.25263331132186595\n",
      "epoch 9256: train loss: 0.12716702201167357, test loss: 0.2526331417656547\n",
      "epoch 9257: train loss: 0.12716313980247984, test loss: 0.2526329725585342\n",
      "epoch 9258: train loss: 0.12715925818881538, test loss: 0.25263280370040286\n",
      "epoch 9259: train loss: 0.12715537717052075, test loss: 0.25263263519113965\n",
      "epoch 9260: train loss: 0.1271514967474366, test loss: 0.2526324670306465\n",
      "epoch 9261: train loss: 0.12714761691940354, test loss: 0.2526322992188082\n",
      "epoch 9262: train loss: 0.12714373768626236, test loss: 0.25263213175551785\n",
      "epoch 9263: train loss: 0.12713985904785383, test loss: 0.2526319646406676\n",
      "epoch 9264: train loss: 0.12713598100401885, test loss: 0.25263179787415113\n",
      "epoch 9265: train loss: 0.12713210355459834, test loss: 0.25263163145586975\n",
      "epoch 9266: train loss: 0.12712822669943322, test loss: 0.2526314653857039\n",
      "epoch 9267: train loss: 0.12712435043836456, test loss: 0.2526312996635488\n",
      "epoch 9268: train loss: 0.12712047477123345, test loss: 0.25263113428930395\n",
      "epoch 9269: train loss: 0.12711659969788108, test loss: 0.2526309692628483\n",
      "epoch 9270: train loss: 0.12711272521814862, test loss: 0.2526308045840859\n",
      "epoch 9271: train loss: 0.12710885133187738, test loss: 0.25263064025290755\n",
      "epoch 9272: train loss: 0.12710497803890866, test loss: 0.25263047626920654\n",
      "epoch 9273: train loss: 0.1271011053390839, test loss: 0.2526303126328642\n",
      "epoch 9274: train loss: 0.12709723323224448, test loss: 0.2526301493437917\n",
      "epoch 9275: train loss: 0.12709336171823196, test loss: 0.25262998640186984\n",
      "epoch 9276: train loss: 0.12708949079688792, test loss: 0.2526298238070007\n",
      "epoch 9277: train loss: 0.12708562046805397, test loss: 0.2526296615590668\n",
      "epoch 9278: train loss: 0.1270817507315718, test loss: 0.2526294996579572\n",
      "epoch 9279: train loss: 0.12707788158728314, test loss: 0.25262933810358407\n",
      "epoch 9280: train loss: 0.12707401303502983, test loss: 0.25262917689582814\n",
      "epoch 9281: train loss: 0.12707014507465375, test loss: 0.2526290160345715\n",
      "epoch 9282: train loss: 0.12706627770599677, test loss: 0.2526288555197301\n",
      "epoch 9283: train loss: 0.12706241092890092, test loss: 0.25262869535118976\n",
      "epoch 9284: train loss: 0.1270585447432082, test loss: 0.2526285355288363\n",
      "epoch 9285: train loss: 0.1270546791487608, test loss: 0.2526283760525691\n",
      "epoch 9286: train loss: 0.1270508141454008, test loss: 0.2526282169222826\n",
      "epoch 9287: train loss: 0.1270469497329704, test loss: 0.25262805813786426\n",
      "epoch 9288: train loss: 0.12704308591131194, test loss: 0.2526278996992127\n",
      "epoch 9289: train loss: 0.12703922268026777, test loss: 0.2526277416062176\n",
      "epoch 9290: train loss: 0.12703536003968022, test loss: 0.25262758385877176\n",
      "epoch 9291: train loss: 0.12703149798939178, test loss: 0.25262742645677766\n",
      "epoch 9292: train loss: 0.12702763652924498, test loss: 0.2526272694001214\n",
      "epoch 9293: train loss: 0.1270237756590824, test loss: 0.2526271126886959\n",
      "epoch 9294: train loss: 0.12701991537874663, test loss: 0.25262695632239174\n",
      "epoch 9295: train loss: 0.12701605568808036, test loss: 0.25262680030111956\n",
      "epoch 9296: train loss: 0.12701219658692636, test loss: 0.25262664462475576\n",
      "epoch 9297: train loss: 0.12700833807512743, test loss: 0.2526264892931994\n",
      "epoch 9298: train loss: 0.1270044801525265, test loss: 0.25262633430634623\n",
      "epoch 9299: train loss: 0.12700062281896637, test loss: 0.2526261796640886\n",
      "epoch 9300: train loss: 0.12699676607429014, test loss: 0.25262602536632117\n",
      "epoch 9301: train loss: 0.12699290991834078, test loss: 0.25262587141293935\n",
      "epoch 9302: train loss: 0.12698905435096142, test loss: 0.2526257178038281\n",
      "epoch 9303: train loss: 0.12698519937199518, test loss: 0.2526255645388957\n",
      "epoch 9304: train loss: 0.12698134498128533, test loss: 0.2526254116180288\n",
      "epoch 9305: train loss: 0.12697749117867513, test loss: 0.2526252590411169\n",
      "epoch 9306: train loss: 0.1269736379640079, test loss: 0.25262510680806716\n",
      "epoch 9307: train loss: 0.12696978533712705, test loss: 0.252624954918764\n",
      "epoch 9308: train loss: 0.12696593329787598, test loss: 0.2526248033731006\n",
      "epoch 9309: train loss: 0.12696208184609825, test loss: 0.2526246521709753\n",
      "epoch 9310: train loss: 0.12695823098163747, test loss: 0.2526245013122907\n",
      "epoch 9311: train loss: 0.12695438070433718, test loss: 0.2526243507969223\n",
      "epoch 9312: train loss: 0.12695053101404108, test loss: 0.2526242006247751\n",
      "epoch 9313: train loss: 0.1269466819105929, test loss: 0.25262405079574907\n",
      "epoch 9314: train loss: 0.12694283339383652, test loss: 0.2526239013097339\n",
      "epoch 9315: train loss: 0.1269389854636157, test loss: 0.25262375216661775\n",
      "epoch 9316: train loss: 0.12693513811977447, test loss: 0.252623603366306\n",
      "epoch 9317: train loss: 0.12693129136215664, test loss: 0.2526234549086861\n",
      "epoch 9318: train loss: 0.1269274451906064, test loss: 0.2526233067936574\n",
      "epoch 9319: train loss: 0.12692359960496777, test loss: 0.2526231590211116\n",
      "epoch 9320: train loss: 0.12691975460508487, test loss: 0.2526230115909416\n",
      "epoch 9321: train loss: 0.12691591019080195, test loss: 0.2526228645030454\n",
      "epoch 9322: train loss: 0.12691206636196328, test loss: 0.25262271775732054\n",
      "epoch 9323: train loss: 0.12690822311841315, test loss: 0.2526225713536597\n",
      "epoch 9324: train loss: 0.12690438045999597, test loss: 0.2526224252919556\n",
      "epoch 9325: train loss: 0.12690053838655616, test loss: 0.2526222795721078\n",
      "epoch 9326: train loss: 0.1268966968979382, test loss: 0.2526221341940078\n",
      "epoch 9327: train loss: 0.12689285599398667, test loss: 0.2526219891575467\n",
      "epoch 9328: train loss: 0.12688901567454616, test loss: 0.2526218444626297\n",
      "epoch 9329: train loss: 0.12688517593946136, test loss: 0.2526217001091514\n",
      "epoch 9330: train loss: 0.126881336788577, test loss: 0.25262155609699205\n",
      "epoch 9331: train loss: 0.12687749822173783, test loss: 0.2526214124260669\n",
      "epoch 9332: train loss: 0.1268736602387887, test loss: 0.2526212690962617\n",
      "epoch 9333: train loss: 0.12686982283957454, test loss: 0.2526211261074659\n",
      "epoch 9334: train loss: 0.1268659860239403, test loss: 0.2526209834595862\n",
      "epoch 9335: train loss: 0.12686214979173097, test loss: 0.25262084115251704\n",
      "epoch 9336: train loss: 0.1268583141427916, test loss: 0.2526206991861482\n",
      "epoch 9337: train loss: 0.1268544790769674, test loss: 0.2526205575603704\n",
      "epoch 9338: train loss: 0.1268506445941035, test loss: 0.2526204162750896\n",
      "epoch 9339: train loss: 0.12684681069404516, test loss: 0.25262027533020565\n",
      "epoch 9340: train loss: 0.12684297737663766, test loss: 0.25262013472560385\n",
      "epoch 9341: train loss: 0.12683914464172635, test loss: 0.2526199944611789\n",
      "epoch 9342: train loss: 0.1268353124891567, test loss: 0.25261985453682967\n",
      "epoch 9343: train loss: 0.12683148091877416, test loss: 0.2526197149524592\n",
      "epoch 9344: train loss: 0.12682764993042425, test loss: 0.2526195757079541\n",
      "epoch 9345: train loss: 0.1268238195239526, test loss: 0.2526194368032136\n",
      "epoch 9346: train loss: 0.12681998969920477, test loss: 0.2526192982381326\n",
      "epoch 9347: train loss: 0.12681616045602653, test loss: 0.25261916001261453\n",
      "epoch 9348: train loss: 0.12681233179426366, test loss: 0.2526190221265414\n",
      "epoch 9349: train loss: 0.12680850371376187, test loss: 0.2526188845798209\n",
      "epoch 9350: train loss: 0.12680467621436717, test loss: 0.2526187473723449\n",
      "epoch 9351: train loss: 0.1268008492959254, test loss: 0.2526186105040129\n",
      "epoch 9352: train loss: 0.12679702295828257, test loss: 0.25261847397471143\n",
      "epoch 9353: train loss: 0.12679319720128476, test loss: 0.25261833778434833\n",
      "epoch 9354: train loss: 0.12678937202477802, test loss: 0.25261820193281515\n",
      "epoch 9355: train loss: 0.12678554742860854, test loss: 0.25261806642000745\n",
      "epoch 9356: train loss: 0.12678172341262253, test loss: 0.25261793124582516\n",
      "epoch 9357: train loss: 0.1267778999766663, test loss: 0.25261779641015986\n",
      "epoch 9358: train loss: 0.1267740771205861, test loss: 0.2526176619129161\n",
      "epoch 9359: train loss: 0.1267702548442284, test loss: 0.25261752775397683\n",
      "epoch 9360: train loss: 0.1267664331474396, test loss: 0.25261739393324983\n",
      "epoch 9361: train loss: 0.12676261203006617, test loss: 0.25261726045063276\n",
      "epoch 9362: train loss: 0.12675879149195476, test loss: 0.2526171273060118\n",
      "epoch 9363: train loss: 0.12675497153295193, test loss: 0.25261699449928854\n",
      "epoch 9364: train loss: 0.12675115215290433, test loss: 0.25261686203037037\n",
      "epoch 9365: train loss: 0.12674733335165872, test loss: 0.25261672989913775\n",
      "epoch 9366: train loss: 0.12674351512906193, test loss: 0.2526165981054961\n",
      "epoch 9367: train loss: 0.1267396974849607, test loss: 0.25261646664934495\n",
      "epoch 9368: train loss: 0.12673588041920197, test loss: 0.2526163355305723\n",
      "epoch 9369: train loss: 0.12673206393163275, test loss: 0.2526162047490837\n",
      "epoch 9370: train loss: 0.12672824802209998, test loss: 0.2526160743047714\n",
      "epoch 9371: train loss: 0.12672443269045078, test loss: 0.2526159441975328\n",
      "epoch 9372: train loss: 0.12672061793653222, test loss: 0.2526158144272724\n",
      "epoch 9373: train loss: 0.12671680376019154, test loss: 0.2526156849938746\n",
      "epoch 9374: train loss: 0.12671299016127593, test loss: 0.25261555589724405\n",
      "epoch 9375: train loss: 0.12670917713963273, test loss: 0.2526154271372791\n",
      "epoch 9376: train loss: 0.1267053646951093, test loss: 0.25261529871386756\n",
      "epoch 9377: train loss: 0.12670155282755297, test loss: 0.2526151706269239\n",
      "epoch 9378: train loss: 0.12669774153681124, test loss: 0.252615042876328\n",
      "epoch 9379: train loss: 0.12669393082273164, test loss: 0.2526149154619863\n",
      "epoch 9380: train loss: 0.12669012068516178, test loss: 0.2526147883837947\n",
      "epoch 9381: train loss: 0.12668631112394924, test loss: 0.2526146616416521\n",
      "epoch 9382: train loss: 0.12668250213894175, test loss: 0.2526145352354604\n",
      "epoch 9383: train loss: 0.12667869372998702, test loss: 0.2526144091651028\n",
      "epoch 9384: train loss: 0.12667488589693285, test loss: 0.2526142834304955\n",
      "epoch 9385: train loss: 0.12667107863962712, test loss: 0.2526141580315222\n",
      "epoch 9386: train loss: 0.12666727195791777, test loss: 0.2526140329680782\n",
      "epoch 9387: train loss: 0.12666346585165272, test loss: 0.25261390824007246\n",
      "epoch 9388: train loss: 0.12665966032068002, test loss: 0.25261378384739497\n",
      "epoch 9389: train loss: 0.12665585536484775, test loss: 0.25261365978995637\n",
      "epoch 9390: train loss: 0.12665205098400403, test loss: 0.2526135360676383\n",
      "epoch 9391: train loss: 0.12664824717799708, test loss: 0.25261341268034193\n",
      "epoch 9392: train loss: 0.12664444394667515, test loss: 0.252613289627971\n",
      "epoch 9393: train loss: 0.12664064128988653, test loss: 0.2526131669104236\n",
      "epoch 9394: train loss: 0.12663683920747962, test loss: 0.2526130445275975\n",
      "epoch 9395: train loss: 0.1266330376993028, test loss: 0.2526129224793912\n",
      "epoch 9396: train loss: 0.12662923676520454, test loss: 0.25261280076569487\n",
      "epoch 9397: train loss: 0.1266254364050334, test loss: 0.25261267938641124\n",
      "epoch 9398: train loss: 0.12662163661863798, test loss: 0.25261255834143476\n",
      "epoch 9399: train loss: 0.12661783740586685, test loss: 0.25261243763068103\n",
      "epoch 9400: train loss: 0.12661403876656876, test loss: 0.2526123172540322\n",
      "epoch 9401: train loss: 0.12661024070059249, test loss: 0.25261219721138234\n",
      "epoch 9402: train loss: 0.1266064432077868, test loss: 0.2526120775026437\n",
      "epoch 9403: train loss: 0.12660264628800058, test loss: 0.2526119581277056\n",
      "epoch 9404: train loss: 0.12659884994108275, test loss: 0.2526118390864767\n",
      "epoch 9405: train loss: 0.12659505416688227, test loss: 0.25261172037884344\n",
      "epoch 9406: train loss: 0.12659125896524817, test loss: 0.2526116020047083\n",
      "epoch 9407: train loss: 0.12658746433602958, test loss: 0.25261148396396704\n",
      "epoch 9408: train loss: 0.12658367027907558, test loss: 0.25261136625652775\n",
      "epoch 9409: train loss: 0.12657987679423546, test loss: 0.2526112488822848\n",
      "epoch 9410: train loss: 0.1265760838813584, test loss: 0.25261113184113354\n",
      "epoch 9411: train loss: 0.1265722915402937, test loss: 0.25261101513297374\n",
      "epoch 9412: train loss: 0.12656849977089077, test loss: 0.25261089875770926\n",
      "epoch 9413: train loss: 0.12656470857299904, test loss: 0.2526107827152347\n",
      "epoch 9414: train loss: 0.12656091794646798, test loss: 0.2526106670054388\n",
      "epoch 9415: train loss: 0.12655712789114706, test loss: 0.2526105516282411\n",
      "epoch 9416: train loss: 0.12655333840688596, test loss: 0.252610436583534\n",
      "epoch 9417: train loss: 0.12654954949353428, test loss: 0.25261032187120336\n",
      "epoch 9418: train loss: 0.12654576115094168, test loss: 0.25261020749115914\n",
      "epoch 9419: train loss: 0.12654197337895798, test loss: 0.25261009344330937\n",
      "epoch 9420: train loss: 0.12653818617743295, test loss: 0.25260997972753224\n",
      "epoch 9421: train loss: 0.12653439954621645, test loss: 0.25260986634374644\n",
      "epoch 9422: train loss: 0.12653061348515846, test loss: 0.2526097532918303\n",
      "epoch 9423: train loss: 0.12652682799410886, test loss: 0.2526096405717089\n",
      "epoch 9424: train loss: 0.12652304307291776, test loss: 0.25260952818326216\n",
      "epoch 9425: train loss: 0.12651925872143518, test loss: 0.252609416126391\n",
      "epoch 9426: train loss: 0.12651547493951135, test loss: 0.2526093044009998\n",
      "epoch 9427: train loss: 0.1265116917269964, test loss: 0.2526091930069937\n",
      "epoch 9428: train loss: 0.12650790908374054, test loss: 0.2526090819442593\n",
      "epoch 9429: train loss: 0.12650412700959418, test loss: 0.25260897121270787\n",
      "epoch 9430: train loss: 0.12650034550440759, test loss: 0.25260886081222667\n",
      "epoch 9431: train loss: 0.12649656456803127, test loss: 0.25260875074272887\n",
      "epoch 9432: train loss: 0.1264927842003156, test loss: 0.252608641004103\n",
      "epoch 9433: train loss: 0.12648900440111116, test loss: 0.25260853159625535\n",
      "epoch 9434: train loss: 0.1264852251702685, test loss: 0.25260842251908544\n",
      "epoch 9435: train loss: 0.12648144650763832, test loss: 0.25260831377249127\n",
      "epoch 9436: train loss: 0.12647766841307126, test loss: 0.25260820535636536\n",
      "epoch 9437: train loss: 0.12647389088641806, test loss: 0.252608097270617\n",
      "epoch 9438: train loss: 0.12647011392752952, test loss: 0.2526079895151491\n",
      "epoch 9439: train loss: 0.12646633753625652, test loss: 0.2526078820898525\n",
      "epoch 9440: train loss: 0.12646256171244996, test loss: 0.25260777499463055\n",
      "epoch 9441: train loss: 0.1264587864559608, test loss: 0.25260766822938757\n",
      "epoch 9442: train loss: 0.12645501176664006, test loss: 0.2526075617940186\n",
      "epoch 9443: train loss: 0.1264512376443388, test loss: 0.25260745568841925\n",
      "epoch 9444: train loss: 0.12644746408890817, test loss: 0.252607349912505\n",
      "epoch 9445: train loss: 0.12644369110019935, test loss: 0.2526072444661612\n",
      "epoch 9446: train loss: 0.12643991867806356, test loss: 0.2526071393492886\n",
      "epoch 9447: train loss: 0.1264361468223521, test loss: 0.25260703456179817\n",
      "epoch 9448: train loss: 0.12643237553291634, test loss: 0.2526069301035839\n",
      "epoch 9449: train loss: 0.12642860480960766, test loss: 0.2526068259745438\n",
      "epoch 9450: train loss: 0.12642483465227752, test loss: 0.25260672217457913\n",
      "epoch 9451: train loss: 0.1264210650607774, test loss: 0.2526066187035932\n",
      "epoch 9452: train loss: 0.12641729603495894, test loss: 0.25260651556149205\n",
      "epoch 9453: train loss: 0.12641352757467372, test loss: 0.25260641274816126\n",
      "epoch 9454: train loss: 0.12640975967977341, test loss: 0.2526063102635116\n",
      "epoch 9455: train loss: 0.1264059923501097, test loss: 0.25260620810743684\n",
      "epoch 9456: train loss: 0.12640222558553443, test loss: 0.2526061062798496\n",
      "epoch 9457: train loss: 0.12639845938589944, test loss: 0.25260600478064194\n",
      "epoch 9458: train loss: 0.12639469375105655, test loss: 0.25260590360971036\n",
      "epoch 9459: train loss: 0.1263909286808578, test loss: 0.25260580276696404\n",
      "epoch 9460: train loss: 0.1263871641751551, test loss: 0.25260570225230766\n",
      "epoch 9461: train loss: 0.12638340023380057, test loss: 0.2526056020656258\n",
      "epoch 9462: train loss: 0.1263796368566463, test loss: 0.2526055022068306\n",
      "epoch 9463: train loss: 0.12637587404354447, test loss: 0.2526054026758294\n",
      "epoch 9464: train loss: 0.12637211179434726, test loss: 0.25260530347250487\n",
      "epoch 9465: train loss: 0.12636835010890693, test loss: 0.25260520459677105\n",
      "epoch 9466: train loss: 0.12636458898707587, test loss: 0.25260510604852887\n",
      "epoch 9467: train loss: 0.12636082842870638, test loss: 0.2526050078276728\n",
      "epoch 9468: train loss: 0.12635706843365097, test loss: 0.2526049099341092\n",
      "epoch 9469: train loss: 0.12635330900176206, test loss: 0.25260481236773497\n",
      "epoch 9470: train loss: 0.12634955013289226, test loss: 0.25260471512846083\n",
      "epoch 9471: train loss: 0.1263457918268941, test loss: 0.25260461821617414\n",
      "epoch 9472: train loss: 0.12634203408362027, test loss: 0.25260452163079505\n",
      "epoch 9473: train loss: 0.12633827690292346, test loss: 0.25260442537219846\n",
      "epoch 9474: train loss: 0.12633452028465642, test loss: 0.2526043294403087\n",
      "epoch 9475: train loss: 0.12633076422867195, test loss: 0.25260423383501235\n",
      "epoch 9476: train loss: 0.12632700873482297, test loss: 0.25260413855622327\n",
      "epoch 9477: train loss: 0.12632325380296236, test loss: 0.2526040436038311\n",
      "epoch 9478: train loss: 0.12631949943294307, test loss: 0.25260394897774946\n",
      "epoch 9479: train loss: 0.12631574562461817, test loss: 0.2526038546778735\n",
      "epoch 9480: train loss: 0.12631199237784071, test loss: 0.25260376070410406\n",
      "epoch 9481: train loss: 0.12630823969246383, test loss: 0.2526036670563405\n",
      "epoch 9482: train loss: 0.12630448756834073, test loss: 0.2526035737344872\n",
      "epoch 9483: train loss: 0.12630073600532465, test loss: 0.2526034807384572\n",
      "epoch 9484: train loss: 0.12629698500326886, test loss: 0.2526033880681342\n",
      "epoch 9485: train loss: 0.12629323456202673, test loss: 0.2526032957234261\n",
      "epoch 9486: train loss: 0.12628948468145168, test loss: 0.2526032037042382\n",
      "epoch 9487: train loss: 0.12628573536139712, test loss: 0.2526031120104627\n",
      "epoch 9488: train loss: 0.12628198660171658, test loss: 0.25260302064201584\n",
      "epoch 9489: train loss: 0.12627823840226363, test loss: 0.2526029295987954\n",
      "epoch 9490: train loss: 0.1262744907628919, test loss: 0.25260283888068946\n",
      "epoch 9491: train loss: 0.12627074368345503, test loss: 0.2526027484876268\n",
      "epoch 9492: train loss: 0.12626699716380674, test loss: 0.25260265841948826\n",
      "epoch 9493: train loss: 0.12626325120380083, test loss: 0.252602568676179\n",
      "epoch 9494: train loss: 0.12625950580329112, test loss: 0.2526024792575991\n",
      "epoch 9495: train loss: 0.1262557609621315, test loss: 0.2526023901636632\n",
      "epoch 9496: train loss: 0.1262520166801759, test loss: 0.2526023013942632\n",
      "epoch 9497: train loss: 0.12624827295727833, test loss: 0.25260221294930946\n",
      "epoch 9498: train loss: 0.1262445297932928, test loss: 0.25260212482868516\n",
      "epoch 9499: train loss: 0.1262407871880734, test loss: 0.25260203703231754\n",
      "epoch 9500: train loss: 0.12623704514147435, test loss: 0.2526019495600996\n",
      "epoch 9501: train loss: 0.12623330365334978, test loss: 0.2526018624119242\n",
      "epoch 9502: train loss: 0.12622956272355396, test loss: 0.25260177558770963\n",
      "epoch 9503: train loss: 0.12622582235194124, test loss: 0.25260168908734953\n",
      "epoch 9504: train loss: 0.12622208253836592, test loss: 0.25260160291074496\n",
      "epoch 9505: train loss: 0.12621834328268247, test loss: 0.2526015170577922\n",
      "epoch 9506: train loss: 0.12621460458474534, test loss: 0.25260143152841646\n",
      "epoch 9507: train loss: 0.12621086644440907, test loss: 0.25260134632250064\n",
      "epoch 9508: train loss: 0.12620712886152818, test loss: 0.2526012614399522\n",
      "epoch 9509: train loss: 0.12620339183595738, test loss: 0.252601176880681\n",
      "epoch 9510: train loss: 0.1261996553675513, test loss: 0.2526010926445808\n",
      "epoch 9511: train loss: 0.12619591945616462, test loss: 0.2526010087315481\n",
      "epoch 9512: train loss: 0.12619218410165225, test loss: 0.25260092514151256\n",
      "epoch 9513: train loss: 0.12618844930386897, test loss: 0.25260084187434934\n",
      "epoch 9514: train loss: 0.12618471506266965, test loss: 0.25260075892997785\n",
      "epoch 9515: train loss: 0.12618098137790926, test loss: 0.25260067630829286\n",
      "epoch 9516: train loss: 0.1261772482494428, test loss: 0.25260059400919194\n",
      "epoch 9517: train loss: 0.12617351567712531, test loss: 0.2526005120325895\n",
      "epoch 9518: train loss: 0.1261697836608119, test loss: 0.25260043037838925\n",
      "epoch 9519: train loss: 0.12616605220035773, test loss: 0.25260034904649326\n",
      "epoch 9520: train loss: 0.12616232129561802, test loss: 0.25260026803679175\n",
      "epoch 9521: train loss: 0.12615859094644807, test loss: 0.25260018734921225\n",
      "epoch 9522: train loss: 0.12615486115270308, test loss: 0.2526001069836285\n",
      "epoch 9523: train loss: 0.12615113191423852, test loss: 0.2526000269399679\n",
      "epoch 9524: train loss: 0.12614740323090978, test loss: 0.25259994721812684\n",
      "epoch 9525: train loss: 0.12614367510257232, test loss: 0.25259986781799676\n",
      "epoch 9526: train loss: 0.1261399475290817, test loss: 0.2525997887395004\n",
      "epoch 9527: train loss: 0.12613622051029347, test loss: 0.2525997099825315\n",
      "epoch 9528: train loss: 0.12613249404606322, test loss: 0.25259963154698384\n",
      "epoch 9529: train loss: 0.12612876813624674, test loss: 0.25259955343277474\n",
      "epoch 9530: train loss: 0.1261250427806997, test loss: 0.2525994756398069\n",
      "epoch 9531: train loss: 0.12612131797927786, test loss: 0.25259939816798316\n",
      "epoch 9532: train loss: 0.12611759373183715, test loss: 0.25259932101720395\n",
      "epoch 9533: train loss: 0.12611387003823338, test loss: 0.2525992441873759\n",
      "epoch 9534: train loss: 0.12611014689832256, test loss: 0.25259916767839824\n",
      "epoch 9535: train loss: 0.12610642431196062, test loss: 0.2525990914901804\n",
      "epoch 9536: train loss: 0.1261027022790037, test loss: 0.25259901562262393\n",
      "epoch 9537: train loss: 0.12609898079930779, test loss: 0.25259894007563005\n",
      "epoch 9538: train loss: 0.12609525987272918, test loss: 0.2525988648491058\n",
      "epoch 9539: train loss: 0.12609153949912397, test loss: 0.2525987899429516\n",
      "epoch 9540: train loss: 0.12608781967834845, test loss: 0.2525987153570833\n",
      "epoch 9541: train loss: 0.12608410041025894, test loss: 0.2525986410913841\n",
      "epoch 9542: train loss: 0.12608038169471183, test loss: 0.25259856714576656\n",
      "epoch 9543: train loss: 0.1260766635315635, test loss: 0.25259849352015223\n",
      "epoch 9544: train loss: 0.12607294592067045, test loss: 0.2525984202144237\n",
      "epoch 9545: train loss: 0.12606922886188915, test loss: 0.2525983472284852\n",
      "epoch 9546: train loss: 0.1260655123550762, test loss: 0.252598274562258\n",
      "epoch 9547: train loss: 0.12606179640008827, test loss: 0.25259820221563345\n",
      "epoch 9548: train loss: 0.126058080996782, test loss: 0.2525981301885169\n",
      "epoch 9549: train loss: 0.1260543661450141, test loss: 0.25259805848081684\n",
      "epoch 9550: train loss: 0.12605065184464137, test loss: 0.2525979870924377\n",
      "epoch 9551: train loss: 0.12604693809552067, test loss: 0.25259791602327547\n",
      "epoch 9552: train loss: 0.12604322489750885, test loss: 0.2525978452732432\n",
      "epoch 9553: train loss: 0.12603951225046287, test loss: 0.25259777484224616\n",
      "epoch 9554: train loss: 0.12603580015423968, test loss: 0.2525977047301813\n",
      "epoch 9555: train loss: 0.12603208860869639, test loss: 0.25259763493696036\n",
      "epoch 9556: train loss: 0.12602837761369, test loss: 0.25259756546248097\n",
      "epoch 9557: train loss: 0.1260246671690778, test loss: 0.25259749630665285\n",
      "epoch 9558: train loss: 0.12602095727471682, test loss: 0.2525974274693803\n",
      "epoch 9559: train loss: 0.12601724793046437, test loss: 0.2525973589505688\n",
      "epoch 9560: train loss: 0.12601353913617783, test loss: 0.25259729075012755\n",
      "epoch 9561: train loss: 0.12600983089171447, test loss: 0.2525972228679451\n",
      "epoch 9562: train loss: 0.12600612319693172, test loss: 0.25259715530393967\n",
      "epoch 9563: train loss: 0.12600241605168702, test loss: 0.25259708805801223\n",
      "epoch 9564: train loss: 0.1259987094558379, test loss: 0.2525970211300681\n",
      "epoch 9565: train loss: 0.12599500340924194, test loss: 0.252596954520013\n",
      "epoch 9566: train loss: 0.12599129791175667, test loss: 0.25259688822775767\n",
      "epoch 9567: train loss: 0.1259875929632398, test loss: 0.25259682225319346\n",
      "epoch 9568: train loss: 0.12598388856354908, test loss: 0.25259675659623176\n",
      "epoch 9569: train loss: 0.12598018471254224, test loss: 0.25259669125679096\n",
      "epoch 9570: train loss: 0.1259764814100771, test loss: 0.25259662623475465\n",
      "epoch 9571: train loss: 0.12597277865601153, test loss: 0.2525965615300381\n",
      "epoch 9572: train loss: 0.12596907645020347, test loss: 0.25259649714254695\n",
      "epoch 9573: train loss: 0.12596537479251083, test loss: 0.2525964330721882\n",
      "epoch 9574: train loss: 0.1259616736827917, test loss: 0.2525963693188582\n",
      "epoch 9575: train loss: 0.12595797312090412, test loss: 0.2525963058824775\n",
      "epoch 9576: train loss: 0.12595427310670626, test loss: 0.2525962427629341\n",
      "epoch 9577: train loss: 0.12595057364005627, test loss: 0.25259617996014244\n",
      "epoch 9578: train loss: 0.12594687472081234, test loss: 0.2525961174740138\n",
      "epoch 9579: train loss: 0.1259431763488328, test loss: 0.25259605530444695\n",
      "epoch 9580: train loss: 0.12593947852397597, test loss: 0.2525959934513343\n",
      "epoch 9581: train loss: 0.12593578124610025, test loss: 0.2525959319146086\n",
      "epoch 9582: train loss: 0.12593208451506405, test loss: 0.2525958706941552\n",
      "epoch 9583: train loss: 0.12592838833072584, test loss: 0.25259580978988955\n",
      "epoch 9584: train loss: 0.12592469269294418, test loss: 0.25259574920171823\n",
      "epoch 9585: train loss: 0.12592099760157768, test loss: 0.252595688929531\n",
      "epoch 9586: train loss: 0.12591730305648496, test loss: 0.252595628973251\n",
      "epoch 9587: train loss: 0.1259136090575247, test loss: 0.2525955693327868\n",
      "epoch 9588: train loss: 0.12590991560455567, test loss: 0.2525955100080277\n",
      "epoch 9589: train loss: 0.1259062226974366, test loss: 0.25259545099888475\n",
      "epoch 9590: train loss: 0.12590253033602639, test loss: 0.2525953923052654\n",
      "epoch 9591: train loss: 0.1258988385201839, test loss: 0.25259533392708117\n",
      "epoch 9592: train loss: 0.12589514724976814, test loss: 0.2525952758642342\n",
      "epoch 9593: train loss: 0.12589145652463807, test loss: 0.25259521811663366\n",
      "epoch 9594: train loss: 0.1258877663446527, test loss: 0.25259516068417637\n",
      "epoch 9595: train loss: 0.12588407670967114, test loss: 0.2525951035667758\n",
      "epoch 9596: train loss: 0.12588038761955261, test loss: 0.2525950467643308\n",
      "epoch 9597: train loss: 0.12587669907415625, test loss: 0.25259499027675647\n",
      "epoch 9598: train loss: 0.1258730110733413, test loss: 0.25259493410396083\n",
      "epoch 9599: train loss: 0.12586932361696712, test loss: 0.2525948782458385\n",
      "epoch 9600: train loss: 0.125865636704893, test loss: 0.2525948227023067\n",
      "epoch 9601: train loss: 0.12586195033697836, test loss: 0.2525947674732649\n",
      "epoch 9602: train loss: 0.1258582645130827, test loss: 0.2525947125586243\n",
      "epoch 9603: train loss: 0.1258545792330655, test loss: 0.25259465795828395\n",
      "epoch 9604: train loss: 0.1258508944967863, test loss: 0.25259460367215836\n",
      "epoch 9605: train loss: 0.12584721030410473, test loss: 0.25259454970015366\n",
      "epoch 9606: train loss: 0.1258435266548804, test loss: 0.2525944960421701\n",
      "epoch 9607: train loss: 0.12583984354897307, test loss: 0.2525944426981163\n",
      "epoch 9608: train loss: 0.1258361609862425, test loss: 0.2525943896678996\n",
      "epoch 9609: train loss: 0.12583247896654848, test loss: 0.2525943369514289\n",
      "epoch 9610: train loss: 0.12582879748975084, test loss: 0.25259428454860905\n",
      "epoch 9611: train loss: 0.12582511655570958, test loss: 0.2525942324593473\n",
      "epoch 9612: train loss: 0.12582143616428457, test loss: 0.25259418068354833\n",
      "epoch 9613: train loss: 0.12581775631533587, test loss: 0.2525941292211256\n",
      "epoch 9614: train loss: 0.12581407700872355, test loss: 0.2525940780719791\n",
      "epoch 9615: train loss: 0.12581039824430765, test loss: 0.2525940272360128\n",
      "epoch 9616: train loss: 0.12580672002194845, test loss: 0.2525939767131407\n",
      "epoch 9617: train loss: 0.12580304234150608, test loss: 0.25259392650326545\n",
      "epoch 9618: train loss: 0.12579936520284082, test loss: 0.2525938766062984\n",
      "epoch 9619: train loss: 0.12579568860581303, test loss: 0.25259382702214284\n",
      "epoch 9620: train loss: 0.12579201255028302, test loss: 0.2525937777507029\n",
      "epoch 9621: train loss: 0.1257883370361112, test loss: 0.2525937287918987\n",
      "epoch 9622: train loss: 0.12578466206315808, test loss: 0.25259368014561984\n",
      "epoch 9623: train loss: 0.1257809876312842, test loss: 0.2525936318117809\n",
      "epoch 9624: train loss: 0.12577731374035003, test loss: 0.2525935837902996\n",
      "epoch 9625: train loss: 0.12577364039021624, test loss: 0.2525935360810648\n",
      "epoch 9626: train loss: 0.12576996758074355, test loss: 0.2525934886839928\n",
      "epoch 9627: train loss: 0.12576629531179256, test loss: 0.25259344159900304\n",
      "epoch 9628: train loss: 0.12576262358322418, test loss: 0.2525933948259738\n",
      "epoch 9629: train loss: 0.1257589523948991, test loss: 0.2525933483648336\n",
      "epoch 9630: train loss: 0.12575528174667824, test loss: 0.25259330221549026\n",
      "epoch 9631: train loss: 0.12575161163842252, test loss: 0.25259325637784213\n",
      "epoch 9632: train loss: 0.12574794206999293, test loss: 0.25259321085180286\n",
      "epoch 9633: train loss: 0.12574427304125047, test loss: 0.2525931656372776\n",
      "epoch 9634: train loss: 0.12574060455205616, test loss: 0.25259312073417634\n",
      "epoch 9635: train loss: 0.1257369366022712, test loss: 0.2525930761424053\n",
      "epoch 9636: train loss: 0.12573326919175667, test loss: 0.2525930318618613\n",
      "epoch 9637: train loss: 0.1257296023203739, test loss: 0.252592987892476\n",
      "epoch 9638: train loss: 0.1257259359879841, test loss: 0.25259294423413825\n",
      "epoch 9639: train loss: 0.12572227019444854, test loss: 0.25259290088675823\n",
      "epoch 9640: train loss: 0.12571860493962864, test loss: 0.2525928578502417\n",
      "epoch 9641: train loss: 0.12571494022338583, test loss: 0.2525928151245039\n",
      "epoch 9642: train loss: 0.12571127604558155, test loss: 0.2525927727094552\n",
      "epoch 9643: train loss: 0.12570761240607733, test loss: 0.2525927306049921\n",
      "epoch 9644: train loss: 0.12570394930473475, test loss: 0.2525926888110311\n",
      "epoch 9645: train loss: 0.1257002867414154, test loss: 0.2525926473274815\n",
      "epoch 9646: train loss: 0.12569662471598098, test loss: 0.25259260615424195\n",
      "epoch 9647: train loss: 0.12569296322829313, test loss: 0.2525925652912266\n",
      "epoch 9648: train loss: 0.12568930227821373, test loss: 0.2525925247383472\n",
      "epoch 9649: train loss: 0.12568564186560452, test loss: 0.2525924844955008\n",
      "epoch 9650: train loss: 0.1256819819903274, test loss: 0.25259244456261265\n",
      "epoch 9651: train loss: 0.12567832265224424, test loss: 0.25259240493956653\n",
      "epoch 9652: train loss: 0.1256746638512171, test loss: 0.252592365626291\n",
      "epoch 9653: train loss: 0.12567100558710786, test loss: 0.2525923266226893\n",
      "epoch 9654: train loss: 0.12566734785977868, test loss: 0.2525922879286673\n",
      "epoch 9655: train loss: 0.1256636906690917, test loss: 0.2525922495441323\n",
      "epoch 9656: train loss: 0.125660034014909, test loss: 0.25259221146900573\n",
      "epoch 9657: train loss: 0.12565637789709283, test loss: 0.25259217370317016\n",
      "epoch 9658: train loss: 0.12565272231550545, test loss: 0.252592136246557\n",
      "epoch 9659: train loss: 0.12564906727000916, test loss: 0.25259209909906294\n",
      "epoch 9660: train loss: 0.12564541276046637, test loss: 0.25259206226060027\n",
      "epoch 9661: train loss: 0.12564175878673944, test loss: 0.25259202573108075\n",
      "epoch 9662: train loss: 0.12563810534869085, test loss: 0.252591989510403\n",
      "epoch 9663: train loss: 0.12563445244618315, test loss: 0.2525919535984877\n",
      "epoch 9664: train loss: 0.12563080007907879, test loss: 0.25259191799523706\n",
      "epoch 9665: train loss: 0.1256271482472405, test loss: 0.25259188270056343\n",
      "epoch 9666: train loss: 0.12562349695053088, test loss: 0.2525918477143709\n",
      "epoch 9667: train loss: 0.1256198461888126, test loss: 0.252591813036566\n",
      "epoch 9668: train loss: 0.1256161959619485, test loss: 0.2525917786670706\n",
      "epoch 9669: train loss: 0.12561254626980134, test loss: 0.2525917446057738\n",
      "epoch 9670: train loss: 0.12560889711223397, test loss: 0.25259171085260224\n",
      "epoch 9671: train loss: 0.12560524848910934, test loss: 0.2525916774074576\n",
      "epoch 9672: train loss: 0.12560160040029034, test loss: 0.25259164427025416\n",
      "epoch 9673: train loss: 0.12559795284564, test loss: 0.2525916114408862\n",
      "epoch 9674: train loss: 0.12559430582502137, test loss: 0.2525915789192694\n",
      "epoch 9675: train loss: 0.12559065933829758, test loss: 0.2525915467053315\n",
      "epoch 9676: train loss: 0.12558701338533174, test loss: 0.25259151479895764\n",
      "epoch 9677: train loss: 0.12558336796598707, test loss: 0.2525914832000552\n",
      "epoch 9678: train loss: 0.1255797230801268, test loss: 0.25259145190856064\n",
      "epoch 9679: train loss: 0.12557607872761423, test loss: 0.25259142092435943\n",
      "epoch 9680: train loss: 0.12557243490831277, test loss: 0.2525913902473597\n",
      "epoch 9681: train loss: 0.1255687916220857, test loss: 0.25259135987749287\n",
      "epoch 9682: train loss: 0.12556514886879658, test loss: 0.25259132981464505\n",
      "epoch 9683: train loss: 0.1255615066483088, test loss: 0.25259130005873387\n",
      "epoch 9684: train loss: 0.12555786496048602, test loss: 0.25259127060967207\n",
      "epoch 9685: train loss: 0.1255542238051917, test loss: 0.25259124146735945\n",
      "epoch 9686: train loss: 0.12555058318228954, test loss: 0.2525912126317229\n",
      "epoch 9687: train loss: 0.1255469430916433, test loss: 0.25259118410265385\n",
      "epoch 9688: train loss: 0.1255433035331166, test loss: 0.25259115588006537\n",
      "epoch 9689: train loss: 0.1255396645065733, test loss: 0.2525911279638791\n",
      "epoch 9690: train loss: 0.1255360260118772, test loss: 0.2525911003539874\n",
      "epoch 9691: train loss: 0.1255323880488922, test loss: 0.2525910730503209\n",
      "epoch 9692: train loss: 0.1255287506174822, test loss: 0.2525910460527673\n",
      "epoch 9693: train loss: 0.12552511371751124, test loss: 0.25259101936125256\n",
      "epoch 9694: train loss: 0.12552147734884334, test loss: 0.25259099297567394\n",
      "epoch 9695: train loss: 0.12551784151134251, test loss: 0.25259096689594895\n",
      "epoch 9696: train loss: 0.12551420620487297, test loss: 0.252590941121991\n",
      "epoch 9697: train loss: 0.1255105714292988, test loss: 0.25259091565370545\n",
      "epoch 9698: train loss: 0.12550693718448433, test loss: 0.25259089049098954\n",
      "epoch 9699: train loss: 0.12550330347029376, test loss: 0.2525908656337824\n",
      "epoch 9700: train loss: 0.12549967028659148, test loss: 0.2525908410819635\n",
      "epoch 9701: train loss: 0.12549603763324174, test loss: 0.25259081683545587\n",
      "epoch 9702: train loss: 0.12549240551010907, test loss: 0.2525907928941738\n",
      "epoch 9703: train loss: 0.1254887739170579, test loss: 0.25259076925802176\n",
      "epoch 9704: train loss: 0.12548514285395274, test loss: 0.25259074592692005\n",
      "epoch 9705: train loss: 0.12548151232065816, test loss: 0.25259072290076046\n",
      "epoch 9706: train loss: 0.12547788231703877, test loss: 0.252590700179461\n",
      "epoch 9707: train loss: 0.12547425284295924, test loss: 0.25259067776293653\n",
      "epoch 9708: train loss: 0.12547062389828428, test loss: 0.25259065565109795\n",
      "epoch 9709: train loss: 0.12546699548287862, test loss: 0.25259063384384595\n",
      "epoch 9710: train loss: 0.12546336759660714, test loss: 0.25259061234110475\n",
      "epoch 9711: train loss: 0.1254597402393346, test loss: 0.2525905911427742\n",
      "epoch 9712: train loss: 0.12545611341092594, test loss: 0.2525905702487627\n",
      "epoch 9713: train loss: 0.12545248711124615, test loss: 0.2525905496589875\n",
      "epoch 9714: train loss: 0.12544886134016017, test loss: 0.25259052937335597\n",
      "epoch 9715: train loss: 0.1254452360975331, test loss: 0.2525905093917777\n",
      "epoch 9716: train loss: 0.12544161138323, test loss: 0.2525904897141734\n",
      "epoch 9717: train loss: 0.12543798719711602, test loss: 0.25259047034044374\n",
      "epoch 9718: train loss: 0.12543436353905638, test loss: 0.2525904512704914\n",
      "epoch 9719: train loss: 0.12543074040891627, test loss: 0.2525904325042444\n",
      "epoch 9720: train loss: 0.12542711780656102, test loss: 0.2525904140416052\n",
      "epoch 9721: train loss: 0.12542349573185596, test loss: 0.25259039588248394\n",
      "epoch 9722: train loss: 0.12541987418466646, test loss: 0.2525903780267923\n",
      "epoch 9723: train loss: 0.12541625316485797, test loss: 0.2525903604744329\n",
      "epoch 9724: train loss: 0.12541263267229597, test loss: 0.2525903432253337\n",
      "epoch 9725: train loss: 0.12540901270684596, test loss: 0.25259032627939737\n",
      "epoch 9726: train loss: 0.12540539326837355, test loss: 0.2525903096365311\n",
      "epoch 9727: train loss: 0.12540177435674432, test loss: 0.25259029329664623\n",
      "epoch 9728: train loss: 0.12539815597182402, test loss: 0.25259027725966166\n",
      "epoch 9729: train loss: 0.1253945381134783, test loss: 0.25259026152548186\n",
      "epoch 9730: train loss: 0.125390920781573, test loss: 0.2525902460940168\n",
      "epoch 9731: train loss: 0.12538730397597384, test loss: 0.2525902309651739\n",
      "epoch 9732: train loss: 0.12538368769654673, test loss: 0.2525902161388743\n",
      "epoch 9733: train loss: 0.12538007194315764, test loss: 0.2525902016150292\n",
      "epoch 9734: train loss: 0.12537645671567244, test loss: 0.2525901873935371\n",
      "epoch 9735: train loss: 0.12537284201395718, test loss: 0.2525901734743185\n",
      "epoch 9736: train loss: 0.12536922783787793, test loss: 0.2525901598572945\n",
      "epoch 9737: train loss: 0.12536561418730077, test loss: 0.25259014654235395\n",
      "epoch 9738: train loss: 0.12536200106209183, test loss: 0.2525901335294181\n",
      "epoch 9739: train loss: 0.1253583884621174, test loss: 0.2525901208184048\n",
      "epoch 9740: train loss: 0.1253547763872436, test loss: 0.2525901084092242\n",
      "epoch 9741: train loss: 0.1253511648373368, test loss: 0.2525900963017799\n",
      "epoch 9742: train loss: 0.12534755381226334, test loss: 0.25259008449598996\n",
      "epoch 9743: train loss: 0.12534394331188958, test loss: 0.2525900729917522\n",
      "epoch 9744: train loss: 0.125340333336082, test loss: 0.2525900617890002\n",
      "epoch 9745: train loss: 0.12533672388470707, test loss: 0.25259005088762615\n",
      "epoch 9746: train loss: 0.12533311495763125, test loss: 0.25259004028755483\n",
      "epoch 9747: train loss: 0.1253295065547212, test loss: 0.25259002998869384\n",
      "epoch 9748: train loss: 0.12532589867584357, test loss: 0.25259001999095626\n",
      "epoch 9749: train loss: 0.12532229132086495, test loss: 0.2525900102942504\n",
      "epoch 9750: train loss: 0.12531868448965214, test loss: 0.25259000089848477\n",
      "epoch 9751: train loss: 0.12531507818207185, test loss: 0.2525899918035759\n",
      "epoch 9752: train loss: 0.12531147239799093, test loss: 0.2525899830094368\n",
      "epoch 9753: train loss: 0.12530786713727626, test loss: 0.2525899745159758\n",
      "epoch 9754: train loss: 0.1253042623997947, test loss: 0.25258996632310987\n",
      "epoch 9755: train loss: 0.12530065818541322, test loss: 0.2525899584307445\n",
      "epoch 9756: train loss: 0.12529705449399886, test loss: 0.2525899508387959\n",
      "epoch 9757: train loss: 0.12529345132541866, test loss: 0.25258994354718195\n",
      "epoch 9758: train loss: 0.12528984867953974, test loss: 0.252589936555794\n",
      "epoch 9759: train loss: 0.1252862465562292, test loss: 0.2525899298645669\n",
      "epoch 9760: train loss: 0.12528264495535427, test loss: 0.25258992347340015\n",
      "epoch 9761: train loss: 0.1252790438767822, test loss: 0.2525899173822045\n",
      "epoch 9762: train loss: 0.12527544332038024, test loss: 0.252589911590909\n",
      "epoch 9763: train loss: 0.12527184328601576, test loss: 0.2525899060994061\n",
      "epoch 9764: train loss: 0.12526824377355614, test loss: 0.2525899009076088\n",
      "epoch 9765: train loss: 0.12526464478286883, test loss: 0.2525898960154487\n",
      "epoch 9766: train loss: 0.1252610463138213, test loss: 0.2525898914228168\n",
      "epoch 9767: train loss: 0.12525744836628103, test loss: 0.25258988712963665\n",
      "epoch 9768: train loss: 0.12525385094011562, test loss: 0.2525898831358164\n",
      "epoch 9769: train loss: 0.12525025403519272, test loss: 0.25258987944127964\n",
      "epoch 9770: train loss: 0.12524665765137996, test loss: 0.25258987604591776\n",
      "epoch 9771: train loss: 0.12524306178854508, test loss: 0.2525898729496537\n",
      "epoch 9772: train loss: 0.1252394664465558, test loss: 0.2525898701524047\n",
      "epoch 9773: train loss: 0.12523587162528002, test loss: 0.25258986765408215\n",
      "epoch 9774: train loss: 0.12523227732458547, test loss: 0.25258986545459444\n",
      "epoch 9775: train loss: 0.12522868354434016, test loss: 0.25258986355384794\n",
      "epoch 9776: train loss: 0.12522509028441198, test loss: 0.25258986195177785\n",
      "epoch 9777: train loss: 0.12522149754466894, test loss: 0.25258986064826755\n",
      "epoch 9778: train loss: 0.12521790532497906, test loss: 0.2525898596432547\n",
      "epoch 9779: train loss: 0.1252143136252105, test loss: 0.25258985893663016\n",
      "epoch 9780: train loss: 0.1252107224452313, test loss: 0.2525898585283272\n",
      "epoch 9781: train loss: 0.12520713178490972, test loss: 0.2525898584182455\n",
      "epoch 9782: train loss: 0.12520354164411393, test loss: 0.25258985860630573\n",
      "epoch 9783: train loss: 0.12519995202271228, test loss: 0.25258985909240467\n",
      "epoch 9784: train loss: 0.12519636292057304, test loss: 0.2525898598764872\n",
      "epoch 9785: train loss: 0.12519277433756457, test loss: 0.2525898609584284\n",
      "epoch 9786: train loss: 0.12518918627355533, test loss: 0.2525898623381692\n",
      "epoch 9787: train loss: 0.12518559872841373, test loss: 0.25258986401560773\n",
      "epoch 9788: train loss: 0.12518201170200835, test loss: 0.2525898659906629\n",
      "epoch 9789: train loss: 0.1251784251942077, test loss: 0.2525898682632451\n",
      "epoch 9790: train loss: 0.12517483920488037, test loss: 0.2525898708332692\n",
      "epoch 9791: train loss: 0.12517125373389504, test loss: 0.25258987370065067\n",
      "epoch 9792: train loss: 0.12516766878112043, test loss: 0.25258987686530066\n",
      "epoch 9793: train loss: 0.1251640843464252, test loss: 0.2525898803271288\n",
      "epoch 9794: train loss: 0.12516050042967825, test loss: 0.25258988408605965\n",
      "epoch 9795: train loss: 0.12515691703074833, test loss: 0.25258988814198263\n",
      "epoch 9796: train loss: 0.12515333414950436, test loss: 0.25258989249483943\n",
      "epoch 9797: train loss: 0.12514975178581522, test loss: 0.25258989714452235\n",
      "epoch 9798: train loss: 0.12514616993954997, test loss: 0.2525899020909568\n",
      "epoch 9799: train loss: 0.1251425886105776, test loss: 0.2525899073340623\n",
      "epoch 9800: train loss: 0.12513900779876713, test loss: 0.2525899128737287\n",
      "epoch 9801: train loss: 0.1251354275039877, test loss: 0.25258991870988756\n",
      "epoch 9802: train loss: 0.12513184772610852, test loss: 0.2525899248424523\n",
      "epoch 9803: train loss: 0.12512826846499872, test loss: 0.25258993127132406\n",
      "epoch 9804: train loss: 0.12512468972052765, test loss: 0.2525899379964278\n",
      "epoch 9805: train loss: 0.1251211114925645, test loss: 0.25258994501767534\n",
      "epoch 9806: train loss: 0.1251175337809787, test loss: 0.2525899523349783\n",
      "epoch 9807: train loss: 0.1251139565856396, test loss: 0.2525899599482519\n",
      "epoch 9808: train loss: 0.12511037990641666, test loss: 0.2525899678574087\n",
      "epoch 9809: train loss: 0.12510680374317937, test loss: 0.2525899760623637\n",
      "epoch 9810: train loss: 0.12510322809579721, test loss: 0.25258998456302995\n",
      "epoch 9811: train loss: 0.12509965296413983, test loss: 0.2525899933593142\n",
      "epoch 9812: train loss: 0.12509607834807682, test loss: 0.25259000245113683\n",
      "epoch 9813: train loss: 0.12509250424747784, test loss: 0.25259001183842367\n",
      "epoch 9814: train loss: 0.1250889306622126, test loss: 0.25259002152106835\n",
      "epoch 9815: train loss: 0.1250853575921509, test loss: 0.25259003149899734\n",
      "epoch 9816: train loss: 0.12508178503716252, test loss: 0.2525900417721131\n",
      "epoch 9817: train loss: 0.1250782129971173, test loss: 0.2525900523403408\n",
      "epoch 9818: train loss: 0.12507464147188518, test loss: 0.2525900632035933\n",
      "epoch 9819: train loss: 0.1250710704613361, test loss: 0.25259007436178704\n",
      "epoch 9820: train loss: 0.12506749996534, test loss: 0.25259008581481596\n",
      "epoch 9821: train loss: 0.12506392998376697, test loss: 0.2525900975626251\n",
      "epoch 9822: train loss: 0.1250603605164871, test loss: 0.2525901096051036\n",
      "epoch 9823: train loss: 0.12505679156337052, test loss: 0.2525901219421818\n",
      "epoch 9824: train loss: 0.12505322312428735, test loss: 0.2525901345737582\n",
      "epoch 9825: train loss: 0.12504965519910785, test loss: 0.2525901474997646\n",
      "epoch 9826: train loss: 0.1250460877877023, test loss: 0.25259016072010426\n",
      "epoch 9827: train loss: 0.125042520889941, test loss: 0.25259017423469005\n",
      "epoch 9828: train loss: 0.1250389545056943, test loss: 0.2525901880434522\n",
      "epoch 9829: train loss: 0.1250353886348326, test loss: 0.2525902021462899\n",
      "epoch 9830: train loss: 0.1250318232772264, test loss: 0.2525902165431166\n",
      "epoch 9831: train loss: 0.12502825843274612, test loss: 0.2525902312338455\n",
      "epoch 9832: train loss: 0.12502469410126235, test loss: 0.2525902462184025\n",
      "epoch 9833: train loss: 0.12502113028264572, test loss: 0.2525902614967054\n",
      "epoch 9834: train loss: 0.12501756697676678, test loss: 0.25259027706864906\n",
      "epoch 9835: train loss: 0.12501400418349626, test loss: 0.2525902929341555\n",
      "epoch 9836: train loss: 0.12501044190270486, test loss: 0.25259030909315255\n",
      "epoch 9837: train loss: 0.12500688013426337, test loss: 0.25259032554553856\n",
      "epoch 9838: train loss: 0.12500331887804259, test loss: 0.252590342291241\n",
      "epoch 9839: train loss: 0.1249997581339134, test loss: 0.2525903593301647\n",
      "epoch 9840: train loss: 0.12499619790174668, test loss: 0.2525903766622318\n",
      "epoch 9841: train loss: 0.12499263818141339, test loss: 0.2525903942873506\n",
      "epoch 9842: train loss: 0.12498907897278456, test loss: 0.25259041220544426\n",
      "epoch 9843: train loss: 0.12498552027573123, test loss: 0.25259043041641355\n",
      "epoch 9844: train loss: 0.12498196209012445, test loss: 0.25259044892018956\n",
      "epoch 9845: train loss: 0.12497840441583541, test loss: 0.2525904677166764\n",
      "epoch 9846: train loss: 0.12497484725273524, test loss: 0.2525904868057931\n",
      "epoch 9847: train loss: 0.12497129060069517, test loss: 0.2525905061874532\n",
      "epoch 9848: train loss: 0.12496773445958649, test loss: 0.2525905258615716\n",
      "epoch 9849: train loss: 0.12496417882928053, test loss: 0.2525905458280741\n",
      "epoch 9850: train loss: 0.12496062370964862, test loss: 0.25259056608684893\n",
      "epoch 9851: train loss: 0.1249570691005622, test loss: 0.25259058663784617\n",
      "epoch 9852: train loss: 0.12495351500189271, test loss: 0.25259060748095186\n",
      "epoch 9853: train loss: 0.12494996141351161, test loss: 0.25259062861609055\n",
      "epoch 9854: train loss: 0.12494640833529053, test loss: 0.25259065004318904\n",
      "epoch 9855: train loss: 0.12494285576710096, test loss: 0.25259067176214356\n",
      "epoch 9856: train loss: 0.12493930370881462, test loss: 0.2525906937728884\n",
      "epoch 9857: train loss: 0.1249357521603031, test loss: 0.25259071607532585\n",
      "epoch 9858: train loss: 0.12493220112143823, test loss: 0.2525907386693777\n",
      "epoch 9859: train loss: 0.1249286505920917, test loss: 0.25259076155495674\n",
      "epoch 9860: train loss: 0.12492510057213532, test loss: 0.25259078473197366\n",
      "epoch 9861: train loss: 0.12492155106144102, test loss: 0.2525908082003425\n",
      "epoch 9862: train loss: 0.12491800205988067, test loss: 0.25259083195999726\n",
      "epoch 9863: train loss: 0.1249144535673262, test loss: 0.2525908560108387\n",
      "epoch 9864: train loss: 0.12491090558364959, test loss: 0.2525908803527806\n",
      "epoch 9865: train loss: 0.12490735810872294, test loss: 0.25259090498574366\n",
      "epoch 9866: train loss: 0.12490381114241829, test loss: 0.25259092990964227\n",
      "epoch 9867: train loss: 0.1249002646846078, test loss: 0.25259095512439195\n",
      "epoch 9868: train loss: 0.12489671873516363, test loss: 0.25259098062991575\n",
      "epoch 9869: train loss: 0.124893173293958, test loss: 0.25259100642611093\n",
      "epoch 9870: train loss: 0.12488962836086319, test loss: 0.2525910325129112\n",
      "epoch 9871: train loss: 0.12488608393575146, test loss: 0.2525910588902317\n",
      "epoch 9872: train loss: 0.12488254001849525, test loss: 0.25259108555797827\n",
      "epoch 9873: train loss: 0.12487899660896687, test loss: 0.2525911125160669\n",
      "epoch 9874: train loss: 0.12487545370703883, test loss: 0.25259113976442665\n",
      "epoch 9875: train loss: 0.1248719113125836, test loss: 0.25259116730295306\n",
      "epoch 9876: train loss: 0.1248683694254737, test loss: 0.2525911951315819\n",
      "epoch 9877: train loss: 0.12486482804558172, test loss: 0.2525912232502134\n",
      "epoch 9878: train loss: 0.12486128717278028, test loss: 0.2525912516587828\n",
      "epoch 9879: train loss: 0.12485774680694207, test loss: 0.2525912803571868\n",
      "epoch 9880: train loss: 0.1248542069479398, test loss: 0.25259130934535073\n",
      "epoch 9881: train loss: 0.12485066759564621, test loss: 0.2525913386231848\n",
      "epoch 9882: train loss: 0.12484712874993409, test loss: 0.2525913681906106\n",
      "epoch 9883: train loss: 0.12484359041067634, test loss: 0.2525913980475518\n",
      "epoch 9884: train loss: 0.1248400525777458, test loss: 0.25259142819390745\n",
      "epoch 9885: train loss: 0.12483651525101544, test loss: 0.25259145862961074\n",
      "epoch 9886: train loss: 0.1248329784303582, test loss: 0.2525914893545556\n",
      "epoch 9887: train loss: 0.12482944211564717, test loss: 0.25259152036868054\n",
      "epoch 9888: train loss: 0.1248259063067554, test loss: 0.25259155167189307\n",
      "epoch 9889: train loss: 0.12482237100355598, test loss: 0.25259158326411774\n",
      "epoch 9890: train loss: 0.12481883620592209, test loss: 0.25259161514524914\n",
      "epoch 9891: train loss: 0.12481530191372692, test loss: 0.2525916473152293\n",
      "epoch 9892: train loss: 0.12481176812684373, test loss: 0.25259167977396335\n",
      "epoch 9893: train loss: 0.1248082348451458, test loss: 0.25259171252136065\n",
      "epoch 9894: train loss: 0.12480470206850651, test loss: 0.25259174555734765\n",
      "epoch 9895: train loss: 0.1248011697967992, test loss: 0.2525917788818472\n",
      "epoch 9896: train loss: 0.1247976380298973, test loss: 0.25259181249475515\n",
      "epoch 9897: train loss: 0.12479410676767433, test loss: 0.2525918463960027\n",
      "epoch 9898: train loss: 0.12479057601000373, test loss: 0.252591880585512\n",
      "epoch 9899: train loss: 0.12478704575675913, test loss: 0.25259191506318535\n",
      "epoch 9900: train loss: 0.12478351600781408, test loss: 0.2525919498289444\n",
      "epoch 9901: train loss: 0.12477998676304228, test loss: 0.2525919848827052\n",
      "epoch 9902: train loss: 0.12477645802231739, test loss: 0.2525920202243908\n",
      "epoch 9903: train loss: 0.12477292978551315, test loss: 0.2525920558539123\n",
      "epoch 9904: train loss: 0.12476940205250335, test loss: 0.25259209177118735\n",
      "epoch 9905: train loss: 0.12476587482316182, test loss: 0.25259212797613795\n",
      "epoch 9906: train loss: 0.12476234809736245, test loss: 0.2525921644686759\n",
      "epoch 9907: train loss: 0.1247588218749791, test loss: 0.25259220124871457\n",
      "epoch 9908: train loss: 0.12475529615588574, test loss: 0.25259223831618516\n",
      "epoch 9909: train loss: 0.12475177093995642, test loss: 0.2525922756709811\n",
      "epoch 9910: train loss: 0.12474824622706515, test loss: 0.2525923133130428\n",
      "epoch 9911: train loss: 0.12474472201708604, test loss: 0.252592351242279\n",
      "epoch 9912: train loss: 0.12474119830989323, test loss: 0.25259238945860324\n",
      "epoch 9913: train loss: 0.12473767510536087, test loss: 0.25259242796193204\n",
      "epoch 9914: train loss: 0.12473415240336319, test loss: 0.25259246675219704\n",
      "epoch 9915: train loss: 0.12473063020377449, test loss: 0.2525925058292931\n",
      "epoch 9916: train loss: 0.12472710850646906, test loss: 0.25259254519315194\n",
      "epoch 9917: train loss: 0.12472358731132126, test loss: 0.2525925848436911\n",
      "epoch 9918: train loss: 0.12472006661820548, test loss: 0.2525926247808141\n",
      "epoch 9919: train loss: 0.12471654642699616, test loss: 0.25259266500445837\n",
      "epoch 9920: train loss: 0.12471302673756782, test loss: 0.2525927055145263\n",
      "epoch 9921: train loss: 0.12470950754979494, test loss: 0.2525927463109433\n",
      "epoch 9922: train loss: 0.12470598886355216, test loss: 0.25259278739361574\n",
      "epoch 9923: train loss: 0.12470247067871405, test loss: 0.2525928287624762\n",
      "epoch 9924: train loss: 0.1246989529951553, test loss: 0.25259287041744266\n",
      "epoch 9925: train loss: 0.12469543581275061, test loss: 0.2525929123584163\n",
      "epoch 9926: train loss: 0.12469191913137471, test loss: 0.25259295458532277\n",
      "epoch 9927: train loss: 0.12468840295090244, test loss: 0.2525929970980816\n",
      "epoch 9928: train loss: 0.1246848872712086, test loss: 0.2525930398966159\n",
      "epoch 9929: train loss: 0.12468137209216809, test loss: 0.2525930829808338\n",
      "epoch 9930: train loss: 0.12467785741365583, test loss: 0.2525931263506454\n",
      "epoch 9931: train loss: 0.12467434323554681, test loss: 0.2525931700059919\n",
      "epoch 9932: train loss: 0.12467082955771602, test loss: 0.2525932139467736\n",
      "epoch 9933: train loss: 0.1246673163800385, test loss: 0.25259325817291933\n",
      "epoch 9934: train loss: 0.12466380370238941, test loss: 0.25259330268433655\n",
      "epoch 9935: train loss: 0.12466029152464383, test loss: 0.25259334748094164\n",
      "epoch 9936: train loss: 0.124656779846677, test loss: 0.2525933925626613\n",
      "epoch 9937: train loss: 0.12465326866836415, test loss: 0.25259343792940997\n",
      "epoch 9938: train loss: 0.1246497579895805, test loss: 0.25259348358111167\n",
      "epoch 9939: train loss: 0.12464624781020144, test loss: 0.25259352951767605\n",
      "epoch 9940: train loss: 0.12464273813010228, test loss: 0.25259357573902164\n",
      "epoch 9941: train loss: 0.12463922894915846, test loss: 0.25259362224506327\n",
      "epoch 9942: train loss: 0.12463572026724541, test loss: 0.2525936690357346\n",
      "epoch 9943: train loss: 0.12463221208423865, test loss: 0.2525937161109354\n",
      "epoch 9944: train loss: 0.12462870440001371, test loss: 0.25259376347059465\n",
      "epoch 9945: train loss: 0.12462519721444613, test loss: 0.2525938111146359\n",
      "epoch 9946: train loss: 0.12462169052741158, test loss: 0.25259385904295123\n",
      "epoch 9947: train loss: 0.1246181843387857, test loss: 0.25259390725548825\n",
      "epoch 9948: train loss: 0.12461467864844425, test loss: 0.2525939557521518\n",
      "epoch 9949: train loss: 0.1246111734562629, test loss: 0.2525940045328664\n",
      "epoch 9950: train loss: 0.12460766876211754, test loss: 0.2525940535975417\n",
      "epoch 9951: train loss: 0.12460416456588393, test loss: 0.25259410294610296\n",
      "epoch 9952: train loss: 0.12460066086743801, test loss: 0.2525941525784648\n",
      "epoch 9953: train loss: 0.12459715766665568, test loss: 0.25259420249455217\n",
      "epoch 9954: train loss: 0.12459365496341296, test loss: 0.2525942526942704\n",
      "epoch 9955: train loss: 0.12459015275758578, test loss: 0.2525943031775485\n",
      "epoch 9956: train loss: 0.12458665104905026, test loss: 0.2525943539443024\n",
      "epoch 9957: train loss: 0.12458314983768247, test loss: 0.25259440499445174\n",
      "epoch 9958: train loss: 0.1245796491233586, test loss: 0.252594456327912\n",
      "epoch 9959: train loss: 0.12457614890595478, test loss: 0.25259450794460336\n",
      "epoch 9960: train loss: 0.1245726491853473, test loss: 0.2525945598444503\n",
      "epoch 9961: train loss: 0.12456914996141237, test loss: 0.25259461202736444\n",
      "epoch 9962: train loss: 0.12456565123402635, test loss: 0.2525946644932602\n",
      "epoch 9963: train loss: 0.12456215300306557, test loss: 0.2525947172420682\n",
      "epoch 9964: train loss: 0.12455865526840647, test loss: 0.25259477027370897\n",
      "epoch 9965: train loss: 0.12455515802992548, test loss: 0.25259482358807483\n",
      "epoch 9966: train loss: 0.12455166128749907, test loss: 0.25259487718511714\n",
      "epoch 9967: train loss: 0.1245481650410038, test loss: 0.25259493106473646\n",
      "epoch 9968: train loss: 0.12454466929031623, test loss: 0.25259498522685997\n",
      "epoch 9969: train loss: 0.124541174035313, test loss: 0.25259503967139535\n",
      "epoch 9970: train loss: 0.12453767927587073, test loss: 0.2525950943982771\n",
      "epoch 9971: train loss: 0.12453418501186615, test loss: 0.2525951494074169\n",
      "epoch 9972: train loss: 0.12453069124317602, test loss: 0.2525952046987313\n",
      "epoch 9973: train loss: 0.12452719796967714, test loss: 0.25259526027213164\n",
      "epoch 9974: train loss: 0.12452370519124628, test loss: 0.2525953161275564\n",
      "epoch 9975: train loss: 0.12452021290776039, test loss: 0.2525953722649181\n",
      "epoch 9976: train loss: 0.1245167211190963, test loss: 0.2525954286841287\n",
      "epoch 9977: train loss: 0.12451322982513108, test loss: 0.2525954853851086\n",
      "epoch 9978: train loss: 0.12450973902574165, test loss: 0.2525955423677844\n",
      "epoch 9979: train loss: 0.12450624872080512, test loss: 0.25259559963206696\n",
      "epoch 9980: train loss: 0.12450275891019855, test loss: 0.25259565717787896\n",
      "epoch 9981: train loss: 0.12449926959379905, test loss: 0.25259571500514033\n",
      "epoch 9982: train loss: 0.12449578077148382, test loss: 0.252595773113774\n",
      "epoch 9983: train loss: 0.12449229244313009, test loss: 0.2525958315036898\n",
      "epoch 9984: train loss: 0.1244888046086151, test loss: 0.25259589017482076\n",
      "epoch 9985: train loss: 0.12448531726781617, test loss: 0.2525959491270727\n",
      "epoch 9986: train loss: 0.12448183042061065, test loss: 0.2525960083603734\n",
      "epoch 9987: train loss: 0.12447834406687591, test loss: 0.252596067874629\n",
      "epoch 9988: train loss: 0.12447485820648937, test loss: 0.2525961276697813\n",
      "epoch 9989: train loss: 0.12447137283932858, test loss: 0.25259618774574133\n",
      "epoch 9990: train loss: 0.12446788796527095, test loss: 0.2525962481024177\n",
      "epoch 9991: train loss: 0.12446440358419411, test loss: 0.2525963087397416\n",
      "epoch 9992: train loss: 0.12446091969597566, test loss: 0.2525963696576256\n",
      "epoch 9993: train loss: 0.12445743630049322, test loss: 0.252596430855991\n",
      "epoch 9994: train loss: 0.1244539533976245, test loss: 0.2525964923347609\n",
      "epoch 9995: train loss: 0.12445047098724721, test loss: 0.2525965540938566\n",
      "epoch 9996: train loss: 0.12444698906923912, test loss: 0.25259661613319534\n",
      "epoch 9997: train loss: 0.12444350764347806, test loss: 0.25259667845269995\n",
      "epoch 9998: train loss: 0.1244400267098419, test loss: 0.2525967410522785\n",
      "epoch 9999: train loss: 0.12443654626820852, test loss: 0.25259680393185774\n",
      "epoch 10000: train loss: 0.12443306631845587, test loss: 0.25259686709135976\n",
      "epoch 10001: train loss: 0.12442958686046193, test loss: 0.252596930530703\n",
      "epoch 10002: train loss: 0.12442610789410473, test loss: 0.2525969942498142\n",
      "epoch 10003: train loss: 0.12442262941926234, test loss: 0.2525970582486063\n",
      "epoch 10004: train loss: 0.12441915143581289, test loss: 0.25259712252699784\n",
      "epoch 10005: train loss: 0.12441567394363448, test loss: 0.2525971870849057\n",
      "epoch 10006: train loss: 0.12441219694260537, test loss: 0.2525972519222629\n",
      "epoch 10007: train loss: 0.12440872043260376, test loss: 0.2525973170389772\n",
      "epoch 10008: train loss: 0.12440524441350793, test loss: 0.252597382434977\n",
      "epoch 10009: train loss: 0.12440176888519625, test loss: 0.2525974481101706\n",
      "epoch 10010: train loss: 0.12439829384754703, test loss: 0.25259751406449504\n",
      "epoch 10011: train loss: 0.12439481930043868, test loss: 0.25259758029785045\n",
      "epoch 10012: train loss: 0.12439134524374972, test loss: 0.2525976468101858\n",
      "epoch 10013: train loss: 0.12438787167735857, test loss: 0.2525977136013938\n",
      "epoch 10014: train loss: 0.12438439860114377, test loss: 0.2525977806714054\n",
      "epoch 10015: train loss: 0.12438092601498393, test loss: 0.25259784802014407\n",
      "epoch 10016: train loss: 0.12437745391875765, test loss: 0.2525979156475202\n",
      "epoch 10017: train loss: 0.12437398231234358, test loss: 0.25259798355346896\n",
      "epoch 10018: train loss: 0.12437051119562045, test loss: 0.25259805173789673\n",
      "epoch 10019: train loss: 0.12436704056846701, test loss: 0.25259812020072525\n",
      "epoch 10020: train loss: 0.124363570430762, test loss: 0.25259818894189384\n",
      "epoch 10021: train loss: 0.12436010078238428, test loss: 0.2525982579613057\n",
      "epoch 10022: train loss: 0.12435663162321273, test loss: 0.2525983272588758\n",
      "epoch 10023: train loss: 0.12435316295312625, test loss: 0.25259839683454094\n",
      "epoch 10024: train loss: 0.12434969477200379, test loss: 0.2525984666882026\n",
      "epoch 10025: train loss: 0.12434622707972437, test loss: 0.2525985368198023\n",
      "epoch 10026: train loss: 0.124342759876167, test loss: 0.25259860722925437\n",
      "epoch 10027: train loss: 0.12433929316121077, test loss: 0.25259867791646834\n",
      "epoch 10028: train loss: 0.12433582693473481, test loss: 0.25259874888137873\n",
      "epoch 10029: train loss: 0.12433236119661828, test loss: 0.25259882012389395\n",
      "epoch 10030: train loss: 0.12432889594674039, test loss: 0.2525988916439531\n",
      "epoch 10031: train loss: 0.1243254311849804, test loss: 0.25259896344145605\n",
      "epoch 10032: train loss: 0.12432196691121757, test loss: 0.2525990355163408\n",
      "epoch 10033: train loss: 0.12431850312533127, test loss: 0.2525991078685177\n",
      "epoch 10034: train loss: 0.12431503982720084, test loss: 0.25259918049790137\n",
      "epoch 10035: train loss: 0.12431157701670571, test loss: 0.2525992534044299\n",
      "epoch 10036: train loss: 0.12430811469372532, test loss: 0.25259932658802364\n",
      "epoch 10037: train loss: 0.12430465285813921, test loss: 0.25259940004857845\n",
      "epoch 10038: train loss: 0.1243011915098269, test loss: 0.2525994737860467\n",
      "epoch 10039: train loss: 0.12429773064866793, test loss: 0.2525995478003283\n",
      "epoch 10040: train loss: 0.124294270274542, test loss: 0.25259962209135894\n",
      "epoch 10041: train loss: 0.12429081038732875, test loss: 0.252599696659044\n",
      "epoch 10042: train loss: 0.12428735098690784, test loss: 0.25259977150332635\n",
      "epoch 10043: train loss: 0.12428389207315908, test loss: 0.2525998466241069\n",
      "epoch 10044: train loss: 0.12428043364596221, test loss: 0.2525999220213157\n",
      "epoch 10045: train loss: 0.12427697570519712, test loss: 0.2525999976948677\n",
      "epoch 10046: train loss: 0.12427351825074362, test loss: 0.2526000736446873\n",
      "epoch 10047: train loss: 0.12427006128248169, test loss: 0.25260014987070945\n",
      "epoch 10048: train loss: 0.12426660480029122, test loss: 0.2526002263728308\n",
      "epoch 10049: train loss: 0.12426314880405227, test loss: 0.2526003031509952\n",
      "epoch 10050: train loss: 0.12425969329364485, test loss: 0.2526003802051134\n",
      "epoch 10051: train loss: 0.12425623826894905, test loss: 0.2526004575351004\n",
      "epoch 10052: train loss: 0.12425278372984497, test loss: 0.252600535140882\n",
      "epoch 10053: train loss: 0.12424932967621281, test loss: 0.25260061302239256\n",
      "epoch 10054: train loss: 0.12424587610793277, test loss: 0.25260069117954415\n",
      "epoch 10055: train loss: 0.12424242302488508, test loss: 0.2526007696122554\n",
      "epoch 10056: train loss: 0.12423897042695002, test loss: 0.252600848320444\n",
      "epoch 10057: train loss: 0.12423551831400793, test loss: 0.2526009273040442\n",
      "epoch 10058: train loss: 0.1242320666859392, test loss: 0.2526010065629664\n",
      "epoch 10059: train loss: 0.12422861554262424, test loss: 0.25260108609714765\n",
      "epoch 10060: train loss: 0.12422516488394347, test loss: 0.2526011659064918\n",
      "epoch 10061: train loss: 0.12422171470977741, test loss: 0.25260124599091716\n",
      "epoch 10062: train loss: 0.12421826502000659, test loss: 0.2526013263503773\n",
      "epoch 10063: train loss: 0.12421481581451163, test loss: 0.2526014069847553\n",
      "epoch 10064: train loss: 0.12421136709317308, test loss: 0.25260148789400466\n",
      "epoch 10065: train loss: 0.12420791885587164, test loss: 0.2526015690780234\n",
      "epoch 10066: train loss: 0.124204471102488, test loss: 0.2526016505367469\n",
      "epoch 10067: train loss: 0.12420102383290291, test loss: 0.2526017322700961\n",
      "epoch 10068: train loss: 0.12419757704699715, test loss: 0.2526018142779881\n",
      "epoch 10069: train loss: 0.12419413074465155, test loss: 0.2526018965603432\n",
      "epoch 10070: train loss: 0.12419068492574695, test loss: 0.25260197911709276\n",
      "epoch 10071: train loss: 0.12418723959016428, test loss: 0.2526020619481448\n",
      "epoch 10072: train loss: 0.12418379473778453, test loss: 0.2526021450534337\n",
      "epoch 10073: train loss: 0.1241803503684886, test loss: 0.25260222843287666\n",
      "epoch 10074: train loss: 0.12417690648215761, test loss: 0.2526023120863997\n",
      "epoch 10075: train loss: 0.12417346307867255, test loss: 0.2526023960139206\n",
      "epoch 10076: train loss: 0.12417002015791458, test loss: 0.25260248021536524\n",
      "epoch 10077: train loss: 0.12416657771976486, test loss: 0.2526025646906495\n",
      "epoch 10078: train loss: 0.12416313576410452, test loss: 0.25260264943970234\n",
      "epoch 10079: train loss: 0.12415969429081491, test loss: 0.252602734462448\n",
      "epoch 10080: train loss: 0.12415625329977723, test loss: 0.2526028197587903\n",
      "epoch 10081: train loss: 0.12415281279087279, test loss: 0.25260290532868035\n",
      "epoch 10082: train loss: 0.12414937276398294, test loss: 0.2526029911720154\n",
      "epoch 10083: train loss: 0.12414593321898916, test loss: 0.25260307728872733\n",
      "epoch 10084: train loss: 0.1241424941557728, test loss: 0.2526031636787428\n",
      "epoch 10085: train loss: 0.12413905557421541, test loss: 0.2526032503419784\n",
      "epoch 10086: train loss: 0.12413561747419845, test loss: 0.25260333727836237\n",
      "epoch 10087: train loss: 0.12413217985560354, test loss: 0.25260342448781237\n",
      "epoch 10088: train loss: 0.12412874271831223, test loss: 0.25260351197025005\n",
      "epoch 10089: train loss: 0.1241253060622062, test loss: 0.25260359972560176\n",
      "epoch 10090: train loss: 0.12412186988716713, test loss: 0.25260368775378694\n",
      "epoch 10091: train loss: 0.12411843419307672, test loss: 0.25260377605472606\n",
      "epoch 10092: train loss: 0.12411499897981677, test loss: 0.2526038646283482\n",
      "epoch 10093: train loss: 0.1241115642472691, test loss: 0.2526039534745793\n",
      "epoch 10094: train loss: 0.12410812999531551, test loss: 0.25260404259332825\n",
      "epoch 10095: train loss: 0.1241046962238379, test loss: 0.2526041319845266\n",
      "epoch 10096: train loss: 0.12410126293271823, test loss: 0.25260422164810314\n",
      "epoch 10097: train loss: 0.12409783012183845, test loss: 0.25260431158396496\n",
      "epoch 10098: train loss: 0.12409439779108053, test loss: 0.252604401792045\n",
      "epoch 10099: train loss: 0.1240909659403266, test loss: 0.2526044922722616\n",
      "epoch 10100: train loss: 0.12408753456945874, test loss: 0.25260458302453764\n",
      "epoch 10101: train loss: 0.12408410367835902, test loss: 0.25260467404880976\n",
      "epoch 10102: train loss: 0.12408067326690965, test loss: 0.2526047653449867\n",
      "epoch 10103: train loss: 0.12407724333499286, test loss: 0.2526048569129871\n",
      "epoch 10104: train loss: 0.12407381388249085, test loss: 0.2526049487527486\n",
      "epoch 10105: train loss: 0.12407038490928599, test loss: 0.25260504086417995\n",
      "epoch 10106: train loss: 0.12406695641526057, test loss: 0.25260513324721345\n",
      "epoch 10107: train loss: 0.12406352840029695, test loss: 0.2526052259017782\n",
      "epoch 10108: train loss: 0.1240601008642776, test loss: 0.2526053188277768\n",
      "epoch 10109: train loss: 0.1240566738070849, test loss: 0.2526054120251528\n",
      "epoch 10110: train loss: 0.12405324722860142, test loss: 0.25260550549381555\n",
      "epoch 10111: train loss: 0.12404982112870969, test loss: 0.2526055992337019\n",
      "epoch 10112: train loss: 0.12404639550729221, test loss: 0.252605693244727\n",
      "epoch 10113: train loss: 0.1240429703642317, test loss: 0.252605787526808\n",
      "epoch 10114: train loss: 0.12403954569941074, test loss: 0.25260588207987467\n",
      "epoch 10115: train loss: 0.12403612151271207, test loss: 0.25260597690384556\n",
      "epoch 10116: train loss: 0.12403269780401843, test loss: 0.2526060719986643\n",
      "epoch 10117: train loss: 0.12402927457321254, test loss: 0.2526061673642239\n",
      "epoch 10118: train loss: 0.12402585182017731, test loss: 0.25260626300046934\n",
      "epoch 10119: train loss: 0.12402242954479557, test loss: 0.25260635890730904\n",
      "epoch 10120: train loss: 0.12401900774695018, test loss: 0.2526064550846868\n",
      "epoch 10121: train loss: 0.1240155864265241, test loss: 0.25260655153250694\n",
      "epoch 10122: train loss: 0.12401216558340035, test loss: 0.2526066482506933\n",
      "epoch 10123: train loss: 0.1240087452174619, test loss: 0.2526067452391804\n",
      "epoch 10124: train loss: 0.12400532532859182, test loss: 0.2526068424978822\n",
      "epoch 10125: train loss: 0.12400190591667323, test loss: 0.2526069400267321\n",
      "epoch 10126: train loss: 0.12399848698158927, test loss: 0.2526070378256602\n",
      "epoch 10127: train loss: 0.12399506852322309, test loss: 0.25260713589456146\n",
      "epoch 10128: train loss: 0.12399165054145794, test loss: 0.25260723423338843\n",
      "epoch 10129: train loss: 0.12398823303617705, test loss: 0.2526073328420477\n",
      "epoch 10130: train loss: 0.12398481600726377, test loss: 0.2526074317204716\n",
      "epoch 10131: train loss: 0.12398139945460143, test loss: 0.25260753086857635\n",
      "epoch 10132: train loss: 0.12397798337807338, test loss: 0.2526076302862956\n",
      "epoch 10133: train loss: 0.12397456777756305, test loss: 0.2526077299735443\n",
      "epoch 10134: train loss: 0.12397115265295394, test loss: 0.2526078299302478\n",
      "epoch 10135: train loss: 0.1239677380041295, test loss: 0.2526079301563392\n",
      "epoch 10136: train loss: 0.12396432383097329, test loss: 0.25260803065172044\n",
      "epoch 10137: train loss: 0.1239609101333689, test loss: 0.2526081314163441\n",
      "epoch 10138: train loss: 0.12395749691119995, test loss: 0.25260823245012143\n",
      "epoch 10139: train loss: 0.12395408416435011, test loss: 0.25260833375296704\n",
      "epoch 10140: train loss: 0.12395067189270303, test loss: 0.2526084353248112\n",
      "epoch 10141: train loss: 0.12394726009614251, test loss: 0.252608537165589\n",
      "epoch 10142: train loss: 0.1239438487745523, test loss: 0.25260863927521476\n",
      "epoch 10143: train loss: 0.12394043792781625, test loss: 0.2526087416536134\n",
      "epoch 10144: train loss: 0.12393702755581819, test loss: 0.25260884430070246\n",
      "epoch 10145: train loss: 0.12393361765844203, test loss: 0.25260894721640936\n",
      "epoch 10146: train loss: 0.12393020823557171, test loss: 0.25260905040067144\n",
      "epoch 10147: train loss: 0.12392679928709122, test loss: 0.25260915385340277\n",
      "epoch 10148: train loss: 0.12392339081288455, test loss: 0.2526092575745152\n",
      "epoch 10149: train loss: 0.12391998281283578, test loss: 0.2526093615639617\n",
      "epoch 10150: train loss: 0.12391657528682902, test loss: 0.2526094658216405\n",
      "epoch 10151: train loss: 0.12391316823474838, test loss: 0.25260957034749526\n",
      "epoch 10152: train loss: 0.12390976165647807, test loss: 0.25260967514143273\n",
      "epoch 10153: train loss: 0.1239063555519023, test loss: 0.25260978020338365\n",
      "epoch 10154: train loss: 0.12390294992090531, test loss: 0.2526098855332774\n",
      "epoch 10155: train loss: 0.12389954476337138, test loss: 0.25260999113104077\n",
      "epoch 10156: train loss: 0.12389614007918491, test loss: 0.2526100969965806\n",
      "epoch 10157: train loss: 0.12389273586823023, test loss: 0.25261020312984495\n",
      "epoch 10158: train loss: 0.12388933213039177, test loss: 0.2526103095307422\n",
      "epoch 10159: train loss: 0.12388592886555397, test loss: 0.2526104161992071\n",
      "epoch 10160: train loss: 0.12388252607360134, test loss: 0.2526105231351518\n",
      "epoch 10161: train loss: 0.1238791237544184, test loss: 0.25261063033850584\n",
      "epoch 10162: train loss: 0.12387572190788976, test loss: 0.25261073780920656\n",
      "epoch 10163: train loss: 0.12387232053389999, test loss: 0.25261084554716\n",
      "epoch 10164: train loss: 0.12386891963233378, test loss: 0.2526109535522966\n",
      "epoch 10165: train loss: 0.12386551920307579, test loss: 0.2526110618245488\n",
      "epoch 10166: train loss: 0.12386211924601076, test loss: 0.25261117036384184\n",
      "epoch 10167: train loss: 0.12385871976102349, test loss: 0.25261127917007803\n",
      "epoch 10168: train loss: 0.12385532074799877, test loss: 0.2526113882432174\n",
      "epoch 10169: train loss: 0.12385192220682142, test loss: 0.2526114975831487\n",
      "epoch 10170: train loss: 0.12384852413737636, test loss: 0.25261160718982145\n",
      "epoch 10171: train loss: 0.12384512653954854, test loss: 0.2526117170631623\n",
      "epoch 10172: train loss: 0.12384172941322288, test loss: 0.2526118272030747\n",
      "epoch 10173: train loss: 0.12383833275828442, test loss: 0.2526119376095079\n",
      "epoch 10174: train loss: 0.12383493657461823, test loss: 0.25261204828236117\n",
      "epoch 10175: train loss: 0.12383154086210935, test loss: 0.252612159221584\n",
      "epoch 10176: train loss: 0.1238281456206429, test loss: 0.2526122704270891\n",
      "epoch 10177: train loss: 0.12382475085010408, test loss: 0.2526123818988\n",
      "epoch 10178: train loss: 0.12382135655037808, test loss: 0.2526124936366464\n",
      "epoch 10179: train loss: 0.12381796272135012, test loss: 0.25261260564055077\n",
      "epoch 10180: train loss: 0.12381456936290554, test loss: 0.25261271791044065\n",
      "epoch 10181: train loss: 0.12381117647492962, test loss: 0.25261283044623783\n",
      "epoch 10182: train loss: 0.12380778405730773, test loss: 0.25261294324787525\n",
      "epoch 10183: train loss: 0.12380439210992523, test loss: 0.25261305631526904\n",
      "epoch 10184: train loss: 0.12380100063266763, test loss: 0.2526131696483464\n",
      "epoch 10185: train loss: 0.12379760962542036, test loss: 0.2526132832470403\n",
      "epoch 10186: train loss: 0.12379421908806897, test loss: 0.2526133971112683\n",
      "epoch 10187: train loss: 0.12379082902049897, test loss: 0.25261351124095166\n",
      "epoch 10188: train loss: 0.123787439422596, test loss: 0.25261362563602413\n",
      "epoch 10189: train loss: 0.12378405029424566, test loss: 0.2526137402964126\n",
      "epoch 10190: train loss: 0.12378066163533365, test loss: 0.2526138552220392\n",
      "epoch 10191: train loss: 0.12377727344574567, test loss: 0.25261397041282174\n",
      "epoch 10192: train loss: 0.12377388572536745, test loss: 0.2526140858687035\n",
      "epoch 10193: train loss: 0.12377049847408485, test loss: 0.25261420158958703\n",
      "epoch 10194: train loss: 0.1237671116917836, test loss: 0.2526143175754099\n",
      "epoch 10195: train loss: 0.12376372537834966, test loss: 0.25261443382610405\n",
      "epoch 10196: train loss: 0.12376033953366886, test loss: 0.2526145503415885\n",
      "epoch 10197: train loss: 0.1237569541576272, test loss: 0.25261466712179576\n",
      "epoch 10198: train loss: 0.12375356925011069, test loss: 0.25261478416663435\n",
      "epoch 10199: train loss: 0.12375018481100523, test loss: 0.2526149014760456\n",
      "epoch 10200: train loss: 0.12374680084019704, test loss: 0.2526150190499447\n",
      "epoch 10201: train loss: 0.1237434173375721, test loss: 0.2526151368882684\n",
      "epoch 10202: train loss: 0.12374003430301662, test loss: 0.25261525499093535\n",
      "epoch 10203: train loss: 0.12373665173641672, test loss: 0.25261537335787776\n",
      "epoch 10204: train loss: 0.1237332696376587, test loss: 0.25261549198900574\n",
      "epoch 10205: train loss: 0.12372988800662874, test loss: 0.25261561088426154\n",
      "epoch 10206: train loss: 0.12372650684321317, test loss: 0.2526157300435633\n",
      "epoch 10207: train loss: 0.12372312614729833, test loss: 0.25261584946684384\n",
      "epoch 10208: train loss: 0.12371974591877055, test loss: 0.2526159691540186\n",
      "epoch 10209: train loss: 0.1237163661575163, test loss: 0.25261608910502015\n",
      "epoch 10210: train loss: 0.123712986863422, test loss: 0.25261620931978107\n",
      "epoch 10211: train loss: 0.12370960803637412, test loss: 0.2526163297982109\n",
      "epoch 10212: train loss: 0.12370622967625923, test loss: 0.25261645054024323\n",
      "epoch 10213: train loss: 0.12370285178296388, test loss: 0.2526165715458071\n",
      "epoch 10214: train loss: 0.12369947435637467, test loss: 0.2526166928148351\n",
      "epoch 10215: train loss: 0.12369609739637824, test loss: 0.2526168143472424\n",
      "epoch 10216: train loss: 0.12369272090286124, test loss: 0.25261693614294684\n",
      "epoch 10217: train loss: 0.12368934487571046, test loss: 0.2526170582019021\n",
      "epoch 10218: train loss: 0.12368596931481263, test loss: 0.2526171805240085\n",
      "epoch 10219: train loss: 0.12368259422005452, test loss: 0.2526173031091949\n",
      "epoch 10220: train loss: 0.12367921959132304, test loss: 0.2526174259574035\n",
      "epoch 10221: train loss: 0.12367584542850496, test loss: 0.25261754906855055\n",
      "epoch 10222: train loss: 0.12367247173148727, test loss: 0.2526176724425692\n",
      "epoch 10223: train loss: 0.12366909850015688, test loss: 0.2526177960793684\n",
      "epoch 10224: train loss: 0.12366572573440081, test loss: 0.25261791997888783\n",
      "epoch 10225: train loss: 0.12366235343410609, test loss: 0.2526180441410617\n",
      "epoch 10226: train loss: 0.12365898159915975, test loss: 0.2526181685657994\n",
      "epoch 10227: train loss: 0.12365561022944893, test loss: 0.25261829325303103\n",
      "epoch 10228: train loss: 0.12365223932486076, test loss: 0.2526184182026903\n",
      "epoch 10229: train loss: 0.12364886888528243, test loss: 0.2526185434147056\n",
      "epoch 10230: train loss: 0.12364549891060117, test loss: 0.25261866888899354\n",
      "epoch 10231: train loss: 0.1236421294007042, test loss: 0.2526187946254822\n",
      "epoch 10232: train loss: 0.12363876035547886, test loss: 0.25261892062410296\n",
      "epoch 10233: train loss: 0.12363539177481243, test loss: 0.2526190468847879\n",
      "epoch 10234: train loss: 0.12363202365859234, test loss: 0.25261917340744966\n",
      "epoch 10235: train loss: 0.12362865600670599, test loss: 0.252619300192016\n",
      "epoch 10236: train loss: 0.12362528881904083, test loss: 0.2526194272384273\n",
      "epoch 10237: train loss: 0.12362192209548432, test loss: 0.25261955454659335\n",
      "epoch 10238: train loss: 0.12361855583592402, test loss: 0.2526196821164599\n",
      "epoch 10239: train loss: 0.12361519004024749, test loss: 0.25261980994793604\n",
      "epoch 10240: train loss: 0.1236118247083423, test loss: 0.25261993804096233\n",
      "epoch 10241: train loss: 0.12360845984009615, test loss: 0.2526200663954544\n",
      "epoch 10242: train loss: 0.12360509543539665, test loss: 0.25262019501134536\n",
      "epoch 10243: train loss: 0.12360173149413155, test loss: 0.25262032388855765\n",
      "epoch 10244: train loss: 0.12359836801618863, test loss: 0.25262045302703534\n",
      "epoch 10245: train loss: 0.12359500500145566, test loss: 0.25262058242667856\n",
      "epoch 10246: train loss: 0.12359164244982047, test loss: 0.25262071208742887\n",
      "epoch 10247: train loss: 0.12358828036117092, test loss: 0.25262084200920554\n",
      "epoch 10248: train loss: 0.12358491873539496, test loss: 0.25262097219194907\n",
      "epoch 10249: train loss: 0.12358155757238048, test loss: 0.2526211026355799\n",
      "epoch 10250: train loss: 0.12357819687201552, test loss: 0.25262123334001935\n",
      "epoch 10251: train loss: 0.12357483663418804, test loss: 0.2526213643052021\n",
      "epoch 10252: train loss: 0.12357147685878615, test loss: 0.2526214955310547\n",
      "epoch 10253: train loss: 0.12356811754569794, test loss: 0.252621627017497\n",
      "epoch 10254: train loss: 0.12356475869481151, test loss: 0.25262175876446025\n",
      "epoch 10255: train loss: 0.12356140030601509, test loss: 0.2526218907718747\n",
      "epoch 10256: train loss: 0.12355804237919685, test loss: 0.25262202303967224\n",
      "epoch 10257: train loss: 0.12355468491424507, test loss: 0.25262215556775774\n",
      "epoch 10258: train loss: 0.123551327911048, test loss: 0.25262228835608735\n",
      "epoch 10259: train loss: 0.12354797136949397, test loss: 0.2526224214045722\n",
      "epoch 10260: train loss: 0.12354461528947137, test loss: 0.25262255471314327\n",
      "epoch 10261: train loss: 0.1235412596708686, test loss: 0.25262268828172657\n",
      "epoch 10262: train loss: 0.1235379045135741, test loss: 0.252622822110243\n",
      "epoch 10263: train loss: 0.1235345498174763, test loss: 0.2526229561986344\n",
      "epoch 10264: train loss: 0.12353119558246377, test loss: 0.25262309054681686\n",
      "epoch 10265: train loss: 0.12352784180842505, test loss: 0.25262322515472624\n",
      "epoch 10266: train loss: 0.1235244884952487, test loss: 0.2526233600222831\n",
      "epoch 10267: train loss: 0.12352113564282337, test loss: 0.25262349514941573\n",
      "epoch 10268: train loss: 0.12351778325103777, test loss: 0.2526236305360594\n",
      "epoch 10269: train loss: 0.1235144313197805, test loss: 0.2526237661821257\n",
      "epoch 10270: train loss: 0.12351107984894039, test loss: 0.2526239020875704\n",
      "epoch 10271: train loss: 0.12350772883840619, test loss: 0.2526240382522818\n",
      "epoch 10272: train loss: 0.1235043782880667, test loss: 0.2526241746762248\n",
      "epoch 10273: train loss: 0.12350102819781078, test loss: 0.25262431135930213\n",
      "epoch 10274: train loss: 0.12349767856752734, test loss: 0.25262444830145536\n",
      "epoch 10275: train loss: 0.12349432939710532, test loss: 0.25262458550260614\n",
      "epoch 10276: train loss: 0.12349098068643365, test loss: 0.252624722962681\n",
      "epoch 10277: train loss: 0.12348763243540134, test loss: 0.2526248606816122\n",
      "epoch 10278: train loss: 0.12348428464389745, test loss: 0.25262499865931626\n",
      "epoch 10279: train loss: 0.12348093731181103, test loss: 0.2526251368957401\n",
      "epoch 10280: train loss: 0.12347759043903124, test loss: 0.25262527539080076\n",
      "epoch 10281: train loss: 0.12347424402544721, test loss: 0.2526254141444258\n",
      "epoch 10282: train loss: 0.12347089807094813, test loss: 0.25262555315654134\n",
      "epoch 10283: train loss: 0.12346755257542323, test loss: 0.2526256924270813\n",
      "epoch 10284: train loss: 0.12346420753876179, test loss: 0.25262583195597055\n",
      "epoch 10285: train loss: 0.12346086296085311, test loss: 0.252625971743143\n",
      "epoch 10286: train loss: 0.12345751884158648, test loss: 0.25262611178851574\n",
      "epoch 10287: train loss: 0.12345417518085139, test loss: 0.2526262520920255\n",
      "epoch 10288: train loss: 0.12345083197853718, test loss: 0.2526263926535847\n",
      "epoch 10289: train loss: 0.1234474892345333, test loss: 0.2526265334731462\n",
      "epoch 10290: train loss: 0.12344414694872924, test loss: 0.25262667455062776\n",
      "epoch 10291: train loss: 0.12344080512101455, test loss: 0.2526268158859457\n",
      "epoch 10292: train loss: 0.12343746375127881, test loss: 0.252626957479044\n",
      "epoch 10293: train loss: 0.12343412283941162, test loss: 0.25262709932983973\n",
      "epoch 10294: train loss: 0.12343078238530258, test loss: 0.2526272414382698\n",
      "epoch 10295: train loss: 0.1234274423888414, test loss: 0.2526273838042631\n",
      "epoch 10296: train loss: 0.12342410284991778, test loss: 0.2526275264277423\n",
      "epoch 10297: train loss: 0.12342076376842151, test loss: 0.25262766930862435\n",
      "epoch 10298: train loss: 0.12341742514424232, test loss: 0.25262781244686694\n",
      "epoch 10299: train loss: 0.1234140869772701, test loss: 0.2526279558423752\n",
      "epoch 10300: train loss: 0.12341074926739466, test loss: 0.25262809949508463\n",
      "epoch 10301: train loss: 0.12340741201450595, test loss: 0.2526282434049241\n",
      "epoch 10302: train loss: 0.12340407521849386, test loss: 0.2526283875718117\n",
      "epoch 10303: train loss: 0.12340073887924838, test loss: 0.2526285319956959\n",
      "epoch 10304: train loss: 0.12339740299665955, test loss: 0.2526286766764943\n",
      "epoch 10305: train loss: 0.12339406757061738, test loss: 0.25262882161413014\n",
      "epoch 10306: train loss: 0.12339073260101201, test loss: 0.2526289668085425\n",
      "epoch 10307: train loss: 0.1233873980877335, test loss: 0.25262911225965345\n",
      "epoch 10308: train loss: 0.12338406403067204, test loss: 0.252629257967397\n",
      "epoch 10309: train loss: 0.12338073042971784, test loss: 0.2526294039316915\n",
      "epoch 10310: train loss: 0.12337739728476114, test loss: 0.2526295501524756\n",
      "epoch 10311: train loss: 0.1233740645956922, test loss: 0.25262969662968215\n",
      "epoch 10312: train loss: 0.12337073236240131, test loss: 0.25262984336321515\n",
      "epoch 10313: train loss: 0.12336740058477887, test loss: 0.25262999035303346\n",
      "epoch 10314: train loss: 0.1233640692627152, test loss: 0.2526301375990446\n",
      "epoch 10315: train loss: 0.12336073839610075, test loss: 0.25263028510119895\n",
      "epoch 10316: train loss: 0.12335740798482597, test loss: 0.2526304328593935\n",
      "epoch 10317: train loss: 0.12335407802878137, test loss: 0.25263058087358814\n",
      "epoch 10318: train loss: 0.12335074852785746, test loss: 0.25263072914370155\n",
      "epoch 10319: train loss: 0.12334741948194484, test loss: 0.2526308776696521\n",
      "epoch 10320: train loss: 0.1233440908909341, test loss: 0.25263102645137925\n",
      "epoch 10321: train loss: 0.12334076275471585, test loss: 0.2526311754888113\n",
      "epoch 10322: train loss: 0.12333743507318079, test loss: 0.2526313247818783\n",
      "epoch 10323: train loss: 0.12333410784621966, test loss: 0.2526314743305034\n",
      "epoch 10324: train loss: 0.1233307810737232, test loss: 0.25263162413461604\n",
      "epoch 10325: train loss: 0.12332745475558216, test loss: 0.2526317741941457\n",
      "epoch 10326: train loss: 0.1233241288916874, test loss: 0.2526319245090306\n",
      "epoch 10327: train loss: 0.12332080348192981, test loss: 0.2526320750791908\n",
      "epoch 10328: train loss: 0.12331747852620026, test loss: 0.2526322259045551\n",
      "epoch 10329: train loss: 0.12331415402438965, test loss: 0.25263237698505864\n",
      "epoch 10330: train loss: 0.123310829976389, test loss: 0.2526325283206204\n",
      "epoch 10331: train loss: 0.12330750638208933, test loss: 0.25263267991117955\n",
      "epoch 10332: train loss: 0.12330418324138165, test loss: 0.2526328317566701\n",
      "epoch 10333: train loss: 0.12330086055415705, test loss: 0.2526329838570034\n",
      "epoch 10334: train loss: 0.12329753832030667, test loss: 0.25263313621211786\n",
      "epoch 10335: train loss: 0.12329421653972165, test loss: 0.252633288821952\n",
      "epoch 10336: train loss: 0.12329089521229317, test loss: 0.2526334416864222\n",
      "epoch 10337: train loss: 0.1232875743379125, test loss: 0.25263359480545805\n",
      "epoch 10338: train loss: 0.12328425391647087, test loss: 0.25263374817899825\n",
      "epoch 10339: train loss: 0.12328093394785959, test loss: 0.25263390180696893\n",
      "epoch 10340: train loss: 0.12327761443197001, test loss: 0.2526340556892936\n",
      "epoch 10341: train loss: 0.12327429536869353, test loss: 0.25263420982590257\n",
      "epoch 10342: train loss: 0.1232709767579215, test loss: 0.2526343642167371\n",
      "epoch 10343: train loss: 0.12326765859954542, test loss: 0.25263451886171073\n",
      "epoch 10344: train loss: 0.12326434089345673, test loss: 0.2526346737607637\n",
      "epoch 10345: train loss: 0.123261023639547, test loss: 0.2526348289138257\n",
      "epoch 10346: train loss: 0.12325770683770773, test loss: 0.25263498432082293\n",
      "epoch 10347: train loss: 0.1232543904878306, test loss: 0.2526351399816804\n",
      "epoch 10348: train loss: 0.12325107458980718, test loss: 0.25263529589633854\n",
      "epoch 10349: train loss: 0.12324775914352916, test loss: 0.25263545206471194\n",
      "epoch 10350: train loss: 0.1232444441488882, test loss: 0.2526356084867405\n",
      "epoch 10351: train loss: 0.12324112960577611, test loss: 0.252635765162366\n",
      "epoch 10352: train loss: 0.12323781551408462, test loss: 0.2526359220914878\n",
      "epoch 10353: train loss: 0.12323450187370556, test loss: 0.2526360792740667\n",
      "epoch 10354: train loss: 0.12323118868453077, test loss: 0.2526362367100037\n",
      "epoch 10355: train loss: 0.12322787594645215, test loss: 0.2526363943992484\n",
      "epoch 10356: train loss: 0.12322456365936162, test loss: 0.252636552341732\n",
      "epoch 10357: train loss: 0.12322125182315112, test loss: 0.2526367105373752\n",
      "epoch 10358: train loss: 0.12321794043771267, test loss: 0.25263686898610815\n",
      "epoch 10359: train loss: 0.1232146295029383, test loss: 0.2526370276878724\n",
      "epoch 10360: train loss: 0.12321131901872004, test loss: 0.2526371866425785\n",
      "epoch 10361: train loss: 0.12320800898495002, test loss: 0.25263734585016734\n",
      "epoch 10362: train loss: 0.1232046994015204, test loss: 0.2526375053105788\n",
      "epoch 10363: train loss: 0.12320139026832333, test loss: 0.25263766502372037\n",
      "epoch 10364: train loss: 0.12319808158525104, test loss: 0.2526378249895474\n",
      "epoch 10365: train loss: 0.12319477335219575, test loss: 0.25263798520796993\n",
      "epoch 10366: train loss: 0.12319146556904975, test loss: 0.25263814567892595\n",
      "epoch 10367: train loss: 0.12318815823570542, test loss: 0.25263830640234014\n",
      "epoch 10368: train loss: 0.12318485135205502, test loss: 0.2526384673781494\n",
      "epoch 10369: train loss: 0.12318154491799102, test loss: 0.25263862860627945\n",
      "epoch 10370: train loss: 0.12317823893340582, test loss: 0.2526387900866668\n",
      "epoch 10371: train loss: 0.12317493339819188, test loss: 0.2526389518192376\n",
      "epoch 10372: train loss: 0.12317162831224174, test loss: 0.25263911380392334\n",
      "epoch 10373: train loss: 0.12316832367544787, test loss: 0.2526392760406505\n",
      "epoch 10374: train loss: 0.12316501948770289, test loss: 0.25263943852935083\n",
      "epoch 10375: train loss: 0.1231617157488994, test loss: 0.25263960126996493\n",
      "epoch 10376: train loss: 0.12315841245893008, test loss: 0.25263976426240015\n",
      "epoch 10377: train loss: 0.12315510961768751, test loss: 0.252639927506613\n",
      "epoch 10378: train loss: 0.12315180722506452, test loss: 0.2526400910025093\n",
      "epoch 10379: train loss: 0.12314850528095382, test loss: 0.2526402547500467\n",
      "epoch 10380: train loss: 0.12314520378524818, test loss: 0.2526404187491314\n",
      "epoch 10381: train loss: 0.12314190273784047, test loss: 0.2526405829997053\n",
      "epoch 10382: train loss: 0.12313860213862349, test loss: 0.25264074750169546\n",
      "epoch 10383: train loss: 0.12313530198749018, test loss: 0.2526409122550332\n",
      "epoch 10384: train loss: 0.12313200228433345, test loss: 0.25264107725965557\n",
      "epoch 10385: train loss: 0.12312870302904631, test loss: 0.2526412425154752\n",
      "epoch 10386: train loss: 0.12312540422152175, test loss: 0.2526414080224487\n",
      "epoch 10387: train loss: 0.12312210586165274, test loss: 0.25264157378048746\n",
      "epoch 10388: train loss: 0.12311880794933244, test loss: 0.2526417397895293\n",
      "epoch 10389: train loss: 0.12311551048445396, test loss: 0.25264190604949677\n",
      "epoch 10390: train loss: 0.12311221346691043, test loss: 0.25264207256033083\n",
      "epoch 10391: train loss: 0.12310891689659499, test loss: 0.25264223932195956\n",
      "epoch 10392: train loss: 0.12310562077340094, test loss: 0.2526424063343148\n",
      "epoch 10393: train loss: 0.12310232509722147, test loss: 0.25264257359732467\n",
      "epoch 10394: train loss: 0.12309902986794988, test loss: 0.2526427411109112\n",
      "epoch 10395: train loss: 0.12309573508547954, test loss: 0.2526429088750259\n",
      "epoch 10396: train loss: 0.12309244074970377, test loss: 0.25264307688957716\n",
      "epoch 10397: train loss: 0.12308914686051597, test loss: 0.2526432451545119\n",
      "epoch 10398: train loss: 0.12308585341780962, test loss: 0.2526434136697583\n",
      "epoch 10399: train loss: 0.12308256042147814, test loss: 0.25264358243524415\n",
      "epoch 10400: train loss: 0.12307926787141504, test loss: 0.252643751450901\n",
      "epoch 10401: train loss: 0.12307597576751388, test loss: 0.2526439207166673\n",
      "epoch 10402: train loss: 0.12307268410966823, test loss: 0.25264409023246115\n",
      "epoch 10403: train loss: 0.12306939289777169, test loss: 0.2526442599982142\n",
      "epoch 10404: train loss: 0.12306610213171794, test loss: 0.25264443001386505\n",
      "epoch 10405: train loss: 0.12306281181140065, test loss: 0.2526446002793514\n",
      "epoch 10406: train loss: 0.12305952193671349, test loss: 0.2526447707945868\n",
      "epoch 10407: train loss: 0.12305623250755028, test loss: 0.2526449415595036\n",
      "epoch 10408: train loss: 0.12305294352380479, test loss: 0.25264511257404915\n",
      "epoch 10409: train loss: 0.12304965498537081, test loss: 0.25264528383815255\n",
      "epoch 10410: train loss: 0.12304636689214224, test loss: 0.2526454553517262\n",
      "epoch 10411: train loss: 0.12304307924401296, test loss: 0.25264562711471894\n",
      "epoch 10412: train loss: 0.12303979204087692, test loss: 0.25264579912705365\n",
      "epoch 10413: train loss: 0.12303650528262806, test loss: 0.2526459713886638\n",
      "epoch 10414: train loss: 0.12303321896916042, test loss: 0.252646143899487\n",
      "epoch 10415: train loss: 0.123029933100368, test loss: 0.2526463166594422\n",
      "epoch 10416: train loss: 0.1230266476761449, test loss: 0.2526464896684775\n",
      "epoch 10417: train loss: 0.12302336269638524, test loss: 0.2526466629265013\n",
      "epoch 10418: train loss: 0.12302007816098309, test loss: 0.2526468364334676\n",
      "epoch 10419: train loss: 0.12301679406983271, test loss: 0.2526470101892986\n",
      "epoch 10420: train loss: 0.12301351042282826, test loss: 0.2526471841939195\n",
      "epoch 10421: train loss: 0.12301022721986404, test loss: 0.2526473584472726\n",
      "epoch 10422: train loss: 0.12300694446083434, test loss: 0.25264753294928216\n",
      "epoch 10423: train loss: 0.1230036621456334, test loss: 0.25264770769988226\n",
      "epoch 10424: train loss: 0.12300038027415564, test loss: 0.25264788269900573\n",
      "epoch 10425: train loss: 0.12299709884629545, test loss: 0.25264805794657724\n",
      "epoch 10426: train loss: 0.12299381786194724, test loss: 0.25264823344253473\n",
      "epoch 10427: train loss: 0.1229905373210055, test loss: 0.2526484091868171\n",
      "epoch 10428: train loss: 0.12298725722336469, test loss: 0.25264858517933964\n",
      "epoch 10429: train loss: 0.12298397756891935, test loss: 0.25264876142004405\n",
      "epoch 10430: train loss: 0.12298069835756406, test loss: 0.25264893790886167\n",
      "epoch 10431: train loss: 0.12297741958919342, test loss: 0.25264911464572537\n",
      "epoch 10432: train loss: 0.12297414126370207, test loss: 0.25264929163055905\n",
      "epoch 10433: train loss: 0.12297086338098465, test loss: 0.25264946886330736\n",
      "epoch 10434: train loss: 0.12296758594093593, test loss: 0.25264964634388964\n",
      "epoch 10435: train loss: 0.12296430894345062, test loss: 0.2526498240722496\n",
      "epoch 10436: train loss: 0.12296103238842346, test loss: 0.25265000204829663\n",
      "epoch 10437: train loss: 0.12295775627574933, test loss: 0.2526501802719901\n",
      "epoch 10438: train loss: 0.12295448060532305, test loss: 0.2526503587432444\n",
      "epoch 10439: train loss: 0.12295120537703949, test loss: 0.25265053746200367\n",
      "epoch 10440: train loss: 0.12294793059079359, test loss: 0.2526507164281833\n",
      "epoch 10441: train loss: 0.12294465624648028, test loss: 0.252650895641737\n",
      "epoch 10442: train loss: 0.12294138234399458, test loss: 0.2526510751025841\n",
      "epoch 10443: train loss: 0.1229381088832315, test loss: 0.252651254810645\n",
      "epoch 10444: train loss: 0.12293483586408609, test loss: 0.25265143476587393\n",
      "epoch 10445: train loss: 0.12293156328645345, test loss: 0.2526516149681905\n",
      "epoch 10446: train loss: 0.12292829115022869, test loss: 0.2526517954175243\n",
      "epoch 10447: train loss: 0.12292501945530702, test loss: 0.25265197611381984\n",
      "epoch 10448: train loss: 0.12292174820158358, test loss: 0.25265215705700866\n",
      "epoch 10449: train loss: 0.12291847738895363, test loss: 0.252652338247008\n",
      "epoch 10450: train loss: 0.12291520701731247, test loss: 0.2526525196837553\n",
      "epoch 10451: train loss: 0.12291193708655535, test loss: 0.25265270136719353\n",
      "epoch 10452: train loss: 0.12290866759657763, test loss: 0.25265288329724883\n",
      "epoch 10453: train loss: 0.1229053985472747, test loss: 0.25265306547383576\n",
      "epoch 10454: train loss: 0.12290212993854194, test loss: 0.2526532478969194\n",
      "epoch 10455: train loss: 0.12289886177027479, test loss: 0.25265343056641476\n",
      "epoch 10456: train loss: 0.12289559404236876, test loss: 0.25265361348224313\n",
      "epoch 10457: train loss: 0.12289232675471932, test loss: 0.2526537966443582\n",
      "epoch 10458: train loss: 0.12288905990722206, test loss: 0.2526539800526866\n",
      "epoch 10459: train loss: 0.12288579349977255, test loss: 0.2526541637071521\n",
      "epoch 10460: train loss: 0.12288252753226636, test loss: 0.25265434760769145\n",
      "epoch 10461: train loss: 0.12287926200459921, test loss: 0.25265453175423924\n",
      "epoch 10462: train loss: 0.12287599691666674, test loss: 0.25265471614672635\n",
      "epoch 10463: train loss: 0.12287273226836469, test loss: 0.2526549007850877\n",
      "epoch 10464: train loss: 0.12286946805958879, test loss: 0.2526550856692491\n",
      "epoch 10465: train loss: 0.12286620429023486, test loss: 0.2526552707991418\n",
      "epoch 10466: train loss: 0.12286294096019873, test loss: 0.25265545617471075\n",
      "epoch 10467: train loss: 0.1228596780693762, test loss: 0.2526556417958911\n",
      "epoch 10468: train loss: 0.12285641561766322, test loss: 0.25265582766260036\n",
      "epoch 10469: train loss: 0.12285315360495572, test loss: 0.2526560137747672\n",
      "epoch 10470: train loss: 0.12284989203114963, test loss: 0.25265620013234835\n",
      "epoch 10471: train loss: 0.12284663089614095, test loss: 0.25265638673525975\n",
      "epoch 10472: train loss: 0.12284337019982576, test loss: 0.2526565735834319\n",
      "epoch 10473: train loss: 0.12284010994210007, test loss: 0.252656760676801\n",
      "epoch 10474: train loss: 0.12283685012286, test loss: 0.25265694801530836\n",
      "epoch 10475: train loss: 0.12283359074200169, test loss: 0.2526571355988749\n",
      "epoch 10476: train loss: 0.12283033179942131, test loss: 0.25265732342744013\n",
      "epoch 10477: train loss: 0.12282707329501508, test loss: 0.2526575115009381\n",
      "epoch 10478: train loss: 0.12282381522867919, test loss: 0.2526576998192972\n",
      "epoch 10479: train loss: 0.12282055760030997, test loss: 0.2526578883824442\n",
      "epoch 10480: train loss: 0.12281730040980367, test loss: 0.25265807719033706\n",
      "epoch 10481: train loss: 0.1228140436570567, test loss: 0.25265826624287546\n",
      "epoch 10482: train loss: 0.12281078734196539, test loss: 0.2526584555400113\n",
      "epoch 10483: train loss: 0.12280753146442618, test loss: 0.25265864508168046\n",
      "epoch 10484: train loss: 0.12280427602433544, test loss: 0.25265883486780455\n",
      "epoch 10485: train loss: 0.12280102102158975, test loss: 0.25265902489833036\n",
      "epoch 10486: train loss: 0.12279776645608559, test loss: 0.25265921517317474\n",
      "epoch 10487: train loss: 0.12279451232771951, test loss: 0.25265940569228795\n",
      "epoch 10488: train loss: 0.12279125863638805, test loss: 0.2526595964555821\n",
      "epoch 10489: train loss: 0.12278800538198789, test loss: 0.25265978746300943\n",
      "epoch 10490: train loss: 0.12278475256441564, test loss: 0.25265997871449164\n",
      "epoch 10491: train loss: 0.12278150018356801, test loss: 0.25266017020996434\n",
      "epoch 10492: train loss: 0.12277824823934168, test loss: 0.25266036194937547\n",
      "epoch 10493: train loss: 0.12277499673163346, test loss: 0.2526605539326357\n",
      "epoch 10494: train loss: 0.12277174566034012, test loss: 0.25266074615968803\n",
      "epoch 10495: train loss: 0.12276849502535846, test loss: 0.25266093863046396\n",
      "epoch 10496: train loss: 0.12276524482658538, test loss: 0.25266113134490575\n",
      "epoch 10497: train loss: 0.12276199506391769, test loss: 0.25266132430293864\n",
      "epoch 10498: train loss: 0.1227587457372524, test loss: 0.25266151750449223\n",
      "epoch 10499: train loss: 0.12275549684648646, test loss: 0.2526617109495016\n",
      "epoch 10500: train loss: 0.1227522483915168, test loss: 0.2526619046379104\n",
      "epoch 10501: train loss: 0.12274900037224054, test loss: 0.25266209856964444\n",
      "epoch 10502: train loss: 0.12274575278855465, test loss: 0.25266229274463664\n",
      "epoch 10503: train loss: 0.1227425056403563, test loss: 0.25266248716281864\n",
      "epoch 10504: train loss: 0.12273925892754256, test loss: 0.25266268182412377\n",
      "epoch 10505: train loss: 0.12273601265001063, test loss: 0.2526628767284988\n",
      "epoch 10506: train loss: 0.12273276680765774, test loss: 0.25266307187586573\n",
      "epoch 10507: train loss: 0.12272952140038104, test loss: 0.25266326726615296\n",
      "epoch 10508: train loss: 0.12272627642807783, test loss: 0.2526634628993014\n",
      "epoch 10509: train loss: 0.12272303189064548, test loss: 0.25266365877524\n",
      "epoch 10510: train loss: 0.12271978778798126, test loss: 0.2526638548939109\n",
      "epoch 10511: train loss: 0.12271654411998252, test loss: 0.25266405125524694\n",
      "epoch 10512: train loss: 0.12271330088654668, test loss: 0.25266424785917985\n",
      "epoch 10513: train loss: 0.12271005808757121, test loss: 0.25266444470563276\n",
      "epoch 10514: train loss: 0.12270681572295355, test loss: 0.2526646417945489\n",
      "epoch 10515: train loss: 0.12270357379259121, test loss: 0.252664839125871\n",
      "epoch 10516: train loss: 0.12270033229638169, test loss: 0.2526650366995106\n",
      "epoch 10517: train loss: 0.12269709123422264, test loss: 0.2526652345154217\n",
      "epoch 10518: train loss: 0.12269385060601162, test loss: 0.2526654325735275\n",
      "epoch 10519: train loss: 0.12269061041164629, test loss: 0.25266563087377064\n",
      "epoch 10520: train loss: 0.12268737065102432, test loss: 0.252665829416073\n",
      "epoch 10521: train loss: 0.12268413132404338, test loss: 0.25266602820037926\n",
      "epoch 10522: train loss: 0.12268089243060125, test loss: 0.2526662272266113\n",
      "epoch 10523: train loss: 0.12267765397059566, test loss: 0.25266642649471754\n",
      "epoch 10524: train loss: 0.12267441594392452, test loss: 0.2526666260046218\n",
      "epoch 10525: train loss: 0.12267117835048559, test loss: 0.2526668257562615\n",
      "epoch 10526: train loss: 0.12266794119017675, test loss: 0.25266702574956507\n",
      "epoch 10527: train loss: 0.12266470446289592, test loss: 0.25266722598448094\n",
      "epoch 10528: train loss: 0.1226614681685411, test loss: 0.2526674264609376\n",
      "epoch 10529: train loss: 0.12265823230701015, test loss: 0.25266762717885805\n",
      "epoch 10530: train loss: 0.12265499687820118, test loss: 0.25266782813818023\n",
      "epoch 10531: train loss: 0.1226517618820122, test loss: 0.2526680293388493\n",
      "epoch 10532: train loss: 0.12264852731834132, test loss: 0.25266823078078726\n",
      "epoch 10533: train loss: 0.12264529318708661, test loss: 0.252668432463938\n",
      "epoch 10534: train loss: 0.12264205948814622, test loss: 0.25266863438822484\n",
      "epoch 10535: train loss: 0.12263882622141839, test loss: 0.2526688365535973\n",
      "epoch 10536: train loss: 0.12263559338680124, test loss: 0.2526690389599733\n",
      "epoch 10537: train loss: 0.1226323609841931, test loss: 0.2526692416072968\n",
      "epoch 10538: train loss: 0.12262912901349221, test loss: 0.25266944449549655\n",
      "epoch 10539: train loss: 0.12262589747459689, test loss: 0.25266964762451155\n",
      "epoch 10540: train loss: 0.12262266636740549, test loss: 0.25266985099427985\n",
      "epoch 10541: train loss: 0.1226194356918164, test loss: 0.2526700546047262\n",
      "epoch 10542: train loss: 0.12261620544772804, test loss: 0.2526702584557945\n",
      "epoch 10543: train loss: 0.12261297563503883, test loss: 0.2526704625474084\n",
      "epoch 10544: train loss: 0.12260974625364728, test loss: 0.2526706668795065\n",
      "epoch 10545: train loss: 0.12260651730345189, test loss: 0.25267087145202943\n",
      "epoch 10546: train loss: 0.1226032887843512, test loss: 0.2526710762648992\n",
      "epoch 10547: train loss: 0.12260006069624384, test loss: 0.252671281318072\n",
      "epoch 10548: train loss: 0.1225968330390284, test loss: 0.252671486611455\n",
      "epoch 10549: train loss: 0.12259360581260349, test loss: 0.25267169214500385\n",
      "epoch 10550: train loss: 0.12259037901686785, test loss: 0.25267189791863753\n",
      "epoch 10551: train loss: 0.12258715265172018, test loss: 0.2526721039323053\n",
      "epoch 10552: train loss: 0.12258392671705924, test loss: 0.2526723101859317\n",
      "epoch 10553: train loss: 0.12258070121278378, test loss: 0.25267251667946\n",
      "epoch 10554: train loss: 0.12257747613879263, test loss: 0.25267272341282676\n",
      "epoch 10555: train loss: 0.12257425149498466, test loss: 0.2526729303859448\n",
      "epoch 10556: train loss: 0.12257102728125875, test loss: 0.25267313759876364\n",
      "epoch 10557: train loss: 0.12256780349751381, test loss: 0.2526733450512337\n",
      "epoch 10558: train loss: 0.12256458014364878, test loss: 0.25267355274326475\n",
      "epoch 10559: train loss: 0.12256135721956266, test loss: 0.25267376067479497\n",
      "epoch 10560: train loss: 0.12255813472515445, test loss: 0.2526739688457703\n",
      "epoch 10561: train loss: 0.12255491266032323, test loss: 0.25267417725611985\n",
      "epoch 10562: train loss: 0.12255169102496805, test loss: 0.25267438590578445\n",
      "epoch 10563: train loss: 0.12254846981898805, test loss: 0.25267459479468274\n",
      "epoch 10564: train loss: 0.12254524904228235, test loss: 0.25267480392277014\n",
      "epoch 10565: train loss: 0.12254202869475017, test loss: 0.2526750132899725\n",
      "epoch 10566: train loss: 0.12253880877629073, test loss: 0.2526752228962225\n",
      "epoch 10567: train loss: 0.12253558928680323, test loss: 0.2526754327414519\n",
      "epoch 10568: train loss: 0.12253237022618702, test loss: 0.252675642825605\n",
      "epoch 10569: train loss: 0.12252915159434137, test loss: 0.25267585314861796\n",
      "epoch 10570: train loss: 0.12252593339116563, test loss: 0.25267606371040946\n",
      "epoch 10571: train loss: 0.1225227156165592, test loss: 0.25267627451093544\n",
      "epoch 10572: train loss: 0.12251949827042148, test loss: 0.25267648555012084\n",
      "epoch 10573: train loss: 0.12251628135265193, test loss: 0.25267669682788546\n",
      "epoch 10574: train loss: 0.12251306486315006, test loss: 0.2526769083442018\n",
      "epoch 10575: train loss: 0.1225098488018153, test loss: 0.25267712009897203\n",
      "epoch 10576: train loss: 0.1225066331685473, test loss: 0.252677332092139\n",
      "epoch 10577: train loss: 0.12250341796324557, test loss: 0.25267754432365136\n",
      "epoch 10578: train loss: 0.12250020318580976, test loss: 0.252677756793431\n",
      "epoch 10579: train loss: 0.12249698883613949, test loss: 0.25267796950140764\n",
      "epoch 10580: train loss: 0.12249377491413448, test loss: 0.2526781824475325\n",
      "epoch 10581: train loss: 0.1224905614196944, test loss: 0.25267839563173544\n",
      "epoch 10582: train loss: 0.12248734835271904, test loss: 0.2526786090539473\n",
      "epoch 10583: train loss: 0.12248413571310811, test loss: 0.2526788227141086\n",
      "epoch 10584: train loss: 0.12248092350076151, test loss: 0.2526790366121501\n",
      "epoch 10585: train loss: 0.122477711715579, test loss: 0.25267925074801295\n",
      "epoch 10586: train loss: 0.12247450035746056, test loss: 0.2526794651216332\n",
      "epoch 10587: train loss: 0.12247128942630599, test loss: 0.25267967973294064\n",
      "epoch 10588: train loss: 0.1224680789220153, test loss: 0.2526798945818622\n",
      "epoch 10589: train loss: 0.12246486884448846, test loss: 0.25268010966834825\n",
      "epoch 10590: train loss: 0.12246165919362546, test loss: 0.2526803249923394\n",
      "epoch 10591: train loss: 0.12245844996932637, test loss: 0.2526805405537473\n",
      "epoch 10592: train loss: 0.12245524117149124, test loss: 0.25268075635253273\n",
      "epoch 10593: train loss: 0.1224520328000202, test loss: 0.25268097238861703\n",
      "epoch 10594: train loss: 0.12244882485481336, test loss: 0.25268118866193134\n",
      "epoch 10595: train loss: 0.12244561733577092, test loss: 0.25268140517242593\n",
      "epoch 10596: train loss: 0.12244241024279308, test loss: 0.2526816219200333\n",
      "epoch 10597: train loss: 0.12243920357578009, test loss: 0.2526818389046848\n",
      "epoch 10598: train loss: 0.12243599733463219, test loss: 0.2526820561263102\n",
      "epoch 10599: train loss: 0.12243279151924974, test loss: 0.252682273584857\n",
      "epoch 10600: train loss: 0.122429586129533, test loss: 0.2526824912802558\n",
      "epoch 10601: train loss: 0.12242638116538242, test loss: 0.2526827092124294\n",
      "epoch 10602: train loss: 0.12242317662669837, test loss: 0.25268292738134285\n",
      "epoch 10603: train loss: 0.12241997251338127, test loss: 0.2526831457869142\n",
      "epoch 10604: train loss: 0.12241676882533159, test loss: 0.25268336442907313\n",
      "epoch 10605: train loss: 0.12241356556244991, test loss: 0.252683583307767\n",
      "epoch 10606: train loss: 0.12241036272463664, test loss: 0.2526838024229173\n",
      "epoch 10607: train loss: 0.12240716031179241, test loss: 0.252684021774482\n",
      "epoch 10608: train loss: 0.12240395832381783, test loss: 0.2526842413623813\n",
      "epoch 10609: train loss: 0.12240075676061352, test loss: 0.25268446118655774\n",
      "epoch 10610: train loss: 0.12239755562208013, test loss: 0.25268468124693755\n",
      "epoch 10611: train loss: 0.1223943549081184, test loss: 0.25268490154346307\n",
      "epoch 10612: train loss: 0.12239115461862898, test loss: 0.2526851220760857\n",
      "epoch 10613: train loss: 0.1223879547535127, test loss: 0.2526853428447183\n",
      "epoch 10614: train loss: 0.12238475531267035, test loss: 0.2526855638493015\n",
      "epoch 10615: train loss: 0.12238155629600271, test loss: 0.25268578508977313\n",
      "epoch 10616: train loss: 0.1223783577034107, test loss: 0.25268600656608114\n",
      "epoch 10617: train loss: 0.12237515953479519, test loss: 0.25268622827814524\n",
      "epoch 10618: train loss: 0.12237196179005709, test loss: 0.2526864502259097\n",
      "epoch 10619: train loss: 0.12236876446909736, test loss: 0.2526866724093109\n",
      "epoch 10620: train loss: 0.12236556757181702, test loss: 0.2526868948282913\n",
      "epoch 10621: train loss: 0.12236237109811705, test loss: 0.2526871174827719\n",
      "epoch 10622: train loss: 0.12235917504789855, test loss: 0.2526873403726871\n",
      "epoch 10623: train loss: 0.12235597942106256, test loss: 0.25268756349798704\n",
      "epoch 10624: train loss: 0.12235278421751022, test loss: 0.2526877868586063\n",
      "epoch 10625: train loss: 0.1223495894371427, test loss: 0.2526880104544813\n",
      "epoch 10626: train loss: 0.12234639507986117, test loss: 0.2526882342855441\n",
      "epoch 10627: train loss: 0.12234320114556682, test loss: 0.2526884583517272\n",
      "epoch 10628: train loss: 0.12234000763416099, test loss: 0.2526886826529793\n",
      "epoch 10629: train loss: 0.12233681454554482, test loss: 0.2526889071892272\n",
      "epoch 10630: train loss: 0.12233362187961973, test loss: 0.25268913196040804\n",
      "epoch 10631: train loss: 0.12233042963628706, test loss: 0.252689356966455\n",
      "epoch 10632: train loss: 0.12232723781544816, test loss: 0.25268958220732113\n",
      "epoch 10633: train loss: 0.12232404641700441, test loss: 0.25268980768291843\n",
      "epoch 10634: train loss: 0.12232085544085733, test loss: 0.25269003339320006\n",
      "epoch 10635: train loss: 0.12231766488690832, test loss: 0.2526902593380993\n",
      "epoch 10636: train loss: 0.12231447475505895, test loss: 0.25269048551755346\n",
      "epoch 10637: train loss: 0.12231128504521072, test loss: 0.25269071193150067\n",
      "epoch 10638: train loss: 0.12230809575726521, test loss: 0.25269093857986874\n",
      "epoch 10639: train loss: 0.12230490689112404, test loss: 0.252691165462601\n",
      "epoch 10640: train loss: 0.12230171844668883, test loss: 0.252691392579635\n",
      "epoch 10641: train loss: 0.12229853042386125, test loss: 0.2526916199308986\n",
      "epoch 10642: train loss: 0.12229534282254301, test loss: 0.2526918475163461\n",
      "epoch 10643: train loss: 0.12229215564263583, test loss: 0.2526920753358944\n",
      "epoch 10644: train loss: 0.1222889688840415, test loss: 0.2526923033895034\n",
      "epoch 10645: train loss: 0.12228578254666181, test loss: 0.2526925316770788\n",
      "epoch 10646: train loss: 0.12228259663039856, test loss: 0.25269276019858106\n",
      "epoch 10647: train loss: 0.12227941113515364, test loss: 0.2526929889539432\n",
      "epoch 10648: train loss: 0.12227622606082893, test loss: 0.25269321794310223\n",
      "epoch 10649: train loss: 0.12227304140732638, test loss: 0.2526934471659883\n",
      "epoch 10650: train loss: 0.12226985717454789, test loss: 0.25269367662253883\n",
      "epoch 10651: train loss: 0.12226667336239552, test loss: 0.25269390631269106\n",
      "epoch 10652: train loss: 0.12226348997077126, test loss: 0.25269413623639203\n",
      "epoch 10653: train loss: 0.12226030699957718, test loss: 0.2526943663935674\n",
      "epoch 10654: train loss: 0.12225712444871534, test loss: 0.2526945967841512\n",
      "epoch 10655: train loss: 0.12225394231808785, test loss: 0.2526948274080978\n",
      "epoch 10656: train loss: 0.1222507606075969, test loss: 0.25269505826532573\n",
      "epoch 10657: train loss: 0.12224757931714464, test loss: 0.25269528935579405\n",
      "epoch 10658: train loss: 0.1222443984466333, test loss: 0.2526955206794101\n",
      "epoch 10659: train loss: 0.12224121799596512, test loss: 0.2526957522361293\n",
      "epoch 10660: train loss: 0.12223803796504239, test loss: 0.2526959840258954\n",
      "epoch 10661: train loss: 0.12223485835376739, test loss: 0.2526962160486313\n",
      "epoch 10662: train loss: 0.1222316791620425, test loss: 0.25269644830427046\n",
      "epoch 10663: train loss: 0.12222850038977008, test loss: 0.2526966807927745\n",
      "epoch 10664: train loss: 0.1222253220368525, test loss: 0.2526969135140498\n",
      "epoch 10665: train loss: 0.12222214410319225, test loss: 0.25269714646805136\n",
      "epoch 10666: train loss: 0.12221896658869179, test loss: 0.2526973796547132\n",
      "epoch 10667: train loss: 0.12221578949325357, test loss: 0.2526976130739754\n",
      "epoch 10668: train loss: 0.12221261281678018, test loss: 0.2526978467257755\n",
      "epoch 10669: train loss: 0.12220943655917417, test loss: 0.2526980806100449\n",
      "epoch 10670: train loss: 0.12220626072033815, test loss: 0.25269831472671456\n",
      "epoch 10671: train loss: 0.12220308530017468, test loss: 0.25269854907573647\n",
      "epoch 10672: train loss: 0.1221999102985865, test loss: 0.25269878365705467\n",
      "epoch 10673: train loss: 0.12219673571547628, test loss: 0.25269901847058274\n",
      "epoch 10674: train loss: 0.1221935615507467, test loss: 0.2526992535162705\n",
      "epoch 10675: train loss: 0.12219038780430058, test loss: 0.25269948879405746\n",
      "epoch 10676: train loss: 0.12218721447604067, test loss: 0.25269972430386795\n",
      "epoch 10677: train loss: 0.12218404156586979, test loss: 0.2526999600456672\n",
      "epoch 10678: train loss: 0.1221808690736908, test loss: 0.2527001960193583\n",
      "epoch 10679: train loss: 0.12217769699940659, test loss: 0.2527004322249122\n",
      "epoch 10680: train loss: 0.12217452534292006, test loss: 0.25270066866223806\n",
      "epoch 10681: train loss: 0.12217135410413414, test loss: 0.2527009053312807\n",
      "epoch 10682: train loss: 0.12216818328295183, test loss: 0.25270114223198936\n",
      "epoch 10683: train loss: 0.12216501287927616, test loss: 0.25270137936429526\n",
      "epoch 10684: train loss: 0.12216184289301012, test loss: 0.25270161672813635\n",
      "epoch 10685: train loss: 0.1221586733240568, test loss: 0.25270185432344905\n",
      "epoch 10686: train loss: 0.12215550417231931, test loss: 0.2527020921501619\n",
      "epoch 10687: train loss: 0.12215233543770082, test loss: 0.2527023302082365\n",
      "epoch 10688: train loss: 0.12214916712010443, test loss: 0.25270256849758677\n",
      "epoch 10689: train loss: 0.12214599921943335, test loss: 0.2527028070181525\n",
      "epoch 10690: train loss: 0.12214283173559086, test loss: 0.2527030457698947\n",
      "epoch 10691: train loss: 0.1221396646684802, test loss: 0.25270328475271653\n",
      "epoch 10692: train loss: 0.12213649801800461, test loss: 0.25270352396659107\n",
      "epoch 10693: train loss: 0.12213333178406749, test loss: 0.25270376341143647\n",
      "epoch 10694: train loss: 0.12213016596657215, test loss: 0.25270400308718877\n",
      "epoch 10695: train loss: 0.122127000565422, test loss: 0.2527042429937813\n",
      "epoch 10696: train loss: 0.12212383558052045, test loss: 0.2527044831311783\n",
      "epoch 10697: train loss: 0.12212067101177095, test loss: 0.25270472349928613\n",
      "epoch 10698: train loss: 0.12211750685907698, test loss: 0.2527049640980619\n",
      "epoch 10699: train loss: 0.12211434312234204, test loss: 0.25270520492743664\n",
      "epoch 10700: train loss: 0.1221111798014697, test loss: 0.252705445987355\n",
      "epoch 10701: train loss: 0.12210801689636351, test loss: 0.25270568727774323\n",
      "epoch 10702: train loss: 0.12210485440692709, test loss: 0.25270592879855586\n",
      "epoch 10703: train loss: 0.12210169233306409, test loss: 0.2527061705497188\n",
      "epoch 10704: train loss: 0.12209853067467819, test loss: 0.2527064125311675\n",
      "epoch 10705: train loss: 0.12209536943167304, test loss: 0.25270665474284343\n",
      "epoch 10706: train loss: 0.12209220860395241, test loss: 0.2527068971846919\n",
      "epoch 10707: train loss: 0.12208904819142008, test loss: 0.2527071398566483\n",
      "epoch 10708: train loss: 0.12208588819397981, test loss: 0.2527073827586382\n",
      "epoch 10709: train loss: 0.12208272861153542, test loss: 0.25270762589061324\n",
      "epoch 10710: train loss: 0.12207956944399082, test loss: 0.2527078692525141\n",
      "epoch 10711: train loss: 0.12207641069124983, test loss: 0.25270811284426653\n",
      "epoch 10712: train loss: 0.12207325235321644, test loss: 0.2527083566658166\n",
      "epoch 10713: train loss: 0.12207009442979455, test loss: 0.2527086007171051\n",
      "epoch 10714: train loss: 0.12206693692088816, test loss: 0.2527088449980575\n",
      "epoch 10715: train loss: 0.12206377982640129, test loss: 0.25270908950862053\n",
      "epoch 10716: train loss: 0.12206062314623796, test loss: 0.2527093342487357\n",
      "epoch 10717: train loss: 0.12205746688030232, test loss: 0.2527095792183388\n",
      "epoch 10718: train loss: 0.12205431102849836, test loss: 0.2527098244173758\n",
      "epoch 10719: train loss: 0.1220511555907303, test loss: 0.25271006984576294\n",
      "epoch 10720: train loss: 0.12204800056690232, test loss: 0.252710315503456\n",
      "epoch 10721: train loss: 0.12204484595691856, test loss: 0.2527105613903977\n",
      "epoch 10722: train loss: 0.1220416917606833, test loss: 0.25271080750651337\n",
      "epoch 10723: train loss: 0.12203853797810081, test loss: 0.2527110538517496\n",
      "epoch 10724: train loss: 0.12203538460907533, test loss: 0.25271130042603734\n",
      "epoch 10725: train loss: 0.12203223165351122, test loss: 0.25271154722932415\n",
      "epoch 10726: train loss: 0.12202907911131286, test loss: 0.2527117942615453\n",
      "epoch 10727: train loss: 0.12202592698238458, test loss: 0.25271204152263793\n",
      "epoch 10728: train loss: 0.12202277526663086, test loss: 0.2527122890125387\n",
      "epoch 10729: train loss: 0.12201962396395612, test loss: 0.2527125367311839\n",
      "epoch 10730: train loss: 0.12201647307426482, test loss: 0.2527127846785205\n",
      "epoch 10731: train loss: 0.12201332259746149, test loss: 0.25271303285448526\n",
      "epoch 10732: train loss: 0.12201017253345067, test loss: 0.25271328125902026\n",
      "epoch 10733: train loss: 0.12200702288213698, test loss: 0.2527135298920507\n",
      "epoch 10734: train loss: 0.12200387364342495, test loss: 0.25271377875351964\n",
      "epoch 10735: train loss: 0.12200072481721926, test loss: 0.2527140278433789\n",
      "epoch 10736: train loss: 0.12199757640342454, test loss: 0.25271427716154526\n",
      "epoch 10737: train loss: 0.12199442840194553, test loss: 0.2527145267079859\n",
      "epoch 10738: train loss: 0.12199128081268695, test loss: 0.2527147764826173\n",
      "epoch 10739: train loss: 0.12198813363555354, test loss: 0.2527150264853863\n",
      "epoch 10740: train loss: 0.1219849868704501, test loss: 0.25271527671621974\n",
      "epoch 10741: train loss: 0.12198184051728145, test loss: 0.25271552717508183\n",
      "epoch 10742: train loss: 0.12197869457595246, test loss: 0.2527157778618927\n",
      "epoch 10743: train loss: 0.121975549046368, test loss: 0.2527160287765895\n",
      "epoch 10744: train loss: 0.12197240392843295, test loss: 0.25271627991911416\n",
      "epoch 10745: train loss: 0.1219692592220523, test loss: 0.2527165312894215\n",
      "epoch 10746: train loss: 0.121966114927131, test loss: 0.25271678288742516\n",
      "epoch 10747: train loss: 0.12196297104357406, test loss: 0.2527170347130802\n",
      "epoch 10748: train loss: 0.12195982757128654, test loss: 0.2527172867663128\n",
      "epoch 10749: train loss: 0.12195668451017347, test loss: 0.25271753904708083\n",
      "epoch 10750: train loss: 0.12195354186013997, test loss: 0.252717791555311\n",
      "epoch 10751: train loss: 0.12195039962109117, test loss: 0.25271804429094175\n",
      "epoch 10752: train loss: 0.12194725779293225, test loss: 0.2527182972539208\n",
      "epoch 10753: train loss: 0.12194411637556835, test loss: 0.2527185504441845\n",
      "epoch 10754: train loss: 0.1219409753689047, test loss: 0.25271880386166024\n",
      "epoch 10755: train loss: 0.12193783477284661, test loss: 0.2527190575062921\n",
      "epoch 10756: train loss: 0.1219346945872993, test loss: 0.2527193113780377\n",
      "epoch 10757: train loss: 0.12193155481216814, test loss: 0.2527195654768131\n",
      "epoch 10758: train loss: 0.12192841544735838, test loss: 0.252719819802563\n",
      "epoch 10759: train loss: 0.12192527649277549, test loss: 0.25272007435522315\n",
      "epoch 10760: train loss: 0.12192213794832485, test loss: 0.25272032913475323\n",
      "epoch 10761: train loss: 0.12191899981391184, test loss: 0.2527205841410696\n",
      "epoch 10762: train loss: 0.12191586208944201, test loss: 0.25272083937412193\n",
      "epoch 10763: train loss: 0.12191272477482082, test loss: 0.2527210948338518\n",
      "epoch 10764: train loss: 0.12190958786995378, test loss: 0.252721350520198\n",
      "epoch 10765: train loss: 0.12190645137474648, test loss: 0.2527216064330874\n",
      "epoch 10766: train loss: 0.12190331528910449, test loss: 0.25272186257246376\n",
      "epoch 10767: train loss: 0.1219001796129334, test loss: 0.25272211893828656\n",
      "epoch 10768: train loss: 0.12189704434613895, test loss: 0.2527223755304709\n",
      "epoch 10769: train loss: 0.12189390948862672, test loss: 0.25272263234896236\n",
      "epoch 10770: train loss: 0.12189077504030246, test loss: 0.2527228893937027\n",
      "epoch 10771: train loss: 0.12188764100107194, test loss: 0.25272314666464774\n",
      "epoch 10772: train loss: 0.12188450737084089, test loss: 0.2527234041616973\n",
      "epoch 10773: train loss: 0.12188137414951512, test loss: 0.252723661884827\n",
      "epoch 10774: train loss: 0.12187824133700051, test loss: 0.2527239198339698\n",
      "epoch 10775: train loss: 0.12187510893320287, test loss: 0.2527241780090591\n",
      "epoch 10776: train loss: 0.12187197693802808, test loss: 0.2527244364100273\n",
      "epoch 10777: train loss: 0.12186884535138214, test loss: 0.25272469503682427\n",
      "epoch 10778: train loss: 0.12186571417317094, test loss: 0.25272495388938276\n",
      "epoch 10779: train loss: 0.12186258340330046, test loss: 0.25272521296764583\n",
      "epoch 10780: train loss: 0.12185945304167677, test loss: 0.25272547227155356\n",
      "epoch 10781: train loss: 0.12185632308820588, test loss: 0.2527257318010541\n",
      "epoch 10782: train loss: 0.12185319354279384, test loss: 0.2527259915560862\n",
      "epoch 10783: train loss: 0.1218500644053468, test loss: 0.2527262515365625\n",
      "epoch 10784: train loss: 0.12184693567577089, test loss: 0.252726511742458\n",
      "epoch 10785: train loss: 0.12184380735397227, test loss: 0.2527267721736855\n",
      "epoch 10786: train loss: 0.12184067943985713, test loss: 0.25272703283020476\n",
      "epoch 10787: train loss: 0.12183755193333173, test loss: 0.252727293711944\n",
      "epoch 10788: train loss: 0.12183442483430226, test loss: 0.25272755481884773\n",
      "epoch 10789: train loss: 0.12183129814267507, test loss: 0.25272781615084844\n",
      "epoch 10790: train loss: 0.12182817185835645, test loss: 0.252728077707903\n",
      "epoch 10791: train loss: 0.12182504598125277, test loss: 0.25272833948993784\n",
      "epoch 10792: train loss: 0.12182192051127039, test loss: 0.25272860149688736\n",
      "epoch 10793: train loss: 0.12181879544831574, test loss: 0.2527288637287124\n",
      "epoch 10794: train loss: 0.12181567079229523, test loss: 0.2527291261853238\n",
      "epoch 10795: train loss: 0.12181254654311535, test loss: 0.25272938886668894\n",
      "epoch 10796: train loss: 0.12180942270068261, test loss: 0.25272965177273643\n",
      "epoch 10797: train loss: 0.12180629926490351, test loss: 0.2527299149033993\n",
      "epoch 10798: train loss: 0.12180317623568461, test loss: 0.2527301782586328\n",
      "epoch 10799: train loss: 0.12180005361293254, test loss: 0.2527304418383607\n",
      "epoch 10800: train loss: 0.12179693139655388, test loss: 0.2527307056425322\n",
      "epoch 10801: train loss: 0.12179380958645529, test loss: 0.2527309696710931\n",
      "epoch 10802: train loss: 0.12179068818254347, test loss: 0.2527312339239768\n",
      "epoch 10803: train loss: 0.12178756718472511, test loss: 0.25273149840111736\n",
      "epoch 10804: train loss: 0.12178444659290698, test loss: 0.25273176310246\n",
      "epoch 10805: train loss: 0.12178132640699581, test loss: 0.25273202802795425\n",
      "epoch 10806: train loss: 0.1217782066268984, test loss: 0.2527322931775243\n",
      "epoch 10807: train loss: 0.12177508725252162, test loss: 0.25273255855112553\n",
      "epoch 10808: train loss: 0.1217719682837723, test loss: 0.25273282414868137\n",
      "epoch 10809: train loss: 0.12176884972055735, test loss: 0.2527330899701477\n",
      "epoch 10810: train loss: 0.1217657315627837, test loss: 0.2527333560154588\n",
      "epoch 10811: train loss: 0.12176261381035826, test loss: 0.25273362228456\n",
      "epoch 10812: train loss: 0.12175949646318804, test loss: 0.2527338887773805\n",
      "epoch 10813: train loss: 0.12175637952118006, test loss: 0.25273415549386485\n",
      "epoch 10814: train loss: 0.12175326298424131, test loss: 0.2527344224339586\n",
      "epoch 10815: train loss: 0.12175014685227893, test loss: 0.25273468959759643\n",
      "epoch 10816: train loss: 0.12174703112519998, test loss: 0.25273495698472387\n",
      "epoch 10817: train loss: 0.1217439158029116, test loss: 0.252735224595275\n",
      "epoch 10818: train loss: 0.12174080088532094, test loss: 0.25273549242919535\n",
      "epoch 10819: train loss: 0.12173768637233522, test loss: 0.25273576048641977\n",
      "epoch 10820: train loss: 0.12173457226386163, test loss: 0.2527360287669044\n",
      "epoch 10821: train loss: 0.12173145855980742, test loss: 0.25273629727057334\n",
      "epoch 10822: train loss: 0.12172834526007989, test loss: 0.2527365659973564\n",
      "epoch 10823: train loss: 0.12172523236458636, test loss: 0.25273683494723553\n",
      "epoch 10824: train loss: 0.12172211987323413, test loss: 0.252737104120104\n",
      "epoch 10825: train loss: 0.12171900778593059, test loss: 0.25273737351593367\n",
      "epoch 10826: train loss: 0.12171589610258315, test loss: 0.2527376431346539\n",
      "epoch 10827: train loss: 0.12171278482309923, test loss: 0.2527379129762116\n",
      "epoch 10828: train loss: 0.12170967394738628, test loss: 0.25273818304054063\n",
      "epoch 10829: train loss: 0.12170656347535182, test loss: 0.2527384533275761\n",
      "epoch 10830: train loss: 0.1217034534069033, test loss: 0.2527387238372703\n",
      "epoch 10831: train loss: 0.12170034374194834, test loss: 0.25273899456956805\n",
      "epoch 10832: train loss: 0.1216972344803945, test loss: 0.25273926552438297\n",
      "epoch 10833: train loss: 0.12169412562214937, test loss: 0.2527395367016942\n",
      "epoch 10834: train loss: 0.12169101716712062, test loss: 0.25273980810141494\n",
      "epoch 10835: train loss: 0.12168790911521586, test loss: 0.25274007972348644\n",
      "epoch 10836: train loss: 0.12168480146634283, test loss: 0.2527403515678753\n",
      "epoch 10837: train loss: 0.12168169422040925, test loss: 0.25274062363448563\n",
      "epoch 10838: train loss: 0.12167858737732289, test loss: 0.25274089592329524\n",
      "epoch 10839: train loss: 0.12167548093699151, test loss: 0.2527411684342079\n",
      "epoch 10840: train loss: 0.12167237489932294, test loss: 0.2527414411672013\n",
      "epoch 10841: train loss: 0.12166926926422503, test loss: 0.25274171412219054\n",
      "epoch 10842: train loss: 0.12166616403160564, test loss: 0.2527419872991262\n",
      "epoch 10843: train loss: 0.12166305920137269, test loss: 0.2527422606979393\n",
      "epoch 10844: train loss: 0.12165995477343408, test loss: 0.2527425343185866\n",
      "epoch 10845: train loss: 0.12165685074769782, test loss: 0.2527428081609934\n",
      "epoch 10846: train loss: 0.12165374712407186, test loss: 0.252743082225117\n",
      "epoch 10847: train loss: 0.12165064390246426, test loss: 0.2527433565108926\n",
      "epoch 10848: train loss: 0.12164754108278306, test loss: 0.25274363101825154\n",
      "epoch 10849: train loss: 0.12164443866493632, test loss: 0.2527439057471503\n",
      "epoch 10850: train loss: 0.1216413366488322, test loss: 0.25274418069751436\n",
      "epoch 10851: train loss: 0.12163823503437879, test loss: 0.25274445586929545\n",
      "epoch 10852: train loss: 0.12163513382148426, test loss: 0.2527447312624244\n",
      "epoch 10853: train loss: 0.12163203301005687, test loss: 0.2527450068768593\n",
      "epoch 10854: train loss: 0.12162893260000479, test loss: 0.2527452827125249\n",
      "epoch 10855: train loss: 0.12162583259123631, test loss: 0.2527455587693791\n",
      "epoch 10856: train loss: 0.12162273298365969, test loss: 0.25274583504734077\n",
      "epoch 10857: train loss: 0.12161963377718327, test loss: 0.25274611154637483\n",
      "epoch 10858: train loss: 0.1216165349717154, test loss: 0.2527463882664102\n",
      "epoch 10859: train loss: 0.12161343656716443, test loss: 0.2527466652073892\n",
      "epoch 10860: train loss: 0.12161033856343881, test loss: 0.25274694236924417\n",
      "epoch 10861: train loss: 0.12160724096044694, test loss: 0.2527472197519311\n",
      "epoch 10862: train loss: 0.12160414375809729, test loss: 0.2527474973553916\n",
      "epoch 10863: train loss: 0.12160104695629836, test loss: 0.25274777517955277\n",
      "epoch 10864: train loss: 0.12159795055495866, test loss: 0.2527480532243661\n",
      "epoch 10865: train loss: 0.12159485455398679, test loss: 0.25274833148977416\n",
      "epoch 10866: train loss: 0.12159175895329126, test loss: 0.25274860997571796\n",
      "epoch 10867: train loss: 0.12158866375278073, test loss: 0.25274888868212864\n",
      "epoch 10868: train loss: 0.12158556895236382, test loss: 0.25274916760896005\n",
      "epoch 10869: train loss: 0.1215824745519492, test loss: 0.252749446756143\n",
      "epoch 10870: train loss: 0.12157938055144558, test loss: 0.2527497261236295\n",
      "epoch 10871: train loss: 0.12157628695076171, test loss: 0.2527500057113628\n",
      "epoch 10872: train loss: 0.12157319374980631, test loss: 0.25275028551927364\n",
      "epoch 10873: train loss: 0.12157010094848818, test loss: 0.2527505655473139\n",
      "epoch 10874: train loss: 0.12156700854671615, test loss: 0.252750845795406\n",
      "epoch 10875: train loss: 0.121563916544399, test loss: 0.2527511262635134\n",
      "epoch 10876: train loss: 0.12156082494144572, test loss: 0.25275140695157217\n",
      "epoch 10877: train loss: 0.12155773373776511, test loss: 0.2527516878595191\n",
      "epoch 10878: train loss: 0.12155464293326616, test loss: 0.25275196898729846\n",
      "epoch 10879: train loss: 0.12155155252785782, test loss: 0.25275225033484944\n",
      "epoch 10880: train loss: 0.12154846252144907, test loss: 0.25275253190211755\n",
      "epoch 10881: train loss: 0.12154537291394892, test loss: 0.25275281368904323\n",
      "epoch 10882: train loss: 0.12154228370526646, test loss: 0.25275309569556514\n",
      "epoch 10883: train loss: 0.12153919489531073, test loss: 0.2527533779216242\n",
      "epoch 10884: train loss: 0.12153610648399087, test loss: 0.25275366036717534\n",
      "epoch 10885: train loss: 0.12153301847121596, test loss: 0.25275394303214316\n",
      "epoch 10886: train loss: 0.12152993085689523, test loss: 0.2527542259164829\n",
      "epoch 10887: train loss: 0.12152684364093788, test loss: 0.25275450902012575\n",
      "epoch 10888: train loss: 0.1215237568232531, test loss: 0.2527547923430137\n",
      "epoch 10889: train loss: 0.12152067040375013, test loss: 0.25275507588510143\n",
      "epoch 10890: train loss: 0.12151758438233827, test loss: 0.2527553596463256\n",
      "epoch 10891: train loss: 0.12151449875892684, test loss: 0.25275564362661934\n",
      "epoch 10892: train loss: 0.12151141353342516, test loss: 0.25275592782593453\n",
      "epoch 10893: train loss: 0.12150832870574263, test loss: 0.2527562122442057\n",
      "epoch 10894: train loss: 0.12150524427578863, test loss: 0.25275649688137297\n",
      "epoch 10895: train loss: 0.12150216024347257, test loss: 0.2527567817373925\n",
      "epoch 10896: train loss: 0.12149907660870393, test loss: 0.25275706681219523\n",
      "epoch 10897: train loss: 0.1214959933713922, test loss: 0.2527573521057246\n",
      "epoch 10898: train loss: 0.12149291053144687, test loss: 0.25275763761791226\n",
      "epoch 10899: train loss: 0.12148982808877751, test loss: 0.2527579233487302\n",
      "epoch 10900: train loss: 0.12148674604329368, test loss: 0.2527582092980986\n",
      "epoch 10901: train loss: 0.12148366439490496, test loss: 0.2527584954659547\n",
      "epoch 10902: train loss: 0.12148058314352102, test loss: 0.2527587818522483\n",
      "epoch 10903: train loss: 0.12147750228905148, test loss: 0.2527590684569228\n",
      "epoch 10904: train loss: 0.12147442183140605, test loss: 0.2527593552799261\n",
      "epoch 10905: train loss: 0.12147134177049444, test loss: 0.25275964232119663\n",
      "epoch 10906: train loss: 0.12146826210622642, test loss: 0.2527599295806675\n",
      "epoch 10907: train loss: 0.12146518283851171, test loss: 0.2527602170582863\n",
      "epoch 10908: train loss: 0.12146210396726015, test loss: 0.2527605047540025\n",
      "epoch 10909: train loss: 0.1214590254923816, test loss: 0.25276079266773804\n",
      "epoch 10910: train loss: 0.12145594741378586, test loss: 0.2527610807994639\n",
      "epoch 10911: train loss: 0.12145286973138283, test loss: 0.2527613691491019\n",
      "epoch 10912: train loss: 0.1214497924450825, test loss: 0.2527616577165951\n",
      "epoch 10913: train loss: 0.1214467155547947, test loss: 0.2527619465018986\n",
      "epoch 10914: train loss: 0.12144363906042951, test loss: 0.25276223550494487\n",
      "epoch 10915: train loss: 0.12144056296189687, test loss: 0.2527625247256782\n",
      "epoch 10916: train loss: 0.12143748725910683, test loss: 0.2527628141640527\n",
      "epoch 10917: train loss: 0.12143441195196948, test loss: 0.2527631038199909\n",
      "epoch 10918: train loss: 0.12143133704039485, test loss: 0.2527633936934473\n",
      "epoch 10919: train loss: 0.12142826252429315, test loss: 0.25276368378434905\n",
      "epoch 10920: train loss: 0.12142518840357443, test loss: 0.252763974092663\n",
      "epoch 10921: train loss: 0.12142211467814892, test loss: 0.2527642646183101\n",
      "epoch 10922: train loss: 0.12141904134792682, test loss: 0.2527645553612463\n",
      "epoch 10923: train loss: 0.12141596841281839, test loss: 0.2527648463214157\n",
      "epoch 10924: train loss: 0.12141289587273384, test loss: 0.2527651374987443\n",
      "epoch 10925: train loss: 0.12140982372758348, test loss: 0.2527654288931999\n",
      "epoch 10926: train loss: 0.12140675197727763, test loss: 0.2527657205047049\n",
      "epoch 10927: train loss: 0.12140368062172666, test loss: 0.2527660123332021\n",
      "epoch 10928: train loss: 0.12140060966084092, test loss: 0.2527663043786475\n",
      "epoch 10929: train loss: 0.12139753909453081, test loss: 0.25276659664096923\n",
      "epoch 10930: train loss: 0.1213944689227068, test loss: 0.252766889120117\n",
      "epoch 10931: train loss: 0.12139139914527933, test loss: 0.2527671818160351\n",
      "epoch 10932: train loss: 0.12138832976215888, test loss: 0.25276747472867295\n",
      "epoch 10933: train loss: 0.121385260773256, test loss: 0.25276776785794786\n",
      "epoch 10934: train loss: 0.12138219217848122, test loss: 0.25276806120383244\n",
      "epoch 10935: train loss: 0.1213791239777451, test loss: 0.25276835476626486\n",
      "epoch 10936: train loss: 0.1213760561709583, test loss: 0.25276864854516257\n",
      "epoch 10937: train loss: 0.1213729887580314, test loss: 0.2527689425404971\n",
      "epoch 10938: train loss: 0.12136992173887508, test loss: 0.25276923675219215\n",
      "epoch 10939: train loss: 0.12136685511340004, test loss: 0.25276953118020257\n",
      "epoch 10940: train loss: 0.12136378888151697, test loss: 0.25276982582445656\n",
      "epoch 10941: train loss: 0.12136072304313665, test loss: 0.25277012068491583\n",
      "epoch 10942: train loss: 0.12135765759816985, test loss: 0.2527704157615202\n",
      "epoch 10943: train loss: 0.12135459254652738, test loss: 0.2527707110541962\n",
      "epoch 10944: train loss: 0.12135152788812004, test loss: 0.25277100656290713\n",
      "epoch 10945: train loss: 0.12134846362285873, test loss: 0.2527713022875855\n",
      "epoch 10946: train loss: 0.12134539975065431, test loss: 0.25277159822817663\n",
      "epoch 10947: train loss: 0.12134233627141772, test loss: 0.25277189438462666\n",
      "epoch 10948: train loss: 0.1213392731850599, test loss: 0.25277219075685714\n",
      "epoch 10949: train loss: 0.1213362104914918, test loss: 0.2527724873448461\n",
      "epoch 10950: train loss: 0.12133314819062449, test loss: 0.25277278414850674\n",
      "epoch 10951: train loss: 0.12133008628236894, test loss: 0.25277308116779973\n",
      "epoch 10952: train loss: 0.1213270247666362, test loss: 0.25277337840266506\n",
      "epoch 10953: train loss: 0.12132396364333742, test loss: 0.2527736758530372\n",
      "epoch 10954: train loss: 0.12132090291238366, test loss: 0.25277397351887193\n",
      "epoch 10955: train loss: 0.12131784257368611, test loss: 0.2527742714001042\n",
      "epoch 10956: train loss: 0.1213147826271559, test loss: 0.25277456949667837\n",
      "epoch 10957: train loss: 0.12131172307270427, test loss: 0.25277486780854014\n",
      "epoch 10958: train loss: 0.12130866391024242, test loss: 0.2527751663356304\n",
      "epoch 10959: train loss: 0.1213056051396816, test loss: 0.25277546507788784\n",
      "epoch 10960: train loss: 0.12130254676093313, test loss: 0.2527757640352692\n",
      "epoch 10961: train loss: 0.12129948877390831, test loss: 0.2527760632077033\n",
      "epoch 10962: train loss: 0.12129643117851849, test loss: 0.2527763625951419\n",
      "epoch 10963: train loss: 0.12129337397467502, test loss: 0.25277666219752976\n",
      "epoch 10964: train loss: 0.12129031716228933, test loss: 0.2527769620148074\n",
      "epoch 10965: train loss: 0.12128726074127279, test loss: 0.2527772620469214\n",
      "epoch 10966: train loss: 0.12128420471153692, test loss: 0.2527775622937993\n",
      "epoch 10967: train loss: 0.12128114907299317, test loss: 0.25277786275540365\n",
      "epoch 10968: train loss: 0.12127809382555306, test loss: 0.25277816343166465\n",
      "epoch 10969: train loss: 0.12127503896912813, test loss: 0.25277846432253304\n",
      "epoch 10970: train loss: 0.12127198450362991, test loss: 0.2527787654279545\n",
      "epoch 10971: train loss: 0.12126893042897006, test loss: 0.25277906674786926\n",
      "epoch 10972: train loss: 0.12126587674506016, test loss: 0.25277936828221803\n",
      "epoch 10973: train loss: 0.12126282345181189, test loss: 0.25277967003094665\n",
      "epoch 10974: train loss: 0.1212597705491369, test loss: 0.2527799719940073\n",
      "epoch 10975: train loss: 0.12125671803694693, test loss: 0.25278027417132337\n",
      "epoch 10976: train loss: 0.1212536659151537, test loss: 0.25278057656286285\n",
      "epoch 10977: train loss: 0.12125061418366893, test loss: 0.2527808791685497\n",
      "epoch 10978: train loss: 0.1212475628424045, test loss: 0.25278118198833033\n",
      "epoch 10979: train loss: 0.12124451189127214, test loss: 0.2527814850221562\n",
      "epoch 10980: train loss: 0.1212414613301838, test loss: 0.25278178826996334\n",
      "epoch 10981: train loss: 0.12123841115905126, test loss: 0.252782091731702\n",
      "epoch 10982: train loss: 0.12123536137778651, test loss: 0.25278239540732517\n",
      "epoch 10983: train loss: 0.12123231198630137, test loss: 0.25278269929675096\n",
      "epoch 10984: train loss: 0.1212292629845079, test loss: 0.25278300339994264\n",
      "epoch 10985: train loss: 0.12122621437231805, test loss: 0.25278330771683066\n",
      "epoch 10986: train loss: 0.12122316614964383, test loss: 0.252783612247366\n",
      "epoch 10987: train loss: 0.12122011831639729, test loss: 0.25278391699150654\n",
      "epoch 10988: train loss: 0.12121707087249049, test loss: 0.2527842219491706\n",
      "epoch 10989: train loss: 0.12121402381783558, test loss: 0.252784527120316\n",
      "epoch 10990: train loss: 0.12121097715234462, test loss: 0.2527848325048849\n",
      "epoch 10991: train loss: 0.12120793087592982, test loss: 0.25278513810282294\n",
      "epoch 10992: train loss: 0.12120488498850333, test loss: 0.25278544391407154\n",
      "epoch 10993: train loss: 0.12120183948997734, test loss: 0.25278574993857644\n",
      "epoch 10994: train loss: 0.12119879438026417, test loss: 0.2527860561762693\n",
      "epoch 10995: train loss: 0.12119574965927597, test loss: 0.2527863626271184\n",
      "epoch 10996: train loss: 0.12119270532692514, test loss: 0.2527866692910423\n",
      "epoch 10997: train loss: 0.12118966138312398, test loss: 0.25278697616800017\n",
      "epoch 10998: train loss: 0.1211866178277848, test loss: 0.2527872832579428\n",
      "epoch 10999: train loss: 0.12118357466082, test loss: 0.2527875905607963\n",
      "epoch 11000: train loss: 0.12118053188214198, test loss: 0.2527878980765077\n",
      "epoch 11001: train loss: 0.12117748949166318, test loss: 0.2527882058050282\n",
      "epoch 11002: train loss: 0.12117444748929605, test loss: 0.2527885137463008\n",
      "epoch 11003: train loss: 0.12117140587495308, test loss: 0.25278882190026564\n",
      "epoch 11004: train loss: 0.12116836464854681, test loss: 0.25278913026686534\n",
      "epoch 11005: train loss: 0.12116532380998979, test loss: 0.2527894388460584\n",
      "epoch 11006: train loss: 0.12116228335919456, test loss: 0.2527897476377795\n",
      "epoch 11007: train loss: 0.12115924329607371, test loss: 0.2527900566419545\n",
      "epoch 11008: train loss: 0.12115620362053989, test loss: 0.25279036585856995\n",
      "epoch 11009: train loss: 0.12115316433250577, test loss: 0.2527906752875223\n",
      "epoch 11010: train loss: 0.121150125431884, test loss: 0.2527909849287876\n",
      "epoch 11011: train loss: 0.1211470869185873, test loss: 0.2527912947823065\n",
      "epoch 11012: train loss: 0.12114404879252842, test loss: 0.2527916048480051\n",
      "epoch 11013: train loss: 0.12114101105362014, test loss: 0.252791915125847\n",
      "epoch 11014: train loss: 0.1211379737017752, test loss: 0.2527922256157739\n",
      "epoch 11015: train loss: 0.12113493673690647, test loss: 0.2527925363177288\n",
      "epoch 11016: train loss: 0.12113190015892678, test loss: 0.2527928472316374\n",
      "epoch 11017: train loss: 0.12112886396774898, test loss: 0.25279315835747246\n",
      "epoch 11018: train loss: 0.12112582816328604, test loss: 0.25279346969516786\n",
      "epoch 11019: train loss: 0.12112279274545085, test loss: 0.2527937812446633\n",
      "epoch 11020: train loss: 0.12111975771415633, test loss: 0.25279409300589745\n",
      "epoch 11021: train loss: 0.12111672306931555, test loss: 0.2527944049788335\n",
      "epoch 11022: train loss: 0.12111368881084143, test loss: 0.2527947171634026\n",
      "epoch 11023: train loss: 0.12111065493864709, test loss: 0.25279502955955263\n",
      "epoch 11024: train loss: 0.12110762145264559, test loss: 0.2527953421672149\n",
      "epoch 11025: train loss: 0.12110458835274997, test loss: 0.25279565498636003\n",
      "epoch 11026: train loss: 0.1211015556388734, test loss: 0.2527959680169189\n",
      "epoch 11027: train loss: 0.12109852331092902, test loss: 0.25279628125883397\n",
      "epoch 11028: train loss: 0.12109549136883001, test loss: 0.25279659471205745\n",
      "epoch 11029: train loss: 0.12109245981248958, test loss: 0.2527969083765175\n",
      "epoch 11030: train loss: 0.12108942864182094, test loss: 0.25279722225218215\n",
      "epoch 11031: train loss: 0.1210863978567374, test loss: 0.25279753633897906\n",
      "epoch 11032: train loss: 0.1210833674571522, test loss: 0.2527978506368554\n",
      "epoch 11033: train loss: 0.12108033744297866, test loss: 0.2527981651457592\n",
      "epoch 11034: train loss: 0.12107730781413016, test loss: 0.2527984798656346\n",
      "epoch 11035: train loss: 0.12107427857052003, test loss: 0.2527987947964168\n",
      "epoch 11036: train loss: 0.12107124971206168, test loss: 0.2527991099380716\n",
      "epoch 11037: train loss: 0.12106822123866856, test loss: 0.2527994252905199\n",
      "epoch 11038: train loss: 0.12106519315025409, test loss: 0.2527997408537319\n",
      "epoch 11039: train loss: 0.12106216544673176, test loss: 0.25280005662762234\n",
      "epoch 11040: train loss: 0.12105913812801507, test loss: 0.25280037261216737\n",
      "epoch 11041: train loss: 0.12105611119401757, test loss: 0.25280068880728895\n",
      "epoch 11042: train loss: 0.12105308464465284, test loss: 0.2528010052129451\n",
      "epoch 11043: train loss: 0.1210500584798344, test loss: 0.25280132182906284\n",
      "epoch 11044: train loss: 0.12104703269947592, test loss: 0.2528016386556191\n",
      "epoch 11045: train loss: 0.12104400730349106, test loss: 0.2528019556925168\n",
      "epoch 11046: train loss: 0.12104098229179343, test loss: 0.2528022729397386\n",
      "epoch 11047: train loss: 0.1210379576642968, test loss: 0.2528025903972056\n",
      "epoch 11048: train loss: 0.12103493342091484, test loss: 0.252802908064872\n",
      "epoch 11049: train loss: 0.12103190956156132, test loss: 0.2528032259426862\n",
      "epoch 11050: train loss: 0.12102888608615002, test loss: 0.2528035440305854\n",
      "epoch 11051: train loss: 0.12102586299459475, test loss: 0.25280386232851465\n",
      "epoch 11052: train loss: 0.12102284028680935, test loss: 0.2528041808364317\n",
      "epoch 11053: train loss: 0.12101981796270765, test loss: 0.2528044995542646\n",
      "epoch 11054: train loss: 0.12101679602220358, test loss: 0.25280481848196173\n",
      "epoch 11055: train loss: 0.12101377446521103, test loss: 0.25280513761947865\n",
      "epoch 11056: train loss: 0.12101075329164396, test loss: 0.2528054569667578\n",
      "epoch 11057: train loss: 0.1210077325014163, test loss: 0.2528057765237309\n",
      "epoch 11058: train loss: 0.12100471209444211, test loss: 0.25280609629036\n",
      "epoch 11059: train loss: 0.12100169207063537, test loss: 0.25280641626657613\n",
      "epoch 11060: train loss: 0.12099867242991014, test loss: 0.25280673645233465\n",
      "epoch 11061: train loss: 0.12099565317218049, test loss: 0.2528070568475728\n",
      "epoch 11062: train loss: 0.12099263429736055, test loss: 0.25280737745224635\n",
      "epoch 11063: train loss: 0.12098961580536445, test loss: 0.2528076982662985\n",
      "epoch 11064: train loss: 0.12098659769610631, test loss: 0.252808019289662\n",
      "epoch 11065: train loss: 0.12098357996950038, test loss: 0.25280834052229745\n",
      "epoch 11066: train loss: 0.12098056262546081, test loss: 0.252808661964137\n",
      "epoch 11067: train loss: 0.12097754566390188, test loss: 0.2528089836151303\n",
      "epoch 11068: train loss: 0.12097452908473785, test loss: 0.2528093054752324\n",
      "epoch 11069: train loss: 0.120971512887883, test loss: 0.25280962754437636\n",
      "epoch 11070: train loss: 0.12096849707325169, test loss: 0.2528099498225112\n",
      "epoch 11071: train loss: 0.12096548164075821, test loss: 0.2528102723095816\n",
      "epoch 11072: train loss: 0.120962466590317, test loss: 0.2528105950055416\n",
      "epoch 11073: train loss: 0.1209594519218424, test loss: 0.25281091791031945\n",
      "epoch 11074: train loss: 0.12095643763524891, test loss: 0.25281124102388075\n",
      "epoch 11075: train loss: 0.1209534237304509, test loss: 0.2528115643461485\n",
      "epoch 11076: train loss: 0.12095041020736293, test loss: 0.25281188787708864\n",
      "epoch 11077: train loss: 0.12094739706589948, test loss: 0.2528122116166336\n",
      "epoch 11078: train loss: 0.12094438430597508, test loss: 0.25281253556474453\n",
      "epoch 11079: train loss: 0.12094137192750433, test loss: 0.25281285972133793\n",
      "epoch 11080: train loss: 0.12093835993040176, test loss: 0.2528131840863874\n",
      "epoch 11081: train loss: 0.12093534831458204, test loss: 0.2528135086598348\n",
      "epoch 11082: train loss: 0.12093233707995978, test loss: 0.25281383344160974\n",
      "epoch 11083: train loss: 0.12092932622644968, test loss: 0.25281415843166605\n",
      "epoch 11084: train loss: 0.12092631575396644, test loss: 0.2528144836299556\n",
      "epoch 11085: train loss: 0.12092330566242473, test loss: 0.2528148090364214\n",
      "epoch 11086: train loss: 0.12092029595173939, test loss: 0.2528151346510025\n",
      "epoch 11087: train loss: 0.12091728662182512, test loss: 0.2528154604736445\n",
      "epoch 11088: train loss: 0.12091427767259677, test loss: 0.25281578650429687\n",
      "epoch 11089: train loss: 0.12091126910396918, test loss: 0.2528161127429157\n",
      "epoch 11090: train loss: 0.12090826091585717, test loss: 0.2528164391894226\n",
      "epoch 11091: train loss: 0.12090525310817565, test loss: 0.25281676584379076\n",
      "epoch 11092: train loss: 0.12090224568083953, test loss: 0.25281709270594876\n",
      "epoch 11093: train loss: 0.12089923863376377, test loss: 0.2528174197758455\n",
      "epoch 11094: train loss: 0.12089623196686329, test loss: 0.2528177470534203\n",
      "epoch 11095: train loss: 0.12089322568005312, test loss: 0.2528180745386367\n",
      "epoch 11096: train loss: 0.12089021977324826, test loss: 0.25281840223141994\n",
      "epoch 11097: train loss: 0.12088721424636378, test loss: 0.25281873013173445\n",
      "epoch 11098: train loss: 0.12088420909931476, test loss: 0.2528190582395181\n",
      "epoch 11099: train loss: 0.12088120433201624, test loss: 0.2528193865547044\n",
      "epoch 11100: train loss: 0.12087819994438337, test loss: 0.25281971507726775\n",
      "epoch 11101: train loss: 0.12087519593633138, test loss: 0.25282004380712325\n",
      "epoch 11102: train loss: 0.12087219230777534, test loss: 0.25282037274423386\n",
      "epoch 11103: train loss: 0.12086918905863055, test loss: 0.252820701888551\n",
      "epoch 11104: train loss: 0.12086618618881215, test loss: 0.2528210312399952\n",
      "epoch 11105: train loss: 0.12086318369823548, test loss: 0.25282136079854217\n",
      "epoch 11106: train loss: 0.12086018158681577, test loss: 0.2528216905641191\n",
      "epoch 11107: train loss: 0.12085717985446838, test loss: 0.2528220205366887\n",
      "epoch 11108: train loss: 0.12085417850110862, test loss: 0.2528223507161779\n",
      "epoch 11109: train loss: 0.12085117752665184, test loss: 0.25282268110253303\n",
      "epoch 11110: train loss: 0.12084817693101348, test loss: 0.2528230116957173\n",
      "epoch 11111: train loss: 0.12084517671410894, test loss: 0.2528233424956686\n",
      "epoch 11112: train loss: 0.12084217687585364, test loss: 0.2528236735023221\n",
      "epoch 11113: train loss: 0.12083917741616307, test loss: 0.25282400471565153\n",
      "epoch 11114: train loss: 0.12083617833495275, test loss: 0.25282433613557276\n",
      "epoch 11115: train loss: 0.12083317963213819, test loss: 0.2528246677620448\n",
      "epoch 11116: train loss: 0.12083018130763494, test loss: 0.25282499959501775\n",
      "epoch 11117: train loss: 0.12082718336135857, test loss: 0.2528253316344318\n",
      "epoch 11118: train loss: 0.12082418579322468, test loss: 0.25282566388023797\n",
      "epoch 11119: train loss: 0.12082118860314893, test loss: 0.25282599633237074\n",
      "epoch 11120: train loss: 0.12081819179104698, test loss: 0.2528263289907921\n",
      "epoch 11121: train loss: 0.12081519535683448, test loss: 0.2528266618554436\n",
      "epoch 11122: train loss: 0.12081219930042714, test loss: 0.2528269949262703\n",
      "epoch 11123: train loss: 0.12080920362174073, test loss: 0.252827328203206\n",
      "epoch 11124: train loss: 0.12080620832069097, test loss: 0.25282766168621373\n",
      "epoch 11125: train loss: 0.12080321339719373, test loss: 0.25282799537523465\n",
      "epoch 11126: train loss: 0.12080021885116472, test loss: 0.2528283292702257\n",
      "epoch 11127: train loss: 0.12079722468251987, test loss: 0.2528286633711145\n",
      "epoch 11128: train loss: 0.120794230891175, test loss: 0.25282899767785516\n",
      "epoch 11129: train loss: 0.12079123747704601, test loss: 0.2528293321903906\n",
      "epoch 11130: train loss: 0.12078824444004885, test loss: 0.25282966690867037\n",
      "epoch 11131: train loss: 0.12078525178009945, test loss: 0.25283000183265436\n",
      "epoch 11132: train loss: 0.12078225949711378, test loss: 0.25283033696226176\n",
      "epoch 11133: train loss: 0.12077926759100784, test loss: 0.2528306722974619\n",
      "epoch 11134: train loss: 0.12077627606169766, test loss: 0.2528310078381891\n",
      "epoch 11135: train loss: 0.12077328490909933, test loss: 0.25283134358439535\n",
      "epoch 11136: train loss: 0.12077029413312886, test loss: 0.2528316795360326\n",
      "epoch 11137: train loss: 0.12076730373370241, test loss: 0.25283201569302355\n",
      "epoch 11138: train loss: 0.12076431371073607, test loss: 0.2528323520553495\n",
      "epoch 11139: train loss: 0.12076132406414604, test loss: 0.2528326886229293\n",
      "epoch 11140: train loss: 0.12075833479384848, test loss: 0.25283302539571856\n",
      "epoch 11141: train loss: 0.12075534589975961, test loss: 0.25283336237366577\n",
      "epoch 11142: train loss: 0.12075235738179568, test loss: 0.2528336995567216\n",
      "epoch 11143: train loss: 0.12074936923987291, test loss: 0.25283403694482703\n",
      "epoch 11144: train loss: 0.12074638147390765, test loss: 0.2528343745379187\n",
      "epoch 11145: train loss: 0.12074339408381617, test loss: 0.2528347123359643\n",
      "epoch 11146: train loss: 0.12074040706951482, test loss: 0.2528350503389053\n",
      "epoch 11147: train loss: 0.12073742043091996, test loss: 0.2528353885466659\n",
      "epoch 11148: train loss: 0.12073443416794802, test loss: 0.252835726959226\n",
      "epoch 11149: train loss: 0.1207314482805154, test loss: 0.25283606557649935\n",
      "epoch 11150: train loss: 0.12072846276853856, test loss: 0.2528364043984717\n",
      "epoch 11151: train loss: 0.12072547763193392, test loss: 0.252836743425051\n",
      "epoch 11152: train loss: 0.12072249287061805, test loss: 0.25283708265621146\n",
      "epoch 11153: train loss: 0.1207195084845074, test loss: 0.2528374220918769\n",
      "epoch 11154: train loss: 0.12071652447351859, test loss: 0.25283776173202394\n",
      "epoch 11155: train loss: 0.12071354083756816, test loss: 0.25283810157656966\n",
      "epoch 11156: train loss: 0.12071055757657273, test loss: 0.2528384416254854\n",
      "epoch 11157: train loss: 0.12070757469044892, test loss: 0.2528387818786945\n",
      "epoch 11158: train loss: 0.12070459217911339, test loss: 0.2528391223361545\n",
      "epoch 11159: train loss: 0.12070161004248282, test loss: 0.25283946299782534\n",
      "epoch 11160: train loss: 0.12069862828047392, test loss: 0.25283980386363053\n",
      "epoch 11161: train loss: 0.12069564689300342, test loss: 0.2528401449335349\n",
      "epoch 11162: train loss: 0.12069266587998807, test loss: 0.2528404862074845\n",
      "epoch 11163: train loss: 0.12068968524134467, test loss: 0.2528408276854165\n",
      "epoch 11164: train loss: 0.12068670497699002, test loss: 0.25284116936728274\n",
      "epoch 11165: train loss: 0.120683725086841, test loss: 0.2528415112530253\n",
      "epoch 11166: train loss: 0.12068074557081442, test loss: 0.2528418533425916\n",
      "epoch 11167: train loss: 0.12067776642882716, test loss: 0.25284219563594507\n",
      "epoch 11168: train loss: 0.12067478766079619, test loss: 0.25284253813301194\n",
      "epoch 11169: train loss: 0.12067180926663841, test loss: 0.25284288083375067\n",
      "epoch 11170: train loss: 0.12066883124627081, test loss: 0.25284322373810736\n",
      "epoch 11171: train loss: 0.12066585359961036, test loss: 0.25284356684603\n",
      "epoch 11172: train loss: 0.1206628763265741, test loss: 0.25284391015746166\n",
      "epoch 11173: train loss: 0.12065989942707908, test loss: 0.25284425367235985\n",
      "epoch 11174: train loss: 0.12065692290104234, test loss: 0.252844597390644\n",
      "epoch 11175: train loss: 0.12065394674838104, test loss: 0.25284494131229673\n",
      "epoch 11176: train loss: 0.12065097096901221, test loss: 0.25284528543724777\n",
      "epoch 11177: train loss: 0.12064799556285308, test loss: 0.2528456297654329\n",
      "epoch 11178: train loss: 0.12064502052982075, test loss: 0.25284597429681865\n",
      "epoch 11179: train loss: 0.1206420458698325, test loss: 0.25284631903135674\n",
      "epoch 11180: train loss: 0.12063907158280551, test loss: 0.2528466639689731\n",
      "epoch 11181: train loss: 0.12063609766865702, test loss: 0.252847009109621\n",
      "epoch 11182: train loss: 0.12063312412730437, test loss: 0.25284735445326595\n",
      "epoch 11183: train loss: 0.12063015095866478, test loss: 0.25284769999983225\n",
      "epoch 11184: train loss: 0.12062717816265564, test loss: 0.2528480457492786\n",
      "epoch 11185: train loss: 0.12062420573919425, test loss: 0.25284839170155465\n",
      "epoch 11186: train loss: 0.12062123368819808, test loss: 0.252848737856591\n",
      "epoch 11187: train loss: 0.12061826200958446, test loss: 0.25284908421435703\n",
      "epoch 11188: train loss: 0.12061529070327083, test loss: 0.25284943077479677\n",
      "epoch 11189: train loss: 0.12061231976917469, test loss: 0.25284977753784577\n",
      "epoch 11190: train loss: 0.12060934920721347, test loss: 0.25285012450345806\n",
      "epoch 11191: train loss: 0.12060637901730473, test loss: 0.25285047167157126\n",
      "epoch 11192: train loss: 0.12060340919936596, test loss: 0.2528508190421557\n",
      "epoch 11193: train loss: 0.12060043975331478, test loss: 0.25285116661513607\n",
      "epoch 11194: train loss: 0.12059747067906869, test loss: 0.25285151439047376\n",
      "epoch 11195: train loss: 0.12059450197654538, test loss: 0.2528518623681157\n",
      "epoch 11196: train loss: 0.12059153364566245, test loss: 0.2528522105479995\n",
      "epoch 11197: train loss: 0.12058856568633758, test loss: 0.25285255893007874\n",
      "epoch 11198: train loss: 0.12058559809848844, test loss: 0.25285290751431316\n",
      "epoch 11199: train loss: 0.12058263088203276, test loss: 0.252853256300618\n",
      "epoch 11200: train loss: 0.12057966403688829, test loss: 0.2528536052889752\n",
      "epoch 11201: train loss: 0.12057669756297276, test loss: 0.252853954479322\n",
      "epoch 11202: train loss: 0.12057373146020402, test loss: 0.25285430387158986\n",
      "epoch 11203: train loss: 0.12057076572849981, test loss: 0.25285465346575\n",
      "epoch 11204: train loss: 0.12056780036777806, test loss: 0.25285500326172805\n",
      "epoch 11205: train loss: 0.12056483537795656, test loss: 0.2528553532594959\n",
      "epoch 11206: train loss: 0.12056187075895325, test loss: 0.2528557034589796\n",
      "epoch 11207: train loss: 0.12055890651068606, test loss: 0.25285605386014454\n",
      "epoch 11208: train loss: 0.12055594263307289, test loss: 0.25285640446292795\n",
      "epoch 11209: train loss: 0.12055297912603173, test loss: 0.25285675526726686\n",
      "epoch 11210: train loss: 0.12055001598948058, test loss: 0.2528571062731336\n",
      "epoch 11211: train loss: 0.12054705322333746, test loss: 0.25285745748046035\n",
      "epoch 11212: train loss: 0.12054409082752045, test loss: 0.25285780888920045\n",
      "epoch 11213: train loss: 0.12054112880194756, test loss: 0.2528581604992985\n",
      "epoch 11214: train loss: 0.12053816714653692, test loss: 0.25285851231071416\n",
      "epoch 11215: train loss: 0.12053520586120664, test loss: 0.2528588643233674\n",
      "epoch 11216: train loss: 0.12053224494587492, test loss: 0.2528592165372357\n",
      "epoch 11217: train loss: 0.12052928440045989, test loss: 0.25285956895224687\n",
      "epoch 11218: train loss: 0.12052632422487974, test loss: 0.25285992156837134\n",
      "epoch 11219: train loss: 0.12052336441905272, test loss: 0.25286027438552966\n",
      "epoch 11220: train loss: 0.12052040498289707, test loss: 0.2528606274036829\n",
      "epoch 11221: train loss: 0.12051744591633107, test loss: 0.2528609806227929\n",
      "epoch 11222: train loss: 0.120514487219273, test loss: 0.2528613340427779\n",
      "epoch 11223: train loss: 0.12051152889164125, test loss: 0.25286168766361156\n",
      "epoch 11224: train loss: 0.12050857093335411, test loss: 0.2528620414852306\n",
      "epoch 11225: train loss: 0.12050561334432999, test loss: 0.25286239550759226\n",
      "epoch 11226: train loss: 0.12050265612448725, test loss: 0.25286274973062617\n",
      "epoch 11227: train loss: 0.12049969927374438, test loss: 0.2528631041542945\n",
      "epoch 11228: train loss: 0.1204967427920198, test loss: 0.252863458778547\n",
      "epoch 11229: train loss: 0.120493786679232, test loss: 0.25286381360331533\n",
      "epoch 11230: train loss: 0.12049083093529948, test loss: 0.25286416862857214\n",
      "epoch 11231: train loss: 0.12048787556014075, test loss: 0.252864523854249\n",
      "epoch 11232: train loss: 0.1204849205536744, test loss: 0.25286487928030255\n",
      "epoch 11233: train loss: 0.12048196591581899, test loss: 0.2528652349066754\n",
      "epoch 11234: train loss: 0.1204790116464931, test loss: 0.2528655907333116\n",
      "epoch 11235: train loss: 0.12047605774561543, test loss: 0.25286594676016827\n",
      "epoch 11236: train loss: 0.1204731042131046, test loss: 0.252866302987187\n",
      "epoch 11237: train loss: 0.12047015104887926, test loss: 0.2528666594143306\n",
      "epoch 11238: train loss: 0.12046719825285816, test loss: 0.25286701604152534\n",
      "epoch 11239: train loss: 0.12046424582496001, test loss: 0.25286737286873284\n",
      "epoch 11240: train loss: 0.12046129376510357, test loss: 0.25286772989590217\n",
      "epoch 11241: train loss: 0.12045834207320764, test loss: 0.2528680871229727\n",
      "epoch 11242: train loss: 0.12045539074919102, test loss: 0.2528684445498989\n",
      "epoch 11243: train loss: 0.1204524397929725, test loss: 0.2528688021766315\n",
      "epoch 11244: train loss: 0.12044948920447103, test loss: 0.25286916000311493\n",
      "epoch 11245: train loss: 0.1204465389836054, test loss: 0.25286951802930496\n",
      "epoch 11246: train loss: 0.12044358913029458, test loss: 0.25286987625513935\n",
      "epoch 11247: train loss: 0.12044063964445746, test loss: 0.25287023468057407\n",
      "epoch 11248: train loss: 0.12043769052601302, test loss: 0.25287059330555467\n",
      "epoch 11249: train loss: 0.1204347417748803, test loss: 0.25287095213002403\n",
      "epoch 11250: train loss: 0.12043179339097819, test loss: 0.2528713111539448\n",
      "epoch 11251: train loss: 0.1204288453742258, test loss: 0.2528716703772547\n",
      "epoch 11252: train loss: 0.12042589772454218, test loss: 0.2528720297998935\n",
      "epoch 11253: train loss: 0.12042295044184641, test loss: 0.2528723894218292\n",
      "epoch 11254: train loss: 0.12042000352605758, test loss: 0.25287274924299824\n",
      "epoch 11255: train loss: 0.12041705697709486, test loss: 0.2528731092633658\n",
      "epoch 11256: train loss: 0.12041411079487739, test loss: 0.25287346948286177\n",
      "epoch 11257: train loss: 0.12041116497932436, test loss: 0.2528738299014332\n",
      "epoch 11258: train loss: 0.12040821953035495, test loss: 0.25287419051904597\n",
      "epoch 11259: train loss: 0.12040527444788847, test loss: 0.2528745513356345\n",
      "epoch 11260: train loss: 0.12040232973184412, test loss: 0.25287491235114945\n",
      "epoch 11261: train loss: 0.12039938538214116, test loss: 0.2528752735655463\n",
      "epoch 11262: train loss: 0.12039644139869896, test loss: 0.25287563497877075\n",
      "epoch 11263: train loss: 0.1203934977814368, test loss: 0.2528759965907668\n",
      "epoch 11264: train loss: 0.1203905545302741, test loss: 0.25287635840147943\n",
      "epoch 11265: train loss: 0.1203876116451302, test loss: 0.25287672041087766\n",
      "epoch 11266: train loss: 0.12038466912592455, test loss: 0.25287708261888775\n",
      "epoch 11267: train loss: 0.12038172697257654, test loss: 0.25287744502546716\n",
      "epoch 11268: train loss: 0.12037878518500562, test loss: 0.25287780763058343\n",
      "epoch 11269: train loss: 0.12037584376313135, test loss: 0.2528781704341474\n",
      "epoch 11270: train loss: 0.12037290270687315, test loss: 0.252878533436138\n",
      "epoch 11271: train loss: 0.1203699620161506, test loss: 0.2528788966364899\n",
      "epoch 11272: train loss: 0.12036702169088326, test loss: 0.25287926003515904\n",
      "epoch 11273: train loss: 0.12036408173099072, test loss: 0.2528796236320857\n",
      "epoch 11274: train loss: 0.12036114213639255, test loss: 0.2528799874272375\n",
      "epoch 11275: train loss: 0.12035820290700844, test loss: 0.2528803514205361\n",
      "epoch 11276: train loss: 0.120355264042758, test loss: 0.2528807156119512\n",
      "epoch 11277: train loss: 0.12035232554356094, test loss: 0.2528810800014265\n",
      "epoch 11278: train loss: 0.12034938740933697, test loss: 0.2528814445889028\n",
      "epoch 11279: train loss: 0.1203464496400058, test loss: 0.25288180937433724\n",
      "epoch 11280: train loss: 0.1203435122354872, test loss: 0.2528821743576797\n",
      "epoch 11281: train loss: 0.12034057519570096, test loss: 0.2528825395388893\n",
      "epoch 11282: train loss: 0.12033763852056686, test loss: 0.2528829049178858\n",
      "epoch 11283: train loss: 0.12033470221000479, test loss: 0.2528832704946506\n",
      "epoch 11284: train loss: 0.12033176626393455, test loss: 0.25288363626910665\n",
      "epoch 11285: train loss: 0.12032883068227605, test loss: 0.25288400224122165\n",
      "epoch 11286: train loss: 0.1203258954649492, test loss: 0.25288436841093004\n",
      "epoch 11287: train loss: 0.12032296061187392, test loss: 0.25288473477818985\n",
      "epoch 11288: train loss: 0.12032002612297016, test loss: 0.2528851013429452\n",
      "epoch 11289: train loss: 0.1203170919981579, test loss: 0.2528854681051554\n",
      "epoch 11290: train loss: 0.12031415823735717, test loss: 0.2528858350647634\n",
      "epoch 11291: train loss: 0.12031122484048798, test loss: 0.25288620222170044\n",
      "epoch 11292: train loss: 0.12030829180747038, test loss: 0.2528865695759571\n",
      "epoch 11293: train loss: 0.12030535913822447, test loss: 0.2528869371274391\n",
      "epoch 11294: train loss: 0.12030242683267035, test loss: 0.25288730487612127\n",
      "epoch 11295: train loss: 0.12029949489072812, test loss: 0.2528876728219487\n",
      "epoch 11296: train loss: 0.12029656331231799, test loss: 0.2528880409648686\n",
      "epoch 11297: train loss: 0.12029363209736008, test loss: 0.2528884093048379\n",
      "epoch 11298: train loss: 0.12029070124577464, test loss: 0.25288877784179126\n",
      "epoch 11299: train loss: 0.12028777075748188, test loss: 0.25288914657568545\n",
      "epoch 11300: train loss: 0.12028484063240202, test loss: 0.25288951550646754\n",
      "epoch 11301: train loss: 0.12028191087045538, test loss: 0.25288988463408896\n",
      "epoch 11302: train loss: 0.12027898147156225, test loss: 0.25289025395849624\n",
      "epoch 11303: train loss: 0.12027605243564295, test loss: 0.25289062347964686\n",
      "epoch 11304: train loss: 0.12027312376261783, test loss: 0.25289099319747443\n",
      "epoch 11305: train loss: 0.12027019545240726, test loss: 0.2528913631119444\n",
      "epoch 11306: train loss: 0.12026726750493168, test loss: 0.25289173322300185\n",
      "epoch 11307: train loss: 0.1202643399201115, test loss: 0.2528921035305942\n",
      "epoch 11308: train loss: 0.12026141269786711, test loss: 0.25289247403467785\n",
      "epoch 11309: train loss: 0.12025848583811906, test loss: 0.25289284473518836\n",
      "epoch 11310: train loss: 0.12025555934078777, test loss: 0.2528932156320899\n",
      "epoch 11311: train loss: 0.12025263320579384, test loss: 0.2528935867253165\n",
      "epoch 11312: train loss: 0.1202497074330578, test loss: 0.25289395801483794\n",
      "epoch 11313: train loss: 0.12024678202250018, test loss: 0.25289432950057145\n",
      "epoch 11314: train loss: 0.12024385697404162, test loss: 0.25289470118251167\n",
      "epoch 11315: train loss: 0.12024093228760271, test loss: 0.25289507306056835\n",
      "epoch 11316: train loss: 0.12023800796310415, test loss: 0.25289544513470635\n",
      "epoch 11317: train loss: 0.12023508400046651, test loss: 0.25289581740488454\n",
      "epoch 11318: train loss: 0.12023216039961056, test loss: 0.25289618987103557\n",
      "epoch 11319: train loss: 0.12022923716045705, test loss: 0.25289656253312054\n",
      "epoch 11320: train loss: 0.12022631428292663, test loss: 0.25289693539107977\n",
      "epoch 11321: train loss: 0.12022339176694011, test loss: 0.2528973084448818\n",
      "epoch 11322: train loss: 0.12022046961241831, test loss: 0.2528976816944511\n",
      "epoch 11323: train loss: 0.12021754781928201, test loss: 0.25289805513975216\n",
      "epoch 11324: train loss: 0.12021462638745203, test loss: 0.2528984287807251\n",
      "epoch 11325: train loss: 0.1202117053168493, test loss: 0.25289880261734105\n",
      "epoch 11326: train loss: 0.12020878460739468, test loss: 0.25289917664953515\n",
      "epoch 11327: train loss: 0.12020586425900906, test loss: 0.2528995508772484\n",
      "epoch 11328: train loss: 0.1202029442716134, test loss: 0.2528999253004388\n",
      "epoch 11329: train loss: 0.12020002464512868, test loss: 0.2529002999190654\n",
      "epoch 11330: train loss: 0.12019710537947585, test loss: 0.2529006747330584\n",
      "epoch 11331: train loss: 0.12019418647457591, test loss: 0.2529010497423993\n",
      "epoch 11332: train loss: 0.12019126793034995, test loss: 0.25290142494699963\n",
      "epoch 11333: train loss: 0.12018834974671902, test loss: 0.252901800346831\n",
      "epoch 11334: train loss: 0.12018543192360416, test loss: 0.2529021759418395\n",
      "epoch 11335: train loss: 0.12018251446092652, test loss: 0.2529025517319727\n",
      "epoch 11336: train loss: 0.12017959735860721, test loss: 0.25290292771718476\n",
      "epoch 11337: train loss: 0.1201766806165674, test loss: 0.2529033038974323\n",
      "epoch 11338: train loss: 0.12017376423472827, test loss: 0.25290368027264715\n",
      "epoch 11339: train loss: 0.12017084821301101, test loss: 0.2529040568427823\n",
      "epoch 11340: train loss: 0.12016793255133688, test loss: 0.252904433607807\n",
      "epoch 11341: train loss: 0.1201650172496271, test loss: 0.2529048105676581\n",
      "epoch 11342: train loss: 0.12016210230780297, test loss: 0.25290518772227716\n",
      "epoch 11343: train loss: 0.12015918772578577, test loss: 0.2529055650716292\n",
      "epoch 11344: train loss: 0.12015627350349688, test loss: 0.2529059426156608\n",
      "epoch 11345: train loss: 0.12015335964085758, test loss: 0.2529063203543204\n",
      "epoch 11346: train loss: 0.12015044613778926, test loss: 0.2529066982875496\n",
      "epoch 11347: train loss: 0.12014753299421338, test loss: 0.2529070764153131\n",
      "epoch 11348: train loss: 0.1201446202100513, test loss: 0.2529074547375531\n",
      "epoch 11349: train loss: 0.12014170778522448, test loss: 0.252907833254216\n",
      "epoch 11350: train loss: 0.12013879571965441, test loss: 0.25290821196526403\n",
      "epoch 11351: train loss: 0.1201358840132626, test loss: 0.25290859087062983\n",
      "epoch 11352: train loss: 0.1201329726659705, test loss: 0.2529089699702812\n",
      "epoch 11353: train loss: 0.12013006167769973, test loss: 0.25290934926415304\n",
      "epoch 11354: train loss: 0.12012715104837181, test loss: 0.2529097287522169\n",
      "epoch 11355: train loss: 0.12012424077790836, test loss: 0.25291010843440254\n",
      "epoch 11356: train loss: 0.12012133086623099, test loss: 0.2529104883106642\n",
      "epoch 11357: train loss: 0.12011842131326134, test loss: 0.2529108683809615\n",
      "epoch 11358: train loss: 0.12011551211892105, test loss: 0.2529112486452359\n",
      "epoch 11359: train loss: 0.12011260328313184, test loss: 0.2529116291034308\n",
      "epoch 11360: train loss: 0.1201096948058154, test loss: 0.2529120097555106\n",
      "epoch 11361: train loss: 0.12010678668689351, test loss: 0.2529123906014296\n",
      "epoch 11362: train loss: 0.12010387892628785, test loss: 0.252912771641124\n",
      "epoch 11363: train loss: 0.1201009715239203, test loss: 0.25291315287454774\n",
      "epoch 11364: train loss: 0.12009806447971261, test loss: 0.2529135343016553\n",
      "epoch 11365: train loss: 0.12009515779358662, test loss: 0.25291391592240614\n",
      "epoch 11366: train loss: 0.12009225146546418, test loss: 0.25291429773671925\n",
      "epoch 11367: train loss: 0.12008934549526719, test loss: 0.2529146797445834\n",
      "epoch 11368: train loss: 0.12008643988291755, test loss: 0.25291506194591806\n",
      "epoch 11369: train loss: 0.12008353462833721, test loss: 0.2529154443406888\n",
      "epoch 11370: train loss: 0.12008062973144806, test loss: 0.2529158269288557\n",
      "epoch 11371: train loss: 0.12007772519217216, test loss: 0.2529162097103388\n",
      "epoch 11372: train loss: 0.12007482101043142, test loss: 0.25291659268512606\n",
      "epoch 11373: train loss: 0.12007191718614792, test loss: 0.2529169758531372\n",
      "epoch 11374: train loss: 0.12006901371924371, test loss: 0.25291735921433833\n",
      "epoch 11375: train loss: 0.12006611060964087, test loss: 0.25291774276867196\n",
      "epoch 11376: train loss: 0.12006320785726146, test loss: 0.25291812651609286\n",
      "epoch 11377: train loss: 0.12006030546202762, test loss: 0.2529185104565606\n",
      "epoch 11378: train loss: 0.12005740342386147, test loss: 0.2529188945900177\n",
      "epoch 11379: train loss: 0.12005450174268521, test loss: 0.25291927891640176\n",
      "epoch 11380: train loss: 0.12005160041842103, test loss: 0.25291966343568545\n",
      "epoch 11381: train loss: 0.12004869945099113, test loss: 0.25292004814781066\n",
      "epoch 11382: train loss: 0.12004579884031774, test loss: 0.2529204330527315\n",
      "epoch 11383: train loss: 0.12004289858632317, test loss: 0.2529208181503797\n",
      "epoch 11384: train loss: 0.12003999868892964, test loss: 0.25292120344073354\n",
      "epoch 11385: train loss: 0.12003709914805955, test loss: 0.2529215889237242\n",
      "epoch 11386: train loss: 0.12003419996363514, test loss: 0.2529219745993062\n",
      "epoch 11387: train loss: 0.12003130113557882, test loss: 0.2529223604674402\n",
      "epoch 11388: train loss: 0.12002840266381298, test loss: 0.25292274652806285\n",
      "epoch 11389: train loss: 0.12002550454825997, test loss: 0.25292313278114154\n",
      "epoch 11390: train loss: 0.1200226067888423, test loss: 0.2529235192266185\n",
      "epoch 11391: train loss: 0.12001970938548237, test loss: 0.2529239058644379\n",
      "epoch 11392: train loss: 0.12001681233810264, test loss: 0.2529242926945539\n",
      "epoch 11393: train loss: 0.12001391564662565, test loss: 0.2529246797169213\n",
      "epoch 11394: train loss: 0.12001101931097395, test loss: 0.252925066931484\n",
      "epoch 11395: train loss: 0.12000812333107003, test loss: 0.2529254543382078\n",
      "epoch 11396: train loss: 0.12000522770683648, test loss: 0.25292584193703727\n",
      "epoch 11397: train loss: 0.12000233243819594, test loss: 0.25292622972791373\n",
      "epoch 11398: train loss: 0.11999943752507097, test loss: 0.2529266177107942\n",
      "epoch 11399: train loss: 0.11999654296738421, test loss: 0.25292700588563305\n",
      "epoch 11400: train loss: 0.11999364876505837, test loss: 0.25292739425237387\n",
      "epoch 11401: train loss: 0.11999075491801613, test loss: 0.2529277828109779\n",
      "epoch 11402: train loss: 0.11998786142618022, test loss: 0.2529281715613871\n",
      "epoch 11403: train loss: 0.11998496828947332, test loss: 0.2529285605035529\n",
      "epoch 11404: train loss: 0.11998207550781823, test loss: 0.2529289496374349\n",
      "epoch 11405: train loss: 0.11997918308113775, test loss: 0.2529293389629777\n",
      "epoch 11406: train loss: 0.11997629100935465, test loss: 0.2529297284801244\n",
      "epoch 11407: train loss: 0.1199733992923918, test loss: 0.2529301181888367\n",
      "epoch 11408: train loss: 0.11997050793017204, test loss: 0.2529305080890747\n",
      "epoch 11409: train loss: 0.11996761692261824, test loss: 0.25293089818077247\n",
      "epoch 11410: train loss: 0.11996472626965334, test loss: 0.25293128846388946\n",
      "epoch 11411: train loss: 0.11996183597120023, test loss: 0.25293167893836926\n",
      "epoch 11412: train loss: 0.11995894602718185, test loss: 0.2529320696041686\n",
      "epoch 11413: train loss: 0.11995605643752125, test loss: 0.25293246046123663\n",
      "epoch 11414: train loss: 0.1199531672021413, test loss: 0.25293285150952954\n",
      "epoch 11415: train loss: 0.11995027832096512, test loss: 0.25293324274899\n",
      "epoch 11416: train loss: 0.11994738979391574, test loss: 0.25293363417957465\n",
      "epoch 11417: train loss: 0.11994450162091624, test loss: 0.2529340258012395\n",
      "epoch 11418: train loss: 0.11994161380188967, test loss: 0.25293441761393914\n",
      "epoch 11419: train loss: 0.11993872633675913, test loss: 0.25293480961760023\n",
      "epoch 11420: train loss: 0.11993583922544784, test loss: 0.252935201812197\n",
      "epoch 11421: train loss: 0.11993295246787891, test loss: 0.25293559419767875\n",
      "epoch 11422: train loss: 0.11993006606397552, test loss: 0.25293598677398904\n",
      "epoch 11423: train loss: 0.1199271800136609, test loss: 0.2529363795410787\n",
      "epoch 11424: train loss: 0.11992429431685826, test loss: 0.2529367724988976\n",
      "epoch 11425: train loss: 0.11992140897349088, test loss: 0.25293716564740215\n",
      "epoch 11426: train loss: 0.119918523983482, test loss: 0.25293755898655895\n",
      "epoch 11427: train loss: 0.11991563934675499, test loss: 0.25293795251628987\n",
      "epoch 11428: train loss: 0.11991275506323312, test loss: 0.2529383462365737\n",
      "epoch 11429: train loss: 0.11990987113283977, test loss: 0.25293874014733736\n",
      "epoch 11430: train loss: 0.11990698755549829, test loss: 0.25293913424854303\n",
      "epoch 11431: train loss: 0.11990410433113208, test loss: 0.25293952854014634\n",
      "epoch 11432: train loss: 0.1199012214596646, test loss: 0.25293992302209245\n",
      "epoch 11433: train loss: 0.11989833894101923, test loss: 0.2529403176943307\n",
      "epoch 11434: train loss: 0.11989545677511948, test loss: 0.2529407125568305\n",
      "epoch 11435: train loss: 0.11989257496188882, test loss: 0.2529411076095225\n",
      "epoch 11436: train loss: 0.11988969350125077, test loss: 0.25294150285237077\n",
      "epoch 11437: train loss: 0.11988681239312887, test loss: 0.2529418982853188\n",
      "epoch 11438: train loss: 0.11988393163744668, test loss: 0.2529422939083246\n",
      "epoch 11439: train loss: 0.11988105123412779, test loss: 0.25294268972132455\n",
      "epoch 11440: train loss: 0.11987817118309581, test loss: 0.25294308572429386\n",
      "epoch 11441: train loss: 0.11987529148427434, test loss: 0.2529434819171655\n",
      "epoch 11442: train loss: 0.11987241213758706, test loss: 0.25294387829990866\n",
      "epoch 11443: train loss: 0.11986953314295765, test loss: 0.25294427487244797\n",
      "epoch 11444: train loss: 0.11986665450030977, test loss: 0.2529446716347599\n",
      "epoch 11445: train loss: 0.11986377620956717, test loss: 0.2529450685868004\n",
      "epoch 11446: train loss: 0.11986089827065363, test loss: 0.2529454657284899\n",
      "epoch 11447: train loss: 0.11985802068349287, test loss: 0.2529458630598041\n",
      "epoch 11448: train loss: 0.11985514344800868, test loss: 0.2529462605806927\n",
      "epoch 11449: train loss: 0.11985226656412497, test loss: 0.25294665829110086\n",
      "epoch 11450: train loss: 0.11984939003176544, test loss: 0.25294705619098534\n",
      "epoch 11451: train loss: 0.11984651385085403, test loss: 0.2529474542802978\n",
      "epoch 11452: train loss: 0.11984363802131462, test loss: 0.2529478525589877\n",
      "epoch 11453: train loss: 0.1198407625430711, test loss: 0.2529482510269953\n",
      "epoch 11454: train loss: 0.11983788741604745, test loss: 0.2529486496843009\n",
      "epoch 11455: train loss: 0.11983501264016758, test loss: 0.2529490485308379\n",
      "epoch 11456: train loss: 0.11983213821535546, test loss: 0.25294944756655674\n",
      "epoch 11457: train loss: 0.11982926414153512, test loss: 0.25294984679140886\n",
      "epoch 11458: train loss: 0.1198263904186306, test loss: 0.2529502462053517\n",
      "epoch 11459: train loss: 0.11982351704656591, test loss: 0.2529506458083355\n",
      "epoch 11460: train loss: 0.11982064402526511, test loss: 0.25295104560031195\n",
      "epoch 11461: train loss: 0.11981777135465237, test loss: 0.2529514455812383\n",
      "epoch 11462: train loss: 0.11981489903465173, test loss: 0.25295184575105323\n",
      "epoch 11463: train loss: 0.11981202706518737, test loss: 0.25295224610972716\n",
      "epoch 11464: train loss: 0.11980915544618341, test loss: 0.2529526466571979\n",
      "epoch 11465: train loss: 0.1198062841775641, test loss: 0.2529530473934177\n",
      "epoch 11466: train loss: 0.11980341325925359, test loss: 0.25295344831834315\n",
      "epoch 11467: train loss: 0.11980054269117617, test loss: 0.2529538494319265\n",
      "epoch 11468: train loss: 0.11979767247325607, test loss: 0.25295425073411903\n",
      "epoch 11469: train loss: 0.11979480260541753, test loss: 0.2529546522248588\n",
      "epoch 11470: train loss: 0.11979193308758491, test loss: 0.25295505390412937\n",
      "epoch 11471: train loss: 0.11978906391968253, test loss: 0.25295545577185674\n",
      "epoch 11472: train loss: 0.11978619510163468, test loss: 0.2529558578280056\n",
      "epoch 11473: train loss: 0.1197833266333658, test loss: 0.25295626007252026\n",
      "epoch 11474: train loss: 0.11978045851480026, test loss: 0.252956662505353\n",
      "epoch 11475: train loss: 0.11977759074586243, test loss: 0.2529570651264552\n",
      "epoch 11476: train loss: 0.11977472332647685, test loss: 0.252957467935791\n",
      "epoch 11477: train loss: 0.11977185625656789, test loss: 0.2529578709332991\n",
      "epoch 11478: train loss: 0.11976898953606009, test loss: 0.2529582741189443\n",
      "epoch 11479: train loss: 0.11976612316487792, test loss: 0.2529586774926599\n",
      "epoch 11480: train loss: 0.11976325714294594, test loss: 0.25295908105442066\n",
      "epoch 11481: train loss: 0.1197603914701887, test loss: 0.2529594848041564\n",
      "epoch 11482: train loss: 0.11975752614653079, test loss: 0.2529598887418477\n",
      "epoch 11483: train loss: 0.11975466117189679, test loss: 0.2529602928674101\n",
      "epoch 11484: train loss: 0.11975179654621132, test loss: 0.25296069718082687\n",
      "epoch 11485: train loss: 0.11974893226939902, test loss: 0.2529611016820373\n",
      "epoch 11486: train loss: 0.11974606834138461, test loss: 0.25296150637098636\n",
      "epoch 11487: train loss: 0.11974320476209271, test loss: 0.2529619112476509\n",
      "epoch 11488: train loss: 0.11974034153144811, test loss: 0.2529623163119656\n",
      "epoch 11489: train loss: 0.11973747864937549, test loss: 0.25296272156386906\n",
      "epoch 11490: train loss: 0.11973461611579964, test loss: 0.25296312700334467\n",
      "epoch 11491: train loss: 0.11973175393064533, test loss: 0.25296353263032034\n",
      "epoch 11492: train loss: 0.11972889209383737, test loss: 0.2529639384447653\n",
      "epoch 11493: train loss: 0.1197260306053006, test loss: 0.2529643444466196\n",
      "epoch 11494: train loss: 0.11972316946495985, test loss: 0.25296475063584906\n",
      "epoch 11495: train loss: 0.11972030867274004, test loss: 0.2529651570123812\n",
      "epoch 11496: train loss: 0.119717448228566, test loss: 0.2529655635761976\n",
      "epoch 11497: train loss: 0.1197145881323627, test loss: 0.25296597032723356\n",
      "epoch 11498: train loss: 0.11971172838405508, test loss: 0.25296637726544086\n",
      "epoch 11499: train loss: 0.1197088689835681, test loss: 0.252966784390783\n",
      "epoch 11500: train loss: 0.11970600993082674, test loss: 0.2529671917032147\n",
      "epoch 11501: train loss: 0.11970315122575602, test loss: 0.25296759920267375\n",
      "epoch 11502: train loss: 0.11970029286828093, test loss: 0.2529680068891029\n",
      "epoch 11503: train loss: 0.11969743485832667, test loss: 0.25296841476248916\n",
      "epoch 11504: train loss: 0.11969457719581815, test loss: 0.25296882282276706\n",
      "epoch 11505: train loss: 0.11969171988068053, test loss: 0.25296923106988395\n",
      "epoch 11506: train loss: 0.11968886291283898, test loss: 0.2529696395037987\n",
      "epoch 11507: train loss: 0.11968600629221861, test loss: 0.25297004812446855\n",
      "epoch 11508: train loss: 0.11968315001874462, test loss: 0.2529704569318224\n",
      "epoch 11509: train loss: 0.11968029409234215, test loss: 0.25297086592585016\n",
      "epoch 11510: train loss: 0.11967743851293648, test loss: 0.25297127510647993\n",
      "epoch 11511: train loss: 0.11967458328045279, test loss: 0.2529716844736703\n",
      "epoch 11512: train loss: 0.11967172839481638, test loss: 0.2529720940273678\n",
      "epoch 11513: train loss: 0.11966887385595251, test loss: 0.25297250376753655\n",
      "epoch 11514: train loss: 0.11966601966378651, test loss: 0.2529729136941134\n",
      "epoch 11515: train loss: 0.11966316581824371, test loss: 0.2529733238070737\n",
      "epoch 11516: train loss: 0.11966031231924944, test loss: 0.2529737341063465\n",
      "epoch 11517: train loss: 0.11965745916672908, test loss: 0.25297414459190953\n",
      "epoch 11518: train loss: 0.11965460636060804, test loss: 0.2529745552636901\n",
      "epoch 11519: train loss: 0.11965175390081177, test loss: 0.2529749661216546\n",
      "epoch 11520: train loss: 0.11964890178726563, test loss: 0.25297537716575663\n",
      "epoch 11521: train loss: 0.11964605001989514, test loss: 0.2529757883959409\n",
      "epoch 11522: train loss: 0.11964319859862581, test loss: 0.2529761998121755\n",
      "epoch 11523: train loss: 0.11964034752338311, test loss: 0.2529766114143987\n",
      "epoch 11524: train loss: 0.11963749679409254, test loss: 0.25297702320256493\n",
      "epoch 11525: train loss: 0.11963464641067975, test loss: 0.2529774351766282\n",
      "epoch 11526: train loss: 0.11963179637307025, test loss: 0.25297784733655776\n",
      "epoch 11527: train loss: 0.11962894668118967, test loss: 0.25297825968227167\n",
      "epoch 11528: train loss: 0.11962609733496361, test loss: 0.25297867221375986\n",
      "epoch 11529: train loss: 0.11962324833431776, test loss: 0.2529790849309515\n",
      "epoch 11530: train loss: 0.11962039967917772, test loss: 0.2529794978338108\n",
      "epoch 11531: train loss: 0.11961755136946924, test loss: 0.25297991092229294\n",
      "epoch 11532: train loss: 0.11961470340511798, test loss: 0.25298032419633093\n",
      "epoch 11533: train loss: 0.11961185578604973, test loss: 0.25298073765589796\n",
      "epoch 11534: train loss: 0.11960900851219021, test loss: 0.25298115130094073\n",
      "epoch 11535: train loss: 0.11960616158346521, test loss: 0.25298156513141357\n",
      "epoch 11536: train loss: 0.11960331499980058, test loss: 0.2529819791472628\n",
      "epoch 11537: train loss: 0.11960046876112206, test loss: 0.25298239334844874\n",
      "epoch 11538: train loss: 0.11959762286735554, test loss: 0.2529828077349249\n",
      "epoch 11539: train loss: 0.11959477731842692, test loss: 0.2529832223066316\n",
      "epoch 11540: train loss: 0.11959193211426206, test loss: 0.25298363706354837\n",
      "epoch 11541: train loss: 0.11958908725478685, test loss: 0.2529840520056045\n",
      "epoch 11542: train loss: 0.11958624273992728, test loss: 0.2529844671327708\n",
      "epoch 11543: train loss: 0.1195833985696093, test loss: 0.2529848824449774\n",
      "epoch 11544: train loss: 0.11958055474375884, test loss: 0.2529852979421958\n",
      "epoch 11545: train loss: 0.11957771126230199, test loss: 0.25298571362438077\n",
      "epoch 11546: train loss: 0.11957486812516471, test loss: 0.25298612949147303\n",
      "epoch 11547: train loss: 0.11957202533227308, test loss: 0.2529865455434324\n",
      "epoch 11548: train loss: 0.11956918288355317, test loss: 0.2529869617802134\n",
      "epoch 11549: train loss: 0.11956634077893105, test loss: 0.25298737820176287\n",
      "epoch 11550: train loss: 0.11956349901833287, test loss: 0.25298779480803657\n",
      "epoch 11551: train loss: 0.11956065760168474, test loss: 0.2529882115989994\n",
      "epoch 11552: train loss: 0.11955781652891284, test loss: 0.25298862857458687\n",
      "epoch 11553: train loss: 0.11955497579994337, test loss: 0.25298904573476544\n",
      "epoch 11554: train loss: 0.11955213541470247, test loss: 0.2529894630794711\n",
      "epoch 11555: train loss: 0.11954929537311645, test loss: 0.25298988060868227\n",
      "epoch 11556: train loss: 0.11954645567511152, test loss: 0.25299029832234066\n",
      "epoch 11557: train loss: 0.11954361632061396, test loss: 0.2529907162203881\n",
      "epoch 11558: train loss: 0.11954077730955005, test loss: 0.2529911343028046\n",
      "epoch 11559: train loss: 0.11953793864184613, test loss: 0.25299155256950523\n",
      "epoch 11560: train loss: 0.11953510031742852, test loss: 0.25299197102048265\n",
      "epoch 11561: train loss: 0.11953226233622358, test loss: 0.2529923896556731\n",
      "epoch 11562: train loss: 0.11952942469815771, test loss: 0.25299280847502303\n",
      "epoch 11563: train loss: 0.11952658740315733, test loss: 0.2529932274784887\n",
      "epoch 11564: train loss: 0.11952375045114881, test loss: 0.25299364666604157\n",
      "epoch 11565: train loss: 0.11952091384205868, test loss: 0.25299406603761176\n",
      "epoch 11566: train loss: 0.11951807757581337, test loss: 0.2529944855931664\n",
      "epoch 11567: train loss: 0.11951524165233934, test loss: 0.25299490533265334\n",
      "epoch 11568: train loss: 0.1195124060715632, test loss: 0.2529953252560223\n",
      "epoch 11569: train loss: 0.1195095708334114, test loss: 0.25299574536323755\n",
      "epoch 11570: train loss: 0.11950673593781054, test loss: 0.25299616565424937\n",
      "epoch 11571: train loss: 0.1195039013846872, test loss: 0.252996586129006\n",
      "epoch 11572: train loss: 0.119501067173968, test loss: 0.25299700678747294\n",
      "epoch 11573: train loss: 0.11949823330557953, test loss: 0.25299742762958227\n",
      "epoch 11574: train loss: 0.11949539977944848, test loss: 0.2529978486553059\n",
      "epoch 11575: train loss: 0.11949256659550152, test loss: 0.25299826986459323\n",
      "epoch 11576: train loss: 0.11948973375366531, test loss: 0.2529986912573914\n",
      "epoch 11577: train loss: 0.11948690125386659, test loss: 0.2529991128336631\n",
      "epoch 11578: train loss: 0.1194840690960321, test loss: 0.2529995345933622\n",
      "epoch 11579: train loss: 0.1194812372800886, test loss: 0.252999956536443\n",
      "epoch 11580: train loss: 0.11947840580596285, test loss: 0.25300037866284303\n",
      "epoch 11581: train loss: 0.11947557467358169, test loss: 0.25300080097253397\n",
      "epoch 11582: train loss: 0.1194727438828719, test loss: 0.2530012234654606\n",
      "epoch 11583: train loss: 0.11946991343376041, test loss: 0.25300164614158294\n",
      "epoch 11584: train loss: 0.11946708332617403, test loss: 0.2530020690008499\n",
      "epoch 11585: train loss: 0.11946425356003967, test loss: 0.2530024920432104\n",
      "epoch 11586: train loss: 0.11946142413528422, test loss: 0.25300291526863095\n",
      "epoch 11587: train loss: 0.11945859505183463, test loss: 0.25300333867705604\n",
      "epoch 11588: train loss: 0.11945576630961792, test loss: 0.2530037622684392\n",
      "epoch 11589: train loss: 0.11945293790856094, test loss: 0.2530041860427415\n",
      "epoch 11590: train loss: 0.11945010984859081, test loss: 0.25300460999992014\n",
      "epoch 11591: train loss: 0.11944728212963454, test loss: 0.2530050341399102\n",
      "epoch 11592: train loss: 0.11944445475161913, test loss: 0.2530054584626859\n",
      "epoch 11593: train loss: 0.11944162771447168, test loss: 0.2530058829681902\n",
      "epoch 11594: train loss: 0.11943880101811927, test loss: 0.2530063076563717\n",
      "epoch 11595: train loss: 0.119435974662489, test loss: 0.2530067325271926\n",
      "epoch 11596: train loss: 0.11943314864750804, test loss: 0.25300715758061315\n",
      "epoch 11597: train loss: 0.11943032297310349, test loss: 0.25300758281657193\n",
      "epoch 11598: train loss: 0.11942749763920261, test loss: 0.253008008235036\n",
      "epoch 11599: train loss: 0.11942467264573253, test loss: 0.25300843383595406\n",
      "epoch 11600: train loss: 0.11942184799262051, test loss: 0.25300885961927644\n",
      "epoch 11601: train loss: 0.1194190236797938, test loss: 0.25300928558496416\n",
      "epoch 11602: train loss: 0.11941619970717962, test loss: 0.25300971173296644\n",
      "epoch 11603: train loss: 0.11941337607470533, test loss: 0.25301013806324524\n",
      "epoch 11604: train loss: 0.11941055278229819, test loss: 0.25301056457573745\n",
      "epoch 11605: train loss: 0.11940772982988551, test loss: 0.2530109912704116\n",
      "epoch 11606: train loss: 0.1194049072173947, test loss: 0.25301141814722855\n",
      "epoch 11607: train loss: 0.11940208494475313, test loss: 0.2530118452061266\n",
      "epoch 11608: train loss: 0.11939926301188816, test loss: 0.2530122724470541\n",
      "epoch 11609: train loss: 0.11939644141872723, test loss: 0.2530126998699859\n",
      "epoch 11610: train loss: 0.11939362016519775, test loss: 0.2530131274748598\n",
      "epoch 11611: train loss: 0.11939079925122727, test loss: 0.2530135552616492\n",
      "epoch 11612: train loss: 0.11938797867674317, test loss: 0.2530139832302982\n",
      "epoch 11613: train loss: 0.11938515844167302, test loss: 0.25301441138074376\n",
      "epoch 11614: train loss: 0.11938233854594436, test loss: 0.2530148397129538\n",
      "epoch 11615: train loss: 0.11937951898948469, test loss: 0.25301526822689185\n",
      "epoch 11616: train loss: 0.11937669977222161, test loss: 0.25301569692251197\n",
      "epoch 11617: train loss: 0.1193738808940827, test loss: 0.2530161257997532\n",
      "epoch 11618: train loss: 0.11937106235499559, test loss: 0.2530165548585762\n",
      "epoch 11619: train loss: 0.11936824415488793, test loss: 0.25301698409893225\n",
      "epoch 11620: train loss: 0.11936542629368735, test loss: 0.25301741352077656\n",
      "epoch 11621: train loss: 0.11936260877132153, test loss: 0.25301784312407266\n",
      "epoch 11622: train loss: 0.11935979158771819, test loss: 0.2530182729087749\n",
      "epoch 11623: train loss: 0.11935697474280506, test loss: 0.2530187028748212\n",
      "epoch 11624: train loss: 0.11935415823650987, test loss: 0.25301913302218754\n",
      "epoch 11625: train loss: 0.1193513420687604, test loss: 0.25301956335081077\n",
      "epoch 11626: train loss: 0.11934852623948443, test loss: 0.25301999386064683\n",
      "epoch 11627: train loss: 0.11934571074860977, test loss: 0.2530204245516593\n",
      "epoch 11628: train loss: 0.11934289559606427, test loss: 0.25302085542379055\n",
      "epoch 11629: train loss: 0.11934008078177573, test loss: 0.2530212864770046\n",
      "epoch 11630: train loss: 0.11933726630567211, test loss: 0.2530217177112565\n",
      "epoch 11631: train loss: 0.11933445216768127, test loss: 0.2530221491264978\n",
      "epoch 11632: train loss: 0.11933163836773109, test loss: 0.25302258072269007\n",
      "epoch 11633: train loss: 0.11932882490574956, test loss: 0.2530230124997718\n",
      "epoch 11634: train loss: 0.11932601178166465, test loss: 0.2530234444577056\n",
      "epoch 11635: train loss: 0.11932319899540428, test loss: 0.2530238765964534\n",
      "epoch 11636: train loss: 0.11932038654689656, test loss: 0.2530243089159543\n",
      "epoch 11637: train loss: 0.11931757443606937, test loss: 0.2530247414161832\n",
      "epoch 11638: train loss: 0.11931476266285092, test loss: 0.25302517409706515\n",
      "epoch 11639: train loss: 0.11931195122716919, test loss: 0.2530256069585818\n",
      "epoch 11640: train loss: 0.11930914012895226, test loss: 0.2530260400006837\n",
      "epoch 11641: train loss: 0.1193063293681283, test loss: 0.2530264732233093\n",
      "epoch 11642: train loss: 0.11930351894462539, test loss: 0.25302690662643323\n",
      "epoch 11643: train loss: 0.11930070885837175, test loss: 0.253027340210007\n",
      "epoch 11644: train loss: 0.1192978991092955, test loss: 0.2530277739739614\n",
      "epoch 11645: train loss: 0.11929508969732489, test loss: 0.2530282079182863\n",
      "epoch 11646: train loss: 0.11929228062238806, test loss: 0.25302864204290487\n",
      "epoch 11647: train loss: 0.11928947188441334, test loss: 0.2530290763477941\n",
      "epoch 11648: train loss: 0.11928666348332896, test loss: 0.25302951083289205\n",
      "epoch 11649: train loss: 0.11928385541906322, test loss: 0.2530299454981682\n",
      "epoch 11650: train loss: 0.11928104769154439, test loss: 0.25303038034357317\n",
      "epoch 11651: train loss: 0.11927824030070085, test loss: 0.2530308153690579\n",
      "epoch 11652: train loss: 0.11927543324646091, test loss: 0.25303125057458586\n",
      "epoch 11653: train loss: 0.11927262652875296, test loss: 0.2530316859600945\n",
      "epoch 11654: train loss: 0.1192698201475054, test loss: 0.25303212152554183\n",
      "epoch 11655: train loss: 0.1192670141026466, test loss: 0.25303255727090335\n",
      "epoch 11656: train loss: 0.11926420839410508, test loss: 0.25303299319611783\n",
      "epoch 11657: train loss: 0.11926140302180924, test loss: 0.25303342930112943\n",
      "epoch 11658: train loss: 0.11925859798568755, test loss: 0.25303386558592694\n",
      "epoch 11659: train loss: 0.11925579328566854, test loss: 0.2530343020504231\n",
      "epoch 11660: train loss: 0.11925298892168072, test loss: 0.2530347386946014\n",
      "epoch 11661: train loss: 0.11925018489365265, test loss: 0.2530351755184124\n",
      "epoch 11662: train loss: 0.11924738120151286, test loss: 0.25303561252180734\n",
      "epoch 11663: train loss: 0.11924457784518994, test loss: 0.2530360497047495\n",
      "epoch 11664: train loss: 0.11924177482461255, test loss: 0.2530364870671722\n",
      "epoch 11665: train loss: 0.11923897213970927, test loss: 0.25303692460904315\n",
      "epoch 11666: train loss: 0.11923616979040876, test loss: 0.25303736233033464\n",
      "epoch 11667: train loss: 0.11923336777663969, test loss: 0.25303780023096617\n",
      "epoch 11668: train loss: 0.11923056609833077, test loss: 0.25303823831091904\n",
      "epoch 11669: train loss: 0.1192277647554107, test loss: 0.2530386765701523\n",
      "epoch 11670: train loss: 0.11922496374780821, test loss: 0.25303911500858584\n",
      "epoch 11671: train loss: 0.11922216307545208, test loss: 0.2530395536262131\n",
      "epoch 11672: train loss: 0.11921936273827108, test loss: 0.25303999242297465\n",
      "epoch 11673: train loss: 0.11921656273619397, test loss: 0.2530404313988219\n",
      "epoch 11674: train loss: 0.11921376306914962, test loss: 0.2530408705537177\n",
      "epoch 11675: train loss: 0.11921096373706686, test loss: 0.2530413098876076\n",
      "epoch 11676: train loss: 0.11920816473987456, test loss: 0.2530417494004508\n",
      "epoch 11677: train loss: 0.11920536607750158, test loss: 0.25304218909220927\n",
      "epoch 11678: train loss: 0.11920256774987685, test loss: 0.2530426289628361\n",
      "epoch 11679: train loss: 0.11919976975692925, test loss: 0.25304306901227563\n",
      "epoch 11680: train loss: 0.11919697209858782, test loss: 0.2530435092404866\n",
      "epoch 11681: train loss: 0.11919417477478149, test loss: 0.25304394964743215\n",
      "epoch 11682: train loss: 0.1191913777854392, test loss: 0.2530443902330525\n",
      "epoch 11683: train loss: 0.11918858113049, test loss: 0.253044830997317\n",
      "epoch 11684: train loss: 0.11918578480986294, test loss: 0.25304527194018506\n",
      "epoch 11685: train loss: 0.11918298882348705, test loss: 0.2530457130616069\n",
      "epoch 11686: train loss: 0.1191801931712914, test loss: 0.25304615436152295\n",
      "epoch 11687: train loss: 0.11917739785320514, test loss: 0.2530465958399035\n",
      "epoch 11688: train loss: 0.11917460286915732, test loss: 0.2530470374966938\n",
      "epoch 11689: train loss: 0.11917180821907712, test loss: 0.25304747933187066\n",
      "epoch 11690: train loss: 0.11916901390289368, test loss: 0.2530479213453748\n",
      "epoch 11691: train loss: 0.1191662199205362, test loss: 0.2530483635371509\n",
      "epoch 11692: train loss: 0.11916342627193391, test loss: 0.2530488059071685\n",
      "epoch 11693: train loss: 0.11916063295701594, test loss: 0.25304924845537535\n",
      "epoch 11694: train loss: 0.11915783997571161, test loss: 0.2530496911817356\n",
      "epoch 11695: train loss: 0.11915504732795018, test loss: 0.25305013408618743\n",
      "epoch 11696: train loss: 0.11915225501366095, test loss: 0.2530505771687158\n",
      "epoch 11697: train loss: 0.11914946303277318, test loss: 0.2530510204292416\n",
      "epoch 11698: train loss: 0.11914667138521623, test loss: 0.2530514638677528\n",
      "epoch 11699: train loss: 0.11914388007091946, test loss: 0.2530519074841717\n",
      "epoch 11700: train loss: 0.11914108908981222, test loss: 0.2530523512784813\n",
      "epoch 11701: train loss: 0.11913829844182391, test loss: 0.2530527952506347\n",
      "epoch 11702: train loss: 0.11913550812688395, test loss: 0.25305323940056385\n",
      "epoch 11703: train loss: 0.11913271814492177, test loss: 0.25305368372825343\n",
      "epoch 11704: train loss: 0.11912992849586683, test loss: 0.25305412823364287\n",
      "epoch 11705: train loss: 0.1191271391796486, test loss: 0.25305457291668393\n",
      "epoch 11706: train loss: 0.11912435019619658, test loss: 0.2530550177773437\n",
      "epoch 11707: train loss: 0.1191215615454403, test loss: 0.2530554628155657\n",
      "epoch 11708: train loss: 0.1191187732273093, test loss: 0.2530559080313224\n",
      "epoch 11709: train loss: 0.11911598524173311, test loss: 0.2530563534245472\n",
      "epoch 11710: train loss: 0.11911319758864136, test loss: 0.2530567989952183\n",
      "epoch 11711: train loss: 0.11911041026796364, test loss: 0.2530572447432747\n",
      "epoch 11712: train loss: 0.11910762327962952, test loss: 0.2530576906686822\n",
      "epoch 11713: train loss: 0.1191048366235687, test loss: 0.25305813677138006\n",
      "epoch 11714: train loss: 0.11910205029971088, test loss: 0.2530585830513404\n",
      "epoch 11715: train loss: 0.11909926430798567, test loss: 0.25305902950852865\n",
      "epoch 11716: train loss: 0.1190964786483228, test loss: 0.2530594761428722\n",
      "epoch 11717: train loss: 0.11909369332065203, test loss: 0.2530599229543414\n",
      "epoch 11718: train loss: 0.1190909083249031, test loss: 0.25306036994289033\n",
      "epoch 11719: train loss: 0.11908812366100573, test loss: 0.25306081710848327\n",
      "epoch 11720: train loss: 0.11908533932888979, test loss: 0.2530612644510739\n",
      "epoch 11721: train loss: 0.11908255532848504, test loss: 0.25306171197058974\n",
      "epoch 11722: train loss: 0.11907977165972135, test loss: 0.2530621596670272\n",
      "epoch 11723: train loss: 0.11907698832252855, test loss: 0.2530626075403142\n",
      "epoch 11724: train loss: 0.11907420531683649, test loss: 0.2530630555904231\n",
      "epoch 11725: train loss: 0.1190714226425751, test loss: 0.2530635038172989\n",
      "epoch 11726: train loss: 0.11906864029967432, test loss: 0.2530639522209024\n",
      "epoch 11727: train loss: 0.11906585828806407, test loss: 0.2530644008011857\n",
      "epoch 11728: train loss: 0.11906307660767429, test loss: 0.2530648495581087\n",
      "epoch 11729: train loss: 0.11906029525843496, test loss: 0.2530652984916302\n",
      "epoch 11730: train loss: 0.1190575142402761, test loss: 0.2530657476016973\n",
      "epoch 11731: train loss: 0.11905473355312773, test loss: 0.2530661968882692\n",
      "epoch 11732: train loss: 0.11905195319691987, test loss: 0.2530666463513058\n",
      "epoch 11733: train loss: 0.11904917317158262, test loss: 0.25306709599075305\n",
      "epoch 11734: train loss: 0.11904639347704603, test loss: 0.2530675458065775\n",
      "epoch 11735: train loss: 0.11904361411324021, test loss: 0.253067995798732\n",
      "epoch 11736: train loss: 0.11904083508009532, test loss: 0.25306844596716344\n",
      "epoch 11737: train loss: 0.11903805637754146, test loss: 0.2530688963118498\n",
      "epoch 11738: train loss: 0.11903527800550882, test loss: 0.2530693468327265\n",
      "epoch 11739: train loss: 0.11903249996392759, test loss: 0.25306979752975256\n",
      "epoch 11740: train loss: 0.11902972225272798, test loss: 0.2530702484029006\n",
      "epoch 11741: train loss: 0.11902694487184022, test loss: 0.2530706994521049\n",
      "epoch 11742: train loss: 0.11902416782119453, test loss: 0.25307115067732455\n",
      "epoch 11743: train loss: 0.11902139110072123, test loss: 0.2530716020785201\n",
      "epoch 11744: train loss: 0.11901861471035056, test loss: 0.25307205365566365\n",
      "epoch 11745: train loss: 0.11901583865001288, test loss: 0.25307250540867654\n",
      "epoch 11746: train loss: 0.11901306291963848, test loss: 0.25307295733754476\n",
      "epoch 11747: train loss: 0.11901028751915778, test loss: 0.2530734094422151\n",
      "epoch 11748: train loss: 0.1190075124485011, test loss: 0.2530738617226479\n",
      "epoch 11749: train loss: 0.11900473770759884, test loss: 0.2530743141787777\n",
      "epoch 11750: train loss: 0.11900196329638142, test loss: 0.2530747668105882\n",
      "epoch 11751: train loss: 0.11899918921477928, test loss: 0.25307521961802854\n",
      "epoch 11752: train loss: 0.11899641546272288, test loss: 0.2530756726010459\n",
      "epoch 11753: train loss: 0.1189936420401427, test loss: 0.25307612575959987\n",
      "epoch 11754: train loss: 0.11899086894696924, test loss: 0.2530765790936444\n",
      "epoch 11755: train loss: 0.11898809618313301, test loss: 0.25307703260313835\n",
      "epoch 11756: train loss: 0.11898532374856455, test loss: 0.25307748628803706\n",
      "epoch 11757: train loss: 0.11898255164319443, test loss: 0.2530779401483133\n",
      "epoch 11758: train loss: 0.11897977986695324, test loss: 0.2530783941839013\n",
      "epoch 11759: train loss: 0.11897700841977155, test loss: 0.25307884839475514\n",
      "epoch 11760: train loss: 0.11897423730158, test loss: 0.25307930278084684\n",
      "epoch 11761: train loss: 0.11897146651230922, test loss: 0.25307975734213173\n",
      "epoch 11762: train loss: 0.11896869605188992, test loss: 0.2530802120785506\n",
      "epoch 11763: train loss: 0.11896592592025275, test loss: 0.2530806669900771\n",
      "epoch 11764: train loss: 0.11896315611732838, test loss: 0.25308112207665756\n",
      "epoch 11765: train loss: 0.11896038664304762, test loss: 0.25308157733825204\n",
      "epoch 11766: train loss: 0.11895761749734113, test loss: 0.2530820327748104\n",
      "epoch 11767: train loss: 0.11895484868013974, test loss: 0.2530824883862985\n",
      "epoch 11768: train loss: 0.1189520801913742, test loss: 0.25308294417266275\n",
      "epoch 11769: train loss: 0.11894931203097535, test loss: 0.25308340013387864\n",
      "epoch 11770: train loss: 0.11894654419887399, test loss: 0.25308385626987895\n",
      "epoch 11771: train loss: 0.11894377669500095, test loss: 0.2530843125806264\n",
      "epoch 11772: train loss: 0.11894100951928716, test loss: 0.25308476906609234\n",
      "epoch 11773: train loss: 0.11893824267166345, test loss: 0.25308522572621345\n",
      "epoch 11774: train loss: 0.11893547615206074, test loss: 0.2530856825609566\n",
      "epoch 11775: train loss: 0.11893270996041004, test loss: 0.2530861395702815\n",
      "epoch 11776: train loss: 0.1189299440966422, test loss: 0.2530865967541367\n",
      "epoch 11777: train loss: 0.11892717856068824, test loss: 0.25308705411247673\n",
      "epoch 11778: train loss: 0.11892441335247912, test loss: 0.25308751164526905\n",
      "epoch 11779: train loss: 0.11892164847194589, test loss: 0.2530879693524608\n",
      "epoch 11780: train loss: 0.11891888391901957, test loss: 0.2530884272340199\n",
      "epoch 11781: train loss: 0.11891611969363122, test loss: 0.25308888528987544\n",
      "epoch 11782: train loss: 0.11891335579571191, test loss: 0.2530893435200254\n",
      "epoch 11783: train loss: 0.11891059222519271, test loss: 0.253089801924394\n",
      "epoch 11784: train loss: 0.11890782898200478, test loss: 0.2530902605029538\n",
      "epoch 11785: train loss: 0.1189050660660792, test loss: 0.2530907192556404\n",
      "epoch 11786: train loss: 0.11890230347734716, test loss: 0.25309117818243526\n",
      "epoch 11787: train loss: 0.11889954121573985, test loss: 0.25309163728328515\n",
      "epoch 11788: train loss: 0.11889677928118844, test loss: 0.2530920965581511\n",
      "epoch 11789: train loss: 0.11889401767362416, test loss: 0.25309255600697533\n",
      "epoch 11790: train loss: 0.11889125639297823, test loss: 0.2530930156297331\n",
      "epoch 11791: train loss: 0.11888849543918195, test loss: 0.2530934754263706\n",
      "epoch 11792: train loss: 0.11888573481216655, test loss: 0.25309393539684505\n",
      "epoch 11793: train loss: 0.11888297451186335, test loss: 0.2530943955411152\n",
      "epoch 11794: train loss: 0.11888021453820365, test loss: 0.25309485585914354\n",
      "epoch 11795: train loss: 0.11887745489111883, test loss: 0.25309531635087196\n",
      "epoch 11796: train loss: 0.11887469557054021, test loss: 0.25309577701627506\n",
      "epoch 11797: train loss: 0.11887193657639919, test loss: 0.25309623785528745\n",
      "epoch 11798: train loss: 0.11886917790862714, test loss: 0.25309669886789105\n",
      "epoch 11799: train loss: 0.1188664195671555, test loss: 0.2530971600540204\n",
      "epoch 11800: train loss: 0.11886366155191576, test loss: 0.25309762141364395\n",
      "epoch 11801: train loss: 0.11886090386283932, test loss: 0.2530980829467236\n",
      "epoch 11802: train loss: 0.11885814649985768, test loss: 0.2530985446532073\n",
      "epoch 11803: train loss: 0.11885538946290232, test loss: 0.2530990065330503\n",
      "epoch 11804: train loss: 0.11885263275190479, test loss: 0.2530994685862142\n",
      "epoch 11805: train loss: 0.11884987636679661, test loss: 0.25309993081265425\n",
      "epoch 11806: train loss: 0.11884712030750938, test loss: 0.25310039321233213\n",
      "epoch 11807: train loss: 0.11884436457397463, test loss: 0.253100855785196\n",
      "epoch 11808: train loss: 0.11884160916612398, test loss: 0.2531013185312146\n",
      "epoch 11809: train loss: 0.1188388540838891, test loss: 0.25310178145033035\n",
      "epoch 11810: train loss: 0.11883609932720157, test loss: 0.2531022445425042\n",
      "epoch 11811: train loss: 0.1188333448959931, test loss: 0.25310270780769906\n",
      "epoch 11812: train loss: 0.11883059079019534, test loss: 0.2531031712458697\n",
      "epoch 11813: train loss: 0.11882783700974, test loss: 0.25310363485697146\n",
      "epoch 11814: train loss: 0.11882508355455881, test loss: 0.2531040986409663\n",
      "epoch 11815: train loss: 0.11882233042458352, test loss: 0.2531045625978034\n",
      "epoch 11816: train loss: 0.1188195776197459, test loss: 0.253105026727437\n",
      "epoch 11817: train loss: 0.11881682513997772, test loss: 0.25310549102983726\n",
      "epoch 11818: train loss: 0.1188140729852108, test loss: 0.2531059555049653\n",
      "epoch 11819: train loss: 0.11881132115537692, test loss: 0.25310642015275625\n",
      "epoch 11820: train loss: 0.11880856965040795, test loss: 0.2531068849731685\n",
      "epoch 11821: train loss: 0.11880581847023582, test loss: 0.2531073499661801\n",
      "epoch 11822: train loss: 0.11880306761479231, test loss: 0.25310781513173675\n",
      "epoch 11823: train loss: 0.11880031708400939, test loss: 0.25310828046979883\n",
      "epoch 11824: train loss: 0.11879756687781896, test loss: 0.2531087459803149\n",
      "epoch 11825: train loss: 0.118794816996153, test loss: 0.25310921166324973\n",
      "epoch 11826: train loss: 0.11879206743894344, test loss: 0.25310967751855656\n",
      "epoch 11827: train loss: 0.11878931820612226, test loss: 0.25311014354619893\n",
      "epoch 11828: train loss: 0.11878656929762149, test loss: 0.253110609746126\n",
      "epoch 11829: train loss: 0.11878382071337315, test loss: 0.25311107611829997\n",
      "epoch 11830: train loss: 0.11878107245330931, test loss: 0.25311154266267555\n",
      "epoch 11831: train loss: 0.11877832451736198, test loss: 0.2531120093791977\n",
      "epoch 11832: train loss: 0.1187755769054633, test loss: 0.25311247626785344\n",
      "epoch 11833: train loss: 0.11877282961754533, test loss: 0.25311294332857975\n",
      "epoch 11834: train loss: 0.11877008265354025, test loss: 0.2531134105613381\n",
      "epoch 11835: train loss: 0.11876733601338013, test loss: 0.25311387796607315\n",
      "epoch 11836: train loss: 0.11876458969699721, test loss: 0.2531143455427717\n",
      "epoch 11837: train loss: 0.11876184370432366, test loss: 0.25311481329137187\n",
      "epoch 11838: train loss: 0.11875909803529165, test loss: 0.2531152812118154\n",
      "epoch 11839: train loss: 0.11875635268983345, test loss: 0.25311574930409786\n",
      "epoch 11840: train loss: 0.11875360766788129, test loss: 0.2531162175681433\n",
      "epoch 11841: train loss: 0.11875086296936743, test loss: 0.2531166860039208\n",
      "epoch 11842: train loss: 0.11874811859422416, test loss: 0.25311715461138634\n",
      "epoch 11843: train loss: 0.11874537454238379, test loss: 0.2531176233905088\n",
      "epoch 11844: train loss: 0.11874263081377862, test loss: 0.2531180923412187\n",
      "epoch 11845: train loss: 0.11873988740834104, test loss: 0.2531185614635042\n",
      "epoch 11846: train loss: 0.11873714432600338, test loss: 0.2531190307573095\n",
      "epoch 11847: train loss: 0.11873440156669807, test loss: 0.2531195002225905\n",
      "epoch 11848: train loss: 0.11873165913035749, test loss: 0.2531199698592981\n",
      "epoch 11849: train loss: 0.11872891701691403, test loss: 0.25312043966739933\n",
      "epoch 11850: train loss: 0.11872617522630019, test loss: 0.2531209096468526\n",
      "epoch 11851: train loss: 0.1187234337584484, test loss: 0.2531213797976206\n",
      "epoch 11852: train loss: 0.1187206926132912, test loss: 0.2531218501196461\n",
      "epoch 11853: train loss: 0.11871795179076101, test loss: 0.25312232061289214\n",
      "epoch 11854: train loss: 0.11871521129079042, test loss: 0.2531227912773092\n",
      "epoch 11855: train loss: 0.11871247111331197, test loss: 0.2531232621128738\n",
      "epoch 11856: train loss: 0.1187097312582582, test loss: 0.25312373311952174\n",
      "epoch 11857: train loss: 0.11870699172556172, test loss: 0.25312420429723664\n",
      "epoch 11858: train loss: 0.1187042525151551, test loss: 0.25312467564595426\n",
      "epoch 11859: train loss: 0.11870151362697103, test loss: 0.2531251471656326\n",
      "epoch 11860: train loss: 0.11869877506094209, test loss: 0.2531256188562417\n",
      "epoch 11861: train loss: 0.11869603681700096, test loss: 0.2531260907177238\n",
      "epoch 11862: train loss: 0.11869329889508035, test loss: 0.25312656275004936\n",
      "epoch 11863: train loss: 0.11869056129511293, test loss: 0.25312703495318295\n",
      "epoch 11864: train loss: 0.11868782401703144, test loss: 0.2531275073270656\n",
      "epoch 11865: train loss: 0.11868508706076863, test loss: 0.2531279798716492\n",
      "epoch 11866: train loss: 0.11868235042625724, test loss: 0.2531284525869111\n",
      "epoch 11867: train loss: 0.1186796141134301, test loss: 0.2531289254727936\n",
      "epoch 11868: train loss: 0.11867687812221998, test loss: 0.25312939852927946\n",
      "epoch 11869: train loss: 0.11867414245255968, test loss: 0.25312987175628765\n",
      "epoch 11870: train loss: 0.1186714071043821, test loss: 0.25313034515381405\n",
      "epoch 11871: train loss: 0.11866867207762005, test loss: 0.2531308187217954\n",
      "epoch 11872: train loss: 0.11866593737220646, test loss: 0.25313129246018284\n",
      "epoch 11873: train loss: 0.1186632029880742, test loss: 0.253131766368947\n",
      "epoch 11874: train loss: 0.11866046892515619, test loss: 0.25313224044804933\n",
      "epoch 11875: train loss: 0.11865773518338538, test loss: 0.2531327146974366\n",
      "epoch 11876: train loss: 0.11865500176269471, test loss: 0.25313318911706995\n",
      "epoch 11877: train loss: 0.11865226866301722, test loss: 0.25313366370692153\n",
      "epoch 11878: train loss: 0.11864953588428588, test loss: 0.25313413846692806\n",
      "epoch 11879: train loss: 0.11864680342643366, test loss: 0.25313461339704674\n",
      "epoch 11880: train loss: 0.1186440712893937, test loss: 0.2531350884972616\n",
      "epoch 11881: train loss: 0.11864133947309895, test loss: 0.2531355637675024\n",
      "epoch 11882: train loss: 0.11863860797748257, test loss: 0.2531360392077342\n",
      "epoch 11883: train loss: 0.11863587680247763, test loss: 0.25313651481792\n",
      "epoch 11884: train loss: 0.11863314594801726, test loss: 0.253136990598019\n",
      "epoch 11885: train loss: 0.11863041541403455, test loss: 0.25313746654797986\n",
      "epoch 11886: train loss: 0.11862768520046274, test loss: 0.2531379426677796\n",
      "epoch 11887: train loss: 0.11862495530723492, test loss: 0.2531384189573515\n",
      "epoch 11888: train loss: 0.11862222573428437, test loss: 0.2531388954166711\n",
      "epoch 11889: train loss: 0.11861949648154427, test loss: 0.2531393720456823\n",
      "epoch 11890: train loss: 0.11861676754894783, test loss: 0.2531398488443645\n",
      "epoch 11891: train loss: 0.11861403893642836, test loss: 0.25314032581265944\n",
      "epoch 11892: train loss: 0.1186113106439191, test loss: 0.25314080295052\n",
      "epoch 11893: train loss: 0.11860858267135332, test loss: 0.25314128025791666\n",
      "epoch 11894: train loss: 0.11860585501866443, test loss: 0.253141757734806\n",
      "epoch 11895: train loss: 0.11860312768578567, test loss: 0.25314223538114633\n",
      "epoch 11896: train loss: 0.11860040067265039, test loss: 0.2531427131968831\n",
      "epoch 11897: train loss: 0.11859767397919206, test loss: 0.2531431911819981\n",
      "epoch 11898: train loss: 0.118594947605344, test loss: 0.25314366933642524\n",
      "epoch 11899: train loss: 0.1185922215510396, test loss: 0.25314414766013343\n",
      "epoch 11900: train loss: 0.11858949581621238, test loss: 0.2531446261530825\n",
      "epoch 11901: train loss: 0.11858677040079574, test loss: 0.2531451048152213\n",
      "epoch 11902: train loss: 0.11858404530472313, test loss: 0.25314558364652223\n",
      "epoch 11903: train loss: 0.11858132052792807, test loss: 0.2531460626469304\n",
      "epoch 11904: train loss: 0.11857859607034404, test loss: 0.2531465418164159\n",
      "epoch 11905: train loss: 0.11857587193190464, test loss: 0.25314702115492355\n",
      "epoch 11906: train loss: 0.11857314811254334, test loss: 0.25314750066242536\n",
      "epoch 11907: train loss: 0.11857042461219378, test loss: 0.25314798033885894\n",
      "epoch 11908: train loss: 0.11856770143078948, test loss: 0.25314846018420784\n",
      "epoch 11909: train loss: 0.1185649785682641, test loss: 0.253148940198412\n",
      "epoch 11910: train loss: 0.11856225602455123, test loss: 0.2531494203814481\n",
      "epoch 11911: train loss: 0.11855953379958456, test loss: 0.25314990073325566\n",
      "epoch 11912: train loss: 0.1185568118932977, test loss: 0.25315038125379113\n",
      "epoch 11913: train loss: 0.1185540903056244, test loss: 0.2531508619430266\n",
      "epoch 11914: train loss: 0.11855136903649831, test loss: 0.25315134280092016\n",
      "epoch 11915: train loss: 0.11854864808585319, test loss: 0.25315182382743\n",
      "epoch 11916: train loss: 0.11854592745362275, test loss: 0.2531523050225017\n",
      "epoch 11917: train loss: 0.1185432071397408, test loss: 0.2531527863860996\n",
      "epoch 11918: train loss: 0.11854048714414107, test loss: 0.2531532679181958\n",
      "epoch 11919: train loss: 0.11853776746675741, test loss: 0.2531537496187213\n",
      "epoch 11920: train loss: 0.11853504810752362, test loss: 0.2531542314876542\n",
      "epoch 11921: train loss: 0.11853232906637358, test loss: 0.25315471352495345\n",
      "epoch 11922: train loss: 0.11852961034324107, test loss: 0.25315519573056455\n",
      "epoch 11923: train loss: 0.11852689193806004, test loss: 0.2531556781044584\n",
      "epoch 11924: train loss: 0.11852417385076434, test loss: 0.25315616064658697\n",
      "epoch 11925: train loss: 0.11852145608128793, test loss: 0.2531566433569227\n",
      "epoch 11926: train loss: 0.11851873862956472, test loss: 0.2531571262353959\n",
      "epoch 11927: train loss: 0.1185160214955287, test loss: 0.2531576092819872\n",
      "epoch 11928: train loss: 0.11851330467911379, test loss: 0.2531580924966536\n",
      "epoch 11929: train loss: 0.11851058818025408, test loss: 0.2531585758793539\n",
      "epoch 11930: train loss: 0.11850787199888348, test loss: 0.2531590594300271\n",
      "epoch 11931: train loss: 0.11850515613493609, test loss: 0.2531595431486454\n",
      "epoch 11932: train loss: 0.11850244058834594, test loss: 0.2531600270351807\n",
      "epoch 11933: train loss: 0.1184997253590471, test loss: 0.2531605110895705\n",
      "epoch 11934: train loss: 0.1184970104469737, test loss: 0.25316099531178893\n",
      "epoch 11935: train loss: 0.11849429585205978, test loss: 0.2531614797017794\n",
      "epoch 11936: train loss: 0.11849158157423957, test loss: 0.2531619642595079\n",
      "epoch 11937: train loss: 0.11848886761344715, test loss: 0.253162448984948\n",
      "epoch 11938: train loss: 0.11848615396961669, test loss: 0.2531629338780296\n",
      "epoch 11939: train loss: 0.1184834406426824, test loss: 0.25316341893873406\n",
      "epoch 11940: train loss: 0.11848072763257848, test loss: 0.25316390416699774\n",
      "epoch 11941: train loss: 0.11847801493923919, test loss: 0.2531643895628077\n",
      "epoch 11942: train loss: 0.11847530256259872, test loss: 0.253164875126097\n",
      "epoch 11943: train loss: 0.11847259050259136, test loss: 0.25316536085684227\n",
      "epoch 11944: train loss: 0.11846987875915144, test loss: 0.2531658467549908\n",
      "epoch 11945: train loss: 0.11846716733221321, test loss: 0.25316633282051476\n",
      "epoch 11946: train loss: 0.11846445622171099, test loss: 0.2531668190533536\n",
      "epoch 11947: train loss: 0.11846174542757915, test loss: 0.25316730545348465\n",
      "epoch 11948: train loss: 0.11845903494975206, test loss: 0.25316779202085526\n",
      "epoch 11949: train loss: 0.11845632478816408, test loss: 0.2531682787554245\n",
      "epoch 11950: train loss: 0.11845361494274961, test loss: 0.2531687656571448\n",
      "epoch 11951: train loss: 0.11845090541344307, test loss: 0.2531692527260008\n",
      "epoch 11952: train loss: 0.11844819620017892, test loss: 0.2531697399619206\n",
      "epoch 11953: train loss: 0.11844548730289159, test loss: 0.25317022736488104\n",
      "epoch 11954: train loss: 0.11844277872151558, test loss: 0.25317071493484367\n",
      "epoch 11955: train loss: 0.11844007045598538, test loss: 0.2531712026717526\n",
      "epoch 11956: train loss: 0.1184373625062355, test loss: 0.2531716905755741\n",
      "epoch 11957: train loss: 0.11843465487220048, test loss: 0.2531721786462753\n",
      "epoch 11958: train loss: 0.11843194755381486, test loss: 0.25317266688378814\n",
      "epoch 11959: train loss: 0.11842924055101323, test loss: 0.25317315528811213\n",
      "epoch 11960: train loss: 0.11842653386373017, test loss: 0.25317364385917357\n",
      "epoch 11961: train loss: 0.11842382749190032, test loss: 0.2531741325969377\n",
      "epoch 11962: train loss: 0.11842112143545826, test loss: 0.25317462150137865\n",
      "epoch 11963: train loss: 0.11841841569433868, test loss: 0.2531751105724346\n",
      "epoch 11964: train loss: 0.11841571026847625, test loss: 0.25317559981008\n",
      "epoch 11965: train loss: 0.1184130051578056, test loss: 0.25317608921425333\n",
      "epoch 11966: train loss: 0.11841030036226152, test loss: 0.25317657878494787\n",
      "epoch 11967: train loss: 0.1184075958817787, test loss: 0.2531770685220831\n",
      "epoch 11968: train loss: 0.11840489171629187, test loss: 0.2531775584256513\n",
      "epoch 11969: train loss: 0.1184021878657358, test loss: 0.2531780484956\n",
      "epoch 11970: train loss: 0.11839948433004527, test loss: 0.2531785387318888\n",
      "epoch 11971: train loss: 0.11839678110915507, test loss: 0.2531790291344636\n",
      "epoch 11972: train loss: 0.11839407820300009, test loss: 0.2531795197032906\n",
      "epoch 11973: train loss: 0.1183913756115151, test loss: 0.25318001043833005\n",
      "epoch 11974: train loss: 0.11838867333463497, test loss: 0.2531805013395686\n",
      "epoch 11975: train loss: 0.11838597137229458, test loss: 0.25318099240690606\n",
      "epoch 11976: train loss: 0.11838326972442886, test loss: 0.25318148364036075\n",
      "epoch 11977: train loss: 0.11838056839097268, test loss: 0.2531819750398554\n",
      "epoch 11978: train loss: 0.118377867371861, test loss: 0.2531824666053551\n",
      "epoch 11979: train loss: 0.11837516666702877, test loss: 0.2531829583368382\n",
      "epoch 11980: train loss: 0.11837246627641097, test loss: 0.25318345023423405\n",
      "epoch 11981: train loss: 0.11836976619994256, test loss: 0.2531839422975284\n",
      "epoch 11982: train loss: 0.11836706643755861, test loss: 0.2531844345266603\n",
      "epoch 11983: train loss: 0.11836436698919407, test loss: 0.25318492692159755\n",
      "epoch 11984: train loss: 0.11836166785478404, test loss: 0.2531854194823004\n",
      "epoch 11985: train loss: 0.11835896903426361, test loss: 0.2531859122087349\n",
      "epoch 11986: train loss: 0.11835627052756782, test loss: 0.253186405100841\n",
      "epoch 11987: train loss: 0.1183535723346318, test loss: 0.253186898158586\n",
      "epoch 11988: train loss: 0.11835087445539064, test loss: 0.25318739138195717\n",
      "epoch 11989: train loss: 0.11834817688977953, test loss: 0.2531878847708669\n",
      "epoch 11990: train loss: 0.11834547963773363, test loss: 0.25318837832529684\n",
      "epoch 11991: train loss: 0.11834278269918809, test loss: 0.2531888720452191\n",
      "epoch 11992: train loss: 0.1183400860740781, test loss: 0.25318936593056207\n",
      "epoch 11993: train loss: 0.11833738976233893, test loss: 0.2531898599813192\n",
      "epoch 11994: train loss: 0.11833469376390578, test loss: 0.25319035419741676\n",
      "epoch 11995: train loss: 0.11833199807871393, test loss: 0.25319084857884827\n",
      "epoch 11996: train loss: 0.11832930270669863, test loss: 0.25319134312554176\n",
      "epoch 11997: train loss: 0.11832660764779518, test loss: 0.25319183783747035\n",
      "epoch 11998: train loss: 0.11832391290193892, test loss: 0.25319233271460134\n",
      "epoch 11999: train loss: 0.11832121846906514, test loss: 0.25319282775688917\n",
      "epoch 12000: train loss: 0.11831852434910921, test loss: 0.2531933229642799\n",
      "epoch 12001: train loss: 0.11831583054200653, test loss: 0.2531938183367348\n",
      "epoch 12002: train loss: 0.11831313704769246, test loss: 0.2531943138742337\n",
      "epoch 12003: train loss: 0.11831044386610237, test loss: 0.2531948095767254\n",
      "epoch 12004: train loss: 0.11830775099717177, test loss: 0.253195305444162\n",
      "epoch 12005: train loss: 0.11830505844083601, test loss: 0.25319580147651133\n",
      "epoch 12006: train loss: 0.11830236619703063, test loss: 0.2531962976737286\n",
      "epoch 12007: train loss: 0.11829967426569109, test loss: 0.2531967940357793\n",
      "epoch 12008: train loss: 0.1182969826467529, test loss: 0.25319729056261325\n",
      "epoch 12009: train loss: 0.11829429134015154, test loss: 0.25319778725419545\n",
      "epoch 12010: train loss: 0.11829160034582259, test loss: 0.2531982841104885\n",
      "epoch 12011: train loss: 0.11828890966370156, test loss: 0.2531987811314382\n",
      "epoch 12012: train loss: 0.1182862192937241, test loss: 0.25319927831702577\n",
      "epoch 12013: train loss: 0.11828352923582575, test loss: 0.2531997756671939\n",
      "epoch 12014: train loss: 0.11828083948994216, test loss: 0.2532002731819081\n",
      "epoch 12015: train loss: 0.11827815005600889, test loss: 0.2532007708611227\n",
      "epoch 12016: train loss: 0.11827546093396167, test loss: 0.2532012687048064\n",
      "epoch 12017: train loss: 0.11827277212373616, test loss: 0.2532017667129136\n",
      "epoch 12018: train loss: 0.118270083625268, test loss: 0.25320226488539704\n",
      "epoch 12019: train loss: 0.11826739543849295, test loss: 0.25320276322223945\n",
      "epoch 12020: train loss: 0.11826470756334669, test loss: 0.25320326172337415\n",
      "epoch 12021: train loss: 0.118262019999765, test loss: 0.25320376038876236\n",
      "epoch 12022: train loss: 0.11825933274768363, test loss: 0.25320425921838596\n",
      "epoch 12023: train loss: 0.11825664580703837, test loss: 0.25320475821219174\n",
      "epoch 12024: train loss: 0.11825395917776502, test loss: 0.2532052573701341\n",
      "epoch 12025: train loss: 0.11825127285979936, test loss: 0.25320575669217515\n",
      "epoch 12026: train loss: 0.11824858685307732, test loss: 0.25320625617828246\n",
      "epoch 12027: train loss: 0.11824590115753467, test loss: 0.2532067558284033\n",
      "epoch 12028: train loss: 0.1182432157731073, test loss: 0.2532072556425133\n",
      "epoch 12029: train loss: 0.11824053069973112, test loss: 0.25320775562055303\n",
      "epoch 12030: train loss: 0.11823784593734203, test loss: 0.25320825576249695\n",
      "epoch 12031: train loss: 0.11823516148587598, test loss: 0.25320875606830595\n",
      "epoch 12032: train loss: 0.11823247734526894, test loss: 0.2532092565379241\n",
      "epoch 12033: train loss: 0.11822979351545682, test loss: 0.2532097571713357\n",
      "epoch 12034: train loss: 0.11822710999637565, test loss: 0.2532102579684655\n",
      "epoch 12035: train loss: 0.11822442678796141, test loss: 0.2532107589293063\n",
      "epoch 12036: train loss: 0.11822174389015014, test loss: 0.2532112600537994\n",
      "epoch 12037: train loss: 0.1182190613028779, test loss: 0.2532117613419144\n",
      "epoch 12038: train loss: 0.1182163790260807, test loss: 0.2532122627936048\n",
      "epoch 12039: train loss: 0.11821369705969469, test loss: 0.2532127644088391\n",
      "epoch 12040: train loss: 0.11821101540365589, test loss: 0.2532132661875727\n",
      "epoch 12041: train loss: 0.11820833405790047, test loss: 0.2532137681297539\n",
      "epoch 12042: train loss: 0.11820565302236455, test loss: 0.2532142702353508\n",
      "epoch 12043: train loss: 0.11820297229698426, test loss: 0.2532147725043383\n",
      "epoch 12044: train loss: 0.11820029188169581, test loss: 0.2532152749366445\n",
      "epoch 12045: train loss: 0.11819761177643541, test loss: 0.2532157775322583\n",
      "epoch 12046: train loss: 0.11819493198113921, test loss: 0.25321628029113447\n",
      "epoch 12047: train loss: 0.11819225249574349, test loss: 0.25321678321321434\n",
      "epoch 12048: train loss: 0.11818957332018444, test loss: 0.2532172862984738\n",
      "epoch 12049: train loss: 0.11818689445439837, test loss: 0.2532177895468815\n",
      "epoch 12050: train loss: 0.11818421589832155, test loss: 0.25321829295838433\n",
      "epoch 12051: train loss: 0.11818153765189031, test loss: 0.25321879653293317\n",
      "epoch 12052: train loss: 0.1181788597150409, test loss: 0.2532193002705077\n",
      "epoch 12053: train loss: 0.11817618208770973, test loss: 0.2532198041710514\n",
      "epoch 12054: train loss: 0.11817350476983313, test loss: 0.2532203082345322\n",
      "epoch 12055: train loss: 0.11817082776134746, test loss: 0.2532208124609192\n",
      "epoch 12056: train loss: 0.11816815106218914, test loss: 0.2532213168501605\n",
      "epoch 12057: train loss: 0.11816547467229459, test loss: 0.25322182140221083\n",
      "epoch 12058: train loss: 0.1181627985916002, test loss: 0.25322232611704026\n",
      "epoch 12059: train loss: 0.11816012282004244, test loss: 0.2532228309946104\n",
      "epoch 12060: train loss: 0.1181574473575578, test loss: 0.2532233360348772\n",
      "epoch 12061: train loss: 0.11815477220408274, test loss: 0.2532238412378009\n",
      "epoch 12062: train loss: 0.11815209735955377, test loss: 0.2532243466033325\n",
      "epoch 12063: train loss: 0.11814942282390742, test loss: 0.2532248521314466\n",
      "epoch 12064: train loss: 0.11814674859708024, test loss: 0.25322535782211225\n",
      "epoch 12065: train loss: 0.11814407467900877, test loss: 0.2532258636752577\n",
      "epoch 12066: train loss: 0.1181414010696296, test loss: 0.2532263696908658\n",
      "epoch 12067: train loss: 0.11813872776887932, test loss: 0.25322687586890513\n",
      "epoch 12068: train loss: 0.11813605477669455, test loss: 0.2532273822093183\n",
      "epoch 12069: train loss: 0.11813338209301191, test loss: 0.2532278887120556\n",
      "epoch 12070: train loss: 0.11813070971776807, test loss: 0.25322839537711006\n",
      "epoch 12071: train loss: 0.11812803765089969, test loss: 0.25322890220441235\n",
      "epoch 12072: train loss: 0.11812536589234346, test loss: 0.25322940919393777\n",
      "epoch 12073: train loss: 0.11812269444203612, test loss: 0.25322991634564246\n",
      "epoch 12074: train loss: 0.11812002329991433, test loss: 0.25323042365947473\n",
      "epoch 12075: train loss: 0.11811735246591486, test loss: 0.2532309311354185\n",
      "epoch 12076: train loss: 0.11811468193997449, test loss: 0.2532314387734288\n",
      "epoch 12077: train loss: 0.11811201172203, test loss: 0.25323194657344067\n",
      "epoch 12078: train loss: 0.11810934181201818, test loss: 0.2532324545354515\n",
      "epoch 12079: train loss: 0.11810667220987582, test loss: 0.25323296265940315\n",
      "epoch 12080: train loss: 0.11810400291553982, test loss: 0.2532334709452522\n",
      "epoch 12081: train loss: 0.11810133392894695, test loss: 0.25323397939295916\n",
      "epoch 12082: train loss: 0.11809866525003412, test loss: 0.2532344880024901\n",
      "epoch 12083: train loss: 0.11809599687873827, test loss: 0.253234996773805\n",
      "epoch 12084: train loss: 0.11809332881499623, test loss: 0.25323550570685455\n",
      "epoch 12085: train loss: 0.11809066105874497, test loss: 0.2532360148016271\n",
      "epoch 12086: train loss: 0.11808799360992142, test loss: 0.2532365240580468\n",
      "epoch 12087: train loss: 0.11808532646846252, test loss: 0.2532370334761021\n",
      "epoch 12088: train loss: 0.11808265963430532, test loss: 0.2532375430557287\n",
      "epoch 12089: train loss: 0.11807999310738676, test loss: 0.25323805279691153\n",
      "epoch 12090: train loss: 0.11807732688764382, test loss: 0.2532385626996041\n",
      "epoch 12091: train loss: 0.11807466097501365, test loss: 0.25323907276374524\n",
      "epoch 12092: train loss: 0.11807199536943322, test loss: 0.2532395829893231\n",
      "epoch 12093: train loss: 0.11806933007083963, test loss: 0.2532400933762944\n",
      "epoch 12094: train loss: 0.11806666507916994, test loss: 0.2532406039246016\n",
      "epoch 12095: train loss: 0.11806400039436131, test loss: 0.25324111463422133\n",
      "epoch 12096: train loss: 0.11806133601635083, test loss: 0.25324162550511\n",
      "epoch 12097: train loss: 0.11805867194507565, test loss: 0.2532421365372376\n",
      "epoch 12098: train loss: 0.11805600818047292, test loss: 0.25324264773054\n",
      "epoch 12099: train loss: 0.11805334472247984, test loss: 0.2532431590849942\n",
      "epoch 12100: train loss: 0.11805068157103364, test loss: 0.25324367060057007\n",
      "epoch 12101: train loss: 0.11804801872607147, test loss: 0.25324418227720913\n",
      "epoch 12102: train loss: 0.1180453561875306, test loss: 0.25324469411488343\n",
      "epoch 12103: train loss: 0.11804269395534832, test loss: 0.2532452061135492\n",
      "epoch 12104: train loss: 0.11804003202946182, test loss: 0.2532457182731618\n",
      "epoch 12105: train loss: 0.11803737040980844, test loss: 0.2532462305936985\n",
      "epoch 12106: train loss: 0.11803470909632548, test loss: 0.25324674307510214\n",
      "epoch 12107: train loss: 0.11803204808895028, test loss: 0.25324725571734397\n",
      "epoch 12108: train loss: 0.11802938738762016, test loss: 0.25324776852038544\n",
      "epoch 12109: train loss: 0.1180267269922725, test loss: 0.25324828148417877\n",
      "epoch 12110: train loss: 0.11802406690284466, test loss: 0.2532487946086925\n",
      "epoch 12111: train loss: 0.11802140711927406, test loss: 0.25324930789389\n",
      "epoch 12112: train loss: 0.11801874764149811, test loss: 0.2532498213397089\n",
      "epoch 12113: train loss: 0.11801608846945426, test loss: 0.25325033494614546\n",
      "epoch 12114: train loss: 0.1180134296030799, test loss: 0.2532508487131301\n",
      "epoch 12115: train loss: 0.11801077104231257, test loss: 0.2532513626406392\n",
      "epoch 12116: train loss: 0.11800811278708974, test loss: 0.25325187672863725\n",
      "epoch 12117: train loss: 0.11800545483734888, test loss: 0.25325239097705954\n",
      "epoch 12118: train loss: 0.11800279719302757, test loss: 0.25325290538590517\n",
      "epoch 12119: train loss: 0.11800013985406332, test loss: 0.2532534199551022\n",
      "epoch 12120: train loss: 0.11799748282039371, test loss: 0.2532539346846225\n",
      "epoch 12121: train loss: 0.11799482609195629, test loss: 0.25325444957444987\n",
      "epoch 12122: train loss: 0.1179921696686887, test loss: 0.2532549646245001\n",
      "epoch 12123: train loss: 0.1179895135505285, test loss: 0.25325547983476365\n",
      "epoch 12124: train loss: 0.11798685773741333, test loss: 0.2532559952052059\n",
      "epoch 12125: train loss: 0.11798420222928091, test loss: 0.25325651073576216\n",
      "epoch 12126: train loss: 0.11798154702606883, test loss: 0.25325702642641695\n",
      "epoch 12127: train loss: 0.1179788921277148, test loss: 0.25325754227712166\n",
      "epoch 12128: train loss: 0.11797623753415655, test loss: 0.25325805828784476\n",
      "epoch 12129: train loss: 0.11797358324533176, test loss: 0.2532585744585253\n",
      "epoch 12130: train loss: 0.11797092926117822, test loss: 0.25325909078915365\n",
      "epoch 12131: train loss: 0.11796827558163363, test loss: 0.25325960727967284\n",
      "epoch 12132: train loss: 0.11796562220663581, test loss: 0.2532601239300471\n",
      "epoch 12133: train loss: 0.11796296913612256, test loss: 0.2532606407402269\n",
      "epoch 12134: train loss: 0.11796031637003164, test loss: 0.2532611577101906\n",
      "epoch 12135: train loss: 0.11795766390830094, test loss: 0.2532616748399074\n",
      "epoch 12136: train loss: 0.11795501175086827, test loss: 0.25326219212930284\n",
      "epoch 12137: train loss: 0.1179523598976715, test loss: 0.25326270957836683\n",
      "epoch 12138: train loss: 0.11794970834864853, test loss: 0.25326322718705724\n",
      "epoch 12139: train loss: 0.11794705710373725, test loss: 0.2532637449553241\n",
      "epoch 12140: train loss: 0.11794440616287559, test loss: 0.253264262883146\n",
      "epoch 12141: train loss: 0.11794175552600146, test loss: 0.25326478097045907\n",
      "epoch 12142: train loss: 0.11793910519305285, test loss: 0.25326529921724233\n",
      "epoch 12143: train loss: 0.11793645516396768, test loss: 0.2532658176234519\n",
      "epoch 12144: train loss: 0.11793380543868405, test loss: 0.25326633618903976\n",
      "epoch 12145: train loss: 0.11793115601713983, test loss: 0.25326685491399664\n",
      "epoch 12146: train loss: 0.11792850689927313, test loss: 0.25326737379824726\n",
      "epoch 12147: train loss: 0.117925858085022, test loss: 0.2532678928417681\n",
      "epoch 12148: train loss: 0.11792320957432448, test loss: 0.2532684120445314\n",
      "epoch 12149: train loss: 0.1179205613671186, test loss: 0.253268931406488\n",
      "epoch 12150: train loss: 0.11791791346334257, test loss: 0.25326945092760134\n",
      "epoch 12151: train loss: 0.1179152658629344, test loss: 0.25326997060782963\n",
      "epoch 12152: train loss: 0.1179126185658323, test loss: 0.2532704904471254\n",
      "epoch 12153: train loss: 0.11790997157197439, test loss: 0.25327101044546396\n",
      "epoch 12154: train loss: 0.11790732488129878, test loss: 0.25327153060281193\n",
      "epoch 12155: train loss: 0.11790467849374377, test loss: 0.25327205091910554\n",
      "epoch 12156: train loss: 0.11790203240924747, test loss: 0.25327257139432485\n",
      "epoch 12157: train loss: 0.11789938662774818, test loss: 0.2532730920284323\n",
      "epoch 12158: train loss: 0.11789674114918407, test loss: 0.2532736128213806\n",
      "epoch 12159: train loss: 0.11789409597349344, test loss: 0.2532741337731398\n",
      "epoch 12160: train loss: 0.11789145110061455, test loss: 0.2532746548836623\n",
      "epoch 12161: train loss: 0.11788880653048572, test loss: 0.2532751761529055\n",
      "epoch 12162: train loss: 0.11788616226304521, test loss: 0.2532756975808552\n",
      "epoch 12163: train loss: 0.11788351829823139, test loss: 0.25327621916743387\n",
      "epoch 12164: train loss: 0.1178808746359826, test loss: 0.25327674091263525\n",
      "epoch 12165: train loss: 0.11787823127623719, test loss: 0.25327726281641616\n",
      "epoch 12166: train loss: 0.11787558821893356, test loss: 0.25327778487872854\n",
      "epoch 12167: train loss: 0.11787294546401009, test loss: 0.253278307099544\n",
      "epoch 12168: train loss: 0.11787030301140522, test loss: 0.2532788294788008\n",
      "epoch 12169: train loss: 0.11786766086105736, test loss: 0.2532793520164966\n",
      "epoch 12170: train loss: 0.117865019012905, test loss: 0.25327987471255653\n",
      "epoch 12171: train loss: 0.11786237746688656, test loss: 0.25328039756696624\n",
      "epoch 12172: train loss: 0.11785973622294055, test loss: 0.25328092057968304\n",
      "epoch 12173: train loss: 0.11785709528100549, test loss: 0.2532814437506456\n",
      "epoch 12174: train loss: 0.11785445464101989, test loss: 0.25328196707985273\n",
      "epoch 12175: train loss: 0.1178518143029223, test loss: 0.25328249056723456\n",
      "epoch 12176: train loss: 0.11784917426665126, test loss: 0.25328301421277155\n",
      "epoch 12177: train loss: 0.11784653453214539, test loss: 0.2532835380164277\n",
      "epoch 12178: train loss: 0.11784389509934322, test loss: 0.2532840619781424\n",
      "epoch 12179: train loss: 0.1178412559681834, test loss: 0.2532845860979056\n",
      "epoch 12180: train loss: 0.11783861713860456, test loss: 0.2532851103756435\n",
      "epoch 12181: train loss: 0.11783597861054534, test loss: 0.2532856348113483\n",
      "epoch 12182: train loss: 0.11783334038394443, test loss: 0.25328615940497207\n",
      "epoch 12183: train loss: 0.11783070245874047, test loss: 0.2532866841564795\n",
      "epoch 12184: train loss: 0.11782806483487217, test loss: 0.2532872090658218\n",
      "epoch 12185: train loss: 0.11782542751227826, test loss: 0.2532877341329791\n",
      "epoch 12186: train loss: 0.11782279049089749, test loss: 0.2532882593578891\n",
      "epoch 12187: train loss: 0.11782015377066855, test loss: 0.2532887847405103\n",
      "epoch 12188: train loss: 0.11781751735153029, test loss: 0.2532893102808355\n",
      "epoch 12189: train loss: 0.11781488123342143, test loss: 0.2532898359788237\n",
      "epoch 12190: train loss: 0.11781224541628081, test loss: 0.2532903618343983\n",
      "epoch 12191: train loss: 0.11780960990004724, test loss: 0.25329088784756026\n",
      "epoch 12192: train loss: 0.11780697468465959, test loss: 0.2532914140182479\n",
      "epoch 12193: train loss: 0.11780433977005668, test loss: 0.2532919403464321\n",
      "epoch 12194: train loss: 0.11780170515617738, test loss: 0.2532924668320796\n",
      "epoch 12195: train loss: 0.11779907084296064, test loss: 0.2532929934751491\n",
      "epoch 12196: train loss: 0.11779643683034532, test loss: 0.25329352027559304\n",
      "epoch 12197: train loss: 0.11779380311827037, test loss: 0.25329404723337634\n",
      "epoch 12198: train loss: 0.1177911697066747, test loss: 0.25329457434846464\n",
      "epoch 12199: train loss: 0.11778853659549732, test loss: 0.2532951016208244\n",
      "epoch 12200: train loss: 0.11778590378467718, test loss: 0.2532956290504005\n",
      "epoch 12201: train loss: 0.1177832712741533, test loss: 0.2532961566371719\n",
      "epoch 12202: train loss: 0.11778063906386467, test loss: 0.25329668438109815\n",
      "epoch 12203: train loss: 0.11777800715375034, test loss: 0.2532972122821384\n",
      "epoch 12204: train loss: 0.11777537554374934, test loss: 0.2532977403402576\n",
      "epoch 12205: train loss: 0.11777274423380076, test loss: 0.2532982685553955\n",
      "epoch 12206: train loss: 0.11777011322384368, test loss: 0.25329879692755186\n",
      "epoch 12207: train loss: 0.11776748251381719, test loss: 0.2532993254566575\n",
      "epoch 12208: train loss: 0.11776485210366044, test loss: 0.2532998541426798\n",
      "epoch 12209: train loss: 0.11776222199331254, test loss: 0.2533003829855907\n",
      "epoch 12210: train loss: 0.11775959218271265, test loss: 0.25330091198535615\n",
      "epoch 12211: train loss: 0.11775696267179991, test loss: 0.2533014411419288\n",
      "epoch 12212: train loss: 0.11775433346051357, test loss: 0.2533019704552537\n",
      "epoch 12213: train loss: 0.11775170454879284, test loss: 0.25330249992532544\n",
      "epoch 12214: train loss: 0.11774907593657687, test loss: 0.2533030295520885\n",
      "epoch 12215: train loss: 0.11774644762380497, test loss: 0.2533035593355031\n",
      "epoch 12216: train loss: 0.11774381961041637, test loss: 0.25330408927552694\n",
      "epoch 12217: train loss: 0.11774119189635034, test loss: 0.25330461937214876\n",
      "epoch 12218: train loss: 0.11773856448154621, test loss: 0.25330514962529277\n",
      "epoch 12219: train loss: 0.11773593736594327, test loss: 0.2533056800349457\n",
      "epoch 12220: train loss: 0.11773331054948083, test loss: 0.2533062106010734\n",
      "epoch 12221: train loss: 0.11773068403209823, test loss: 0.25330674132361625\n",
      "epoch 12222: train loss: 0.11772805781373491, test loss: 0.25330727220255894\n",
      "epoch 12223: train loss: 0.11772543189433017, test loss: 0.25330780323784224\n",
      "epoch 12224: train loss: 0.11772280627382344, test loss: 0.2533083344294455\n",
      "epoch 12225: train loss: 0.11772018095215411, test loss: 0.25330886577731493\n",
      "epoch 12226: train loss: 0.11771755592926166, test loss: 0.25330939728142954\n",
      "epoch 12227: train loss: 0.11771493120508549, test loss: 0.25330992894173054\n",
      "epoch 12228: train loss: 0.1177123067795651, test loss: 0.2533104607582029\n",
      "epoch 12229: train loss: 0.11770968265263995, test loss: 0.2533109927308002\n",
      "epoch 12230: train loss: 0.11770705882424956, test loss: 0.2533115248594742\n",
      "epoch 12231: train loss: 0.11770443529433347, test loss: 0.25331205714420724\n",
      "epoch 12232: train loss: 0.11770181206283117, test loss: 0.2533125895849367\n",
      "epoch 12233: train loss: 0.11769918912968225, test loss: 0.253313122181651\n",
      "epoch 12234: train loss: 0.11769656649482627, test loss: 0.25331365493428853\n",
      "epoch 12235: train loss: 0.11769394415820279, test loss: 0.2533141878428227\n",
      "epoch 12236: train loss: 0.11769132211975146, test loss: 0.2533147209072212\n",
      "epoch 12237: train loss: 0.11768870037941187, test loss: 0.2533152541274351\n",
      "epoch 12238: train loss: 0.11768607893712371, test loss: 0.2533157875034268\n",
      "epoch 12239: train loss: 0.11768345779282656, test loss: 0.2533163210351608\n",
      "epoch 12240: train loss: 0.11768083694646016, test loss: 0.2533168547226116\n",
      "epoch 12241: train loss: 0.11767821639796416, test loss: 0.2533173885657324\n",
      "epoch 12242: train loss: 0.11767559614727834, test loss: 0.25331792256447694\n",
      "epoch 12243: train loss: 0.1176729761943423, test loss: 0.2533184567188242\n",
      "epoch 12244: train loss: 0.1176703565390959, test loss: 0.2533189910287081\n",
      "epoch 12245: train loss: 0.11766773718147885, test loss: 0.25331952549412945\n",
      "epoch 12246: train loss: 0.11766511812143092, test loss: 0.2533200601150137\n",
      "epoch 12247: train loss: 0.11766249935889192, test loss: 0.2533205948913552\n",
      "epoch 12248: train loss: 0.11765988089380168, test loss: 0.2533211298230935\n",
      "epoch 12249: train loss: 0.11765726272610001, test loss: 0.25332166491020364\n",
      "epoch 12250: train loss: 0.11765464485572676, test loss: 0.25332220015263046\n",
      "epoch 12251: train loss: 0.11765202728262177, test loss: 0.2533227355503697\n",
      "epoch 12252: train loss: 0.11764941000672495, test loss: 0.25332327110334574\n",
      "epoch 12253: train loss: 0.1176467930279762, test loss: 0.25332380681153366\n",
      "epoch 12254: train loss: 0.11764417634631542, test loss: 0.25332434267491466\n",
      "epoch 12255: train loss: 0.11764155996168256, test loss: 0.25332487869344145\n",
      "epoch 12256: train loss: 0.11763894387401756, test loss: 0.25332541486706006\n",
      "epoch 12257: train loss: 0.11763632808326037, test loss: 0.2533259511957457\n",
      "epoch 12258: train loss: 0.117633712589351, test loss: 0.2533264876794508\n",
      "epoch 12259: train loss: 0.11763109739222945, test loss: 0.25332702431815163\n",
      "epoch 12260: train loss: 0.1176284824918357, test loss: 0.253327561111806\n",
      "epoch 12261: train loss: 0.11762586788810982, test loss: 0.25332809806038936\n",
      "epoch 12262: train loss: 0.11762325358099186, test loss: 0.253328635163834\n",
      "epoch 12263: train loss: 0.11762063957042189, test loss: 0.25332917242211495\n",
      "epoch 12264: train loss: 0.11761802585633997, test loss: 0.25332970983521264\n",
      "epoch 12265: train loss: 0.11761541243868624, test loss: 0.2533302474030609\n",
      "epoch 12266: train loss: 0.1176127993174008, test loss: 0.2533307851256535\n",
      "epoch 12267: train loss: 0.11761018649242379, test loss: 0.2533313230029315\n",
      "epoch 12268: train loss: 0.11760757396369537, test loss: 0.2533318610348551\n",
      "epoch 12269: train loss: 0.1176049617311557, test loss: 0.2533323992213983\n",
      "epoch 12270: train loss: 0.11760234979474499, test loss: 0.2533329375625087\n",
      "epoch 12271: train loss: 0.11759973815440343, test loss: 0.2533334760581808\n",
      "epoch 12272: train loss: 0.11759712681007123, test loss: 0.2533340147083412\n",
      "epoch 12273: train loss: 0.11759451576168865, test loss: 0.25333455351296463\n",
      "epoch 12274: train loss: 0.11759190500919596, test loss: 0.2533350924720245\n",
      "epoch 12275: train loss: 0.11758929455253339, test loss: 0.25333563158546213\n",
      "epoch 12276: train loss: 0.11758668439164126, test loss: 0.2533361708532725\n",
      "epoch 12277: train loss: 0.1175840745264599, test loss: 0.25333671027538096\n",
      "epoch 12278: train loss: 0.11758146495692959, test loss: 0.25333724985177014\n",
      "epoch 12279: train loss: 0.1175788556829907, test loss: 0.25333778958240805\n",
      "epoch 12280: train loss: 0.11757624670458355, test loss: 0.25333832946723306\n",
      "epoch 12281: train loss: 0.11757363802164857, test loss: 0.2533388695062424\n",
      "epoch 12282: train loss: 0.11757102963412612, test loss: 0.2533394096993767\n",
      "epoch 12283: train loss: 0.11756842154195662, test loss: 0.253339950046595\n",
      "epoch 12284: train loss: 0.11756581374508052, test loss: 0.25334049054787305\n",
      "epoch 12285: train loss: 0.1175632062434382, test loss: 0.25334103120317875\n",
      "epoch 12286: train loss: 0.11756059903697018, test loss: 0.2533415720124453\n",
      "epoch 12287: train loss: 0.1175579921256169, test loss: 0.2533421129756616\n",
      "epoch 12288: train loss: 0.11755538550931888, test loss: 0.2533426540927826\n",
      "epoch 12289: train loss: 0.11755277918801664, test loss: 0.25334319536376815\n",
      "epoch 12290: train loss: 0.11755017316165067, test loss: 0.2533437367885799\n",
      "epoch 12291: train loss: 0.11754756743016152, test loss: 0.2533442783671989\n",
      "epoch 12292: train loss: 0.11754496199348981, test loss: 0.2533448200995603\n",
      "epoch 12293: train loss: 0.11754235685157606, test loss: 0.25334536198565183\n",
      "epoch 12294: train loss: 0.11753975200436088, test loss: 0.25334590402542817\n",
      "epoch 12295: train loss: 0.11753714745178488, test loss: 0.25334644621883695\n",
      "epoch 12296: train loss: 0.11753454319378871, test loss: 0.25334698856585225\n",
      "epoch 12297: train loss: 0.11753193923031297, test loss: 0.2533475310664365\n",
      "epoch 12298: train loss: 0.11752933556129841, test loss: 0.25334807372057183\n",
      "epoch 12299: train loss: 0.11752673218668562, test loss: 0.2533486165281772\n",
      "epoch 12300: train loss: 0.11752412910641535, test loss: 0.2533491594892553\n",
      "epoch 12301: train loss: 0.1175215263204283, test loss: 0.2533497026037616\n",
      "epoch 12302: train loss: 0.11751892382866519, test loss: 0.2533502458716436\n",
      "epoch 12303: train loss: 0.11751632163106678, test loss: 0.25335078929287597\n",
      "epoch 12304: train loss: 0.11751371972757382, test loss: 0.2533513328674065\n",
      "epoch 12305: train loss: 0.11751111811812709, test loss: 0.25335187659523156\n",
      "epoch 12306: train loss: 0.11750851680266741, test loss: 0.2533524204762714\n",
      "epoch 12307: train loss: 0.11750591578113562, test loss: 0.2533529645105284\n",
      "epoch 12308: train loss: 0.11750331505347252, test loss: 0.2533535086979432\n",
      "epoch 12309: train loss: 0.11750071461961892, test loss: 0.2533540530384708\n",
      "epoch 12310: train loss: 0.11749811447951572, test loss: 0.2533545975320867\n",
      "epoch 12311: train loss: 0.11749551463310383, test loss: 0.2533551421787608\n",
      "epoch 12312: train loss: 0.1174929150803241, test loss: 0.25335568697845223\n",
      "epoch 12313: train loss: 0.11749031582111749, test loss: 0.2533562319311104\n",
      "epoch 12314: train loss: 0.1174877168554249, test loss: 0.2533567770367181\n",
      "epoch 12315: train loss: 0.11748511818318731, test loss: 0.2533573222952212\n",
      "epoch 12316: train loss: 0.11748251980434564, test loss: 0.2533578677065977\n",
      "epoch 12317: train loss: 0.11747992171884093, test loss: 0.25335841327079406\n",
      "epoch 12318: train loss: 0.11747732392661411, test loss: 0.25335895898778704\n",
      "epoch 12319: train loss: 0.11747472642760627, test loss: 0.2533595048575379\n",
      "epoch 12320: train loss: 0.11747212922175841, test loss: 0.25336005088000735\n",
      "epoch 12321: train loss: 0.11746953230901157, test loss: 0.2533605970551519\n",
      "epoch 12322: train loss: 0.11746693568930684, test loss: 0.25336114338294713\n",
      "epoch 12323: train loss: 0.11746433936258527, test loss: 0.25336168986334\n",
      "epoch 12324: train loss: 0.11746174332878798, test loss: 0.2533622364963074\n",
      "epoch 12325: train loss: 0.1174591475878561, test loss: 0.25336278328181117\n",
      "epoch 12326: train loss: 0.11745655213973077, test loss: 0.25336333021981394\n",
      "epoch 12327: train loss: 0.1174539569843531, test loss: 0.25336387731028276\n",
      "epoch 12328: train loss: 0.11745136212166425, test loss: 0.25336442455315333\n",
      "epoch 12329: train loss: 0.11744876755160547, test loss: 0.2533649719484215\n",
      "epoch 12330: train loss: 0.1174461732741179, test loss: 0.25336551949604386\n",
      "epoch 12331: train loss: 0.11744357928914279, test loss: 0.25336606719596766\n",
      "epoch 12332: train loss: 0.11744098559662136, test loss: 0.25336661504817554\n",
      "epoch 12333: train loss: 0.11743839219649488, test loss: 0.25336716305262347\n",
      "epoch 12334: train loss: 0.11743579908870456, test loss: 0.2533677112092738\n",
      "epoch 12335: train loss: 0.11743320627319175, test loss: 0.2533682595180877\n",
      "epoch 12336: train loss: 0.1174306137498977, test loss: 0.25336880797902817\n",
      "epoch 12337: train loss: 0.11742802151876379, test loss: 0.25336935659205795\n",
      "epoch 12338: train loss: 0.11742542957973125, test loss: 0.25336990535715825\n",
      "epoch 12339: train loss: 0.11742283793274154, test loss: 0.25337045427425875\n",
      "epoch 12340: train loss: 0.11742024657773598, test loss: 0.25337100334334883\n",
      "epoch 12341: train loss: 0.11741765551465594, test loss: 0.25337155256438365\n",
      "epoch 12342: train loss: 0.11741506474344286, test loss: 0.2533721019373324\n",
      "epoch 12343: train loss: 0.1174124742640381, test loss: 0.2533726514621437\n",
      "epoch 12344: train loss: 0.11740988407638313, test loss: 0.25337320113880124\n",
      "epoch 12345: train loss: 0.11740729418041937, test loss: 0.25337375096724707\n",
      "epoch 12346: train loss: 0.11740470457608834, test loss: 0.25337430094745655\n",
      "epoch 12347: train loss: 0.11740211526333148, test loss: 0.25337485107939905\n",
      "epoch 12348: train loss: 0.1173995262420903, test loss: 0.2533754013630163\n",
      "epoch 12349: train loss: 0.11739693751230633, test loss: 0.2533759517982993\n",
      "epoch 12350: train loss: 0.11739434907392109, test loss: 0.2533765023851906\n",
      "epoch 12351: train loss: 0.11739176092687612, test loss: 0.2533770531236571\n",
      "epoch 12352: train loss: 0.117389173071113, test loss: 0.2533776040136779\n",
      "epoch 12353: train loss: 0.1173865855065733, test loss: 0.25337815505520633\n",
      "epoch 12354: train loss: 0.11738399823319863, test loss: 0.25337870624818687\n",
      "epoch 12355: train loss: 0.11738141125093059, test loss: 0.25337925759260815\n",
      "epoch 12356: train loss: 0.11737882455971083, test loss: 0.25337980908843305\n",
      "epoch 12357: train loss: 0.11737623815948098, test loss: 0.2533803607356112\n",
      "epoch 12358: train loss: 0.11737365205018274, test loss: 0.2533809125341045\n",
      "epoch 12359: train loss: 0.11737106623175775, test loss: 0.25338146448389\n",
      "epoch 12360: train loss: 0.11736848070414771, test loss: 0.25338201658493065\n",
      "epoch 12361: train loss: 0.11736589546729437, test loss: 0.2533825688371822\n",
      "epoch 12362: train loss: 0.11736331052113944, test loss: 0.2533831212406145\n",
      "epoch 12363: train loss: 0.11736072586562468, test loss: 0.25338367379517607\n",
      "epoch 12364: train loss: 0.11735814150069183, test loss: 0.2533842265008513\n",
      "epoch 12365: train loss: 0.11735555742628266, test loss: 0.25338477935758924\n",
      "epoch 12366: train loss: 0.11735297364233901, test loss: 0.25338533236536714\n",
      "epoch 12367: train loss: 0.11735039014880268, test loss: 0.253385885524132\n",
      "epoch 12368: train loss: 0.11734780694561547, test loss: 0.2533864388338632\n",
      "epoch 12369: train loss: 0.11734522403271924, test loss: 0.2533869922945082\n",
      "epoch 12370: train loss: 0.11734264141005588, test loss: 0.2533875459060452\n",
      "epoch 12371: train loss: 0.11734005907756724, test loss: 0.25338809966841597\n",
      "epoch 12372: train loss: 0.11733747703519523, test loss: 0.2533886535816245\n",
      "epoch 12373: train loss: 0.11733489528288174, test loss: 0.2533892076455996\n",
      "epoch 12374: train loss: 0.11733231382056872, test loss: 0.25338976186029805\n",
      "epoch 12375: train loss: 0.11732973264819811, test loss: 0.25339031622572433\n",
      "epoch 12376: train loss: 0.11732715176571187, test loss: 0.25339087074181427\n",
      "epoch 12377: train loss: 0.11732457117305199, test loss: 0.2533914254085229\n",
      "epoch 12378: train loss: 0.11732199087016042, test loss: 0.25339198022582826\n",
      "epoch 12379: train loss: 0.11731941085697922, test loss: 0.25339253519370764\n",
      "epoch 12380: train loss: 0.11731683113345039, test loss: 0.25339309031209667\n",
      "epoch 12381: train loss: 0.11731425169951598, test loss: 0.25339364558098454\n",
      "epoch 12382: train loss: 0.11731167255511804, test loss: 0.25339420100030957\n",
      "epoch 12383: train loss: 0.11730909370019868, test loss: 0.25339475657005445\n",
      "epoch 12384: train loss: 0.11730651513469993, test loss: 0.2533953122901692\n",
      "epoch 12385: train loss: 0.11730393685856395, test loss: 0.2533958681606306\n",
      "epoch 12386: train loss: 0.11730135887173286, test loss: 0.2533964241814026\n",
      "epoch 12387: train loss: 0.11729878117414877, test loss: 0.25339698035243374\n",
      "epoch 12388: train loss: 0.1172962037657539, test loss: 0.25339753667370934\n",
      "epoch 12389: train loss: 0.11729362664649035, test loss: 0.2533980931451789\n",
      "epoch 12390: train loss: 0.11729104981630034, test loss: 0.25339864976680504\n",
      "epoch 12391: train loss: 0.11728847327512609, test loss: 0.2533992065385459\n",
      "epoch 12392: train loss: 0.1172858970229098, test loss: 0.25339976346039195\n",
      "epoch 12393: train loss: 0.11728332105959371, test loss: 0.253400320532279\n",
      "epoch 12394: train loss: 0.11728074538512011, test loss: 0.2534008777541769\n",
      "epoch 12395: train loss: 0.11727816999943123, test loss: 0.25340143512606533\n",
      "epoch 12396: train loss: 0.11727559490246939, test loss: 0.253401992647891\n",
      "epoch 12397: train loss: 0.11727302009417684, test loss: 0.25340255031963493\n",
      "epoch 12398: train loss: 0.117270445574496, test loss: 0.25340310814124434\n",
      "epoch 12399: train loss: 0.11726787134336909, test loss: 0.2534036661126902\n",
      "epoch 12400: train loss: 0.11726529740073854, test loss: 0.2534042242339373\n",
      "epoch 12401: train loss: 0.1172627237465467, test loss: 0.25340478250493564\n",
      "epoch 12402: train loss: 0.11726015038073595, test loss: 0.2534053409256676\n",
      "epoch 12403: train loss: 0.11725757730324869, test loss: 0.2534058994960998\n",
      "epoch 12404: train loss: 0.11725500451402734, test loss: 0.2534064582161818\n",
      "epoch 12405: train loss: 0.11725243201301433, test loss: 0.2534070170858676\n",
      "epoch 12406: train loss: 0.11724985980015212, test loss: 0.2534075761051529\n",
      "epoch 12407: train loss: 0.11724728787538317, test loss: 0.2534081352739856\n",
      "epoch 12408: train loss: 0.11724471623864999, test loss: 0.2534086945923223\n",
      "epoch 12409: train loss: 0.11724214488989503, test loss: 0.2534092540601417\n",
      "epoch 12410: train loss: 0.11723957382906086, test loss: 0.25340981367740134\n",
      "epoch 12411: train loss: 0.11723700305608997, test loss: 0.2534103734440506\n",
      "epoch 12412: train loss: 0.11723443257092489, test loss: 0.2534109333600803\n",
      "epoch 12413: train loss: 0.11723186237350823, test loss: 0.2534114934254351\n",
      "epoch 12414: train loss: 0.11722929246378254, test loss: 0.2534120536400933\n",
      "epoch 12415: train loss: 0.11722672284169044, test loss: 0.2534126140040042\n",
      "epoch 12416: train loss: 0.11722415350717455, test loss: 0.2534131745171388\n",
      "epoch 12417: train loss: 0.11722158446017744, test loss: 0.2534137351794552\n",
      "epoch 12418: train loss: 0.11721901570064183, test loss: 0.25341429599093\n",
      "epoch 12419: train loss: 0.11721644722851034, test loss: 0.2534148569515292\n",
      "epoch 12420: train loss: 0.11721387904372563, test loss: 0.2534154180612015\n",
      "epoch 12421: train loss: 0.11721131114623043, test loss: 0.253415979319919\n",
      "epoch 12422: train loss: 0.11720874353596746, test loss: 0.2534165407276469\n",
      "epoch 12423: train loss: 0.11720617621287938, test loss: 0.25341710228434716\n",
      "epoch 12424: train loss: 0.11720360917690897, test loss: 0.253417663989971\n",
      "epoch 12425: train loss: 0.117201042427999, test loss: 0.2534182258445117\n",
      "epoch 12426: train loss: 0.11719847596609222, test loss: 0.2534187878479054\n",
      "epoch 12427: train loss: 0.11719590979113143, test loss: 0.25341935000013666\n",
      "epoch 12428: train loss: 0.11719334390305944, test loss: 0.25341991230117\n",
      "epoch 12429: train loss: 0.11719077830181909, test loss: 0.2534204747509423\n",
      "epoch 12430: train loss: 0.11718821298735317, test loss: 0.2534210373494527\n",
      "epoch 12431: train loss: 0.1171856479596046, test loss: 0.2534216000966458\n",
      "epoch 12432: train loss: 0.11718308321851616, test loss: 0.25342216299248377\n",
      "epoch 12433: train loss: 0.11718051876403082, test loss: 0.2534227260369476\n",
      "epoch 12434: train loss: 0.11717795459609144, test loss: 0.25342328922998\n",
      "epoch 12435: train loss: 0.11717539071464095, test loss: 0.25342385257155103\n",
      "epoch 12436: train loss: 0.11717282711962229, test loss: 0.2534244160616421\n",
      "epoch 12437: train loss: 0.1171702638109784, test loss: 0.2534249797002156\n",
      "epoch 12438: train loss: 0.11716770078865224, test loss: 0.25342554348720175\n",
      "epoch 12439: train loss: 0.11716513805258683, test loss: 0.25342610742259986\n",
      "epoch 12440: train loss: 0.1171625756027251, test loss: 0.2534266715063766\n",
      "epoch 12441: train loss: 0.11716001343901013, test loss: 0.25342723573846593\n",
      "epoch 12442: train loss: 0.1171574515613849, test loss: 0.25342780011885574\n",
      "epoch 12443: train loss: 0.1171548899697925, test loss: 0.2534283646475026\n",
      "epoch 12444: train loss: 0.11715232866417598, test loss: 0.25342892932437056\n",
      "epoch 12445: train loss: 0.11714976764447842, test loss: 0.2534294941494193\n",
      "epoch 12446: train loss: 0.1171472069106429, test loss: 0.25343005912263455\n",
      "epoch 12447: train loss: 0.11714464646261254, test loss: 0.2534306242439513\n",
      "epoch 12448: train loss: 0.11714208630033046, test loss: 0.25343118951336385\n",
      "epoch 12449: train loss: 0.1171395264237398, test loss: 0.25343175493081566\n",
      "epoch 12450: train loss: 0.11713696683278375, test loss: 0.2534323204962796\n",
      "epoch 12451: train loss: 0.11713440752740546, test loss: 0.25343288620970505\n",
      "epoch 12452: train loss: 0.11713184850754808, test loss: 0.2534334520710778\n",
      "epoch 12453: train loss: 0.11712928977315488, test loss: 0.2534340180803587\n",
      "epoch 12454: train loss: 0.11712673132416908, test loss: 0.2534345842374956\n",
      "epoch 12455: train loss: 0.11712417316053388, test loss: 0.253435150542483\n",
      "epoch 12456: train loss: 0.11712161528219255, test loss: 0.25343571699525075\n",
      "epoch 12457: train loss: 0.11711905768908837, test loss: 0.2534362835957852\n",
      "epoch 12458: train loss: 0.11711650038116464, test loss: 0.2534368503440306\n",
      "epoch 12459: train loss: 0.11711394335836463, test loss: 0.25343741723999225\n",
      "epoch 12460: train loss: 0.11711138662063164, test loss: 0.2534379842835888\n",
      "epoch 12461: train loss: 0.11710883016790906, test loss: 0.25343855147480504\n",
      "epoch 12462: train loss: 0.1171062740001402, test loss: 0.2534391188136213\n",
      "epoch 12463: train loss: 0.11710371811726844, test loss: 0.2534396862999659\n",
      "epoch 12464: train loss: 0.11710116251923715, test loss: 0.2534402539338466\n",
      "epoch 12465: train loss: 0.11709860720598975, test loss: 0.2534408217151812\n",
      "epoch 12466: train loss: 0.11709605217746966, test loss: 0.2534413896439687\n",
      "epoch 12467: train loss: 0.11709349743362026, test loss: 0.25344195772017514\n",
      "epoch 12468: train loss: 0.11709094297438502, test loss: 0.2534425259437295\n",
      "epoch 12469: train loss: 0.11708838879970741, test loss: 0.25344309431463347\n",
      "epoch 12470: train loss: 0.11708583490953094, test loss: 0.25344366283284614\n",
      "epoch 12471: train loss: 0.11708328130379902, test loss: 0.25344423149830975\n",
      "epoch 12472: train loss: 0.11708072798245522, test loss: 0.2534448003110045\n",
      "epoch 12473: train loss: 0.11707817494544306, test loss: 0.2534453692709111\n",
      "epoch 12474: train loss: 0.11707562219270602, test loss: 0.253445938377958\n",
      "epoch 12475: train loss: 0.11707306972418778, test loss: 0.2534465076321266\n",
      "epoch 12476: train loss: 0.11707051753983176, test loss: 0.253447077033409\n",
      "epoch 12477: train loss: 0.11706796563958166, test loss: 0.25344764658172964\n",
      "epoch 12478: train loss: 0.11706541402338104, test loss: 0.2534482162770679\n",
      "epoch 12479: train loss: 0.11706286269117352, test loss: 0.25344878611938865\n",
      "epoch 12480: train loss: 0.11706031164290277, test loss: 0.2534493561086515\n",
      "epoch 12481: train loss: 0.11705776087851236, test loss: 0.25344992624484236\n",
      "epoch 12482: train loss: 0.11705521039794604, test loss: 0.25345049652789975\n",
      "epoch 12483: train loss: 0.11705266020114745, test loss: 0.2534510669578096\n",
      "epoch 12484: train loss: 0.11705011028806032, test loss: 0.2534516375345165\n",
      "epoch 12485: train loss: 0.11704756065862831, test loss: 0.25345220825800807\n",
      "epoch 12486: train loss: 0.1170450113127952, test loss: 0.2534527791282348\n",
      "epoch 12487: train loss: 0.11704246225050471, test loss: 0.2534533501451566\n",
      "epoch 12488: train loss: 0.11703991347170062, test loss: 0.2534539213087458\n",
      "epoch 12489: train loss: 0.11703736497632669, test loss: 0.25345449261896036\n",
      "epoch 12490: train loss: 0.1170348167643267, test loss: 0.25345506407578955\n",
      "epoch 12491: train loss: 0.1170322688356445, test loss: 0.2534556356791609\n",
      "epoch 12492: train loss: 0.1170297211902239, test loss: 0.2534562074290647\n",
      "epoch 12493: train loss: 0.1170271738280087, test loss: 0.2534567793254638\n",
      "epoch 12494: train loss: 0.1170246267489428, test loss: 0.2534573513683127\n",
      "epoch 12495: train loss: 0.11702207995297005, test loss: 0.25345792355758934\n",
      "epoch 12496: train loss: 0.11701953344003435, test loss: 0.25345849589325464\n",
      "epoch 12497: train loss: 0.11701698721007961, test loss: 0.253459068375258\n",
      "epoch 12498: train loss: 0.11701444126304975, test loss: 0.25345964100358875\n",
      "epoch 12499: train loss: 0.11701189559888865, test loss: 0.25346021377819133\n",
      "epoch 12500: train loss: 0.1170093502175403, test loss: 0.25346078669904526\n",
      "epoch 12501: train loss: 0.1170068051189487, test loss: 0.2534613597661035\n",
      "epoch 12502: train loss: 0.11700426030305776, test loss: 0.2534619329793452\n",
      "epoch 12503: train loss: 0.11700171576981154, test loss: 0.2534625063387233\n",
      "epoch 12504: train loss: 0.11699917151915401, test loss: 0.25346307984420996\n",
      "epoch 12505: train loss: 0.1169966275510292, test loss: 0.2534636534957724\n",
      "epoch 12506: train loss: 0.11699408386538118, test loss: 0.25346422729335555\n",
      "epoch 12507: train loss: 0.11699154046215401, test loss: 0.2534648012369392\n",
      "epoch 12508: train loss: 0.11698899734129174, test loss: 0.25346537532650454\n",
      "epoch 12509: train loss: 0.1169864545027385, test loss: 0.2534659495619957\n",
      "epoch 12510: train loss: 0.11698391194643833, test loss: 0.25346652394338703\n",
      "epoch 12511: train loss: 0.11698136967233541, test loss: 0.2534670984706224\n",
      "epoch 12512: train loss: 0.11697882768037386, test loss: 0.2534676731437049\n",
      "epoch 12513: train loss: 0.11697628597049782, test loss: 0.2534682479625573\n",
      "epoch 12514: train loss: 0.1169737445426515, test loss: 0.2534688229271756\n",
      "epoch 12515: train loss: 0.11697120339677904, test loss: 0.2534693980375179\n",
      "epoch 12516: train loss: 0.11696866253282465, test loss: 0.2534699732935512\n",
      "epoch 12517: train loss: 0.11696612195073254, test loss: 0.25347054869523505\n",
      "epoch 12518: train loss: 0.11696358165044696, test loss: 0.2534711242425204\n",
      "epoch 12519: train loss: 0.11696104163191215, test loss: 0.2534716999354114\n",
      "epoch 12520: train loss: 0.11695850189507238, test loss: 0.25347227577383075\n",
      "epoch 12521: train loss: 0.1169559624398719, test loss: 0.25347285175778106\n",
      "epoch 12522: train loss: 0.11695342326625503, test loss: 0.25347342788718535\n",
      "epoch 12523: train loss: 0.11695088437416612, test loss: 0.25347400416204663\n",
      "epoch 12524: train loss: 0.11694834576354939, test loss: 0.2534745805823169\n",
      "epoch 12525: train loss: 0.11694580743434925, test loss: 0.2534751571479563\n",
      "epoch 12526: train loss: 0.11694326938651002, test loss: 0.2534757338589374\n",
      "epoch 12527: train loss: 0.11694073161997612, test loss: 0.25347631071522125\n",
      "epoch 12528: train loss: 0.11693819413469192, test loss: 0.25347688771677374\n",
      "epoch 12529: train loss: 0.11693565693060182, test loss: 0.25347746486355477\n",
      "epoch 12530: train loss: 0.11693312000765019, test loss: 0.253478042155545\n",
      "epoch 12531: train loss: 0.1169305833657815, test loss: 0.25347861959270424\n",
      "epoch 12532: train loss: 0.11692804700494026, test loss: 0.2534791971749717\n",
      "epoch 12533: train loss: 0.11692551092507082, test loss: 0.25347977490234747\n",
      "epoch 12534: train loss: 0.11692297512611775, test loss: 0.25348035277480085\n",
      "epoch 12535: train loss: 0.1169204396080255, test loss: 0.2534809307922629\n",
      "epoch 12536: train loss: 0.1169179043707386, test loss: 0.25348150895471333\n",
      "epoch 12537: train loss: 0.11691536941420157, test loss: 0.25348208726212734\n",
      "epoch 12538: train loss: 0.11691283473835895, test loss: 0.2534826657144713\n",
      "epoch 12539: train loss: 0.11691030034315528, test loss: 0.2534832443116835\n",
      "epoch 12540: train loss: 0.11690776622853517, test loss: 0.25348382305376815\n",
      "epoch 12541: train loss: 0.11690523239444317, test loss: 0.2534844019406547\n",
      "epoch 12542: train loss: 0.11690269884082394, test loss: 0.2534849809723319\n",
      "epoch 12543: train loss: 0.11690016556762205, test loss: 0.2534855601487607\n",
      "epoch 12544: train loss: 0.11689763257478213, test loss: 0.2534861394698927\n",
      "epoch 12545: train loss: 0.11689509986224884, test loss: 0.25348671893572494\n",
      "epoch 12546: train loss: 0.11689256742996687, test loss: 0.25348729854618773\n",
      "epoch 12547: train loss: 0.11689003527788089, test loss: 0.2534878783012626\n",
      "epoch 12548: train loss: 0.11688750340593555, test loss: 0.2534884582009111\n",
      "epoch 12549: train loss: 0.11688497181407563, test loss: 0.2534890382451126\n",
      "epoch 12550: train loss: 0.1168824405022458, test loss: 0.253489618433821\n",
      "epoch 12551: train loss: 0.11687990947039087, test loss: 0.253490198766997\n",
      "epoch 12552: train loss: 0.11687737871845555, test loss: 0.253490779244599\n",
      "epoch 12553: train loss: 0.1168748482463846, test loss: 0.25349135986662413\n",
      "epoch 12554: train loss: 0.11687231805412283, test loss: 0.25349194063301234\n",
      "epoch 12555: train loss: 0.11686978814161506, test loss: 0.25349252154373564\n",
      "epoch 12556: train loss: 0.1168672585088061, test loss: 0.25349310259876195\n",
      "epoch 12557: train loss: 0.11686472915564076, test loss: 0.2534936837980535\n",
      "epoch 12558: train loss: 0.11686220008206391, test loss: 0.2534942651415678\n",
      "epoch 12559: train loss: 0.11685967128802041, test loss: 0.2534948466292739\n",
      "epoch 12560: train loss: 0.11685714277345514, test loss: 0.25349542826116084\n",
      "epoch 12561: train loss: 0.11685461453831301, test loss: 0.2534960100371666\n",
      "epoch 12562: train loss: 0.11685208658253891, test loss: 0.25349659195727364\n",
      "epoch 12563: train loss: 0.1168495589060778, test loss: 0.253497174021428\n",
      "epoch 12564: train loss: 0.11684703150887459, test loss: 0.2534977562296181\n",
      "epoch 12565: train loss: 0.11684450439087427, test loss: 0.25349833858179815\n",
      "epoch 12566: train loss: 0.11684197755202176, test loss: 0.25349892107793454\n",
      "epoch 12567: train loss: 0.11683945099226208, test loss: 0.2534995037179818\n",
      "epoch 12568: train loss: 0.11683692471154025, test loss: 0.2535000865019209\n",
      "epoch 12569: train loss: 0.11683439870980125, test loss: 0.2535006694297196\n",
      "epoch 12570: train loss: 0.11683187298699015, test loss: 0.2535012525013389\n",
      "epoch 12571: train loss: 0.11682934754305199, test loss: 0.2535018357167327\n",
      "epoch 12572: train loss: 0.11682682237793181, test loss: 0.2535024190758887\n",
      "epoch 12573: train loss: 0.11682429749157473, test loss: 0.2535030025787545\n",
      "epoch 12574: train loss: 0.1168217728839258, test loss: 0.25350358622529745\n",
      "epoch 12575: train loss: 0.11681924855493019, test loss: 0.25350417001549147\n",
      "epoch 12576: train loss: 0.11681672450453298, test loss: 0.2535047539492985\n",
      "epoch 12577: train loss: 0.1168142007326793, test loss: 0.25350533802669434\n",
      "epoch 12578: train loss: 0.11681167723931435, test loss: 0.25350592224762286\n",
      "epoch 12579: train loss: 0.11680915402438327, test loss: 0.2535065066120681\n",
      "epoch 12580: train loss: 0.11680663108783125, test loss: 0.25350709111998304\n",
      "epoch 12581: train loss: 0.11680410842960352, test loss: 0.2535076757713434\n",
      "epoch 12582: train loss: 0.11680158604964525, test loss: 0.253508260566116\n",
      "epoch 12583: train loss: 0.11679906394790172, test loss: 0.2535088455042553\n",
      "epoch 12584: train loss: 0.11679654212431814, test loss: 0.253509430585729\n",
      "epoch 12585: train loss: 0.1167940205788398, test loss: 0.2535100158105193\n",
      "epoch 12586: train loss: 0.11679149931141197, test loss: 0.25351060117858126\n",
      "epoch 12587: train loss: 0.11678897832197993, test loss: 0.2535111866898658\n",
      "epoch 12588: train loss: 0.11678645761048902, test loss: 0.2535117723443729\n",
      "epoch 12589: train loss: 0.11678393717688451, test loss: 0.2535123581420395\n",
      "epoch 12590: train loss: 0.11678141702111178, test loss: 0.2535129440828352\n",
      "epoch 12591: train loss: 0.11677889714311618, test loss: 0.2535135301667424\n",
      "epoch 12592: train loss: 0.11677637754284309, test loss: 0.253514116393699\n",
      "epoch 12593: train loss: 0.11677385822023786, test loss: 0.2535147027637042\n",
      "epoch 12594: train loss: 0.11677133917524593, test loss: 0.25351528927671074\n",
      "epoch 12595: train loss: 0.11676882040781267, test loss: 0.2535158759326719\n",
      "epoch 12596: train loss: 0.11676630191788354, test loss: 0.2535164627315563\n",
      "epoch 12597: train loss: 0.11676378370540398, test loss: 0.2535170496733401\n",
      "epoch 12598: train loss: 0.11676126577031946, test loss: 0.2535176367579995\n",
      "epoch 12599: train loss: 0.11675874811257543, test loss: 0.2535182239854788\n",
      "epoch 12600: train loss: 0.11675623073211741, test loss: 0.25351881135574766\n",
      "epoch 12601: train loss: 0.11675371362889088, test loss: 0.25351939886878283\n",
      "epoch 12602: train loss: 0.11675119680284136, test loss: 0.25351998652453545\n",
      "epoch 12603: train loss: 0.11674868025391442, test loss: 0.2535205743229921\n",
      "epoch 12604: train loss: 0.11674616398205556, test loss: 0.25352116226410365\n",
      "epoch 12605: train loss: 0.1167436479872104, test loss: 0.2535217503478255\n",
      "epoch 12606: train loss: 0.11674113226932448, test loss: 0.25352233857414086\n",
      "epoch 12607: train loss: 0.11673861682834343, test loss: 0.2535229269430311\n",
      "epoch 12608: train loss: 0.1167361016642128, test loss: 0.2535235154544235\n",
      "epoch 12609: train loss: 0.11673358677687831, test loss: 0.25352410410831483\n",
      "epoch 12610: train loss: 0.11673107216628555, test loss: 0.2535246929046512\n",
      "epoch 12611: train loss: 0.11672855783238013, test loss: 0.2535252818434022\n",
      "epoch 12612: train loss: 0.11672604377510783, test loss: 0.2535258709245579\n",
      "epoch 12613: train loss: 0.11672352999441425, test loss: 0.25352646014806446\n",
      "epoch 12614: train loss: 0.11672101649024512, test loss: 0.25352704951387733\n",
      "epoch 12615: train loss: 0.11671850326254614, test loss: 0.25352763902198594\n",
      "epoch 12616: train loss: 0.11671599031126306, test loss: 0.25352822867233127\n",
      "epoch 12617: train loss: 0.11671347763634163, test loss: 0.25352881846491004\n",
      "epoch 12618: train loss: 0.11671096523772764, test loss: 0.25352940839965465\n",
      "epoch 12619: train loss: 0.11670845311536679, test loss: 0.25352999847655583\n",
      "epoch 12620: train loss: 0.11670594126920493, test loss: 0.25353058869557615\n",
      "epoch 12621: train loss: 0.11670342969918787, test loss: 0.25353117905668326\n",
      "epoch 12622: train loss: 0.11670091840526141, test loss: 0.25353176955983203\n",
      "epoch 12623: train loss: 0.11669840738737136, test loss: 0.2535323602049904\n",
      "epoch 12624: train loss: 0.11669589664546363, test loss: 0.25353295099213663\n",
      "epoch 12625: train loss: 0.11669338617948406, test loss: 0.2535335419212169\n",
      "epoch 12626: train loss: 0.11669087598937854, test loss: 0.25353413299221306\n",
      "epoch 12627: train loss: 0.11668836607509293, test loss: 0.2535347242051038\n",
      "epoch 12628: train loss: 0.1166858564365732, test loss: 0.253535315559813\n",
      "epoch 12629: train loss: 0.11668334707376525, test loss: 0.25353590705635404\n",
      "epoch 12630: train loss: 0.11668083798661502, test loss: 0.2535364986946729\n",
      "epoch 12631: train loss: 0.11667832917506848, test loss: 0.2535370904747332\n",
      "epoch 12632: train loss: 0.11667582063907159, test loss: 0.25353768239648905\n",
      "epoch 12633: train loss: 0.11667331237857032, test loss: 0.25353827445993765\n",
      "epoch 12634: train loss: 0.11667080439351071, test loss: 0.2535388666650275\n",
      "epoch 12635: train loss: 0.11666829668383873, test loss: 0.2535394590117121\n",
      "epoch 12636: train loss: 0.11666578924950045, test loss: 0.2535400514999979\n",
      "epoch 12637: train loss: 0.11666328209044194, test loss: 0.25354064412979554\n",
      "epoch 12638: train loss: 0.11666077520660918, test loss: 0.2535412369011172\n",
      "epoch 12639: train loss: 0.11665826859794834, test loss: 0.2535418298139121\n",
      "epoch 12640: train loss: 0.11665576226440542, test loss: 0.25354242286815554\n",
      "epoch 12641: train loss: 0.1166532562059266, test loss: 0.25354301606380275\n",
      "epoch 12642: train loss: 0.11665075042245798, test loss: 0.25354360940080767\n",
      "epoch 12643: train loss: 0.1166482449139457, test loss: 0.2535442028791763\n",
      "epoch 12644: train loss: 0.11664573968033587, test loss: 0.2535447964988355\n",
      "epoch 12645: train loss: 0.1166432347215747, test loss: 0.2535453902597676\n",
      "epoch 12646: train loss: 0.11664073003760837, test loss: 0.25354598416194307\n",
      "epoch 12647: train loss: 0.11663822562838305, test loss: 0.25354657820532395\n",
      "epoch 12648: train loss: 0.11663572149384499, test loss: 0.2535471723898718\n",
      "epoch 12649: train loss: 0.1166332176339404, test loss: 0.2535477667155569\n",
      "epoch 12650: train loss: 0.11663071404861547, test loss: 0.2535483611823626\n",
      "epoch 12651: train loss: 0.1166282107378165, test loss: 0.25354895579021475\n",
      "epoch 12652: train loss: 0.11662570770148979, test loss: 0.25354955053913536\n",
      "epoch 12653: train loss: 0.11662320493958159, test loss: 0.2535501454290329\n",
      "epoch 12654: train loss: 0.11662070245203818, test loss: 0.25355074045990866\n",
      "epoch 12655: train loss: 0.11661820023880591, test loss: 0.2535513356317241\n",
      "epoch 12656: train loss: 0.1166156982998311, test loss: 0.25355193094444833\n",
      "epoch 12657: train loss: 0.1166131966350601, test loss: 0.253552526398044\n",
      "epoch 12658: train loss: 0.11661069524443925, test loss: 0.2535531219924674\n",
      "epoch 12659: train loss: 0.11660819412791494, test loss: 0.25355371772770074\n",
      "epoch 12660: train loss: 0.11660569328543353, test loss: 0.25355431360370795\n",
      "epoch 12661: train loss: 0.11660319271694149, test loss: 0.25355490962043586\n",
      "epoch 12662: train loss: 0.11660069242238517, test loss: 0.2535555057778772\n",
      "epoch 12663: train loss: 0.11659819240171104, test loss: 0.25355610207599427\n",
      "epoch 12664: train loss: 0.11659569265486552, test loss: 0.2535566985147342\n",
      "epoch 12665: train loss: 0.11659319318179515, test loss: 0.2535572950940892\n",
      "epoch 12666: train loss: 0.11659069398244629, test loss: 0.2535578918140002\n",
      "epoch 12667: train loss: 0.1165881950567655, test loss: 0.2535584886744655\n",
      "epoch 12668: train loss: 0.11658569640469932, test loss: 0.25355908567541907\n",
      "epoch 12669: train loss: 0.11658319802619417, test loss: 0.2535596828168459\n",
      "epoch 12670: train loss: 0.1165806999211967, test loss: 0.25356028009871506\n",
      "epoch 12671: train loss: 0.11657820208965336, test loss: 0.2535608775209894\n",
      "epoch 12672: train loss: 0.11657570453151081, test loss: 0.25356147508361043\n",
      "epoch 12673: train loss: 0.11657320724671559, test loss: 0.25356207278658366\n",
      "epoch 12674: train loss: 0.11657071023521427, test loss: 0.2535626706298572\n",
      "epoch 12675: train loss: 0.1165682134969535, test loss: 0.2535632686134102\n",
      "epoch 12676: train loss: 0.11656571703187986, test loss: 0.253563866737196\n",
      "epoch 12677: train loss: 0.11656322083994006, test loss: 0.2535644650011706\n",
      "epoch 12678: train loss: 0.11656072492108067, test loss: 0.2535650634053191\n",
      "epoch 12679: train loss: 0.11655822927524842, test loss: 0.2535656619496124\n",
      "epoch 12680: train loss: 0.11655573390238999, test loss: 0.2535662606340041\n",
      "epoch 12681: train loss: 0.11655323880245208, test loss: 0.2535668594584733\n",
      "epoch 12682: train loss: 0.11655074397538137, test loss: 0.25356745842296036\n",
      "epoch 12683: train loss: 0.1165482494211246, test loss: 0.253568057527465\n",
      "epoch 12684: train loss: 0.11654575513962855, test loss: 0.25356865677194973\n",
      "epoch 12685: train loss: 0.11654326113083992, test loss: 0.2535692561563486\n",
      "epoch 12686: train loss: 0.11654076739470551, test loss: 0.2535698556806611\n",
      "epoch 12687: train loss: 0.11653827393117211, test loss: 0.25357045534485095\n",
      "epoch 12688: train loss: 0.11653578074018656, test loss: 0.2535710551488722\n",
      "epoch 12689: train loss: 0.1165332878216956, test loss: 0.25357165509269747\n",
      "epoch 12690: train loss: 0.11653079517564612, test loss: 0.2535722551763019\n",
      "epoch 12691: train loss: 0.11652830280198494, test loss: 0.25357285539962826\n",
      "epoch 12692: train loss: 0.11652581070065893, test loss: 0.2535734557626689\n",
      "epoch 12693: train loss: 0.1165233188716149, test loss: 0.25357405626537116\n",
      "epoch 12694: train loss: 0.11652082731479987, test loss: 0.2535746569077222\n",
      "epoch 12695: train loss: 0.11651833603016065, test loss: 0.2535752576896765\n",
      "epoch 12696: train loss: 0.11651584501764418, test loss: 0.25357585861120374\n",
      "epoch 12697: train loss: 0.11651335427719742, test loss: 0.25357645967227593\n",
      "epoch 12698: train loss: 0.11651086380876727, test loss: 0.25357706087284204\n",
      "epoch 12699: train loss: 0.11650837361230071, test loss: 0.253577662212893\n",
      "epoch 12700: train loss: 0.11650588368774475, test loss: 0.25357826369237096\n",
      "epoch 12701: train loss: 0.11650339403504635, test loss: 0.2535788653112687\n",
      "epoch 12702: train loss: 0.1165009046541525, test loss: 0.25357946706952705\n",
      "epoch 12703: train loss: 0.11649841554501027, test loss: 0.25358006896714663\n",
      "epoch 12704: train loss: 0.11649592670756666, test loss: 0.2535806710040531\n",
      "epoch 12705: train loss: 0.11649343814176874, test loss: 0.25358127318024815\n",
      "epoch 12706: train loss: 0.11649094984756354, test loss: 0.25358187549567207\n",
      "epoch 12707: train loss: 0.11648846182489819, test loss: 0.25358247795031014\n",
      "epoch 12708: train loss: 0.11648597407371973, test loss: 0.25358308054414147\n",
      "epoch 12709: train loss: 0.11648348659397534, test loss: 0.25358368327709074\n",
      "epoch 12710: train loss: 0.11648099938561207, test loss: 0.25358428614916806\n",
      "epoch 12711: train loss: 0.11647851244857713, test loss: 0.25358488916029925\n",
      "epoch 12712: train loss: 0.11647602578281757, test loss: 0.2535854923105068\n",
      "epoch 12713: train loss: 0.11647353938828064, test loss: 0.25358609559970136\n",
      "epoch 12714: train loss: 0.11647105326491353, test loss: 0.25358669902787734\n",
      "epoch 12715: train loss: 0.11646856741266337, test loss: 0.25358730259499795\n",
      "epoch 12716: train loss: 0.11646608183147744, test loss: 0.2535879063010351\n",
      "epoch 12717: train loss: 0.11646359652130289, test loss: 0.25358851014595074\n",
      "epoch 12718: train loss: 0.11646111148208702, test loss: 0.25358911412970925\n",
      "epoch 12719: train loss: 0.11645862671377706, test loss: 0.25358971825228244\n",
      "epoch 12720: train loss: 0.11645614221632031, test loss: 0.2535903225136477\n",
      "epoch 12721: train loss: 0.11645365798966399, test loss: 0.2535909269137547\n",
      "epoch 12722: train loss: 0.11645117403375548, test loss: 0.25359153145255897\n",
      "epoch 12723: train loss: 0.116448690348542, test loss: 0.2535921361300768\n",
      "epoch 12724: train loss: 0.11644620693397094, test loss: 0.25359274094621975\n",
      "epoch 12725: train loss: 0.11644372378998963, test loss: 0.2535933459009816\n",
      "epoch 12726: train loss: 0.1164412409165454, test loss: 0.25359395099434057\n",
      "epoch 12727: train loss: 0.11643875831358563, test loss: 0.2535945562262379\n",
      "epoch 12728: train loss: 0.11643627598105773, test loss: 0.25359516159665274\n",
      "epoch 12729: train loss: 0.11643379391890911, test loss: 0.2535957671055643\n",
      "epoch 12730: train loss: 0.11643131212708711, test loss: 0.25359637275291325\n",
      "epoch 12731: train loss: 0.11642883060553923, test loss: 0.25359697853869373\n",
      "epoch 12732: train loss: 0.11642634935421288, test loss: 0.2535975844628479\n",
      "epoch 12733: train loss: 0.11642386837305552, test loss: 0.25359819052536764\n",
      "epoch 12734: train loss: 0.11642138766201462, test loss: 0.2535987967262113\n",
      "epoch 12735: train loss: 0.11641890722103765, test loss: 0.2535994030653415\n",
      "epoch 12736: train loss: 0.11641642705007216, test loss: 0.25360000954271633\n",
      "epoch 12737: train loss: 0.11641394714906562, test loss: 0.2536006161583212\n",
      "epoch 12738: train loss: 0.11641146751796558, test loss: 0.2536012229121132\n",
      "epoch 12739: train loss: 0.11640898815671957, test loss: 0.2536018298040778\n",
      "epoch 12740: train loss: 0.11640650906527515, test loss: 0.25360243683415684\n",
      "epoch 12741: train loss: 0.1164040302435799, test loss: 0.25360304400232336\n",
      "epoch 12742: train loss: 0.1164015516915814, test loss: 0.25360365130856194\n",
      "epoch 12743: train loss: 0.11639907340922725, test loss: 0.2536042587528162\n",
      "epoch 12744: train loss: 0.11639659539646507, test loss: 0.2536048663350636\n",
      "epoch 12745: train loss: 0.1163941176532425, test loss: 0.25360547405527695\n",
      "epoch 12746: train loss: 0.11639164017950715, test loss: 0.25360608191343464\n",
      "epoch 12747: train loss: 0.1163891629752067, test loss: 0.2536066899094716\n",
      "epoch 12748: train loss: 0.11638668604028883, test loss: 0.25360729804337395\n",
      "epoch 12749: train loss: 0.11638420937470119, test loss: 0.2536079063150986\n",
      "epoch 12750: train loss: 0.11638173297839156, test loss: 0.25360851472464746\n",
      "epoch 12751: train loss: 0.11637925685130757, test loss: 0.25360912327194784\n",
      "epoch 12752: train loss: 0.116376780993397, test loss: 0.25360973195697795\n",
      "epoch 12753: train loss: 0.1163743054046076, test loss: 0.25361034077971056\n",
      "epoch 12754: train loss: 0.11637183008488705, test loss: 0.25361094974010273\n",
      "epoch 12755: train loss: 0.11636935503418323, test loss: 0.25361155883815506\n",
      "epoch 12756: train loss: 0.11636688025244388, test loss: 0.25361216807379444\n",
      "epoch 12757: train loss: 0.11636440573961679, test loss: 0.25361277744701\n",
      "epoch 12758: train loss: 0.11636193149564977, test loss: 0.25361338695775726\n",
      "epoch 12759: train loss: 0.11635945752049069, test loss: 0.25361399660601447\n",
      "epoch 12760: train loss: 0.11635698381408738, test loss: 0.2536146063917409\n",
      "epoch 12761: train loss: 0.11635451037638767, test loss: 0.2536152163149076\n",
      "epoch 12762: train loss: 0.11635203720733946, test loss: 0.25361582637550106\n",
      "epoch 12763: train loss: 0.11634956430689064, test loss: 0.2536164365734488\n",
      "epoch 12764: train loss: 0.11634709167498912, test loss: 0.2536170469087455\n",
      "epoch 12765: train loss: 0.11634461931158276, test loss: 0.25361765738134884\n",
      "epoch 12766: train loss: 0.11634214721661955, test loss: 0.25361826799124504\n",
      "epoch 12767: train loss: 0.1163396753900474, test loss: 0.2536188787383759\n",
      "epoch 12768: train loss: 0.11633720383181431, test loss: 0.25361948962272984\n",
      "epoch 12769: train loss: 0.11633473254186821, test loss: 0.2536201006442404\n",
      "epoch 12770: train loss: 0.11633226152015713, test loss: 0.2536207118029259\n",
      "epoch 12771: train loss: 0.11632979076662903, test loss: 0.25362132309871294\n",
      "epoch 12772: train loss: 0.11632732028123195, test loss: 0.25362193453159554\n",
      "epoch 12773: train loss: 0.11632485006391388, test loss: 0.25362254610152474\n",
      "epoch 12774: train loss: 0.11632238011462291, test loss: 0.2536231578084723\n",
      "epoch 12775: train loss: 0.11631991043330714, test loss: 0.25362376965239586\n",
      "epoch 12776: train loss: 0.11631744101991455, test loss: 0.25362438163329737\n",
      "epoch 12777: train loss: 0.11631497187439324, test loss: 0.25362499375111136\n",
      "epoch 12778: train loss: 0.11631250299669137, test loss: 0.25362560600580264\n",
      "epoch 12779: train loss: 0.11631003438675702, test loss: 0.2536262183973601\n",
      "epoch 12780: train loss: 0.11630756604453829, test loss: 0.2536268309257396\n",
      "epoch 12781: train loss: 0.1163050979699834, test loss: 0.25362744359092265\n",
      "epoch 12782: train loss: 0.11630263016304043, test loss: 0.25362805639284985\n",
      "epoch 12783: train loss: 0.11630016262365761, test loss: 0.25362866933152506\n",
      "epoch 12784: train loss: 0.11629769535178308, test loss: 0.25362928240688193\n",
      "epoch 12785: train loss: 0.1162952283473651, test loss: 0.25362989561890936\n",
      "epoch 12786: train loss: 0.11629276161035182, test loss: 0.2536305089675496\n",
      "epoch 12787: train loss: 0.11629029514069149, test loss: 0.25363112245281294\n",
      "epoch 12788: train loss: 0.11628782893833238, test loss: 0.25363173607462536\n",
      "epoch 12789: train loss: 0.11628536300322273, test loss: 0.2536323498329912\n",
      "epoch 12790: train loss: 0.1162828973353108, test loss: 0.2536329637278456\n",
      "epoch 12791: train loss: 0.11628043193454488, test loss: 0.25363357775917506\n",
      "epoch 12792: train loss: 0.11627796680087328, test loss: 0.25363419192693787\n",
      "epoch 12793: train loss: 0.11627550193424431, test loss: 0.2536348062310995\n",
      "epoch 12794: train loss: 0.11627303733460628, test loss: 0.25363542067164746\n",
      "epoch 12795: train loss: 0.11627057300190759, test loss: 0.2536360352485396\n",
      "epoch 12796: train loss: 0.11626810893609654, test loss: 0.2536366499617334\n",
      "epoch 12797: train loss: 0.11626564513712148, test loss: 0.2536372648112084\n",
      "epoch 12798: train loss: 0.11626318160493089, test loss: 0.2536378797969245\n",
      "epoch 12799: train loss: 0.11626071833947307, test loss: 0.2536384949188525\n",
      "epoch 12800: train loss: 0.11625825534069648, test loss: 0.2536391101769588\n",
      "epoch 12801: train loss: 0.11625579260854951, test loss: 0.2536397255712237\n",
      "epoch 12802: train loss: 0.11625333014298066, test loss: 0.2536403411015897\n",
      "epoch 12803: train loss: 0.11625086794393838, test loss: 0.25364095676806053\n",
      "epoch 12804: train loss: 0.11624840601137108, test loss: 0.25364157257056985\n",
      "epoch 12805: train loss: 0.1162459443452273, test loss: 0.2536421885090925\n",
      "epoch 12806: train loss: 0.11624348294545549, test loss: 0.25364280458361593\n",
      "epoch 12807: train loss: 0.11624102181200417, test loss: 0.2536434207940907\n",
      "epoch 12808: train loss: 0.11623856094482193, test loss: 0.25364403714048883\n",
      "epoch 12809: train loss: 0.11623610034385722, test loss: 0.25364465362278343\n",
      "epoch 12810: train loss: 0.11623364000905864, test loss: 0.2536452702409269\n",
      "epoch 12811: train loss: 0.11623117994037477, test loss: 0.2536458869948989\n",
      "epoch 12812: train loss: 0.11622872013775418, test loss: 0.25364650388468696\n",
      "epoch 12813: train loss: 0.11622626060114544, test loss: 0.25364712091021885\n",
      "epoch 12814: train loss: 0.11622380133049717, test loss: 0.25364773807149066\n",
      "epoch 12815: train loss: 0.116221342325758, test loss: 0.2536483553684624\n",
      "epoch 12816: train loss: 0.1162188835868766, test loss: 0.25364897280108145\n",
      "epoch 12817: train loss: 0.11621642511380158, test loss: 0.25364959036936846\n",
      "epoch 12818: train loss: 0.1162139669064816, test loss: 0.2536502080732365\n",
      "epoch 12819: train loss: 0.11621150896486539, test loss: 0.25365082591268007\n",
      "epoch 12820: train loss: 0.11620905128890159, test loss: 0.2536514438876648\n",
      "epoch 12821: train loss: 0.11620659387853895, test loss: 0.25365206199816553\n",
      "epoch 12822: train loss: 0.11620413673372613, test loss: 0.25365268024413995\n",
      "epoch 12823: train loss: 0.11620167985441193, test loss: 0.25365329862555347\n",
      "epoch 12824: train loss: 0.11619922324054509, test loss: 0.2536539171423724\n",
      "epoch 12825: train loss: 0.11619676689207434, test loss: 0.25365453579457053\n",
      "epoch 12826: train loss: 0.11619431080894849, test loss: 0.253655154582135\n",
      "epoch 12827: train loss: 0.1161918549911163, test loss: 0.2536557735050016\n",
      "epoch 12828: train loss: 0.11618939943852663, test loss: 0.25365639256316114\n",
      "epoch 12829: train loss: 0.11618694415112824, test loss: 0.2536570117565613\n",
      "epoch 12830: train loss: 0.11618448912887, test loss: 0.2536576310851858\n",
      "epoch 12831: train loss: 0.11618203437170076, test loss: 0.25365825054900765\n",
      "epoch 12832: train loss: 0.11617957987956933, test loss: 0.25365887014798405\n",
      "epoch 12833: train loss: 0.11617712565242465, test loss: 0.25365948988208187\n",
      "epoch 12834: train loss: 0.11617467169021559, test loss: 0.2536601097512663\n",
      "epoch 12835: train loss: 0.11617221799289103, test loss: 0.2536607297555132\n",
      "epoch 12836: train loss: 0.11616976456039993, test loss: 0.2536613498948085\n",
      "epoch 12837: train loss: 0.11616731139269117, test loss: 0.2536619701690749\n",
      "epoch 12838: train loss: 0.11616485848971372, test loss: 0.25366259057832335\n",
      "epoch 12839: train loss: 0.11616240585141656, test loss: 0.25366321112251244\n",
      "epoch 12840: train loss: 0.1161599534777486, test loss: 0.25366383180159313\n",
      "epoch 12841: train loss: 0.11615750136865892, test loss: 0.25366445261554593\n",
      "epoch 12842: train loss: 0.11615504952409644, test loss: 0.25366507356433127\n",
      "epoch 12843: train loss: 0.1161525979440102, test loss: 0.25366569464793753\n",
      "epoch 12844: train loss: 0.11615014662834926, test loss: 0.25366631586631416\n",
      "epoch 12845: train loss: 0.1161476955770626, test loss: 0.25366693721942946\n",
      "epoch 12846: train loss: 0.11614524479009933, test loss: 0.2536675587072566\n",
      "epoch 12847: train loss: 0.1161427942674085, test loss: 0.2536681803297773\n",
      "epoch 12848: train loss: 0.1161403440089392, test loss: 0.25366880208692677\n",
      "epoch 12849: train loss: 0.11613789401464053, test loss: 0.2536694239787096\n",
      "epoch 12850: train loss: 0.11613544428446161, test loss: 0.25367004600506954\n",
      "epoch 12851: train loss: 0.11613299481835153, test loss: 0.2536706681659885\n",
      "epoch 12852: train loss: 0.11613054561625943, test loss: 0.25367129046141773\n",
      "epoch 12853: train loss: 0.11612809667813451, test loss: 0.25367191289134544\n",
      "epoch 12854: train loss: 0.1161256480039259, test loss: 0.25367253545573865\n",
      "epoch 12855: train loss: 0.1161231995935828, test loss: 0.2536731581545494\n",
      "epoch 12856: train loss: 0.1161207514470544, test loss: 0.2536737809877434\n",
      "epoch 12857: train loss: 0.11611830356428988, test loss: 0.2536744039553254\n",
      "epoch 12858: train loss: 0.11611585594523852, test loss: 0.2536750270572241\n",
      "epoch 12859: train loss: 0.11611340858984953, test loss: 0.25367565029342787\n",
      "epoch 12860: train loss: 0.11611096149807212, test loss: 0.2536762736639117\n",
      "epoch 12861: train loss: 0.11610851466985561, test loss: 0.2536768971686199\n",
      "epoch 12862: train loss: 0.11610606810514927, test loss: 0.25367752080753336\n",
      "epoch 12863: train loss: 0.11610362180390235, test loss: 0.25367814458061916\n",
      "epoch 12864: train loss: 0.1161011757660642, test loss: 0.2536787684878435\n",
      "epoch 12865: train loss: 0.11609872999158412, test loss: 0.2536793925291966\n",
      "epoch 12866: train loss: 0.11609628448041144, test loss: 0.25368001670461443\n",
      "epoch 12867: train loss: 0.11609383923249551, test loss: 0.25368064101407845\n",
      "epoch 12868: train loss: 0.11609139424778568, test loss: 0.2536812654575793\n",
      "epoch 12869: train loss: 0.11608894952623137, test loss: 0.25368189003503716\n",
      "epoch 12870: train loss: 0.1160865050677819, test loss: 0.2536825147464646\n",
      "epoch 12871: train loss: 0.11608406087238674, test loss: 0.2536831395918207\n",
      "epoch 12872: train loss: 0.11608161693999525, test loss: 0.25368376457104264\n",
      "epoch 12873: train loss: 0.11607917327055688, test loss: 0.2536843896841425\n",
      "epoch 12874: train loss: 0.11607672986402108, test loss: 0.25368501493107115\n",
      "epoch 12875: train loss: 0.1160742867203373, test loss: 0.2536856403117817\n",
      "epoch 12876: train loss: 0.11607184383945501, test loss: 0.25368626582626236\n",
      "epoch 12877: train loss: 0.11606940122132371, test loss: 0.25368689147448853\n",
      "epoch 12878: train loss: 0.11606695886589287, test loss: 0.2536875172563973\n",
      "epoch 12879: train loss: 0.11606451677311203, test loss: 0.2536881431719774\n",
      "epoch 12880: train loss: 0.11606207494293067, test loss: 0.25368876922121913\n",
      "epoch 12881: train loss: 0.1160596333752984, test loss: 0.25368939540405094\n",
      "epoch 12882: train loss: 0.1160571920701647, test loss: 0.25369002172044836\n",
      "epoch 12883: train loss: 0.11605475102747916, test loss: 0.2536906481704165\n",
      "epoch 12884: train loss: 0.11605231024719138, test loss: 0.2536912747538749\n",
      "epoch 12885: train loss: 0.11604986972925095, test loss: 0.25369190147082415\n",
      "epoch 12886: train loss: 0.11604742947360748, test loss: 0.2536925283212216\n",
      "epoch 12887: train loss: 0.11604498948021057, test loss: 0.25369315530504943\n",
      "epoch 12888: train loss: 0.11604254974900988, test loss: 0.25369378242224494\n",
      "epoch 12889: train loss: 0.11604011027995503, test loss: 0.25369440967280643\n",
      "epoch 12890: train loss: 0.11603767107299567, test loss: 0.2536950370567012\n",
      "epoch 12891: train loss: 0.11603523212808156, test loss: 0.25369566457387943\n",
      "epoch 12892: train loss: 0.11603279344516229, test loss: 0.2536962922243181\n",
      "epoch 12893: train loss: 0.11603035502418763, test loss: 0.2536969200079979\n",
      "epoch 12894: train loss: 0.11602791686510727, test loss: 0.25369754792487165\n",
      "epoch 12895: train loss: 0.11602547896787094, test loss: 0.2536981759749153\n",
      "epoch 12896: train loss: 0.11602304133242837, test loss: 0.2536988041580942\n",
      "epoch 12897: train loss: 0.11602060395872936, test loss: 0.2536994324743686\n",
      "epoch 12898: train loss: 0.11601816684672364, test loss: 0.25370006092374553\n",
      "epoch 12899: train loss: 0.11601572999636105, test loss: 0.25370068950613023\n",
      "epoch 12900: train loss: 0.11601329340759131, test loss: 0.25370131822155045\n",
      "epoch 12901: train loss: 0.1160108570803643, test loss: 0.2537019470699512\n",
      "epoch 12902: train loss: 0.11600842101462978, test loss: 0.2537025760512932\n",
      "epoch 12903: train loss: 0.11600598521033767, test loss: 0.25370320516556666\n",
      "epoch 12904: train loss: 0.11600354966743776, test loss: 0.2537038344127078\n",
      "epoch 12905: train loss: 0.11600111438587994, test loss: 0.25370446379272904\n",
      "epoch 12906: train loss: 0.11599867936561409, test loss: 0.2537050933055538\n",
      "epoch 12907: train loss: 0.11599624460659012, test loss: 0.25370572295118\n",
      "epoch 12908: train loss: 0.11599381010875788, test loss: 0.25370635272956604\n",
      "epoch 12909: train loss: 0.11599137587206737, test loss: 0.25370698264068864\n",
      "epoch 12910: train loss: 0.1159889418964685, test loss: 0.2537076126845076\n",
      "epoch 12911: train loss: 0.11598650818191114, test loss: 0.2537082428609901\n",
      "epoch 12912: train loss: 0.11598407472834538, test loss: 0.25370887317011176\n",
      "epoch 12913: train loss: 0.1159816415357211, test loss: 0.2537095036118475\n",
      "epoch 12914: train loss: 0.11597920860398833, test loss: 0.253710134186173\n",
      "epoch 12915: train loss: 0.11597677593309705, test loss: 0.2537107648930187\n",
      "epoch 12916: train loss: 0.11597434352299729, test loss: 0.2537113957323804\n",
      "epoch 12917: train loss: 0.11597191137363907, test loss: 0.2537120267042354\n",
      "epoch 12918: train loss: 0.11596947948497249, test loss: 0.2537126578085281\n",
      "epoch 12919: train loss: 0.1159670478569475, test loss: 0.25371328904525614\n",
      "epoch 12920: train loss: 0.11596461648951427, test loss: 0.25371392041436586\n",
      "epoch 12921: train loss: 0.11596218538262282, test loss: 0.25371455191583986\n",
      "epoch 12922: train loss: 0.11595975453622329, test loss: 0.25371518354963013\n",
      "epoch 12923: train loss: 0.11595732395026576, test loss: 0.2537158153157196\n",
      "epoch 12924: train loss: 0.11595489362470034, test loss: 0.25371644721407755\n",
      "epoch 12925: train loss: 0.11595246355947722, test loss: 0.2537170792446706\n",
      "epoch 12926: train loss: 0.1159500337545465, test loss: 0.2537177114074672\n",
      "epoch 12927: train loss: 0.11594760420985839, test loss: 0.25371834370241986\n",
      "epoch 12928: train loss: 0.11594517492536305, test loss: 0.2537189761295275\n",
      "epoch 12929: train loss: 0.11594274590101064, test loss: 0.25371960868874205\n",
      "epoch 12930: train loss: 0.1159403171367514, test loss: 0.2537202413800322\n",
      "epoch 12931: train loss: 0.11593788863253554, test loss: 0.2537208742033817\n",
      "epoch 12932: train loss: 0.11593546038831332, test loss: 0.253721507158742\n",
      "epoch 12933: train loss: 0.11593303240403494, test loss: 0.25372214024608186\n",
      "epoch 12934: train loss: 0.11593060467965068, test loss: 0.2537227734653856\n",
      "epoch 12935: train loss: 0.11592817721511078, test loss: 0.253723406816611\n",
      "epoch 12936: train loss: 0.1159257500103656, test loss: 0.25372404029972967\n",
      "epoch 12937: train loss: 0.11592332306536537, test loss: 0.25372467391469977\n",
      "epoch 12938: train loss: 0.11592089638006046, test loss: 0.2537253076615201\n",
      "epoch 12939: train loss: 0.11591846995440115, test loss: 0.2537259415401147\n",
      "epoch 12940: train loss: 0.11591604378833775, test loss: 0.25372657555049577\n",
      "epoch 12941: train loss: 0.11591361788182071, test loss: 0.2537272096926171\n",
      "epoch 12942: train loss: 0.11591119223480036, test loss: 0.2537278439664474\n",
      "epoch 12943: train loss: 0.11590876684722701, test loss: 0.25372847837196155\n",
      "epoch 12944: train loss: 0.11590634171905113, test loss: 0.2537291129091054\n",
      "epoch 12945: train loss: 0.11590391685022311, test loss: 0.25372974757786326\n",
      "epoch 12946: train loss: 0.11590149224069338, test loss: 0.25373038237822515\n",
      "epoch 12947: train loss: 0.11589906789041234, test loss: 0.25373101731012193\n",
      "epoch 12948: train loss: 0.11589664379933044, test loss: 0.2537316523735526\n",
      "epoch 12949: train loss: 0.1158942199673982, test loss: 0.2537322875684696\n",
      "epoch 12950: train loss: 0.11589179639456602, test loss: 0.2537329228948338\n",
      "epoch 12951: train loss: 0.11588937308078438, test loss: 0.25373355835265315\n",
      "epoch 12952: train loss: 0.11588695002600388, test loss: 0.25373419394184915\n",
      "epoch 12953: train loss: 0.11588452723017495, test loss: 0.25373482966242855\n",
      "epoch 12954: train loss: 0.11588210469324814, test loss: 0.25373546551435266\n",
      "epoch 12955: train loss: 0.11587968241517396, test loss: 0.253736101497574\n",
      "epoch 12956: train loss: 0.11587726039590303, test loss: 0.2537367376120692\n",
      "epoch 12957: train loss: 0.11587483863538588, test loss: 0.25373737385780687\n",
      "epoch 12958: train loss: 0.11587241713357307, test loss: 0.25373801023477843\n",
      "epoch 12959: train loss: 0.11586999589041526, test loss: 0.2537386467429145\n",
      "epoch 12960: train loss: 0.11586757490586294, test loss: 0.2537392833822135\n",
      "epoch 12961: train loss: 0.11586515417986686, test loss: 0.2537399201526151\n",
      "epoch 12962: train loss: 0.11586273371237757, test loss: 0.25374055705413207\n",
      "epoch 12963: train loss: 0.11586031350334576, test loss: 0.2537411940867033\n",
      "epoch 12964: train loss: 0.11585789355272208, test loss: 0.2537418312503046\n",
      "epoch 12965: train loss: 0.11585547386045718, test loss: 0.2537424685449128\n",
      "epoch 12966: train loss: 0.11585305442650178, test loss: 0.2537431059704824\n",
      "epoch 12967: train loss: 0.11585063525080656, test loss: 0.25374374352698775\n",
      "epoch 12968: train loss: 0.11584821633332223, test loss: 0.25374438121441406\n",
      "epoch 12969: train loss: 0.11584579767399955, test loss: 0.25374501903271535\n",
      "epoch 12970: train loss: 0.11584337927278926, test loss: 0.2537456569818585\n",
      "epoch 12971: train loss: 0.11584096112964208, test loss: 0.2537462950618145\n",
      "epoch 12972: train loss: 0.11583854324450876, test loss: 0.25374693327254394\n",
      "epoch 12973: train loss: 0.11583612561734015, test loss: 0.25374757161404565\n",
      "epoch 12974: train loss: 0.11583370824808696, test loss: 0.25374821008627335\n",
      "epoch 12975: train loss: 0.11583129113670008, test loss: 0.253748848689182\n",
      "epoch 12976: train loss: 0.11582887428313027, test loss: 0.2537494874227694\n",
      "epoch 12977: train loss: 0.11582645768732841, test loss: 0.25375012628697496\n",
      "epoch 12978: train loss: 0.1158240413492453, test loss: 0.25375076528179796\n",
      "epoch 12979: train loss: 0.11582162526883183, test loss: 0.2537514044071679\n",
      "epoch 12980: train loss: 0.11581920944603886, test loss: 0.2537520436631027\n",
      "epoch 12981: train loss: 0.11581679388081731, test loss: 0.25375268304953785\n",
      "epoch 12982: train loss: 0.11581437857311801, test loss: 0.2537533225664594\n",
      "epoch 12983: train loss: 0.11581196352289194, test loss: 0.25375396221382057\n",
      "epoch 12984: train loss: 0.11580954873008999, test loss: 0.25375460199159716\n",
      "epoch 12985: train loss: 0.11580713419466311, test loss: 0.2537552418997755\n",
      "epoch 12986: train loss: 0.11580471991656228, test loss: 0.2537558819382933\n",
      "epoch 12987: train loss: 0.11580230589573841, test loss: 0.25375652210715016\n",
      "epoch 12988: train loss: 0.1157998921321425, test loss: 0.2537571624063159\n",
      "epoch 12989: train loss: 0.1157974786257256, test loss: 0.2537578028357198\n",
      "epoch 12990: train loss: 0.11579506537643862, test loss: 0.25375844339537873\n",
      "epoch 12991: train loss: 0.11579265238423264, test loss: 0.2537590840852463\n",
      "epoch 12992: train loss: 0.11579023964905868, test loss: 0.2537597249052677\n",
      "epoch 12993: train loss: 0.11578782717086777, test loss: 0.25376036585544476\n",
      "epoch 12994: train loss: 0.11578541494961098, test loss: 0.25376100693575276\n",
      "epoch 12995: train loss: 0.11578300298523939, test loss: 0.25376164814612345\n",
      "epoch 12996: train loss: 0.11578059127770408, test loss: 0.25376228948654855\n",
      "epoch 12997: train loss: 0.11577817982695614, test loss: 0.2537629309570059\n",
      "epoch 12998: train loss: 0.11577576863294667, test loss: 0.2537635725574417\n",
      "epoch 12999: train loss: 0.11577335769562684, test loss: 0.2537642142878478\n",
      "epoch 13000: train loss: 0.11577094701494772, test loss: 0.25376485614818645\n",
      "epoch 13001: train loss: 0.11576853659086055, test loss: 0.2537654981384269\n",
      "epoch 13002: train loss: 0.11576612642331639, test loss: 0.2537661402585318\n",
      "epoch 13003: train loss: 0.11576371651226648, test loss: 0.25376678250847684\n",
      "epoch 13004: train loss: 0.115761306857662, test loss: 0.25376742488824744\n",
      "epoch 13005: train loss: 0.11575889745945414, test loss: 0.2537680673977824\n",
      "epoch 13006: train loss: 0.11575648831759415, test loss: 0.25376871003706775\n",
      "epoch 13007: train loss: 0.11575407943203322, test loss: 0.25376935280608687\n",
      "epoch 13008: train loss: 0.11575167080272258, test loss: 0.25376999570477116\n",
      "epoch 13009: train loss: 0.11574926242961357, test loss: 0.25377063873313743\n",
      "epoch 13010: train loss: 0.11574685431265737, test loss: 0.2537712818911225\n",
      "epoch 13011: train loss: 0.11574444645180532, test loss: 0.25377192517869035\n",
      "epoch 13012: train loss: 0.11574203884700865, test loss: 0.2537725685958407\n",
      "epoch 13013: train loss: 0.11573963149821871, test loss: 0.25377321214252807\n",
      "epoch 13014: train loss: 0.11573722440538683, test loss: 0.25377385581872886\n",
      "epoch 13015: train loss: 0.11573481756846433, test loss: 0.2537744996243987\n",
      "epoch 13016: train loss: 0.11573241098740256, test loss: 0.25377514355950576\n",
      "epoch 13017: train loss: 0.11573000466215289, test loss: 0.25377578762404523\n",
      "epoch 13018: train loss: 0.11572759859266667, test loss: 0.25377643181794574\n",
      "epoch 13019: train loss: 0.1157251927788953, test loss: 0.25377707614122597\n",
      "epoch 13020: train loss: 0.11572278722079017, test loss: 0.2537777205938309\n",
      "epoch 13021: train loss: 0.11572038191830274, test loss: 0.2537783651757225\n",
      "epoch 13022: train loss: 0.11571797687138435, test loss: 0.25377900988687835\n",
      "epoch 13023: train loss: 0.11571557207998649, test loss: 0.2537796547272851\n",
      "epoch 13024: train loss: 0.11571316754406062, test loss: 0.2537802996968811\n",
      "epoch 13025: train loss: 0.1157107632635582, test loss: 0.25378094479566704\n",
      "epoch 13026: train loss: 0.11570835923843069, test loss: 0.25378159002357326\n",
      "epoch 13027: train loss: 0.11570595546862959, test loss: 0.25378223538060973\n",
      "epoch 13028: train loss: 0.11570355195410642, test loss: 0.2537828808667308\n",
      "epoch 13029: train loss: 0.11570114869481266, test loss: 0.25378352648190633\n",
      "epoch 13030: train loss: 0.11569874569069988, test loss: 0.2537841722260981\n",
      "epoch 13031: train loss: 0.11569634294171958, test loss: 0.2537848180992916\n",
      "epoch 13032: train loss: 0.11569394044782336, test loss: 0.25378546410144204\n",
      "epoch 13033: train loss: 0.11569153820896276, test loss: 0.2537861102325278\n",
      "epoch 13034: train loss: 0.11568913622508935, test loss: 0.25378675649250904\n",
      "epoch 13035: train loss: 0.11568673449615478, test loss: 0.25378740288137386\n",
      "epoch 13036: train loss: 0.1156843330221106, test loss: 0.2537880493990918\n",
      "epoch 13037: train loss: 0.11568193180290846, test loss: 0.2537886960456153\n",
      "epoch 13038: train loss: 0.1156795308385, test loss: 0.25378934282090926\n",
      "epoch 13039: train loss: 0.11567713012883689, test loss: 0.25378998972496697\n",
      "epoch 13040: train loss: 0.11567472967387073, test loss: 0.2537906367577411\n",
      "epoch 13041: train loss: 0.11567232947355319, test loss: 0.2537912839192203\n",
      "epoch 13042: train loss: 0.11566992952783597, test loss: 0.25379193120935706\n",
      "epoch 13043: train loss: 0.11566752983667083, test loss: 0.2537925786281304\n",
      "epoch 13044: train loss: 0.11566513040000942, test loss: 0.2537932261755032\n",
      "epoch 13045: train loss: 0.11566273121780349, test loss: 0.2537938738514369\n",
      "epoch 13046: train loss: 0.11566033229000475, test loss: 0.2537945216559331\n",
      "epoch 13047: train loss: 0.11565793361656498, test loss: 0.25379516958893195\n",
      "epoch 13048: train loss: 0.11565553519743592, test loss: 0.253795817650419\n",
      "epoch 13049: train loss: 0.11565313703256937, test loss: 0.25379646584035676\n",
      "epoch 13050: train loss: 0.1156507391219171, test loss: 0.2537971141587068\n",
      "epoch 13051: train loss: 0.1156483414654309, test loss: 0.2537977626054574\n",
      "epoch 13052: train loss: 0.11564594406306265, test loss: 0.25379841118059304\n",
      "epoch 13053: train loss: 0.1156435469147641, test loss: 0.253799059884029\n",
      "epoch 13054: train loss: 0.11564115002048717, test loss: 0.2537997087157767\n",
      "epoch 13055: train loss: 0.11563875338018363, test loss: 0.25380035767580594\n",
      "epoch 13056: train loss: 0.11563635699380537, test loss: 0.25380100676408573\n",
      "epoch 13057: train loss: 0.11563396086130431, test loss: 0.2538016559805656\n",
      "epoch 13058: train loss: 0.11563156498263233, test loss: 0.25380230532523784\n",
      "epoch 13059: train loss: 0.11562916935774131, test loss: 0.2538029547980583\n",
      "epoch 13060: train loss: 0.11562677398658318, test loss: 0.25380360439898925\n",
      "epoch 13061: train loss: 0.11562437886910987, test loss: 0.2538042541280324\n",
      "epoch 13062: train loss: 0.11562198400527335, test loss: 0.2538049039851442\n",
      "epoch 13063: train loss: 0.11561958939502553, test loss: 0.25380555397028637\n",
      "epoch 13064: train loss: 0.11561719503831842, test loss: 0.2538062040834207\n",
      "epoch 13065: train loss: 0.11561480093510397, test loss: 0.25380685432454314\n",
      "epoch 13066: train loss: 0.11561240708533421, test loss: 0.25380750469359303\n",
      "epoch 13067: train loss: 0.11561001348896113, test loss: 0.2538081551905718\n",
      "epoch 13068: train loss: 0.11560762014593676, test loss: 0.25380880581544246\n",
      "epoch 13069: train loss: 0.11560522705621311, test loss: 0.2538094565681524\n",
      "epoch 13070: train loss: 0.11560283421974227, test loss: 0.25381010744870297\n",
      "epoch 13071: train loss: 0.11560044163647629, test loss: 0.2538107584570274\n",
      "epoch 13072: train loss: 0.11559804930636718, test loss: 0.25381140959313475\n",
      "epoch 13073: train loss: 0.1155956572293671, test loss: 0.25381206085698876\n",
      "epoch 13074: train loss: 0.11559326540542812, test loss: 0.25381271224853597\n",
      "epoch 13075: train loss: 0.11559087383450237, test loss: 0.2538133637677475\n",
      "epoch 13076: train loss: 0.11558848251654194, test loss: 0.25381401541462545\n",
      "epoch 13077: train loss: 0.11558609145149899, test loss: 0.25381466718910944\n",
      "epoch 13078: train loss: 0.11558370063932567, test loss: 0.2538153190911795\n",
      "epoch 13079: train loss: 0.11558131007997413, test loss: 0.2538159711208203\n",
      "epoch 13080: train loss: 0.11557891977339656, test loss: 0.2538166232779806\n",
      "epoch 13081: train loss: 0.11557652971954513, test loss: 0.2538172755626397\n",
      "epoch 13082: train loss: 0.11557413991837205, test loss: 0.253817927974768\n",
      "epoch 13083: train loss: 0.11557175036982956, test loss: 0.2538185805143438\n",
      "epoch 13084: train loss: 0.11556936107386985, test loss: 0.2538192331813155\n",
      "epoch 13085: train loss: 0.11556697203044518, test loss: 0.2538198859756689\n",
      "epoch 13086: train loss: 0.11556458323950779, test loss: 0.25382053889737677\n",
      "epoch 13087: train loss: 0.11556219470100991, test loss: 0.25382119194640923\n",
      "epoch 13088: train loss: 0.11555980641490392, test loss: 0.2538218451227147\n",
      "epoch 13089: train loss: 0.11555741838114202, test loss: 0.2538224984263017\n",
      "epoch 13090: train loss: 0.11555503059967652, test loss: 0.2538231518571191\n",
      "epoch 13091: train loss: 0.11555264307045979, test loss: 0.25382380541513105\n",
      "epoch 13092: train loss: 0.1155502557934441, test loss: 0.25382445910030005\n",
      "epoch 13093: train loss: 0.11554786876858182, test loss: 0.25382511291263676\n",
      "epoch 13094: train loss: 0.1155454819958253, test loss: 0.2538257668520653\n",
      "epoch 13095: train loss: 0.11554309547512691, test loss: 0.25382642091859614\n",
      "epoch 13096: train loss: 0.11554070920643905, test loss: 0.25382707511216956\n",
      "epoch 13097: train loss: 0.11553832318971406, test loss: 0.2538277294327641\n",
      "epoch 13098: train loss: 0.11553593742490438, test loss: 0.2538283838803765\n",
      "epoch 13099: train loss: 0.11553355191196245, test loss: 0.2538290384549289\n",
      "epoch 13100: train loss: 0.11553116665084064, test loss: 0.2538296931564319\n",
      "epoch 13101: train loss: 0.11552878164149143, test loss: 0.25383034798481896\n",
      "epoch 13102: train loss: 0.11552639688386726, test loss: 0.25383100294010064\n",
      "epoch 13103: train loss: 0.11552401237792065, test loss: 0.2538316580222317\n",
      "epoch 13104: train loss: 0.115521628123604, test loss: 0.2538323132311769\n",
      "epoch 13105: train loss: 0.11551924412086986, test loss: 0.2538329685668998\n",
      "epoch 13106: train loss: 0.11551686036967074, test loss: 0.2538336240293866\n",
      "epoch 13107: train loss: 0.11551447686995912, test loss: 0.2538342796186013\n",
      "epoch 13108: train loss: 0.11551209362168756, test loss: 0.25383493533452417\n",
      "epoch 13109: train loss: 0.1155097106248086, test loss: 0.25383559117710297\n",
      "epoch 13110: train loss: 0.1155073278792748, test loss: 0.25383624714634023\n",
      "epoch 13111: train loss: 0.1155049453850387, test loss: 0.25383690324218383\n",
      "epoch 13112: train loss: 0.11550256314205294, test loss: 0.25383755946459785\n",
      "epoch 13113: train loss: 0.11550018115027008, test loss: 0.2538382158135701\n",
      "epoch 13114: train loss: 0.1154977994096427, test loss: 0.25383887228906366\n",
      "epoch 13115: train loss: 0.11549541792012347, test loss: 0.2538395288910585\n",
      "epoch 13116: train loss: 0.11549303668166504, test loss: 0.2538401856195035\n",
      "epoch 13117: train loss: 0.11549065569421998, test loss: 0.2538408424743924\n",
      "epoch 13118: train loss: 0.11548827495774101, test loss: 0.25384149945567386\n",
      "epoch 13119: train loss: 0.11548589447218077, test loss: 0.25384215656335185\n",
      "epoch 13120: train loss: 0.11548351423749197, test loss: 0.2538428137973586\n",
      "epoch 13121: train loss: 0.11548113425362727, test loss: 0.25384347115768896\n",
      "epoch 13122: train loss: 0.1154787545205394, test loss: 0.25384412864431466\n",
      "epoch 13123: train loss: 0.11547637503818113, test loss: 0.2538447862571847\n",
      "epoch 13124: train loss: 0.1154739958065051, test loss: 0.2538454439962792\n",
      "epoch 13125: train loss: 0.11547161682546411, test loss: 0.2538461018615921\n",
      "epoch 13126: train loss: 0.11546923809501092, test loss: 0.2538467598530567\n",
      "epoch 13127: train loss: 0.11546685961509828, test loss: 0.25384741797067684\n",
      "epoch 13128: train loss: 0.11546448138567902, test loss: 0.25384807621440114\n",
      "epoch 13129: train loss: 0.11546210340670589, test loss: 0.2538487345842083\n",
      "epoch 13130: train loss: 0.11545972567813173, test loss: 0.2538493930800623\n",
      "epoch 13131: train loss: 0.11545734819990933, test loss: 0.2538500517019607\n",
      "epoch 13132: train loss: 0.11545497097199156, test loss: 0.25385071044984175\n",
      "epoch 13133: train loss: 0.11545259399433125, test loss: 0.2538513693236784\n",
      "epoch 13134: train loss: 0.11545021726688125, test loss: 0.2538520283234514\n",
      "epoch 13135: train loss: 0.11544784078959446, test loss: 0.2538526874491317\n",
      "epoch 13136: train loss: 0.11544546456242374, test loss: 0.2538533467007075\n",
      "epoch 13137: train loss: 0.11544308858532201, test loss: 0.25385400607811287\n",
      "epoch 13138: train loss: 0.11544071285824216, test loss: 0.25385466558133357\n",
      "epoch 13139: train loss: 0.11543833738113714, test loss: 0.2538553252103496\n",
      "epoch 13140: train loss: 0.11543596215395985, test loss: 0.2538559849651348\n",
      "epoch 13141: train loss: 0.11543358717666326, test loss: 0.25385664484563714\n",
      "epoch 13142: train loss: 0.11543121244920032, test loss: 0.25385730485185287\n",
      "epoch 13143: train loss: 0.11542883797152402, test loss: 0.25385796498374474\n",
      "epoch 13144: train loss: 0.11542646374358732, test loss: 0.2538586252412625\n",
      "epoch 13145: train loss: 0.11542408976534321, test loss: 0.25385928562441007\n",
      "epoch 13146: train loss: 0.11542171603674475, test loss: 0.2538599461331288\n",
      "epoch 13147: train loss: 0.11541934255774498, test loss: 0.2538606067674131\n",
      "epoch 13148: train loss: 0.11541696932829684, test loss: 0.25386126752721994\n",
      "epoch 13149: train loss: 0.11541459634835344, test loss: 0.25386192841253086\n",
      "epoch 13150: train loss: 0.11541222361786783, test loss: 0.2538625894233092\n",
      "epoch 13151: train loss: 0.11540985113679308, test loss: 0.2538632505595197\n",
      "epoch 13152: train loss: 0.11540747890508228, test loss: 0.25386391182114226\n",
      "epoch 13153: train loss: 0.11540510692268857, test loss: 0.2538645732081424\n",
      "epoch 13154: train loss: 0.11540273518956497, test loss: 0.253865234720507\n",
      "epoch 13155: train loss: 0.11540036370566466, test loss: 0.25386589635818607\n",
      "epoch 13156: train loss: 0.11539799247094078, test loss: 0.2538665581211514\n",
      "epoch 13157: train loss: 0.11539562148534645, test loss: 0.2538672200093838\n",
      "epoch 13158: train loss: 0.11539325074883483, test loss: 0.25386788202285604\n",
      "epoch 13159: train loss: 0.11539088026135914, test loss: 0.25386854416153226\n",
      "epoch 13160: train loss: 0.11538851002287252, test loss: 0.25386920642538524\n",
      "epoch 13161: train loss: 0.11538614003332817, test loss: 0.25386986881440293\n",
      "epoch 13162: train loss: 0.11538377029267932, test loss: 0.2538705313285189\n",
      "epoch 13163: train loss: 0.11538140080087922, test loss: 0.25387119396774493\n",
      "epoch 13164: train loss: 0.11537903155788104, test loss: 0.2538718567320139\n",
      "epoch 13165: train loss: 0.11537666256363804, test loss: 0.2538725196213234\n",
      "epoch 13166: train loss: 0.11537429381810352, test loss: 0.2538731826356218\n",
      "epoch 13167: train loss: 0.11537192532123072, test loss: 0.2538738457749213\n",
      "epoch 13168: train loss: 0.11536955707297293, test loss: 0.2538745090391384\n",
      "epoch 13169: train loss: 0.11536718907328346, test loss: 0.2538751724282864\n",
      "epoch 13170: train loss: 0.11536482132211563, test loss: 0.2538758359423229\n",
      "epoch 13171: train loss: 0.11536245381942273, test loss: 0.2538764995812191\n",
      "epoch 13172: train loss: 0.1153600865651581, test loss: 0.25387716334493987\n",
      "epoch 13173: train loss: 0.11535771955927512, test loss: 0.25387782723345165\n",
      "epoch 13174: train loss: 0.11535535280172708, test loss: 0.2538784912467506\n",
      "epoch 13175: train loss: 0.11535298629246743, test loss: 0.25387915538477657\n",
      "epoch 13176: train loss: 0.11535062003144951, test loss: 0.2538798196475192\n",
      "epoch 13177: train loss: 0.11534825401862672, test loss: 0.25388048403496016\n",
      "epoch 13178: train loss: 0.1153458882539525, test loss: 0.2538811485470474\n",
      "epoch 13179: train loss: 0.11534352273738023, test loss: 0.2538818131837616\n",
      "epoch 13180: train loss: 0.11534115746886336, test loss: 0.2538824779450686\n",
      "epoch 13181: train loss: 0.11533879244835536, test loss: 0.2538831428309491\n",
      "epoch 13182: train loss: 0.11533642767580961, test loss: 0.25388380784137604\n",
      "epoch 13183: train loss: 0.11533406315117968, test loss: 0.2538844729763073\n",
      "epoch 13184: train loss: 0.11533169887441899, test loss: 0.2538851382357222\n",
      "epoch 13185: train loss: 0.11532933484548107, test loss: 0.2538858036195867\n",
      "epoch 13186: train loss: 0.1153269710643194, test loss: 0.2538864691278816\n",
      "epoch 13187: train loss: 0.11532460753088752, test loss: 0.2538871347605566\n",
      "epoch 13188: train loss: 0.11532224424513896, test loss: 0.2538878005176172\n",
      "epoch 13189: train loss: 0.11531988120702723, test loss: 0.2538884663990038\n",
      "epoch 13190: train loss: 0.11531751841650595, test loss: 0.2538891324046971\n",
      "epoch 13191: train loss: 0.11531515587352864, test loss: 0.2538897985346948\n",
      "epoch 13192: train loss: 0.11531279357804891, test loss: 0.2538904647889143\n",
      "epoch 13193: train loss: 0.11531043153002031, test loss: 0.25389113116737666\n",
      "epoch 13194: train loss: 0.1153080697293965, test loss: 0.2538917976700156\n",
      "epoch 13195: train loss: 0.11530570817613107, test loss: 0.25389246429683543\n",
      "epoch 13196: train loss: 0.11530334687017768, test loss: 0.25389313104778644\n",
      "epoch 13197: train loss: 0.11530098581148993, test loss: 0.25389379792284034\n",
      "epoch 13198: train loss: 0.1152986250000215, test loss: 0.2538944649219804\n",
      "epoch 13199: train loss: 0.11529626443572605, test loss: 0.25389513204517117\n",
      "epoch 13200: train loss: 0.11529390411855726, test loss: 0.2538957992923771\n",
      "epoch 13201: train loss: 0.11529154404846884, test loss: 0.25389646666358046\n",
      "epoch 13202: train loss: 0.11528918422541447, test loss: 0.25389713415873144\n",
      "epoch 13203: train loss: 0.11528682464934789, test loss: 0.2538978017778338\n",
      "epoch 13204: train loss: 0.1152844653202228, test loss: 0.2538984695208378\n",
      "epoch 13205: train loss: 0.11528210623799297, test loss: 0.25389913738771663\n",
      "epoch 13206: train loss: 0.11527974740261214, test loss: 0.25389980537845214\n",
      "epoch 13207: train loss: 0.11527738881403408, test loss: 0.2539004734930016\n",
      "epoch 13208: train loss: 0.11527503047221258, test loss: 0.2539011417313468\n",
      "epoch 13209: train loss: 0.11527267237710138, test loss: 0.2539018100934527\n",
      "epoch 13210: train loss: 0.11527031452865437, test loss: 0.2539024785792926\n",
      "epoch 13211: train loss: 0.1152679569268253, test loss: 0.2539031471888413\n",
      "epoch 13212: train loss: 0.11526559957156798, test loss: 0.25390381592206385\n",
      "epoch 13213: train loss: 0.11526324246283633, test loss: 0.25390448477893257\n",
      "epoch 13214: train loss: 0.11526088560058412, test loss: 0.2539051537594314\n",
      "epoch 13215: train loss: 0.11525852898476525, test loss: 0.25390582286350827\n",
      "epoch 13216: train loss: 0.11525617261533362, test loss: 0.2539064920911457\n",
      "epoch 13217: train loss: 0.11525381649224307, test loss: 0.2539071614423251\n",
      "epoch 13218: train loss: 0.11525146061544753, test loss: 0.2539078309170112\n",
      "epoch 13219: train loss: 0.1152491049849009, test loss: 0.2539085005151637\n",
      "epoch 13220: train loss: 0.11524674960055713, test loss: 0.2539091702367784\n",
      "epoch 13221: train loss: 0.11524439446237011, test loss: 0.2539098400818058\n",
      "epoch 13222: train loss: 0.11524203957029386, test loss: 0.25391051005022885\n",
      "epoch 13223: train loss: 0.11523968492428226, test loss: 0.2539111801420113\n",
      "epoch 13224: train loss: 0.11523733052428935, test loss: 0.2539118503571117\n",
      "epoch 13225: train loss: 0.11523497637026907, test loss: 0.2539125206955288\n",
      "epoch 13226: train loss: 0.11523262246217542, test loss: 0.25391319115722716\n",
      "epoch 13227: train loss: 0.11523026879996244, test loss: 0.2539138617421734\n",
      "epoch 13228: train loss: 0.11522791538358415, test loss: 0.2539145324503333\n",
      "epoch 13229: train loss: 0.11522556221299458, test loss: 0.25391520328168854\n",
      "epoch 13230: train loss: 0.11522320928814775, test loss: 0.2539158742362135\n",
      "epoch 13231: train loss: 0.11522085660899777, test loss: 0.2539165453138576\n",
      "epoch 13232: train loss: 0.11521850417549866, test loss: 0.2539172165146193\n",
      "epoch 13233: train loss: 0.11521615198760454, test loss: 0.25391788783844793\n",
      "epoch 13234: train loss: 0.11521380004526946, test loss: 0.25391855928532575\n",
      "epoch 13235: train loss: 0.11521144834844758, test loss: 0.2539192308552266\n",
      "epoch 13236: train loss: 0.115209096897093, test loss: 0.25391990254813324\n",
      "epoch 13237: train loss: 0.11520674569115982, test loss: 0.25392057436397153\n",
      "epoch 13238: train loss: 0.11520439473060225, test loss: 0.253921246302771\n",
      "epoch 13239: train loss: 0.11520204401537437, test loss: 0.2539219183644739\n",
      "epoch 13240: train loss: 0.11519969354543043, test loss: 0.25392259054903815\n",
      "epoch 13241: train loss: 0.11519734332072457, test loss: 0.25392326285647077\n",
      "epoch 13242: train loss: 0.11519499334121094, test loss: 0.2539239352867051\n",
      "epoch 13243: train loss: 0.1151926436068438, test loss: 0.25392460783973964\n",
      "epoch 13244: train loss: 0.11519029411757735, test loss: 0.25392528051554\n",
      "epoch 13245: train loss: 0.11518794487336584, test loss: 0.25392595331408957\n",
      "epoch 13246: train loss: 0.11518559587416348, test loss: 0.25392662623533735\n",
      "epoch 13247: train loss: 0.11518324711992452, test loss: 0.25392729927925883\n",
      "epoch 13248: train loss: 0.11518089861060327, test loss: 0.25392797244582677\n",
      "epoch 13249: train loss: 0.11517855034615397, test loss: 0.2539286457350326\n",
      "epoch 13250: train loss: 0.11517620232653089, test loss: 0.253929319146826\n",
      "epoch 13251: train loss: 0.11517385455168835, test loss: 0.2539299926811734\n",
      "epoch 13252: train loss: 0.11517150702158072, test loss: 0.2539306663380576\n",
      "epoch 13253: train loss: 0.11516915973616224, test loss: 0.2539313401174683\n",
      "epoch 13254: train loss: 0.11516681269538727, test loss: 0.2539320140193481\n",
      "epoch 13255: train loss: 0.1151644658992102, test loss: 0.25393268804368774\n",
      "epoch 13256: train loss: 0.11516211934758534, test loss: 0.25393336219044516\n",
      "epoch 13257: train loss: 0.11515977304046711, test loss: 0.25393403645960294\n",
      "epoch 13258: train loss: 0.11515742697780987, test loss: 0.2539347108511115\n",
      "epoch 13259: train loss: 0.115155081159568, test loss: 0.25393538536498556\n",
      "epoch 13260: train loss: 0.11515273558569597, test loss: 0.2539360600011428\n",
      "epoch 13261: train loss: 0.11515039025614811, test loss: 0.25393673475959894\n",
      "epoch 13262: train loss: 0.11514804517087894, test loss: 0.25393740964030465\n",
      "epoch 13263: train loss: 0.11514570032984286, test loss: 0.25393808464324885\n",
      "epoch 13264: train loss: 0.11514335573299438, test loss: 0.2539387597683829\n",
      "epoch 13265: train loss: 0.11514101138028789, test loss: 0.25393943501568195\n",
      "epoch 13266: train loss: 0.11513866727167793, test loss: 0.2539401103851281\n",
      "epoch 13267: train loss: 0.11513632340711895, test loss: 0.2539407858766707\n",
      "epoch 13268: train loss: 0.11513397978656549, test loss: 0.2539414614903191\n",
      "epoch 13269: train loss: 0.11513163640997208, test loss: 0.2539421372260058\n",
      "epoch 13270: train loss: 0.11512929327729321, test loss: 0.2539428130837384\n",
      "epoch 13271: train loss: 0.11512695038848346, test loss: 0.2539434890634663\n",
      "epoch 13272: train loss: 0.11512460774349735, test loss: 0.25394416516515916\n",
      "epoch 13273: train loss: 0.11512226534228943, test loss: 0.2539448413887879\n",
      "epoch 13274: train loss: 0.11511992318481436, test loss: 0.25394551773435503\n",
      "epoch 13275: train loss: 0.11511758127102667, test loss: 0.25394619420179193\n",
      "epoch 13276: train loss: 0.11511523960088098, test loss: 0.25394687079109973\n",
      "epoch 13277: train loss: 0.11511289817433187, test loss: 0.25394754750223614\n",
      "epoch 13278: train loss: 0.115110556991334, test loss: 0.2539482243351756\n",
      "epoch 13279: train loss: 0.11510821605184202, test loss: 0.25394890128988673\n",
      "epoch 13280: train loss: 0.11510587535581054, test loss: 0.2539495783663503\n",
      "epoch 13281: train loss: 0.11510353490319425, test loss: 0.25395025556452683\n",
      "epoch 13282: train loss: 0.1151011946939478, test loss: 0.2539509328843982\n",
      "epoch 13283: train loss: 0.11509885472802589, test loss: 0.2539516103259152\n",
      "epoch 13284: train loss: 0.11509651500538322, test loss: 0.2539522878890855\n",
      "epoch 13285: train loss: 0.11509417552597448, test loss: 0.2539529655738516\n",
      "epoch 13286: train loss: 0.11509183628975445, test loss: 0.253953643380213\n",
      "epoch 13287: train loss: 0.11508949729667776, test loss: 0.2539543213081039\n",
      "epoch 13288: train loss: 0.11508715854669926, test loss: 0.2539549993575327\n",
      "epoch 13289: train loss: 0.11508482003977365, test loss: 0.25395567752844794\n",
      "epoch 13290: train loss: 0.11508248177585573, test loss: 0.2539563558208354\n",
      "epoch 13291: train loss: 0.11508014375490024, test loss: 0.253957034234652\n",
      "epoch 13292: train loss: 0.115077805976862, test loss: 0.2539577127698734\n",
      "epoch 13293: train loss: 0.11507546844169582, test loss: 0.2539583914264838\n",
      "epoch 13294: train loss: 0.11507313114935648, test loss: 0.25395907020444847\n",
      "epoch 13295: train loss: 0.11507079409979887, test loss: 0.25395974910374475\n",
      "epoch 13296: train loss: 0.1150684572929778, test loss: 0.25396042812433817\n",
      "epoch 13297: train loss: 0.1150661207288481, test loss: 0.2539611072661961\n",
      "epoch 13298: train loss: 0.11506378440736466, test loss: 0.2539617865292863\n",
      "epoch 13299: train loss: 0.11506144832848236, test loss: 0.2539624659136074\n",
      "epoch 13300: train loss: 0.11505911249215604, test loss: 0.2539631454190945\n",
      "epoch 13301: train loss: 0.11505677689834068, test loss: 0.25396382504575543\n",
      "epoch 13302: train loss: 0.11505444154699113, test loss: 0.25396450479354127\n",
      "epoch 13303: train loss: 0.11505210643806234, test loss: 0.25396518466243534\n",
      "epoch 13304: train loss: 0.11504977157150922, test loss: 0.25396586465239696\n",
      "epoch 13305: train loss: 0.11504743694728674, test loss: 0.253966544763417\n",
      "epoch 13306: train loss: 0.11504510256534989, test loss: 0.2539672249954478\n",
      "epoch 13307: train loss: 0.11504276842565356, test loss: 0.25396790534846353\n",
      "epoch 13308: train loss: 0.11504043452815281, test loss: 0.2539685858224394\n",
      "epoch 13309: train loss: 0.11503810087280257, test loss: 0.25396926641736023\n",
      "epoch 13310: train loss: 0.1150357674595579, test loss: 0.25396994713317633\n",
      "epoch 13311: train loss: 0.11503343428837381, test loss: 0.25397062796988834\n",
      "epoch 13312: train loss: 0.11503110135920533, test loss: 0.2539713089274386\n",
      "epoch 13313: train loss: 0.11502876867200744, test loss: 0.2539719900058014\n",
      "epoch 13314: train loss: 0.11502643622673528, test loss: 0.25397267120497924\n",
      "epoch 13315: train loss: 0.11502410402334387, test loss: 0.25397335252491265\n",
      "epoch 13316: train loss: 0.1150217720617883, test loss: 0.25397403396557894\n",
      "epoch 13317: train loss: 0.11501944034202367, test loss: 0.25397471552697776\n",
      "epoch 13318: train loss: 0.11501710886400505, test loss: 0.2539753972090439\n",
      "epoch 13319: train loss: 0.11501477762768758, test loss: 0.253976079011769\n",
      "epoch 13320: train loss: 0.1150124466330264, test loss: 0.25397676093512844\n",
      "epoch 13321: train loss: 0.1150101158799766, test loss: 0.25397744297909064\n",
      "epoch 13322: train loss: 0.11500778536849332, test loss: 0.2539781251436147\n",
      "epoch 13323: train loss: 0.11500545509853179, test loss: 0.25397880742868323\n",
      "epoch 13324: train loss: 0.11500312507004713, test loss: 0.25397948983427465\n",
      "epoch 13325: train loss: 0.11500079528299455, test loss: 0.25398017236034537\n",
      "epoch 13326: train loss: 0.11499846573732922, test loss: 0.2539808550068819\n",
      "epoch 13327: train loss: 0.11499613643300638, test loss: 0.2539815377738665\n",
      "epoch 13328: train loss: 0.1149938073699812, test loss: 0.2539822206612431\n",
      "epoch 13329: train loss: 0.11499147854820894, test loss: 0.25398290366900367\n",
      "epoch 13330: train loss: 0.11498914996764485, test loss: 0.2539835867970988\n",
      "epoch 13331: train loss: 0.1149868216282442, test loss: 0.25398427004553875\n",
      "epoch 13332: train loss: 0.11498449352996219, test loss: 0.2539849534142482\n",
      "epoch 13333: train loss: 0.11498216567275416, test loss: 0.2539856369032461\n",
      "epoch 13334: train loss: 0.11497983805657537, test loss: 0.25398632051248216\n",
      "epoch 13335: train loss: 0.11497751068138115, test loss: 0.253987004241925\n",
      "epoch 13336: train loss: 0.11497518354712678, test loss: 0.25398768809155925\n",
      "epoch 13337: train loss: 0.1149728566537676, test loss: 0.25398837206133495\n",
      "epoch 13338: train loss: 0.11497053000125894, test loss: 0.25398905615123735\n",
      "epoch 13339: train loss: 0.11496820358955614, test loss: 0.25398974036124994\n",
      "epoch 13340: train loss: 0.1149658774186146, test loss: 0.25399042469134087\n",
      "epoch 13341: train loss: 0.11496355148838966, test loss: 0.2539911091414705\n",
      "epoch 13342: train loss: 0.11496122579883668, test loss: 0.2539917937116133\n",
      "epoch 13343: train loss: 0.1149589003499111, test loss: 0.25399247840176337\n",
      "epoch 13344: train loss: 0.11495657514156832, test loss: 0.2539931632118557\n",
      "epoch 13345: train loss: 0.11495425017376373, test loss: 0.2539938481418975\n",
      "epoch 13346: train loss: 0.11495192544645279, test loss: 0.2539945331918429\n",
      "epoch 13347: train loss: 0.11494960095959093, test loss: 0.2539952183616661\n",
      "epoch 13348: train loss: 0.11494727671313358, test loss: 0.2539959036513434\n",
      "epoch 13349: train loss: 0.11494495270703625, test loss: 0.25399658906084494\n",
      "epoch 13350: train loss: 0.11494262894125437, test loss: 0.2539972745901276\n",
      "epoch 13351: train loss: 0.1149403054157435, test loss: 0.2539979602392024\n",
      "epoch 13352: train loss: 0.11493798213045907, test loss: 0.2539986460080113\n",
      "epoch 13353: train loss: 0.11493565908535659, test loss: 0.25399933189652396\n",
      "epoch 13354: train loss: 0.11493333628039164, test loss: 0.25400001790473964\n",
      "epoch 13355: train loss: 0.11493101371551971, test loss: 0.2540007040326041\n",
      "epoch 13356: train loss: 0.11492869139069636, test loss: 0.25400139028010776\n",
      "epoch 13357: train loss: 0.11492636930587716, test loss: 0.2540020766472123\n",
      "epoch 13358: train loss: 0.11492404746101766, test loss: 0.2540027631338935\n",
      "epoch 13359: train loss: 0.11492172585607346, test loss: 0.2540034497401188\n",
      "epoch 13360: train loss: 0.11491940449100013, test loss: 0.2540041364658653\n",
      "epoch 13361: train loss: 0.11491708336575329, test loss: 0.2540048233111101\n",
      "epoch 13362: train loss: 0.11491476248028856, test loss: 0.25400551027582063\n",
      "epoch 13363: train loss: 0.11491244183456156, test loss: 0.254006197359972\n",
      "epoch 13364: train loss: 0.11491012142852794, test loss: 0.25400688456352677\n",
      "epoch 13365: train loss: 0.11490780126214335, test loss: 0.2540075718864852\n",
      "epoch 13366: train loss: 0.11490548133536346, test loss: 0.254008259328774\n",
      "epoch 13367: train loss: 0.11490316164814392, test loss: 0.25400894689040326\n",
      "epoch 13368: train loss: 0.11490084220044043, test loss: 0.2540096345713324\n",
      "epoch 13369: train loss: 0.11489852299220871, test loss: 0.2540103223715387\n",
      "epoch 13370: train loss: 0.11489620402340442, test loss: 0.2540110102909903\n",
      "epoch 13371: train loss: 0.1148938852939833, test loss: 0.2540116983296718\n",
      "epoch 13372: train loss: 0.11489156680390111, test loss: 0.2540123864875276\n",
      "epoch 13373: train loss: 0.11488924855311357, test loss: 0.254013074764558\n",
      "epoch 13374: train loss: 0.11488693054157643, test loss: 0.2540137631607233\n",
      "epoch 13375: train loss: 0.1148846127692455, test loss: 0.2540144516759938\n",
      "epoch 13376: train loss: 0.11488229523607654, test loss: 0.2540151403103521\n",
      "epoch 13377: train loss: 0.11487997794202527, test loss: 0.2540158290637603\n",
      "epoch 13378: train loss: 0.1148776608870476, test loss: 0.25401651793620933\n",
      "epoch 13379: train loss: 0.11487534407109928, test loss: 0.25401720692762936\n",
      "epoch 13380: train loss: 0.11487302749413615, test loss: 0.2540178960380443\n",
      "epoch 13381: train loss: 0.11487071115611404, test loss: 0.25401858526739973\n",
      "epoch 13382: train loss: 0.11486839505698881, test loss: 0.2540192746156799\n",
      "epoch 13383: train loss: 0.11486607919671635, test loss: 0.2540199640828287\n",
      "epoch 13384: train loss: 0.11486376357525248, test loss: 0.25402065366887233\n",
      "epoch 13385: train loss: 0.11486144819255309, test loss: 0.2540213433737305\n",
      "epoch 13386: train loss: 0.11485913304857409, test loss: 0.25402203319740313\n",
      "epoch 13387: train loss: 0.11485681814327137, test loss: 0.2540227231398541\n",
      "epoch 13388: train loss: 0.11485450347660088, test loss: 0.25402341320106503\n",
      "epoch 13389: train loss: 0.11485218904851852, test loss: 0.25402410338098214\n",
      "epoch 13390: train loss: 0.11484987485898024, test loss: 0.2540247936796141\n",
      "epoch 13391: train loss: 0.11484756090794197, test loss: 0.25402548409693043\n",
      "epoch 13392: train loss: 0.11484524719535973, test loss: 0.25402617463287536\n",
      "epoch 13393: train loss: 0.11484293372118946, test loss: 0.2540268652874407\n",
      "epoch 13394: train loss: 0.11484062048538714, test loss: 0.2540275560605892\n",
      "epoch 13395: train loss: 0.11483830748790876, test loss: 0.25402824695231363\n",
      "epoch 13396: train loss: 0.11483599472871037, test loss: 0.25402893796257486\n",
      "epoch 13397: train loss: 0.11483368220774796, test loss: 0.2540296290913319\n",
      "epoch 13398: train loss: 0.11483136992497758, test loss: 0.2540303203385717\n",
      "epoch 13399: train loss: 0.11482905788035523, test loss: 0.2540310117042716\n",
      "epoch 13400: train loss: 0.11482674607383704, test loss: 0.25403170318840806\n",
      "epoch 13401: train loss: 0.11482443450537902, test loss: 0.254032394790925\n",
      "epoch 13402: train loss: 0.11482212317493729, test loss: 0.2540330865118164\n",
      "epoch 13403: train loss: 0.1148198120824679, test loss: 0.25403377835106883\n",
      "epoch 13404: train loss: 0.11481750122792694, test loss: 0.25403447030862436\n",
      "epoch 13405: train loss: 0.1148151906112706, test loss: 0.25403516238447027\n",
      "epoch 13406: train loss: 0.11481288023245494, test loss: 0.25403585457859124\n",
      "epoch 13407: train loss: 0.11481057009143608, test loss: 0.25403654689093275\n",
      "epoch 13408: train loss: 0.11480826018817027, test loss: 0.2540372393215029\n",
      "epoch 13409: train loss: 0.11480595052261354, test loss: 0.25403793187023127\n",
      "epoch 13410: train loss: 0.11480364109472214, test loss: 0.2540386245371435\n",
      "epoch 13411: train loss: 0.11480133190445223, test loss: 0.2540393173221607\n",
      "epoch 13412: train loss: 0.11479902295176002, test loss: 0.2540400102252839\n",
      "epoch 13413: train loss: 0.11479671423660166, test loss: 0.25404070324647426\n",
      "epoch 13414: train loss: 0.11479440575893342, test loss: 0.2540413963857171\n",
      "epoch 13415: train loss: 0.11479209751871154, test loss: 0.2540420896429916\n",
      "epoch 13416: train loss: 0.11478978951589223, test loss: 0.25404278301823985\n",
      "epoch 13417: train loss: 0.11478748175043173, test loss: 0.25404347651145626\n",
      "epoch 13418: train loss: 0.11478517422228633, test loss: 0.2540441701226194\n",
      "epoch 13419: train loss: 0.11478286693141225, test loss: 0.25404486385168995\n",
      "epoch 13420: train loss: 0.11478055987776585, test loss: 0.2540455576986449\n",
      "epoch 13421: train loss: 0.11477825306130338, test loss: 0.25404625166345396\n",
      "epoch 13422: train loss: 0.11477594648198114, test loss: 0.2540469457460946\n",
      "epoch 13423: train loss: 0.1147736401397555, test loss: 0.25404763994653584\n",
      "epoch 13424: train loss: 0.11477133403458271, test loss: 0.2540483342647556\n",
      "epoch 13425: train loss: 0.1147690281664192, test loss: 0.25404902870072327\n",
      "epoch 13426: train loss: 0.11476672253522127, test loss: 0.2540497232544168\n",
      "epoch 13427: train loss: 0.11476441714094529, test loss: 0.25405041792578836\n",
      "epoch 13428: train loss: 0.11476211198354765, test loss: 0.2540511127148322\n",
      "epoch 13429: train loss: 0.11475980706298472, test loss: 0.2540518076215352\n",
      "epoch 13430: train loss: 0.1147575023792129, test loss: 0.2540525026458245\n",
      "epoch 13431: train loss: 0.11475519793218862, test loss: 0.25405319778772745\n",
      "epoch 13432: train loss: 0.1147528937218683, test loss: 0.25405389304717313\n",
      "epoch 13433: train loss: 0.11475058974820833, test loss: 0.2540545884241542\n",
      "epoch 13434: train loss: 0.11474828601116521, test loss: 0.25405528391863347\n",
      "epoch 13435: train loss: 0.11474598251069533, test loss: 0.25405597953060477\n",
      "epoch 13436: train loss: 0.11474367924675523, test loss: 0.2540566752600128\n",
      "epoch 13437: train loss: 0.11474137621930136, test loss: 0.25405737110686805\n",
      "epoch 13438: train loss: 0.11473907342829019, test loss: 0.25405806707109957\n",
      "epoch 13439: train loss: 0.11473677087367826, test loss: 0.2540587631527178\n",
      "epoch 13440: train loss: 0.11473446855542202, test loss: 0.25405945935166757\n",
      "epoch 13441: train loss: 0.11473216647347804, test loss: 0.25406015566792617\n",
      "epoch 13442: train loss: 0.11472986462780288, test loss: 0.2540608521014889\n",
      "epoch 13443: train loss: 0.114727563018353, test loss: 0.25406154865231806\n",
      "epoch 13444: train loss: 0.11472526164508505, test loss: 0.2540622453203739\n",
      "epoch 13445: train loss: 0.11472296050795555, test loss: 0.25406294210565045\n",
      "epoch 13446: train loss: 0.11472065960692106, test loss: 0.254063639008093\n",
      "epoch 13447: train loss: 0.11471835894193821, test loss: 0.2540643360276969\n",
      "epoch 13448: train loss: 0.11471605851296361, test loss: 0.2540650331644395\n",
      "epoch 13449: train loss: 0.11471375831995384, test loss: 0.25406573041828256\n",
      "epoch 13450: train loss: 0.11471145836286556, test loss: 0.2540664277891876\n",
      "epoch 13451: train loss: 0.11470915864165537, test loss: 0.25406712527714914\n",
      "epoch 13452: train loss: 0.11470685915627993, test loss: 0.25406782288212837\n",
      "epoch 13453: train loss: 0.11470455990669592, test loss: 0.25406852060410384\n",
      "epoch 13454: train loss: 0.11470226089285998, test loss: 0.25406921844303754\n",
      "epoch 13455: train loss: 0.11469996211472878, test loss: 0.25406991639893195\n",
      "epoch 13456: train loss: 0.11469766357225909, test loss: 0.2540706144717234\n",
      "epoch 13457: train loss: 0.11469536526540754, test loss: 0.25407131266140753\n",
      "epoch 13458: train loss: 0.11469306719413087, test loss: 0.25407201096795423\n",
      "epoch 13459: train loss: 0.1146907693583858, test loss: 0.25407270939133203\n",
      "epoch 13460: train loss: 0.11468847175812912, test loss: 0.2540734079315216\n",
      "epoch 13461: train loss: 0.11468617439331746, test loss: 0.2540741065884746\n",
      "epoch 13462: train loss: 0.11468387726390768, test loss: 0.25407480536220245\n",
      "epoch 13463: train loss: 0.11468158036985653, test loss: 0.25407550425263503\n",
      "epoch 13464: train loss: 0.11467928371112081, test loss: 0.25407620325978286\n",
      "epoch 13465: train loss: 0.11467698728765723, test loss: 0.2540769023835907\n",
      "epoch 13466: train loss: 0.1146746910994227, test loss: 0.25407760162405424\n",
      "epoch 13467: train loss: 0.11467239514637399, test loss: 0.2540783009811355\n",
      "epoch 13468: train loss: 0.1146700994284679, test loss: 0.2540790004548117\n",
      "epoch 13469: train loss: 0.11466780394566133, test loss: 0.25407970004506275\n",
      "epoch 13470: train loss: 0.1146655086979111, test loss: 0.25408039975184993\n",
      "epoch 13471: train loss: 0.11466321368517407, test loss: 0.25408109957513453\n",
      "epoch 13472: train loss: 0.11466091890740712, test loss: 0.25408179951490323\n",
      "epoch 13473: train loss: 0.11465862436456713, test loss: 0.254082499571153\n",
      "epoch 13474: train loss: 0.11465633005661097, test loss: 0.2540831997438103\n",
      "epoch 13475: train loss: 0.11465403598349556, test loss: 0.254083900032888\n",
      "epoch 13476: train loss: 0.11465174214517787, test loss: 0.2540846004383471\n",
      "epoch 13477: train loss: 0.11464944854161477, test loss: 0.25408530096015164\n",
      "epoch 13478: train loss: 0.11464715517276321, test loss: 0.25408600159827804\n",
      "epoch 13479: train loss: 0.11464486203858015, test loss: 0.25408670235272196\n",
      "epoch 13480: train loss: 0.11464256913902254, test loss: 0.25408740322343004\n",
      "epoch 13481: train loss: 0.11464027647404737, test loss: 0.25408810421037104\n",
      "epoch 13482: train loss: 0.11463798404361164, test loss: 0.25408880531354144\n",
      "epoch 13483: train loss: 0.1146356918476723, test loss: 0.2540895065329025\n",
      "epoch 13484: train loss: 0.11463339988618639, test loss: 0.2540902078684332\n",
      "epoch 13485: train loss: 0.1146311081591109, test loss: 0.2540909093200957\n",
      "epoch 13486: train loss: 0.11462881666640289, test loss: 0.25409161088787735\n",
      "epoch 13487: train loss: 0.1146265254080194, test loss: 0.25409231257173936\n",
      "epoch 13488: train loss: 0.11462423438391746, test loss: 0.25409301437166926\n",
      "epoch 13489: train loss: 0.11462194359405413, test loss: 0.25409371628762073\n",
      "epoch 13490: train loss: 0.11461965303838652, test loss: 0.25409441831958224\n",
      "epoch 13491: train loss: 0.11461736271687167, test loss: 0.2540951204675227\n",
      "epoch 13492: train loss: 0.11461507262946671, test loss: 0.25409582273142095\n",
      "epoch 13493: train loss: 0.11461278277612873, test loss: 0.25409652511125663\n",
      "epoch 13494: train loss: 0.11461049315681486, test loss: 0.25409722760696707\n",
      "epoch 13495: train loss: 0.1146082037714822, test loss: 0.2540979302185806\n",
      "epoch 13496: train loss: 0.11460591462008794, test loss: 0.25409863294602664\n",
      "epoch 13497: train loss: 0.1146036257025892, test loss: 0.25409933578929167\n",
      "epoch 13498: train loss: 0.11460133701894314, test loss: 0.254100038748347\n",
      "epoch 13499: train loss: 0.11459904856910695, test loss: 0.2541007418231873\n",
      "epoch 13500: train loss: 0.1145967603530378, test loss: 0.2541014450137516\n",
      "epoch 13501: train loss: 0.1145944723706929, test loss: 0.2541021483200256\n",
      "epoch 13502: train loss: 0.11459218462202944, test loss: 0.2541028517419985\n",
      "epoch 13503: train loss: 0.11458989710700465, test loss: 0.25410355527963036\n",
      "epoch 13504: train loss: 0.11458760982557578, test loss: 0.2541042589329107\n",
      "epoch 13505: train loss: 0.11458532277770005, test loss: 0.25410496270178395\n",
      "epoch 13506: train loss: 0.11458303596333469, test loss: 0.25410566658624656\n",
      "epoch 13507: train loss: 0.114580749382437, test loss: 0.25410637058625973\n",
      "epoch 13508: train loss: 0.1145784630349642, test loss: 0.2541070747017959\n",
      "epoch 13509: train loss: 0.11457617692087366, test loss: 0.25410777893285064\n",
      "epoch 13510: train loss: 0.11457389104012261, test loss: 0.2541084832793684\n",
      "epoch 13511: train loss: 0.11457160539266839, test loss: 0.254109187741338\n",
      "epoch 13512: train loss: 0.11456931997846828, test loss: 0.25410989231873876\n",
      "epoch 13513: train loss: 0.11456703479747966, test loss: 0.25411059701154043\n",
      "epoch 13514: train loss: 0.11456474984965982, test loss: 0.25411130181969854\n",
      "epoch 13515: train loss: 0.11456246513496614, test loss: 0.254112006743216\n",
      "epoch 13516: train loss: 0.11456018065335599, test loss: 0.2541127117820475\n",
      "epoch 13517: train loss: 0.1145578964047867, test loss: 0.25411341693616474\n",
      "epoch 13518: train loss: 0.11455561238921573, test loss: 0.2541141222055382\n",
      "epoch 13519: train loss: 0.11455332860660039, test loss: 0.25411482759017334\n",
      "epoch 13520: train loss: 0.11455104505689814, test loss: 0.25411553309001406\n",
      "epoch 13521: train loss: 0.1145487617400664, test loss: 0.25411623870503164\n",
      "epoch 13522: train loss: 0.11454647865606256, test loss: 0.25411694443522387\n",
      "epoch 13523: train loss: 0.11454419580484412, test loss: 0.25411765028053623\n",
      "epoch 13524: train loss: 0.11454191318636844, test loss: 0.25411835624095497\n",
      "epoch 13525: train loss: 0.11453963080059307, test loss: 0.25411906231646897\n",
      "epoch 13526: train loss: 0.11453734864747546, test loss: 0.2541197685070249\n",
      "epoch 13527: train loss: 0.11453506672697306, test loss: 0.2541204748126098\n",
      "epoch 13528: train loss: 0.11453278503904339, test loss: 0.2541211812331951\n",
      "epoch 13529: train loss: 0.11453050358364393, test loss: 0.25412188776877676\n",
      "epoch 13530: train loss: 0.11452822236073225, test loss: 0.2541225944192841\n",
      "epoch 13531: train loss: 0.11452594137026584, test loss: 0.25412330118472165\n",
      "epoch 13532: train loss: 0.11452366061220223, test loss: 0.2541240080650533\n",
      "epoch 13533: train loss: 0.114521380086499, test loss: 0.25412471506026585\n",
      "epoch 13534: train loss: 0.11451909979311369, test loss: 0.2541254221703064\n",
      "epoch 13535: train loss: 0.11451681973200385, test loss: 0.2541261293951876\n",
      "epoch 13536: train loss: 0.11451453990312713, test loss: 0.2541268367348377\n",
      "epoch 13537: train loss: 0.11451226030644107, test loss: 0.2541275441892633\n",
      "epoch 13538: train loss: 0.11450998094190329, test loss: 0.25412825175843495\n",
      "epoch 13539: train loss: 0.11450770180947138, test loss: 0.25412895944230685\n",
      "epoch 13540: train loss: 0.11450542290910301, test loss: 0.25412966724087616\n",
      "epoch 13541: train loss: 0.11450314424075579, test loss: 0.25413037515409714\n",
      "epoch 13542: train loss: 0.11450086580438736, test loss: 0.25413108318195815\n",
      "epoch 13543: train loss: 0.11449858759995543, test loss: 0.25413179132442143\n",
      "epoch 13544: train loss: 0.11449630962741761, test loss: 0.2541324995814847\n",
      "epoch 13545: train loss: 0.11449403188673159, test loss: 0.2541332079530854\n",
      "epoch 13546: train loss: 0.1144917543778551, test loss: 0.25413391643921995\n",
      "epoch 13547: train loss: 0.1144894771007458, test loss: 0.25413462503986844\n",
      "epoch 13548: train loss: 0.11448720005536145, test loss: 0.25413533375496816\n",
      "epoch 13549: train loss: 0.11448492324165976, test loss: 0.25413604258454947\n",
      "epoch 13550: train loss: 0.11448264665959844, test loss: 0.25413675152854276\n",
      "epoch 13551: train loss: 0.11448037030913526, test loss: 0.2541374605869366\n",
      "epoch 13552: train loss: 0.11447809419022793, test loss: 0.25413816975969283\n",
      "epoch 13553: train loss: 0.11447581830283429, test loss: 0.25413887904680754\n",
      "epoch 13554: train loss: 0.1144735426469121, test loss: 0.2541395884482385\n",
      "epoch 13555: train loss: 0.11447126722241911, test loss: 0.25414029796398035\n",
      "epoch 13556: train loss: 0.11446899202931314, test loss: 0.2541410075939705\n",
      "epoch 13557: train loss: 0.11446671706755204, test loss: 0.2541417173382077\n",
      "epoch 13558: train loss: 0.1144644423370936, test loss: 0.25414242719665375\n",
      "epoch 13559: train loss: 0.11446216783789566, test loss: 0.25414313716931486\n",
      "epoch 13560: train loss: 0.11445989356991607, test loss: 0.2541438472561205\n",
      "epoch 13561: train loss: 0.11445761953311266, test loss: 0.2541445574570585\n",
      "epoch 13562: train loss: 0.11445534572744333, test loss: 0.2541452677721266\n",
      "epoch 13563: train loss: 0.11445307215286593, test loss: 0.25414597820127033\n",
      "epoch 13564: train loss: 0.11445079880933838, test loss: 0.25414668874446206\n",
      "epoch 13565: train loss: 0.11444852569681854, test loss: 0.25414739940170766\n",
      "epoch 13566: train loss: 0.11444625281526438, test loss: 0.25414811017295313\n",
      "epoch 13567: train loss: 0.11444398016463377, test loss: 0.2541488210581784\n",
      "epoch 13568: train loss: 0.11444170774488463, test loss: 0.25414953205735635\n",
      "epoch 13569: train loss: 0.11443943555597492, test loss: 0.25415024317048307\n",
      "epoch 13570: train loss: 0.11443716359786266, test loss: 0.25415095439748947\n",
      "epoch 13571: train loss: 0.1144348918705057, test loss: 0.2541516657383788\n",
      "epoch 13572: train loss: 0.11443262037386209, test loss: 0.2541523771931248\n",
      "epoch 13573: train loss: 0.11443034910788978, test loss: 0.25415308876170806\n",
      "epoch 13574: train loss: 0.11442807807254679, test loss: 0.25415380044407393\n",
      "epoch 13575: train loss: 0.11442580726779113, test loss: 0.2541545122402127\n",
      "epoch 13576: train loss: 0.11442353669358081, test loss: 0.25415522415011954\n",
      "epoch 13577: train loss: 0.11442126634987386, test loss: 0.2541559361737262\n",
      "epoch 13578: train loss: 0.11441899623662831, test loss: 0.2541566483110382\n",
      "epoch 13579: train loss: 0.11441672635380222, test loss: 0.2541573605620088\n",
      "epoch 13580: train loss: 0.11441445670135365, test loss: 0.25415807292663706\n",
      "epoch 13581: train loss: 0.1144121872792407, test loss: 0.25415878540487813\n",
      "epoch 13582: train loss: 0.11440991808742142, test loss: 0.25415949799671234\n",
      "epoch 13583: train loss: 0.1144076491258539, test loss: 0.2541602107021194\n",
      "epoch 13584: train loss: 0.11440538039449627, test loss: 0.254160923521055\n",
      "epoch 13585: train loss: 0.11440311189330661, test loss: 0.2541616364535085\n",
      "epoch 13586: train loss: 0.11440084362224312, test loss: 0.2541623494994604\n",
      "epoch 13587: train loss: 0.11439857558126384, test loss: 0.25416306265885685\n",
      "epoch 13588: train loss: 0.11439630777032699, test loss: 0.2541637759317049\n",
      "epoch 13589: train loss: 0.1143940401893907, test loss: 0.25416448931795854\n",
      "epoch 13590: train loss: 0.11439177283841316, test loss: 0.2541652028175994\n",
      "epoch 13591: train loss: 0.11438950571735251, test loss: 0.25416591643059055\n",
      "epoch 13592: train loss: 0.114387238826167, test loss: 0.25416663015693786\n",
      "epoch 13593: train loss: 0.11438497216481476, test loss: 0.25416734399656404\n",
      "epoch 13594: train loss: 0.11438270573325408, test loss: 0.2541680579494736\n",
      "epoch 13595: train loss: 0.11438043953144314, test loss: 0.2541687720156575\n",
      "epoch 13596: train loss: 0.11437817355934017, test loss: 0.25416948619506974\n",
      "epoch 13597: train loss: 0.11437590781690343, test loss: 0.25417020048768263\n",
      "epoch 13598: train loss: 0.11437364230409118, test loss: 0.2541709148934606\n",
      "epoch 13599: train loss: 0.11437137702086167, test loss: 0.25417162941239435\n",
      "epoch 13600: train loss: 0.11436911196717318, test loss: 0.2541723440444783\n",
      "epoch 13601: train loss: 0.11436684714298401, test loss: 0.25417305878964447\n",
      "epoch 13602: train loss: 0.11436458254825248, test loss: 0.2541737736478823\n",
      "epoch 13603: train loss: 0.11436231818293685, test loss: 0.25417448861918013\n",
      "epoch 13604: train loss: 0.11436005404699547, test loss: 0.2541752037034947\n",
      "epoch 13605: train loss: 0.11435779014038666, test loss: 0.25417591890080676\n",
      "epoch 13606: train loss: 0.11435552646306876, test loss: 0.254176634211096\n",
      "epoch 13607: train loss: 0.11435326301500016, test loss: 0.2541773496343193\n",
      "epoch 13608: train loss: 0.11435099979613916, test loss: 0.2541780651704822\n",
      "epoch 13609: train loss: 0.11434873680644418, test loss: 0.2541787808195235\n",
      "epoch 13610: train loss: 0.11434647404587359, test loss: 0.2541794965814415\n",
      "epoch 13611: train loss: 0.11434421151438577, test loss: 0.2541802124562\n",
      "epoch 13612: train loss: 0.11434194921193919, test loss: 0.254180928443771\n",
      "epoch 13613: train loss: 0.1143396871384922, test loss: 0.2541816445441454\n",
      "epoch 13614: train loss: 0.11433742529400323, test loss: 0.2541823607572859\n",
      "epoch 13615: train loss: 0.11433516367843076, test loss: 0.2541830770831572\n",
      "epoch 13616: train loss: 0.1143329022917332, test loss: 0.2541837935217501\n",
      "epoch 13617: train loss: 0.11433064113386901, test loss: 0.25418451007303444\n",
      "epoch 13618: train loss: 0.11432838020479671, test loss: 0.25418522673696803\n",
      "epoch 13619: train loss: 0.11432611950447473, test loss: 0.2541859435135487\n",
      "epoch 13620: train loss: 0.11432385903286157, test loss: 0.2541866604027326\n",
      "epoch 13621: train loss: 0.11432159878991577, test loss: 0.2541873774045249\n",
      "epoch 13622: train loss: 0.11431933877559577, test loss: 0.2541880945188568\n",
      "epoch 13623: train loss: 0.11431707898986017, test loss: 0.2541888117457443\n",
      "epoch 13624: train loss: 0.11431481943266747, test loss: 0.25418952908512515\n",
      "epoch 13625: train loss: 0.11431256010397618, test loss: 0.25419024653698835\n",
      "epoch 13626: train loss: 0.11431030100374492, test loss: 0.25419096410131653\n",
      "epoch 13627: train loss: 0.11430804213193226, test loss: 0.2541916817780648\n",
      "epoch 13628: train loss: 0.11430578348849668, test loss: 0.25419239956724193\n",
      "epoch 13629: train loss: 0.11430352507339687, test loss: 0.25419311746879225\n",
      "epoch 13630: train loss: 0.1143012668865914, test loss: 0.2541938354826903\n",
      "epoch 13631: train loss: 0.11429900892803885, test loss: 0.2541945536089257\n",
      "epoch 13632: train loss: 0.11429675119769786, test loss: 0.2541952718474629\n",
      "epoch 13633: train loss: 0.11429449369552706, test loss: 0.25419599019827344\n",
      "epoch 13634: train loss: 0.1142922364214851, test loss: 0.2541967086613506\n",
      "epoch 13635: train loss: 0.11428997937553062, test loss: 0.2541974272366557\n",
      "epoch 13636: train loss: 0.11428772255762226, test loss: 0.254198145924154\n",
      "epoch 13637: train loss: 0.11428546596771874, test loss: 0.2541988647238271\n",
      "epoch 13638: train loss: 0.11428320960577873, test loss: 0.25419958363564904\n",
      "epoch 13639: train loss: 0.1142809534717609, test loss: 0.2542003026596259\n",
      "epoch 13640: train loss: 0.11427869756562396, test loss: 0.25420102179567927\n",
      "epoch 13641: train loss: 0.11427644188732664, test loss: 0.2542017410437905\n",
      "epoch 13642: train loss: 0.11427418643682766, test loss: 0.2542024604039762\n",
      "epoch 13643: train loss: 0.11427193121408577, test loss: 0.2542031798761847\n",
      "epoch 13644: train loss: 0.11426967621905966, test loss: 0.2542038994603873\n",
      "epoch 13645: train loss: 0.11426742145170815, test loss: 0.25420461915655684\n",
      "epoch 13646: train loss: 0.11426516691198998, test loss: 0.25420533896468567\n",
      "epoch 13647: train loss: 0.11426291259986394, test loss: 0.25420605888473646\n",
      "epoch 13648: train loss: 0.11426065851528881, test loss: 0.2542067789166659\n",
      "epoch 13649: train loss: 0.1142584046582234, test loss: 0.25420749906048207\n",
      "epoch 13650: train loss: 0.11425615102862652, test loss: 0.25420821931613163\n",
      "epoch 13651: train loss: 0.11425389762645696, test loss: 0.25420893968360403\n",
      "epoch 13652: train loss: 0.11425164445167361, test loss: 0.25420966016287516\n",
      "epoch 13653: train loss: 0.11424939150423526, test loss: 0.25421038075391633\n",
      "epoch 13654: train loss: 0.1142471387841008, test loss: 0.2542111014566915\n",
      "epoch 13655: train loss: 0.11424488629122904, test loss: 0.2542118222711936\n",
      "epoch 13656: train loss: 0.11424263402557891, test loss: 0.25421254319739367\n",
      "epoch 13657: train loss: 0.11424038198710929, test loss: 0.2542132642352575\n",
      "epoch 13658: train loss: 0.11423813017577902, test loss: 0.25421398538475803\n",
      "epoch 13659: train loss: 0.11423587859154706, test loss: 0.25421470664586837\n",
      "epoch 13660: train loss: 0.1142336272343723, test loss: 0.2542154280185805\n",
      "epoch 13661: train loss: 0.11423137610421369, test loss: 0.254216149502858\n",
      "epoch 13662: train loss: 0.11422912520103015, test loss: 0.2542168710986733\n",
      "epoch 13663: train loss: 0.11422687452478061, test loss: 0.2542175928060192\n",
      "epoch 13664: train loss: 0.11422462407542405, test loss: 0.254218314624841\n",
      "epoch 13665: train loss: 0.11422237385291945, test loss: 0.2542190365551146\n",
      "epoch 13666: train loss: 0.11422012385722576, test loss: 0.2542197585968462\n",
      "epoch 13667: train loss: 0.11421787408830199, test loss: 0.25422048074997483\n",
      "epoch 13668: train loss: 0.11421562454610712, test loss: 0.25422120301450873\n",
      "epoch 13669: train loss: 0.11421337523060018, test loss: 0.2542219253903965\n",
      "epoch 13670: train loss: 0.11421112614174017, test loss: 0.25422264787762766\n",
      "epoch 13671: train loss: 0.11420887727948617, test loss: 0.2542233704761586\n",
      "epoch 13672: train loss: 0.11420662864379716, test loss: 0.25422409318598027\n",
      "epoch 13673: train loss: 0.11420438023463224, test loss: 0.25422481600706787\n",
      "epoch 13674: train loss: 0.11420213205195044, test loss: 0.2542255389394007\n",
      "epoch 13675: train loss: 0.11419988409571084, test loss: 0.2542262619829368\n",
      "epoch 13676: train loss: 0.11419763636587253, test loss: 0.25422698513764996\n",
      "epoch 13677: train loss: 0.11419538886239464, test loss: 0.2542277084035387\n",
      "epoch 13678: train loss: 0.11419314158523619, test loss: 0.2542284317805597\n",
      "epoch 13679: train loss: 0.11419089453435637, test loss: 0.25422915526867906\n",
      "epoch 13680: train loss: 0.11418864770971428, test loss: 0.2542298788678955\n",
      "epoch 13681: train loss: 0.11418640111126904, test loss: 0.2542306025781569\n",
      "epoch 13682: train loss: 0.11418415473897983, test loss: 0.2542313263994626\n",
      "epoch 13683: train loss: 0.11418190859280582, test loss: 0.25423205033177015\n",
      "epoch 13684: train loss: 0.11417966267270609, test loss: 0.2542327743750774\n",
      "epoch 13685: train loss: 0.11417741697863992, test loss: 0.2542334985293252\n",
      "epoch 13686: train loss: 0.11417517151056643, test loss: 0.2542342227945204\n",
      "epoch 13687: train loss: 0.11417292626844486, test loss: 0.2542349471706304\n",
      "epoch 13688: train loss: 0.11417068125223437, test loss: 0.2542356716576098\n",
      "epoch 13689: train loss: 0.11416843646189424, test loss: 0.2542363962554409\n",
      "epoch 13690: train loss: 0.11416619189738367, test loss: 0.2542371209641076\n",
      "epoch 13691: train loss: 0.11416394755866191, test loss: 0.2542378457835839\n",
      "epoch 13692: train loss: 0.11416170344568818, test loss: 0.25423857071384204\n",
      "epoch 13693: train loss: 0.1141594595584218, test loss: 0.2542392957548655\n",
      "epoch 13694: train loss: 0.114157215896822, test loss: 0.2542400209066116\n",
      "epoch 13695: train loss: 0.11415497246084806, test loss: 0.25424074616906095\n",
      "epoch 13696: train loss: 0.11415272925045927, test loss: 0.2542414715422066\n",
      "epoch 13697: train loss: 0.11415048626561498, test loss: 0.2542421970259961\n",
      "epoch 13698: train loss: 0.11414824350627445, test loss: 0.2542429226204214\n",
      "epoch 13699: train loss: 0.11414600097239702, test loss: 0.25424364832546276\n",
      "epoch 13700: train loss: 0.11414375866394202, test loss: 0.2542443741410622\n",
      "epoch 13701: train loss: 0.11414151658086884, test loss: 0.25424510006723666\n",
      "epoch 13702: train loss: 0.11413927472313677, test loss: 0.2542458261039324\n",
      "epoch 13703: train loss: 0.1141370330907052, test loss: 0.2542465522511425\n",
      "epoch 13704: train loss: 0.11413479168353353, test loss: 0.2542472785088149\n",
      "epoch 13705: train loss: 0.1141325505015811, test loss: 0.2542480048769664\n",
      "epoch 13706: train loss: 0.11413030954480735, test loss: 0.2542487313555279\n",
      "epoch 13707: train loss: 0.11412806881317167, test loss: 0.25424945794450055\n",
      "epoch 13708: train loss: 0.11412582830663348, test loss: 0.25425018464385674\n",
      "epoch 13709: train loss: 0.11412358802515221, test loss: 0.25425091145358036\n",
      "epoch 13710: train loss: 0.11412134796868728, test loss: 0.254251638373611\n",
      "epoch 13711: train loss: 0.11411910813719815, test loss: 0.25425236540394913\n",
      "epoch 13712: train loss: 0.11411686853064432, test loss: 0.2542530925445853\n",
      "epoch 13713: train loss: 0.11411462914898517, test loss: 0.25425381979546885\n",
      "epoch 13714: train loss: 0.11411238999218024, test loss: 0.25425454715657325\n",
      "epoch 13715: train loss: 0.11411015106018901, test loss: 0.25425527462788977\n",
      "epoch 13716: train loss: 0.11410791235297096, test loss: 0.2542560022093855\n",
      "epoch 13717: train loss: 0.11410567387048566, test loss: 0.25425672990103393\n",
      "epoch 13718: train loss: 0.11410343561269258, test loss: 0.2542574577028167\n",
      "epoch 13719: train loss: 0.11410119757955126, test loss: 0.25425818561471075\n",
      "epoch 13720: train loss: 0.11409895977102125, test loss: 0.2542589136366712\n",
      "epoch 13721: train loss: 0.11409672218706206, test loss: 0.25425964176869953\n",
      "epoch 13722: train loss: 0.11409448482763333, test loss: 0.2542603700107341\n",
      "epoch 13723: train loss: 0.11409224769269459, test loss: 0.25426109836279326\n",
      "epoch 13724: train loss: 0.11409001078220538, test loss: 0.25426182682482656\n",
      "epoch 13725: train loss: 0.1140877740961254, test loss: 0.25426255539682485\n",
      "epoch 13726: train loss: 0.11408553763441415, test loss: 0.254263284078744\n",
      "epoch 13727: train loss: 0.11408330139703128, test loss: 0.25426401287056133\n",
      "epoch 13728: train loss: 0.11408106538393642, test loss: 0.25426474177226643\n",
      "epoch 13729: train loss: 0.11407882959508923, test loss: 0.25426547078383366\n",
      "epoch 13730: train loss: 0.1140765940304493, test loss: 0.2542661999052223\n",
      "epoch 13731: train loss: 0.11407435868997634, test loss: 0.2542669291364223\n",
      "epoch 13732: train loss: 0.11407212357362996, test loss: 0.25426765847739125\n",
      "epoch 13733: train loss: 0.11406988868136989, test loss: 0.2542683879281208\n",
      "epoch 13734: train loss: 0.11406765401315574, test loss: 0.25426911748858655\n",
      "epoch 13735: train loss: 0.11406541956894731, test loss: 0.25426984715875456\n",
      "epoch 13736: train loss: 0.11406318534870423, test loss: 0.2542705769386068\n",
      "epoch 13737: train loss: 0.11406095135238623, test loss: 0.25427130682811844\n",
      "epoch 13738: train loss: 0.11405871757995305, test loss: 0.2542720368272479\n",
      "epoch 13739: train loss: 0.11405648403136442, test loss: 0.25427276693598494\n",
      "epoch 13740: train loss: 0.11405425070658012, test loss: 0.25427349715431535\n",
      "epoch 13741: train loss: 0.11405201760555986, test loss: 0.2542742274821856\n",
      "epoch 13742: train loss: 0.1140497847282634, test loss: 0.2542749579196071\n",
      "epoch 13743: train loss: 0.11404755207465056, test loss: 0.2542756884665262\n",
      "epoch 13744: train loss: 0.11404531964468113, test loss: 0.2542764191229284\n",
      "epoch 13745: train loss: 0.11404308743831486, test loss: 0.25427714988878863\n",
      "epoch 13746: train loss: 0.11404085545551164, test loss: 0.25427788076408897\n",
      "epoch 13747: train loss: 0.1140386236962312, test loss: 0.2542786117487787\n",
      "epoch 13748: train loss: 0.11403639216043342, test loss: 0.25427934284286813\n",
      "epoch 13749: train loss: 0.11403416084807812, test loss: 0.254280074046304\n",
      "epoch 13750: train loss: 0.11403192975912516, test loss: 0.25428080535907155\n",
      "epoch 13751: train loss: 0.11402969889353441, test loss: 0.25428153678116266\n",
      "epoch 13752: train loss: 0.1140274682512657, test loss: 0.25428226831252665\n",
      "epoch 13753: train loss: 0.11402523783227898, test loss: 0.25428299995314674\n",
      "epoch 13754: train loss: 0.11402300763653407, test loss: 0.25428373170299745\n",
      "epoch 13755: train loss: 0.11402077766399092, test loss: 0.25428446356207124\n",
      "epoch 13756: train loss: 0.11401854791460941, test loss: 0.2542851955303258\n",
      "epoch 13757: train loss: 0.1140163183883495, test loss: 0.25428592760773566\n",
      "epoch 13758: train loss: 0.11401408908517109, test loss: 0.25428665979427645\n",
      "epoch 13759: train loss: 0.11401186000503412, test loss: 0.2542873920899406\n",
      "epoch 13760: train loss: 0.11400963114789855, test loss: 0.25428812449468496\n",
      "epoch 13761: train loss: 0.11400740251372433, test loss: 0.2542888570084859\n",
      "epoch 13762: train loss: 0.11400517410247145, test loss: 0.2542895896313344\n",
      "epoch 13763: train loss: 0.11400294591409989, test loss: 0.25429032236317134\n",
      "epoch 13764: train loss: 0.11400071794856964, test loss: 0.2542910552040153\n",
      "epoch 13765: train loss: 0.11399849020584071, test loss: 0.2542917881538152\n",
      "epoch 13766: train loss: 0.11399626268587308, test loss: 0.2542925212125462\n",
      "epoch 13767: train loss: 0.11399403538862682, test loss: 0.25429325438019235\n",
      "epoch 13768: train loss: 0.11399180831406194, test loss: 0.25429398765672895\n",
      "epoch 13769: train loss: 0.1139895814621385, test loss: 0.25429472104213013\n",
      "epoch 13770: train loss: 0.11398735483281654, test loss: 0.25429545453637314\n",
      "epoch 13771: train loss: 0.1139851284260561, test loss: 0.2542961881394305\n",
      "epoch 13772: train loss: 0.11398290224181729, test loss: 0.2542969218512794\n",
      "epoch 13773: train loss: 0.1139806762800602, test loss: 0.25429765567187657\n",
      "epoch 13774: train loss: 0.11397845054074487, test loss: 0.2542983896012321\n",
      "epoch 13775: train loss: 0.11397622502383147, test loss: 0.2542991236392871\n",
      "epoch 13776: train loss: 0.1139739997292801, test loss: 0.254299857786052\n",
      "epoch 13777: train loss: 0.11397177465705084, test loss: 0.25430059204146627\n",
      "epoch 13778: train loss: 0.11396954980710385, test loss: 0.2543013264055404\n",
      "epoch 13779: train loss: 0.11396732517939931, test loss: 0.25430206087820634\n",
      "epoch 13780: train loss: 0.11396510077389735, test loss: 0.2543027954594917\n",
      "epoch 13781: train loss: 0.1139628765905581, test loss: 0.25430353014931933\n",
      "epoch 13782: train loss: 0.1139606526293418, test loss: 0.25430426494769975\n",
      "epoch 13783: train loss: 0.11395842889020859, test loss: 0.2543049998546081\n",
      "epoch 13784: train loss: 0.11395620537311865, test loss: 0.2543057348700016\n",
      "epoch 13785: train loss: 0.11395398207803226, test loss: 0.25430646999387546\n",
      "epoch 13786: train loss: 0.11395175900490956, test loss: 0.25430720522617645\n",
      "epoch 13787: train loss: 0.11394953615371083, test loss: 0.25430794056689815\n",
      "epoch 13788: train loss: 0.11394731352439628, test loss: 0.2543086760160433\n",
      "epoch 13789: train loss: 0.11394509111692613, test loss: 0.2543094115735344\n",
      "epoch 13790: train loss: 0.11394286893126068, test loss: 0.25431014723937206\n",
      "epoch 13791: train loss: 0.1139406469673602, test loss: 0.254310883013542\n",
      "epoch 13792: train loss: 0.11393842522518494, test loss: 0.25431161889600185\n",
      "epoch 13793: train loss: 0.11393620370469516, test loss: 0.2543123548867364\n",
      "epoch 13794: train loss: 0.11393398240585124, test loss: 0.2543130909857377\n",
      "epoch 13795: train loss: 0.11393176132861342, test loss: 0.2543138271929377\n",
      "epoch 13796: train loss: 0.11392954047294206, test loss: 0.254314563508347\n",
      "epoch 13797: train loss: 0.11392731983879742, test loss: 0.25431529993192453\n",
      "epoch 13798: train loss: 0.11392509942613989, test loss: 0.25431603646367196\n",
      "epoch 13799: train loss: 0.11392287923492983, test loss: 0.2543167731035381\n",
      "epoch 13800: train loss: 0.11392065926512757, test loss: 0.25431750985149937\n",
      "epoch 13801: train loss: 0.11391843951669346, test loss: 0.2543182467075398\n",
      "epoch 13802: train loss: 0.1139162199895879, test loss: 0.25431898367163414\n",
      "epoch 13803: train loss: 0.11391400068377129, test loss: 0.25431972074376064\n",
      "epoch 13804: train loss: 0.11391178159920401, test loss: 0.25432045792387464\n",
      "epoch 13805: train loss: 0.11390956273584646, test loss: 0.2543211952119793\n",
      "epoch 13806: train loss: 0.11390734409365906, test loss: 0.25432193260803354\n",
      "epoch 13807: train loss: 0.11390512567260228, test loss: 0.254322670112031\n",
      "epoch 13808: train loss: 0.11390290747263648, test loss: 0.25432340772392836\n",
      "epoch 13809: train loss: 0.11390068949372216, test loss: 0.2543241454437023\n",
      "epoch 13810: train loss: 0.11389847173581978, test loss: 0.2543248832713371\n",
      "epoch 13811: train loss: 0.1138962541988898, test loss: 0.2543256212068185\n",
      "epoch 13812: train loss: 0.1138940368828927, test loss: 0.2543263592501029\n",
      "epoch 13813: train loss: 0.11389181978778895, test loss: 0.25432709740114956\n",
      "epoch 13814: train loss: 0.11388960291353904, test loss: 0.25432783565998834\n",
      "epoch 13815: train loss: 0.11388738626010352, test loss: 0.25432857402653214\n",
      "epoch 13816: train loss: 0.11388516982744287, test loss: 0.25432931250080104\n",
      "epoch 13817: train loss: 0.11388295361551762, test loss: 0.25433005108277046\n",
      "epoch 13818: train loss: 0.11388073762428835, test loss: 0.2543307897723823\n",
      "epoch 13819: train loss: 0.11387852185371557, test loss: 0.2543315285696396\n",
      "epoch 13820: train loss: 0.11387630630375982, test loss: 0.254332267474507\n",
      "epoch 13821: train loss: 0.11387409097438173, test loss: 0.2543330064869803\n",
      "epoch 13822: train loss: 0.1138718758655418, test loss: 0.2543337456069817\n",
      "epoch 13823: train loss: 0.11386966097720069, test loss: 0.2543344848345575\n",
      "epoch 13824: train loss: 0.11386744630931894, test loss: 0.2543352241696406\n",
      "epoch 13825: train loss: 0.1138652318618572, test loss: 0.254335963612216\n",
      "epoch 13826: train loss: 0.11386301763477608, test loss: 0.2543367031622589\n",
      "epoch 13827: train loss: 0.11386080362803616, test loss: 0.254337442819746\n",
      "epoch 13828: train loss: 0.11385858984159813, test loss: 0.25433818258464425\n",
      "epoch 13829: train loss: 0.11385637627542267, test loss: 0.2543389224569475\n",
      "epoch 13830: train loss: 0.11385416292947034, test loss: 0.2543396624366142\n",
      "epoch 13831: train loss: 0.11385194980370189, test loss: 0.254340402523639\n",
      "epoch 13832: train loss: 0.11384973689807794, test loss: 0.2543411427179701\n",
      "epoch 13833: train loss: 0.11384752421255921, test loss: 0.2543418830196028\n",
      "epoch 13834: train loss: 0.11384531174710638, test loss: 0.25434262342851355\n",
      "epoch 13835: train loss: 0.1138430995016802, test loss: 0.25434336394466733\n",
      "epoch 13836: train loss: 0.11384088747624134, test loss: 0.2543441045680607\n",
      "epoch 13837: train loss: 0.11383867567075054, test loss: 0.25434484529865076\n",
      "epoch 13838: train loss: 0.11383646408516852, test loss: 0.25434558613641434\n",
      "epoch 13839: train loss: 0.11383425271945607, test loss: 0.2543463270813289\n",
      "epoch 13840: train loss: 0.1138320415735739, test loss: 0.2543470681333596\n",
      "epoch 13841: train loss: 0.11382983064748284, test loss: 0.2543478092925103\n",
      "epoch 13842: train loss: 0.11382761994114361, test loss: 0.2543485505587489\n",
      "epoch 13843: train loss: 0.113825409454517, test loss: 0.2543492919320168\n",
      "epoch 13844: train loss: 0.11382319918756384, test loss: 0.2543500334123246\n",
      "epoch 13845: train loss: 0.11382098914024491, test loss: 0.2543507749996489\n",
      "epoch 13846: train loss: 0.11381877931252102, test loss: 0.25435151669395833\n",
      "epoch 13847: train loss: 0.11381656970435303, test loss: 0.25435225849521914\n",
      "epoch 13848: train loss: 0.11381436031570175, test loss: 0.2543530004034176\n",
      "epoch 13849: train loss: 0.11381215114652804, test loss: 0.254353742418513\n",
      "epoch 13850: train loss: 0.11380994219679273, test loss: 0.2543544845405145\n",
      "epoch 13851: train loss: 0.11380773346645673, test loss: 0.2543552267693661\n",
      "epoch 13852: train loss: 0.11380552495548088, test loss: 0.2543559691050794\n",
      "epoch 13853: train loss: 0.11380331666382608, test loss: 0.2543567115475852\n",
      "epoch 13854: train loss: 0.11380110859145322, test loss: 0.25435745409686955\n",
      "epoch 13855: train loss: 0.11379890073832319, test loss: 0.2543581967529454\n",
      "epoch 13856: train loss: 0.11379669310439695, test loss: 0.25435893951575156\n",
      "epoch 13857: train loss: 0.11379448568963539, test loss: 0.2543596823852672\n",
      "epoch 13858: train loss: 0.11379227849399945, test loss: 0.2543604253614862\n",
      "epoch 13859: train loss: 0.11379007151745008, test loss: 0.25436116844437445\n",
      "epoch 13860: train loss: 0.11378786475994823, test loss: 0.2543619116339017\n",
      "epoch 13861: train loss: 0.1137856582214549, test loss: 0.2543626549300616\n",
      "epoch 13862: train loss: 0.11378345190193101, test loss: 0.25436339833281296\n",
      "epoch 13863: train loss: 0.11378124580133754, test loss: 0.25436414184213374\n",
      "epoch 13864: train loss: 0.11377903991963556, test loss: 0.2543648854580075\n",
      "epoch 13865: train loss: 0.11377683425678602, test loss: 0.25436562918041206\n",
      "epoch 13866: train loss: 0.11377462881274994, test loss: 0.2543663730093064\n",
      "epoch 13867: train loss: 0.11377242358748835, test loss: 0.25436711694467495\n",
      "epoch 13868: train loss: 0.11377021858096224, test loss: 0.25436786098648895\n",
      "epoch 13869: train loss: 0.11376801379313273, test loss: 0.254368605134757\n",
      "epoch 13870: train loss: 0.11376580922396083, test loss: 0.2543693493894227\n",
      "epoch 13871: train loss: 0.1137636048734076, test loss: 0.25437009375046116\n",
      "epoch 13872: train loss: 0.11376140074143412, test loss: 0.25437083821786843\n",
      "epoch 13873: train loss: 0.11375919682800148, test loss: 0.254371582791585\n",
      "epoch 13874: train loss: 0.1137569931330708, test loss: 0.2543723274716243\n",
      "epoch 13875: train loss: 0.11375478965660311, test loss: 0.25437307225794403\n",
      "epoch 13876: train loss: 0.11375258639855959, test loss: 0.2543738171505311\n",
      "epoch 13877: train loss: 0.11375038335890134, test loss: 0.254374562149361\n",
      "epoch 13878: train loss: 0.11374818053758946, test loss: 0.2543753072543953\n",
      "epoch 13879: train loss: 0.11374597793458514, test loss: 0.25437605246562695\n",
      "epoch 13880: train loss: 0.11374377554984953, test loss: 0.2543767977830068\n",
      "epoch 13881: train loss: 0.11374157338334374, test loss: 0.2543775432065386\n",
      "epoch 13882: train loss: 0.11373937143502898, test loss: 0.25437828873618007\n",
      "epoch 13883: train loss: 0.11373716970486644, test loss: 0.2543790343719109\n",
      "epoch 13884: train loss: 0.11373496819281727, test loss: 0.25437978011372336\n",
      "epoch 13885: train loss: 0.11373276689884274, test loss: 0.2543805259615882\n",
      "epoch 13886: train loss: 0.11373056582290399, test loss: 0.25438127191545395\n",
      "epoch 13887: train loss: 0.11372836496496226, test loss: 0.25438201797533333\n",
      "epoch 13888: train loss: 0.11372616432497878, test loss: 0.25438276414116834\n",
      "epoch 13889: train loss: 0.11372396390291485, test loss: 0.2543835104129624\n",
      "epoch 13890: train loss: 0.11372176369873163, test loss: 0.2543842567906843\n",
      "epoch 13891: train loss: 0.1137195637123904, test loss: 0.25438500327431085\n",
      "epoch 13892: train loss: 0.11371736394385248, test loss: 0.25438574986380275\n",
      "epoch 13893: train loss: 0.1137151643930791, test loss: 0.2543864965591518\n",
      "epoch 13894: train loss: 0.11371296506003155, test loss: 0.254387243360347\n",
      "epoch 13895: train loss: 0.11371076594467118, test loss: 0.2543879902673206\n",
      "epoch 13896: train loss: 0.11370856704695925, test loss: 0.25438873728010203\n",
      "epoch 13897: train loss: 0.11370636836685705, test loss: 0.25438948439863374\n",
      "epoch 13898: train loss: 0.11370416990432598, test loss: 0.25439023162290203\n",
      "epoch 13899: train loss: 0.11370197165932733, test loss: 0.2543909789528749\n",
      "epoch 13900: train loss: 0.11369977363182247, test loss: 0.2543917263885301\n",
      "epoch 13901: train loss: 0.11369757582177276, test loss: 0.2543924739298619\n",
      "epoch 13902: train loss: 0.11369537822913951, test loss: 0.2543932215768313\n",
      "epoch 13903: train loss: 0.11369318085388418, test loss: 0.25439396932940395\n",
      "epoch 13904: train loss: 0.1136909836959681, test loss: 0.25439471718757\n",
      "epoch 13905: train loss: 0.11368878675535267, test loss: 0.2543954651513213\n",
      "epoch 13906: train loss: 0.11368659003199934, test loss: 0.2543962132205931\n",
      "epoch 13907: train loss: 0.11368439352586947, test loss: 0.25439696139540613\n",
      "epoch 13908: train loss: 0.11368219723692449, test loss: 0.2543977096757012\n",
      "epoch 13909: train loss: 0.1136800011651259, test loss: 0.25439845806147543\n",
      "epoch 13910: train loss: 0.11367780531043506, test loss: 0.25439920655270565\n",
      "epoch 13911: train loss: 0.11367560967281348, test loss: 0.25439995514934227\n",
      "epoch 13912: train loss: 0.11367341425222259, test loss: 0.2544007038513898\n",
      "epoch 13913: train loss: 0.11367121904862387, test loss: 0.25440145265882586\n",
      "epoch 13914: train loss: 0.11366902406197883, test loss: 0.25440220157160015\n",
      "epoch 13915: train loss: 0.11366682929224893, test loss: 0.25440295058970963\n",
      "epoch 13916: train loss: 0.11366463473939571, test loss: 0.25440369971313137\n",
      "epoch 13917: train loss: 0.11366244040338064, test loss: 0.25440444894182523\n",
      "epoch 13918: train loss: 0.11366024628416525, test loss: 0.25440519827578517\n",
      "epoch 13919: train loss: 0.11365805238171112, test loss: 0.2544059477149724\n",
      "epoch 13920: train loss: 0.11365585869597972, test loss: 0.2544066972593736\n",
      "epoch 13921: train loss: 0.11365366522693263, test loss: 0.2544074469089655\n",
      "epoch 13922: train loss: 0.11365147197453145, test loss: 0.25440819666371733\n",
      "epoch 13923: train loss: 0.11364927893873769, test loss: 0.25440894652361573\n",
      "epoch 13924: train loss: 0.11364708611951296, test loss: 0.2544096964886205\n",
      "epoch 13925: train loss: 0.11364489351681886, test loss: 0.25441044655874484\n",
      "epoch 13926: train loss: 0.11364270113061696, test loss: 0.25441119673390417\n",
      "epoch 13927: train loss: 0.11364050896086887, test loss: 0.2544119470141376\n",
      "epoch 13928: train loss: 0.11363831700753625, test loss: 0.25441269739937156\n",
      "epoch 13929: train loss: 0.1136361252705807, test loss: 0.25441344788961784\n",
      "epoch 13930: train loss: 0.11363393374996385, test loss: 0.2544141984848283\n",
      "epoch 13931: train loss: 0.11363174244564736, test loss: 0.2544149491850067\n",
      "epoch 13932: train loss: 0.11362955135759285, test loss: 0.2544156999900955\n",
      "epoch 13933: train loss: 0.11362736048576207, test loss: 0.2544164509000995\n",
      "epoch 13934: train loss: 0.11362516983011663, test loss: 0.25441720191496875\n",
      "epoch 13935: train loss: 0.11362297939061823, test loss: 0.2544179530347016\n",
      "epoch 13936: train loss: 0.11362078916722858, test loss: 0.25441870425926383\n",
      "epoch 13937: train loss: 0.11361859915990934, test loss: 0.2544194555886531\n",
      "epoch 13938: train loss: 0.1136164093686223, test loss: 0.25442020702281026\n",
      "epoch 13939: train loss: 0.11361421979332911, test loss: 0.25442095856173275\n",
      "epoch 13940: train loss: 0.11361203043399154, test loss: 0.25442171020539717\n",
      "epoch 13941: train loss: 0.11360984129057136, test loss: 0.2544224619537825\n",
      "epoch 13942: train loss: 0.11360765236303032, test loss: 0.25442321380683913\n",
      "epoch 13943: train loss: 0.11360546365133009, test loss: 0.2544239657645718\n",
      "epoch 13944: train loss: 0.11360327515543255, test loss: 0.254424717826968\n",
      "epoch 13945: train loss: 0.1136010868752994, test loss: 0.2544254699939697\n",
      "epoch 13946: train loss: 0.11359889881089252, test loss: 0.2544262222655721\n",
      "epoch 13947: train loss: 0.11359671096217365, test loss: 0.2544269746417278\n",
      "epoch 13948: train loss: 0.11359452332910465, test loss: 0.2544277271224583\n",
      "epoch 13949: train loss: 0.11359233591164726, test loss: 0.2544284797077069\n",
      "epoch 13950: train loss: 0.1135901487097634, test loss: 0.25442923239746057\n",
      "epoch 13951: train loss: 0.11358796172341484, test loss: 0.2544299851916877\n",
      "epoch 13952: train loss: 0.11358577495256349, test loss: 0.2544307380903677\n",
      "epoch 13953: train loss: 0.11358358839717113, test loss: 0.25443149109348595\n",
      "epoch 13954: train loss: 0.1135814020571997, test loss: 0.2544322442010216\n",
      "epoch 13955: train loss: 0.11357921593261107, test loss: 0.25443299741292635\n",
      "epoch 13956: train loss: 0.1135770300233671, test loss: 0.254433750729204\n",
      "epoch 13957: train loss: 0.11357484432942969, test loss: 0.2544345041498158\n",
      "epoch 13958: train loss: 0.11357265885076076, test loss: 0.2544352576747495\n",
      "epoch 13959: train loss: 0.11357047358732221, test loss: 0.25443601130398236\n",
      "epoch 13960: train loss: 0.11356828853907601, test loss: 0.25443676503746515\n",
      "epoch 13961: train loss: 0.11356610370598404, test loss: 0.25443751887520444\n",
      "epoch 13962: train loss: 0.11356391908800825, test loss: 0.25443827281715115\n",
      "epoch 13963: train loss: 0.11356173468511066, test loss: 0.25443902686330094\n",
      "epoch 13964: train loss: 0.11355955049725315, test loss: 0.2544397810136331\n",
      "epoch 13965: train loss: 0.11355736652439774, test loss: 0.254440535268107\n",
      "epoch 13966: train loss: 0.11355518276650642, test loss: 0.25444128962671053\n",
      "epoch 13967: train loss: 0.11355299922354113, test loss: 0.2544420440894045\n",
      "epoch 13968: train loss: 0.11355081589546394, test loss: 0.254442798656203\n",
      "epoch 13969: train loss: 0.11354863278223683, test loss: 0.254443553327038\n",
      "epoch 13970: train loss: 0.1135464498838218, test loss: 0.25444430810191726\n",
      "epoch 13971: train loss: 0.11354426720018092, test loss: 0.2544450629808184\n",
      "epoch 13972: train loss: 0.11354208473127621, test loss: 0.25444581796369137\n",
      "epoch 13973: train loss: 0.11353990247706972, test loss: 0.2544465730505256\n",
      "epoch 13974: train loss: 0.1135377204375235, test loss: 0.2544473282413172\n",
      "epoch 13975: train loss: 0.11353553861259968, test loss: 0.2544480835359981\n",
      "epoch 13976: train loss: 0.11353335700226025, test loss: 0.254448838934594\n",
      "epoch 13977: train loss: 0.11353117560646733, test loss: 0.25444959443705595\n",
      "epoch 13978: train loss: 0.11352899442518304, test loss: 0.2544503500433609\n",
      "epoch 13979: train loss: 0.11352681345836949, test loss: 0.254451105753489\n",
      "epoch 13980: train loss: 0.11352463270598875, test loss: 0.25445186156741834\n",
      "epoch 13981: train loss: 0.11352245216800298, test loss: 0.25445261748511816\n",
      "epoch 13982: train loss: 0.11352027184437433, test loss: 0.2544533735065855\n",
      "epoch 13983: train loss: 0.1135180917350649, test loss: 0.2544541296317805\n",
      "epoch 13984: train loss: 0.11351591184003688, test loss: 0.2544548858606744\n",
      "epoch 13985: train loss: 0.11351373215925242, test loss: 0.2544556421932433\n",
      "epoch 13986: train loss: 0.1135115526926737, test loss: 0.25445639862948644\n",
      "epoch 13987: train loss: 0.11350937344026288, test loss: 0.2544571551693621\n",
      "epoch 13988: train loss: 0.11350719440198219, test loss: 0.25445791181286004\n",
      "epoch 13989: train loss: 0.11350501557779381, test loss: 0.2544586685599298\n",
      "epoch 13990: train loss: 0.11350283696765995, test loss: 0.2544594254105803\n",
      "epoch 13991: train loss: 0.11350065857154285, test loss: 0.25446018236476076\n",
      "epoch 13992: train loss: 0.1134984803894047, test loss: 0.25446093942246806\n",
      "epoch 13993: train loss: 0.11349630242120778, test loss: 0.25446169658368106\n",
      "epoch 13994: train loss: 0.11349412466691433, test loss: 0.2544624538483623\n",
      "epoch 13995: train loss: 0.1134919471264866, test loss: 0.25446321121647936\n",
      "epoch 13996: train loss: 0.11348976979988684, test loss: 0.25446396868804855\n",
      "epoch 13997: train loss: 0.11348759268707737, test loss: 0.25446472626300265\n",
      "epoch 13998: train loss: 0.11348541578802043, test loss: 0.25446548394134777\n",
      "epoch 13999: train loss: 0.11348323910267837, test loss: 0.25446624172304605\n",
      "epoch 14000: train loss: 0.11348106263101343, test loss: 0.2544669996080835\n",
      "epoch 14001: train loss: 0.11347888637298797, test loss: 0.25446775759643114\n",
      "epoch 14002: train loss: 0.11347671032856432, test loss: 0.2544685156880684\n",
      "epoch 14003: train loss: 0.1134745344977048, test loss: 0.25446927388296353\n",
      "epoch 14004: train loss: 0.11347235888037174, test loss: 0.25447003218109665\n",
      "epoch 14005: train loss: 0.1134701834765275, test loss: 0.2544707905824644\n",
      "epoch 14006: train loss: 0.11346800828613446, test loss: 0.25447154908701\n",
      "epoch 14007: train loss: 0.11346583330915497, test loss: 0.25447230769472967\n",
      "epoch 14008: train loss: 0.11346365854555142, test loss: 0.2544730664056039\n",
      "epoch 14009: train loss: 0.11346148399528619, test loss: 0.25447382521960127\n",
      "epoch 14010: train loss: 0.11345930965832171, test loss: 0.25447458413671087\n",
      "epoch 14011: train loss: 0.11345713553462038, test loss: 0.25447534315689363\n",
      "epoch 14012: train loss: 0.11345496162414458, test loss: 0.2544761022801091\n",
      "epoch 14013: train loss: 0.11345278792685677, test loss: 0.25447686150639187\n",
      "epoch 14014: train loss: 0.11345061444271938, test loss: 0.25447762083565845\n",
      "epoch 14015: train loss: 0.11344844117169489, test loss: 0.2544783802679325\n",
      "epoch 14016: train loss: 0.1134462681137457, test loss: 0.2544791398031553\n",
      "epoch 14017: train loss: 0.1134440952688343, test loss: 0.2544798994413358\n",
      "epoch 14018: train loss: 0.11344192263692317, test loss: 0.2544806591824251\n",
      "epoch 14019: train loss: 0.1134397502179748, test loss: 0.2544814190263926\n",
      "epoch 14020: train loss: 0.11343757801195167, test loss: 0.25448217897324543\n",
      "epoch 14021: train loss: 0.11343540601881628, test loss: 0.2544829390229528\n",
      "epoch 14022: train loss: 0.11343323423853116, test loss: 0.25448369917547775\n",
      "epoch 14023: train loss: 0.1134310626710588, test loss: 0.2544844594308071\n",
      "epoch 14024: train loss: 0.11342889131636177, test loss: 0.2544852197889117\n",
      "epoch 14025: train loss: 0.1134267201744026, test loss: 0.2544859802497612\n",
      "epoch 14026: train loss: 0.11342454924514382, test loss: 0.2544867408133627\n",
      "epoch 14027: train loss: 0.11342237852854802, test loss: 0.2544875014796577\n",
      "epoch 14028: train loss: 0.11342020802457775, test loss: 0.2544882622486553\n",
      "epoch 14029: train loss: 0.11341803773319555, test loss: 0.2544890231203057\n",
      "epoch 14030: train loss: 0.1134158676543641, test loss: 0.25448978409459827\n",
      "epoch 14031: train loss: 0.11341369778804591, test loss: 0.25449054517150294\n",
      "epoch 14032: train loss: 0.11341152813420362, test loss: 0.2544913063510091\n",
      "epoch 14033: train loss: 0.11340935869279983, test loss: 0.2544920676330852\n",
      "epoch 14034: train loss: 0.1134071894637972, test loss: 0.2544928290177117\n",
      "epoch 14035: train loss: 0.11340502044715833, test loss: 0.25449359050485937\n",
      "epoch 14036: train loss: 0.11340285164284587, test loss: 0.2544943520944982\n",
      "epoch 14037: train loss: 0.11340068305082251, test loss: 0.2544951137866354\n",
      "epoch 14038: train loss: 0.11339851467105085, test loss: 0.25449587558122283\n",
      "epoch 14039: train loss: 0.1133963465034936, test loss: 0.254496637478239\n",
      "epoch 14040: train loss: 0.11339417854811343, test loss: 0.2544973994776749\n",
      "epoch 14041: train loss: 0.11339201080487303, test loss: 0.2544981615794895\n",
      "epoch 14042: train loss: 0.11338984327373511, test loss: 0.25449892378365374\n",
      "epoch 14043: train loss: 0.11338767595466237, test loss: 0.2544996860901852\n",
      "epoch 14044: train loss: 0.1133855088476175, test loss: 0.25450044849902553\n",
      "epoch 14045: train loss: 0.11338334195256328, test loss: 0.2545012110101548\n",
      "epoch 14046: train loss: 0.11338117526946241, test loss: 0.2545019736235796\n",
      "epoch 14047: train loss: 0.11337900879827764, test loss: 0.2545027363392263\n",
      "epoch 14048: train loss: 0.11337684253897173, test loss: 0.2545034991571086\n",
      "epoch 14049: train loss: 0.11337467649150745, test loss: 0.2545042620771991\n",
      "epoch 14050: train loss: 0.1133725106558476, test loss: 0.254505025099477\n",
      "epoch 14051: train loss: 0.11337034503195487, test loss: 0.25450578822389447\n",
      "epoch 14052: train loss: 0.11336817961979215, test loss: 0.2545065514504588\n",
      "epoch 14053: train loss: 0.11336601441932219, test loss: 0.2545073147791213\n",
      "epoch 14054: train loss: 0.11336384943050781, test loss: 0.2545080782098903\n",
      "epoch 14055: train loss: 0.11336168465331185, test loss: 0.25450884174272703\n",
      "epoch 14056: train loss: 0.11335952008769709, test loss: 0.25450960537760187\n",
      "epoch 14057: train loss: 0.11335735573362644, test loss: 0.2545103691144772\n",
      "epoch 14058: train loss: 0.1133551915910627, test loss: 0.25451113295337824\n",
      "epoch 14059: train loss: 0.11335302765996874, test loss: 0.25451189689424664\n",
      "epoch 14060: train loss: 0.11335086394030741, test loss: 0.2545126609370546\n",
      "epoch 14061: train loss: 0.1133487004320416, test loss: 0.25451342508181\n",
      "epoch 14062: train loss: 0.11334653713513418, test loss: 0.25451418932844555\n",
      "epoch 14063: train loss: 0.11334437404954807, test loss: 0.25451495367698823\n",
      "epoch 14064: train loss: 0.11334221117524615, test loss: 0.25451571812737134\n",
      "epoch 14065: train loss: 0.11334004851219134, test loss: 0.2545164826796023\n",
      "epoch 14066: train loss: 0.11333788606034659, test loss: 0.2545172473336524\n",
      "epoch 14067: train loss: 0.11333572381967479, test loss: 0.2545180120894919\n",
      "epoch 14068: train loss: 0.11333356179013888, test loss: 0.25451877694709163\n",
      "epoch 14069: train loss: 0.11333139997170184, test loss: 0.2545195419064412\n",
      "epoch 14070: train loss: 0.1133292383643266, test loss: 0.2545203069675387\n",
      "epoch 14071: train loss: 0.11332707696797614, test loss: 0.25452107213029973\n",
      "epoch 14072: train loss: 0.11332491578261344, test loss: 0.25452183739476\n",
      "epoch 14073: train loss: 0.11332275480820148, test loss: 0.254522602760871\n",
      "epoch 14074: train loss: 0.11332059404470327, test loss: 0.25452336822861393\n",
      "epoch 14075: train loss: 0.11331843349208176, test loss: 0.2545241337979605\n",
      "epoch 14076: train loss: 0.11331627315030005, test loss: 0.2545248994689064\n",
      "epoch 14077: train loss: 0.11331411301932111, test loss: 0.25452566524141573\n",
      "epoch 14078: train loss: 0.11331195309910796, test loss: 0.2545264311154397\n",
      "epoch 14079: train loss: 0.1133097933896237, test loss: 0.254527197091014\n",
      "epoch 14080: train loss: 0.11330763389083132, test loss: 0.2545279631680643\n",
      "epoch 14081: train loss: 0.1133054746026939, test loss: 0.2545287293465982\n",
      "epoch 14082: train loss: 0.11330331552517452, test loss: 0.2545294956265864\n",
      "epoch 14083: train loss: 0.11330115665823624, test loss: 0.2545302620079809\n",
      "epoch 14084: train loss: 0.11329899800184215, test loss: 0.25453102849079107\n",
      "epoch 14085: train loss: 0.11329683955595538, test loss: 0.25453179507498647\n",
      "epoch 14086: train loss: 0.113294681320539, test loss: 0.2545325617605576\n",
      "epoch 14087: train loss: 0.11329252329555613, test loss: 0.2545333285474377\n",
      "epoch 14088: train loss: 0.11329036548096988, test loss: 0.2545340954356362\n",
      "epoch 14089: train loss: 0.11328820787674344, test loss: 0.2545348624251412\n",
      "epoch 14090: train loss: 0.11328605048283989, test loss: 0.2545356295159052\n",
      "epoch 14091: train loss: 0.11328389329922242, test loss: 0.25453639670790157\n",
      "epoch 14092: train loss: 0.11328173632585414, test loss: 0.2545371640011449\n",
      "epoch 14093: train loss: 0.1132795795626983, test loss: 0.25453793139558073\n",
      "epoch 14094: train loss: 0.113277423009718, test loss: 0.25453869889117925\n",
      "epoch 14095: train loss: 0.1132752666668765, test loss: 0.25453946648794806\n",
      "epoch 14096: train loss: 0.11327311053413695, test loss: 0.2545402341858409\n",
      "epoch 14097: train loss: 0.11327095461146254, test loss: 0.25454100198486584\n",
      "epoch 14098: train loss: 0.11326879889881653, test loss: 0.2545417698849563\n",
      "epoch 14099: train loss: 0.11326664339616212, test loss: 0.2545425378861117\n",
      "epoch 14100: train loss: 0.11326448810346255, test loss: 0.25454330598831393\n",
      "epoch 14101: train loss: 0.11326233302068107, test loss: 0.25454407419153136\n",
      "epoch 14102: train loss: 0.1132601781477809, test loss: 0.2545448424957557\n",
      "epoch 14103: train loss: 0.11325802348472537, test loss: 0.2545456109009398\n",
      "epoch 14104: train loss: 0.11325586903147768, test loss: 0.25454637940709146\n",
      "epoch 14105: train loss: 0.11325371478800117, test loss: 0.254547148014164\n",
      "epoch 14106: train loss: 0.11325156075425906, test loss: 0.2545479167221375\n",
      "epoch 14107: train loss: 0.1132494069302147, test loss: 0.2545486855310013\n",
      "epoch 14108: train loss: 0.11324725331583135, test loss: 0.254549454440719\n",
      "epoch 14109: train loss: 0.1132450999110724, test loss: 0.2545502234512796\n",
      "epoch 14110: train loss: 0.1132429467159011, test loss: 0.25455099256266406\n",
      "epoch 14111: train loss: 0.11324079373028084, test loss: 0.254551761774824\n",
      "epoch 14112: train loss: 0.11323864095417491, test loss: 0.25455253108776915\n",
      "epoch 14113: train loss: 0.11323648838754671, test loss: 0.25455330050145364\n",
      "epoch 14114: train loss: 0.11323433603035957, test loss: 0.2545540700158735\n",
      "epoch 14115: train loss: 0.11323218388257689, test loss: 0.25455483963099357\n",
      "epoch 14116: train loss: 0.11323003194416202, test loss: 0.2545556093468033\n",
      "epoch 14117: train loss: 0.11322788021507835, test loss: 0.25455637916323726\n",
      "epoch 14118: train loss: 0.1132257286952893, test loss: 0.2545571490803393\n",
      "epoch 14119: train loss: 0.11322357738475826, test loss: 0.2545579190980466\n",
      "epoch 14120: train loss: 0.11322142628344867, test loss: 0.2545586892163379\n",
      "epoch 14121: train loss: 0.11321927539132393, test loss: 0.2545594594352126\n",
      "epoch 14122: train loss: 0.11321712470834747, test loss: 0.25456022975460757\n",
      "epoch 14123: train loss: 0.11321497423448275, test loss: 0.25456100017454714\n",
      "epoch 14124: train loss: 0.11321282396969319, test loss: 0.2545617706949868\n",
      "epoch 14125: train loss: 0.11321067391394232, test loss: 0.2545625413158859\n",
      "epoch 14126: train loss: 0.11320852406719353, test loss: 0.25456331203725663\n",
      "epoch 14127: train loss: 0.11320637442941037, test loss: 0.2545640828590491\n",
      "epoch 14128: train loss: 0.11320422500055628, test loss: 0.254564853781274\n",
      "epoch 14129: train loss: 0.11320207578059477, test loss: 0.2545656248038655\n",
      "epoch 14130: train loss: 0.11319992676948935, test loss: 0.2545663959268319\n",
      "epoch 14131: train loss: 0.11319777796720354, test loss: 0.25456716715012717\n",
      "epoch 14132: train loss: 0.11319562937370087, test loss: 0.25456793847374953\n",
      "epoch 14133: train loss: 0.11319348098894483, test loss: 0.25456870989769015\n",
      "epoch 14134: train loss: 0.11319133281289902, test loss: 0.2545694814218934\n",
      "epoch 14135: train loss: 0.11318918484552697, test loss: 0.2545702530463483\n",
      "epoch 14136: train loss: 0.11318703708679224, test loss: 0.25457102477101906\n",
      "epoch 14137: train loss: 0.11318488953665838, test loss: 0.2545717965959318\n",
      "epoch 14138: train loss: 0.11318274219508902, test loss: 0.25457256852100263\n",
      "epoch 14139: train loss: 0.11318059506204771, test loss: 0.25457334054626\n",
      "epoch 14140: train loss: 0.11317844813749803, test loss: 0.25457411267163815\n",
      "epoch 14141: train loss: 0.11317630142140363, test loss: 0.25457488489713676\n",
      "epoch 14142: train loss: 0.11317415491372806, test loss: 0.2545756572227452\n",
      "epoch 14143: train loss: 0.11317200861443504, test loss: 0.25457642964841937\n",
      "epoch 14144: train loss: 0.11316986252348812, test loss: 0.25457720217414753\n",
      "epoch 14145: train loss: 0.11316771664085098, test loss: 0.2545779747999013\n",
      "epoch 14146: train loss: 0.11316557096648726, test loss: 0.25457874752565496\n",
      "epoch 14147: train loss: 0.11316342550036064, test loss: 0.2545795203514156\n",
      "epoch 14148: train loss: 0.11316128024243476, test loss: 0.25458029327711856\n",
      "epoch 14149: train loss: 0.1131591351926733, test loss: 0.2545810663027641\n",
      "epoch 14150: train loss: 0.11315699035103996, test loss: 0.25458183942834284\n",
      "epoch 14151: train loss: 0.11315484571749845, test loss: 0.2545826126538158\n",
      "epoch 14152: train loss: 0.11315270129201242, test loss: 0.25458338597914787\n",
      "epoch 14153: train loss: 0.11315055707454565, test loss: 0.2545841594043194\n",
      "epoch 14154: train loss: 0.11314841306506183, test loss: 0.2545849329293393\n",
      "epoch 14155: train loss: 0.11314626926352471, test loss: 0.25458570655417057\n",
      "epoch 14156: train loss: 0.11314412566989801, test loss: 0.25458648027878567\n",
      "epoch 14157: train loss: 0.11314198228414547, test loss: 0.2545872541031484\n",
      "epoch 14158: train loss: 0.11313983910623086, test loss: 0.2545880280272366\n",
      "epoch 14159: train loss: 0.11313769613611799, test loss: 0.2545888020510724\n",
      "epoch 14160: train loss: 0.11313555337377057, test loss: 0.2545895761745902\n",
      "epoch 14161: train loss: 0.11313341081915244, test loss: 0.2545903503977885\n",
      "epoch 14162: train loss: 0.11313126847222735, test loss: 0.2545911247206224\n",
      "epoch 14163: train loss: 0.11312912633295913, test loss: 0.25459189914308156\n",
      "epoch 14164: train loss: 0.11312698440131158, test loss: 0.25459267366515675\n",
      "epoch 14165: train loss: 0.11312484267724854, test loss: 0.25459344828681196\n",
      "epoch 14166: train loss: 0.11312270116073382, test loss: 0.25459422300803697\n",
      "epoch 14167: train loss: 0.11312055985173128, test loss: 0.2545949978287956\n",
      "epoch 14168: train loss: 0.11311841875020473, test loss: 0.2545957727490679\n",
      "epoch 14169: train loss: 0.11311627785611807, test loss: 0.25459654776881885\n",
      "epoch 14170: train loss: 0.11311413716943516, test loss: 0.2545973228880657\n",
      "epoch 14171: train loss: 0.11311199669011986, test loss: 0.2545980981067631\n",
      "epoch 14172: train loss: 0.11310985641813603, test loss: 0.25459887342487414\n",
      "epoch 14173: train loss: 0.11310771635344764, test loss: 0.2545996488424067\n",
      "epoch 14174: train loss: 0.1131055764960185, test loss: 0.25460042435929686\n",
      "epoch 14175: train loss: 0.11310343684581259, test loss: 0.254601199975565\n",
      "epoch 14176: train loss: 0.11310129740279379, test loss: 0.2546019756911811\n",
      "epoch 14177: train loss: 0.11309915816692605, test loss: 0.25460275150610123\n",
      "epoch 14178: train loss: 0.1130970191381733, test loss: 0.25460352742031395\n",
      "epoch 14179: train loss: 0.11309488031649949, test loss: 0.25460430343379403\n",
      "epoch 14180: train loss: 0.11309274170186855, test loss: 0.25460507954653905\n",
      "epoch 14181: train loss: 0.11309060329424452, test loss: 0.25460585575850553\n",
      "epoch 14182: train loss: 0.11308846509359124, test loss: 0.2546066320696832\n",
      "epoch 14183: train loss: 0.11308632709987285, test loss: 0.2546074084800262\n",
      "epoch 14184: train loss: 0.11308418931305322, test loss: 0.25460818498953514\n",
      "epoch 14185: train loss: 0.11308205173309639, test loss: 0.254608961598201\n",
      "epoch 14186: train loss: 0.1130799143599664, test loss: 0.25460973830597716\n",
      "epoch 14187: train loss: 0.11307777719362719, test loss: 0.2546105151128469\n",
      "epoch 14188: train loss: 0.11307564023404287, test loss: 0.25461129201879\n",
      "epoch 14189: train loss: 0.11307350348117744, test loss: 0.25461206902377126\n",
      "epoch 14190: train loss: 0.11307136693499494, test loss: 0.2546128461278\n",
      "epoch 14191: train loss: 0.11306923059545942, test loss: 0.25461362333082904\n",
      "epoch 14192: train loss: 0.11306709446253498, test loss: 0.25461440063284335\n",
      "epoch 14193: train loss: 0.11306495853618563, test loss: 0.25461517803381306\n",
      "epoch 14194: train loss: 0.1130628228163755, test loss: 0.25461595553372873\n",
      "epoch 14195: train loss: 0.11306068730306863, test loss: 0.25461673313256616\n",
      "epoch 14196: train loss: 0.11305855199622919, test loss: 0.25461751083028594\n",
      "epoch 14197: train loss: 0.11305641689582119, test loss: 0.2546182886268983\n",
      "epoch 14198: train loss: 0.11305428200180884, test loss: 0.2546190665223584\n",
      "epoch 14199: train loss: 0.1130521473141562, test loss: 0.25461984451662983\n",
      "epoch 14200: train loss: 0.11305001283282745, test loss: 0.25462062260972046\n",
      "epoch 14201: train loss: 0.1130478785577867, test loss: 0.2546214008016056\n",
      "epoch 14202: train loss: 0.11304574448899811, test loss: 0.25462217909224655\n",
      "epoch 14203: train loss: 0.11304361062642582, test loss: 0.25462295748161795\n",
      "epoch 14204: train loss: 0.11304147697003403, test loss: 0.25462373596972754\n",
      "epoch 14205: train loss: 0.1130393435197869, test loss: 0.2546245145565323\n",
      "epoch 14206: train loss: 0.11303721027564861, test loss: 0.2546252932420029\n",
      "epoch 14207: train loss: 0.11303507723758337, test loss: 0.25462607202614085\n",
      "epoch 14208: train loss: 0.11303294440555538, test loss: 0.25462685090889164\n",
      "epoch 14209: train loss: 0.11303081177952884, test loss: 0.25462762989026333\n",
      "epoch 14210: train loss: 0.11302867935946798, test loss: 0.25462840897024025\n",
      "epoch 14211: train loss: 0.11302654714533705, test loss: 0.25462918814874863\n",
      "epoch 14212: train loss: 0.11302441513710025, test loss: 0.25462996742582517\n",
      "epoch 14213: train loss: 0.11302228333472186, test loss: 0.25463074680140596\n",
      "epoch 14214: train loss: 0.1130201517381661, test loss: 0.2546315262755017\n",
      "epoch 14215: train loss: 0.11301802034739727, test loss: 0.2546323058480674\n",
      "epoch 14216: train loss: 0.11301588916237965, test loss: 0.25463308551908304\n",
      "epoch 14217: train loss: 0.11301375818307748, test loss: 0.2546338652885428\n",
      "epoch 14218: train loss: 0.11301162740945511, test loss: 0.2546346451564179\n",
      "epoch 14219: train loss: 0.1130094968414768, test loss: 0.254635425122672\n",
      "epoch 14220: train loss: 0.11300736647910686, test loss: 0.2546362051872979\n",
      "epoch 14221: train loss: 0.11300523632230959, test loss: 0.25463698535026785\n",
      "epoch 14222: train loss: 0.11300310637104939, test loss: 0.2546377656115643\n",
      "epoch 14223: train loss: 0.11300097662529052, test loss: 0.2546385459711513\n",
      "epoch 14224: train loss: 0.11299884708499736, test loss: 0.2546393264290391\n",
      "epoch 14225: train loss: 0.11299671775013426, test loss: 0.25464010698517275\n",
      "epoch 14226: train loss: 0.11299458862066557, test loss: 0.25464088763953435\n",
      "epoch 14227: train loss: 0.11299245969655565, test loss: 0.25464166839212604\n",
      "epoch 14228: train loss: 0.11299033097776895, test loss: 0.25464244924290064\n",
      "epoch 14229: train loss: 0.11298820246426977, test loss: 0.25464323019185137\n",
      "epoch 14230: train loss: 0.11298607415602255, test loss: 0.2546440112389693\n",
      "epoch 14231: train loss: 0.1129839460529917, test loss: 0.2546447923841814\n",
      "epoch 14232: train loss: 0.11298181815514159, test loss: 0.2546455736275069\n",
      "epoch 14233: train loss: 0.11297969046243672, test loss: 0.2546463549689193\n",
      "epoch 14234: train loss: 0.11297756297484146, test loss: 0.2546471364083922\n",
      "epoch 14235: train loss: 0.11297543569232028, test loss: 0.2546479179459166\n",
      "epoch 14236: train loss: 0.11297330861483762, test loss: 0.25464869958143954\n",
      "epoch 14237: train loss: 0.11297118174235792, test loss: 0.2546494813149598\n",
      "epoch 14238: train loss: 0.11296905507484568, test loss: 0.25465026314648087\n",
      "epoch 14239: train loss: 0.11296692861226537, test loss: 0.25465104507592784\n",
      "epoch 14240: train loss: 0.11296480235458146, test loss: 0.2546518271033022\n",
      "epoch 14241: train loss: 0.11296267630175844, test loss: 0.2546526092286064\n",
      "epoch 14242: train loss: 0.11296055045376084, test loss: 0.25465339145176563\n",
      "epoch 14243: train loss: 0.11295842481055317, test loss: 0.2546541737728195\n",
      "epoch 14244: train loss: 0.11295629937209987, test loss: 0.25465495619170425\n",
      "epoch 14245: train loss: 0.1129541741383656, test loss: 0.25465573870841085\n",
      "epoch 14246: train loss: 0.11295204910931478, test loss: 0.25465652132291416\n",
      "epoch 14247: train loss: 0.11294992428491202, test loss: 0.2546573040351954\n",
      "epoch 14248: train loss: 0.11294779966512185, test loss: 0.2546580868452379\n",
      "epoch 14249: train loss: 0.11294567524990885, test loss: 0.25465886975301577\n",
      "epoch 14250: train loss: 0.11294355103923757, test loss: 0.2546596527585016\n",
      "epoch 14251: train loss: 0.11294142703307264, test loss: 0.2546604358616598\n",
      "epoch 14252: train loss: 0.11293930323137859, test loss: 0.25466121906251926\n",
      "epoch 14253: train loss: 0.11293717963412003, test loss: 0.2546620023609889\n",
      "epoch 14254: train loss: 0.11293505624126159, test loss: 0.25466278575711754\n",
      "epoch 14255: train loss: 0.11293293305276787, test loss: 0.2546635692508398\n",
      "epoch 14256: train loss: 0.11293081006860348, test loss: 0.25466435284213923\n",
      "epoch 14257: train loss: 0.11292868728873309, test loss: 0.2546651365309897\n",
      "epoch 14258: train loss: 0.11292656471312132, test loss: 0.2546659203173914\n",
      "epoch 14259: train loss: 0.11292444234173281, test loss: 0.25466670420129267\n",
      "epoch 14260: train loss: 0.11292232017453223, test loss: 0.25466748818272045\n",
      "epoch 14261: train loss: 0.11292019821148427, test loss: 0.25466827226159366\n",
      "epoch 14262: train loss: 0.11291807645255357, test loss: 0.25466905643792215\n",
      "epoch 14263: train loss: 0.11291595489770483, test loss: 0.25466984071169085\n",
      "epoch 14264: train loss: 0.11291383354690274, test loss: 0.25467062508286237\n",
      "epoch 14265: train loss: 0.11291171240011198, test loss: 0.2546714095514202\n",
      "epoch 14266: train loss: 0.11290959145729731, test loss: 0.254672194117329\n",
      "epoch 14267: train loss: 0.11290747071842343, test loss: 0.2546729787806081\n",
      "epoch 14268: train loss: 0.11290535018345504, test loss: 0.25467376354119564\n",
      "epoch 14269: train loss: 0.11290322985235694, test loss: 0.2546745483990719\n",
      "epoch 14270: train loss: 0.11290110972509382, test loss: 0.25467533335423287\n",
      "epoch 14271: train loss: 0.11289898980163042, test loss: 0.25467611840665005\n",
      "epoch 14272: train loss: 0.11289687008193156, test loss: 0.2546769035563068\n",
      "epoch 14273: train loss: 0.11289475056596196, test loss: 0.2546776888031676\n",
      "epoch 14274: train loss: 0.11289263125368645, test loss: 0.2546784741472247\n",
      "epoch 14275: train loss: 0.11289051214506979, test loss: 0.25467925958845267\n",
      "epoch 14276: train loss: 0.11288839324007677, test loss: 0.2546800451268149\n",
      "epoch 14277: train loss: 0.1128862745386722, test loss: 0.25468083076231485\n",
      "epoch 14278: train loss: 0.11288415604082092, test loss: 0.2546816164949248\n",
      "epoch 14279: train loss: 0.11288203774648771, test loss: 0.25468240232462064\n",
      "epoch 14280: train loss: 0.11287991965563744, test loss: 0.254683188251364\n",
      "epoch 14281: train loss: 0.11287780176823492, test loss: 0.25468397427514966\n",
      "epoch 14282: train loss: 0.11287568408424505, test loss: 0.2546847603959599\n",
      "epoch 14283: train loss: 0.11287356660363262, test loss: 0.2546855466137596\n",
      "epoch 14284: train loss: 0.11287144932636256, test loss: 0.2546863329285224\n",
      "epoch 14285: train loss: 0.11286933225239969, test loss: 0.25468711934026017\n",
      "epoch 14286: train loss: 0.11286721538170891, test loss: 0.2546879058489278\n",
      "epoch 14287: train loss: 0.11286509871425515, test loss: 0.2546886924545001\n",
      "epoch 14288: train loss: 0.11286298225000327, test loss: 0.25468947915696843\n",
      "epoch 14289: train loss: 0.11286086598891819, test loss: 0.25469026595628874\n",
      "epoch 14290: train loss: 0.11285874993096483, test loss: 0.25469105285246285\n",
      "epoch 14291: train loss: 0.1128566340761081, test loss: 0.2546918398454657\n",
      "epoch 14292: train loss: 0.11285451842431299, test loss: 0.2546926269352612\n",
      "epoch 14293: train loss: 0.11285240297554439, test loss: 0.2546934141218327\n",
      "epoch 14294: train loss: 0.11285028772976723, test loss: 0.25469420140519183\n",
      "epoch 14295: train loss: 0.11284817268694654, test loss: 0.2546949887852564\n",
      "epoch 14296: train loss: 0.11284605784704727, test loss: 0.2546957762620669\n",
      "epoch 14297: train loss: 0.11284394321003437, test loss: 0.2546965638355495\n",
      "epoch 14298: train loss: 0.11284182877587284, test loss: 0.25469735150571593\n",
      "epoch 14299: train loss: 0.11283971454452768, test loss: 0.25469813927252144\n",
      "epoch 14300: train loss: 0.11283760051596388, test loss: 0.2546989271359699\n",
      "epoch 14301: train loss: 0.1128354866901465, test loss: 0.25469971509602407\n",
      "epoch 14302: train loss: 0.11283337306704048, test loss: 0.2547005031526783\n",
      "epoch 14303: train loss: 0.11283125964661095, test loss: 0.2547012913058777\n",
      "epoch 14304: train loss: 0.11282914642882283, test loss: 0.25470207955562546\n",
      "epoch 14305: train loss: 0.11282703341364128, test loss: 0.2547028679019047\n",
      "epoch 14306: train loss: 0.11282492060103129, test loss: 0.2547036563446799\n",
      "epoch 14307: train loss: 0.11282280799095794, test loss: 0.25470444488393584\n",
      "epoch 14308: train loss: 0.11282069558338628, test loss: 0.2547052335196455\n",
      "epoch 14309: train loss: 0.11281858337828143, test loss: 0.2547060222517933\n",
      "epoch 14310: train loss: 0.11281647137560849, test loss: 0.2547068110803632\n",
      "epoch 14311: train loss: 0.1128143595753325, test loss: 0.25470760000532794\n",
      "epoch 14312: train loss: 0.11281224797741862, test loss: 0.25470838902665477\n",
      "epoch 14313: train loss: 0.11281013658183194, test loss: 0.25470917814434385\n",
      "epoch 14314: train loss: 0.11280802538853756, test loss: 0.25470996735835194\n",
      "epoch 14315: train loss: 0.11280591439750065, test loss: 0.2547107566686729\n",
      "epoch 14316: train loss: 0.11280380360868633, test loss: 0.2547115460752898\n",
      "epoch 14317: train loss: 0.11280169302205978, test loss: 0.2547123355781671\n",
      "epoch 14318: train loss: 0.11279958263758613, test loss: 0.25471312517727895\n",
      "epoch 14319: train loss: 0.11279747245523054, test loss: 0.2547139148726202\n",
      "epoch 14320: train loss: 0.11279536247495824, test loss: 0.25471470466417323\n",
      "epoch 14321: train loss: 0.11279325269673432, test loss: 0.25471549455188525\n",
      "epoch 14322: train loss: 0.11279114312052403, test loss: 0.25471628453577677\n",
      "epoch 14323: train loss: 0.1127890337462926, test loss: 0.25471707461580406\n",
      "epoch 14324: train loss: 0.11278692457400517, test loss: 0.254717864791923\n",
      "epoch 14325: train loss: 0.11278481560362699, test loss: 0.25471865506414487\n",
      "epoch 14326: train loss: 0.11278270683512329, test loss: 0.2547194454324644\n",
      "epoch 14327: train loss: 0.11278059826845933, test loss: 0.25472023589681714\n",
      "epoch 14328: train loss: 0.11277848990360027, test loss: 0.25472102645720585\n",
      "epoch 14329: train loss: 0.11277638174051147, test loss: 0.25472181711359615\n",
      "epoch 14330: train loss: 0.11277427377915811, test loss: 0.2547226078659827\n",
      "epoch 14331: train loss: 0.11277216601950546, test loss: 0.254723398714328\n",
      "epoch 14332: train loss: 0.11277005846151883, test loss: 0.2547241896586179\n",
      "epoch 14333: train loss: 0.1127679511051635, test loss: 0.254724980698827\n",
      "epoch 14334: train loss: 0.11276584395040476, test loss: 0.2547257718349574\n",
      "epoch 14335: train loss: 0.11276373699720793, test loss: 0.2547265630669645\n",
      "epoch 14336: train loss: 0.1127616302455383, test loss: 0.25472735439481536\n",
      "epoch 14337: train loss: 0.11275952369536117, test loss: 0.25472814581851105\n",
      "epoch 14338: train loss: 0.11275741734664191, test loss: 0.25472893733802765\n",
      "epoch 14339: train loss: 0.11275531119934583, test loss: 0.25472972895334783\n",
      "epoch 14340: train loss: 0.11275320525343828, test loss: 0.2547305206644291\n",
      "epoch 14341: train loss: 0.1127510995088846, test loss: 0.2547313124712741\n",
      "epoch 14342: train loss: 0.11274899396565016, test loss: 0.2547321043738464\n",
      "epoch 14343: train loss: 0.11274688862370036, test loss: 0.25473289637212226\n",
      "epoch 14344: train loss: 0.11274478348300054, test loss: 0.2547336884661049\n",
      "epoch 14345: train loss: 0.11274267854351613, test loss: 0.25473448065573917\n",
      "epoch 14346: train loss: 0.11274057380521246, test loss: 0.25473527294103065\n",
      "epoch 14347: train loss: 0.11273846926805497, test loss: 0.25473606532193366\n",
      "epoch 14348: train loss: 0.11273636493200909, test loss: 0.25473685779846084\n",
      "epoch 14349: train loss: 0.11273426079704024, test loss: 0.25473765037055957\n",
      "epoch 14350: train loss: 0.1127321568631138, test loss: 0.25473844303823234\n",
      "epoch 14351: train loss: 0.11273005313019524, test loss: 0.2547392358014246\n",
      "epoch 14352: train loss: 0.11272794959825004, test loss: 0.2547400286601414\n",
      "epoch 14353: train loss: 0.11272584626724359, test loss: 0.25474082161437556\n",
      "epoch 14354: train loss: 0.11272374313714138, test loss: 0.2547416146640837\n",
      "epoch 14355: train loss: 0.11272164020790892, test loss: 0.25474240780923063\n",
      "epoch 14356: train loss: 0.11271953747951166, test loss: 0.2547432010498202\n",
      "epoch 14357: train loss: 0.11271743495191507, test loss: 0.25474399438583556\n",
      "epoch 14358: train loss: 0.11271533262508462, test loss: 0.25474478781723453\n",
      "epoch 14359: train loss: 0.1127132304989859, test loss: 0.2547455813440008\n",
      "epoch 14360: train loss: 0.11271112857358437, test loss: 0.2547463749661178\n",
      "epoch 14361: train loss: 0.11270902684884558, test loss: 0.25474716868356173\n",
      "epoch 14362: train loss: 0.11270692532473502, test loss: 0.25474796249631715\n",
      "epoch 14363: train loss: 0.11270482400121824, test loss: 0.25474875640436756\n",
      "epoch 14364: train loss: 0.11270272287826082, test loss: 0.25474955040766056\n",
      "epoch 14365: train loss: 0.1127006219558283, test loss: 0.2547503445062276\n",
      "epoch 14366: train loss: 0.11269852123388623, test loss: 0.2547511387000052\n",
      "epoch 14367: train loss: 0.11269642071240017, test loss: 0.2547519329889695\n",
      "epoch 14368: train loss: 0.11269432039133573, test loss: 0.25475272737312354\n",
      "epoch 14369: train loss: 0.1126922202706585, test loss: 0.2547535218524525\n",
      "epoch 14370: train loss: 0.11269012035033406, test loss: 0.2547543164269019\n",
      "epoch 14371: train loss: 0.11268802063032801, test loss: 0.2547551110964657\n",
      "epoch 14372: train loss: 0.112685921110606, test loss: 0.25475590586113994\n",
      "epoch 14373: train loss: 0.11268382179113363, test loss: 0.2547567007208977\n",
      "epoch 14374: train loss: 0.11268172267187654, test loss: 0.25475749567567807\n",
      "epoch 14375: train loss: 0.11267962375280036, test loss: 0.25475829072552086\n",
      "epoch 14376: train loss: 0.1126775250338707, test loss: 0.2547590858703733\n",
      "epoch 14377: train loss: 0.11267542651505329, test loss: 0.254759881110202\n",
      "epoch 14378: train loss: 0.11267332819631375, test loss: 0.254760676445\n",
      "epoch 14379: train loss: 0.11267123007761776, test loss: 0.2547614718747619\n",
      "epoch 14380: train loss: 0.112669132158931, test loss: 0.25476226739943525\n",
      "epoch 14381: train loss: 0.11266703444021918, test loss: 0.25476306301902374\n",
      "epoch 14382: train loss: 0.11266493692144795, test loss: 0.2547638587335009\n",
      "epoch 14383: train loss: 0.11266283960258308, test loss: 0.2547646545428342\n",
      "epoch 14384: train loss: 0.11266074248359023, test loss: 0.25476545044703597\n",
      "epoch 14385: train loss: 0.11265864556443518, test loss: 0.25476624644603474\n",
      "epoch 14386: train loss: 0.1126565488450836, test loss: 0.2547670425398431\n",
      "epoch 14387: train loss: 0.11265445232550125, test loss: 0.2547678387284186\n",
      "epoch 14388: train loss: 0.1126523560056539, test loss: 0.2547686350117735\n",
      "epoch 14389: train loss: 0.11265025988550727, test loss: 0.25476943138985503\n",
      "epoch 14390: train loss: 0.11264816396502716, test loss: 0.2547702278626578\n",
      "epoch 14391: train loss: 0.11264606824417933, test loss: 0.2547710244301566\n",
      "epoch 14392: train loss: 0.11264397272292957, test loss: 0.2547718210923282\n",
      "epoch 14393: train loss: 0.11264187740124364, test loss: 0.25477261784914523\n",
      "epoch 14394: train loss: 0.11263978227908737, test loss: 0.2547734147006243\n",
      "epoch 14395: train loss: 0.11263768735642657, test loss: 0.2547742116466919\n",
      "epoch 14396: train loss: 0.11263559263322702, test loss: 0.2547750086873518\n",
      "epoch 14397: train loss: 0.11263349810945457, test loss: 0.25477580582258086\n",
      "epoch 14398: train loss: 0.11263140378507504, test loss: 0.2547766030523813\n",
      "epoch 14399: train loss: 0.11262930966005427, test loss: 0.2547774003766809\n",
      "epoch 14400: train loss: 0.11262721573435812, test loss: 0.2547781977955052\n",
      "epoch 14401: train loss: 0.11262512200795242, test loss: 0.2547789953087991\n",
      "epoch 14402: train loss: 0.11262302848080306, test loss: 0.2547797929165666\n",
      "epoch 14403: train loss: 0.11262093515287594, test loss: 0.25478059061878505\n",
      "epoch 14404: train loss: 0.11261884202413688, test loss: 0.25478138841541875\n",
      "epoch 14405: train loss: 0.11261674909455179, test loss: 0.25478218630647254\n",
      "epoch 14406: train loss: 0.11261465636408659, test loss: 0.25478298429190327\n",
      "epoch 14407: train loss: 0.11261256383270717, test loss: 0.25478378237167604\n",
      "epoch 14408: train loss: 0.11261047150037942, test loss: 0.2547845805458153\n",
      "epoch 14409: train loss: 0.1126083793670693, test loss: 0.2547853788142487\n",
      "epoch 14410: train loss: 0.11260628743274273, test loss: 0.25478617717699853\n",
      "epoch 14411: train loss: 0.11260419569736566, test loss: 0.2547869756340129\n",
      "epoch 14412: train loss: 0.112602104160904, test loss: 0.25478777418528714\n",
      "epoch 14413: train loss: 0.11260001282332376, test loss: 0.2547885728307857\n",
      "epoch 14414: train loss: 0.11259792168459086, test loss: 0.25478937157053394\n",
      "epoch 14415: train loss: 0.11259583074467126, test loss: 0.25479017040444896\n",
      "epoch 14416: train loss: 0.112593740003531, test loss: 0.2547909693325257\n",
      "epoch 14417: train loss: 0.11259164946113603, test loss: 0.25479176835476935\n",
      "epoch 14418: train loss: 0.11258955911745236, test loss: 0.25479256747115386\n",
      "epoch 14419: train loss: 0.112587468972446, test loss: 0.2547933666816291\n",
      "epoch 14420: train loss: 0.11258537902608293, test loss: 0.25479416598621585\n",
      "epoch 14421: train loss: 0.11258328927832918, test loss: 0.2547949653848537\n",
      "epoch 14422: train loss: 0.11258119972915083, test loss: 0.25479576487754596\n",
      "epoch 14423: train loss: 0.11257911037851386, test loss: 0.2547965644642498\n",
      "epoch 14424: train loss: 0.11257702122638434, test loss: 0.2547973641449797\n",
      "epoch 14425: train loss: 0.11257493227272836, test loss: 0.2547981639196829\n",
      "epoch 14426: train loss: 0.11257284351751191, test loss: 0.25479896378836214\n",
      "epoch 14427: train loss: 0.11257075496070111, test loss: 0.25479976375097685\n",
      "epoch 14428: train loss: 0.11256866660226204, test loss: 0.25480056380751087\n",
      "epoch 14429: train loss: 0.11256657844216075, test loss: 0.2548013639579396\n",
      "epoch 14430: train loss: 0.1125644904803634, test loss: 0.25480216420226925\n",
      "epoch 14431: train loss: 0.11256240271683604, test loss: 0.25480296454045537\n",
      "epoch 14432: train loss: 0.11256031515154481, test loss: 0.2548037649724748\n",
      "epoch 14433: train loss: 0.1125582277844558, test loss: 0.254804565498312\n",
      "epoch 14434: train loss: 0.11255614061553515, test loss: 0.2548053661179447\n",
      "epoch 14435: train loss: 0.11255405364474903, test loss: 0.25480616683135665\n",
      "epoch 14436: train loss: 0.11255196687206359, test loss: 0.25480696763854355\n",
      "epoch 14437: train loss: 0.11254988029744491, test loss: 0.2548077685394342\n",
      "epoch 14438: train loss: 0.11254779392085922, test loss: 0.2548085695340522\n",
      "epoch 14439: train loss: 0.11254570774227267, test loss: 0.25480937062236303\n",
      "epoch 14440: train loss: 0.11254362176165143, test loss: 0.2548101718043544\n",
      "epoch 14441: train loss: 0.1125415359789617, test loss: 0.25481097307999984\n",
      "epoch 14442: train loss: 0.11253945039416964, test loss: 0.2548117744492588\n",
      "epoch 14443: train loss: 0.11253736500724151, test loss: 0.25481257591215273\n",
      "epoch 14444: train loss: 0.11253527981814351, test loss: 0.2548133774686224\n",
      "epoch 14445: train loss: 0.1125331948268418, test loss: 0.2548141791186699\n",
      "epoch 14446: train loss: 0.11253111003330264, test loss: 0.2548149808622629\n",
      "epoch 14447: train loss: 0.11252902543749231, test loss: 0.2548157826993689\n",
      "epoch 14448: train loss: 0.112526941039377, test loss: 0.2548165846300001\n",
      "epoch 14449: train loss: 0.112524856838923, test loss: 0.2548173866541265\n",
      "epoch 14450: train loss: 0.11252277283609655, test loss: 0.25481818877170154\n",
      "epoch 14451: train loss: 0.11252068903086389, test loss: 0.2548189909827127\n",
      "epoch 14452: train loss: 0.11251860542319132, test loss: 0.2548197932871753\n",
      "epoch 14453: train loss: 0.11251652201304518, test loss: 0.2548205956850164\n",
      "epoch 14454: train loss: 0.11251443880039166, test loss: 0.2548213981762612\n",
      "epoch 14455: train loss: 0.11251235578519715, test loss: 0.25482220076085565\n",
      "epoch 14456: train loss: 0.11251027296742791, test loss: 0.25482300343880665\n",
      "epoch 14457: train loss: 0.11250819034705024, test loss: 0.25482380621006\n",
      "epoch 14458: train loss: 0.11250610792403055, test loss: 0.2548246090746232\n",
      "epoch 14459: train loss: 0.11250402569833512, test loss: 0.25482541203246983\n",
      "epoch 14460: train loss: 0.11250194366993024, test loss: 0.25482621508357717\n",
      "epoch 14461: train loss: 0.11249986183878236, test loss: 0.25482701822791215\n",
      "epoch 14462: train loss: 0.11249778020485778, test loss: 0.2548278214654709\n",
      "epoch 14463: train loss: 0.11249569876812285, test loss: 0.2548286247962196\n",
      "epoch 14464: train loss: 0.11249361752854398, test loss: 0.2548294282201634\n",
      "epoch 14465: train loss: 0.11249153648608758, test loss: 0.2548302317372404\n",
      "epoch 14466: train loss: 0.11248945564071997, test loss: 0.25483103534747586\n",
      "epoch 14467: train loss: 0.11248737499240759, test loss: 0.25483183905080525\n",
      "epoch 14468: train loss: 0.11248529454111686, test loss: 0.2548326428472375\n",
      "epoch 14469: train loss: 0.11248321428681413, test loss: 0.254833446736746\n",
      "epoch 14470: train loss: 0.11248113422946591, test loss: 0.2548342507193182\n",
      "epoch 14471: train loss: 0.11247905436903856, test loss: 0.25483505479491164\n",
      "epoch 14472: train loss: 0.11247697470549861, test loss: 0.25483585896351074\n",
      "epoch 14473: train loss: 0.11247489523881239, test loss: 0.2548366632251031\n",
      "epoch 14474: train loss: 0.11247281596894641, test loss: 0.2548374675796755\n",
      "epoch 14475: train loss: 0.11247073689586717, test loss: 0.25483827202718323\n",
      "epoch 14476: train loss: 0.11246865801954112, test loss: 0.254839076567633\n",
      "epoch 14477: train loss: 0.11246657933993472, test loss: 0.25483988120098217\n",
      "epoch 14478: train loss: 0.11246450085701444, test loss: 0.25484068592723547\n",
      "epoch 14479: train loss: 0.1124624225707468, test loss: 0.25484149074635076\n",
      "epoch 14480: train loss: 0.11246034448109833, test loss: 0.25484229565829486\n",
      "epoch 14481: train loss: 0.11245826658803555, test loss: 0.25484310066308324\n",
      "epoch 14482: train loss: 0.11245618889152492, test loss: 0.2548439057606924\n",
      "epoch 14483: train loss: 0.112454111391533, test loss: 0.2548447109510509\n",
      "epoch 14484: train loss: 0.11245203408802634, test loss: 0.2548455162342017\n",
      "epoch 14485: train loss: 0.11244995698097146, test loss: 0.2548463216100951\n",
      "epoch 14486: train loss: 0.11244788007033496, test loss: 0.2548471270787045\n",
      "epoch 14487: train loss: 0.11244580335608333, test loss: 0.25484793264002853\n",
      "epoch 14488: train loss: 0.1124437268381832, test loss: 0.2548487382940135\n",
      "epoch 14489: train loss: 0.11244165051660109, test loss: 0.2548495440406848\n",
      "epoch 14490: train loss: 0.11243957439130364, test loss: 0.25485034987999017\n",
      "epoch 14491: train loss: 0.11243749846225742, test loss: 0.2548511558119166\n",
      "epoch 14492: train loss: 0.11243542272942907, test loss: 0.25485196183644926\n",
      "epoch 14493: train loss: 0.11243334719278511, test loss: 0.2548527679535563\n",
      "epoch 14494: train loss: 0.11243127185229224, test loss: 0.25485357416322496\n",
      "epoch 14495: train loss: 0.11242919670791704, test loss: 0.2548543804654392\n",
      "epoch 14496: train loss: 0.1124271217596262, test loss: 0.2548551868601488\n",
      "epoch 14497: train loss: 0.1124250470073863, test loss: 0.25485599334737796\n",
      "epoch 14498: train loss: 0.112422972451164, test loss: 0.2548567999270951\n",
      "epoch 14499: train loss: 0.11242089809092598, test loss: 0.2548576065992465\n",
      "epoch 14500: train loss: 0.1124188239266389, test loss: 0.2548584133638587\n",
      "epoch 14501: train loss: 0.11241674995826945, test loss: 0.2548592202208789\n",
      "epoch 14502: train loss: 0.11241467618578425, test loss: 0.25486002717029344\n",
      "epoch 14503: train loss: 0.11241260260915009, test loss: 0.2548608342120893\n",
      "epoch 14504: train loss: 0.11241052922833356, test loss: 0.25486164134623357\n",
      "epoch 14505: train loss: 0.11240845604330145, test loss: 0.2548624485727123\n",
      "epoch 14506: train loss: 0.11240638305402044, test loss: 0.2548632558915142\n",
      "epoch 14507: train loss: 0.11240431026045726, test loss: 0.2548640633025938\n",
      "epoch 14508: train loss: 0.11240223766257863, test loss: 0.2548648708059703\n",
      "epoch 14509: train loss: 0.11240016526035129, test loss: 0.25486567840158847\n",
      "epoch 14510: train loss: 0.11239809305374199, test loss: 0.25486648608943563\n",
      "epoch 14511: train loss: 0.1123960210427175, test loss: 0.25486729386950085\n",
      "epoch 14512: train loss: 0.11239394922724454, test loss: 0.254868101741758\n",
      "epoch 14513: train loss: 0.11239187760728991, test loss: 0.25486890970620574\n",
      "epoch 14514: train loss: 0.11238980618282042, test loss: 0.2548697177627919\n",
      "epoch 14515: train loss: 0.1123877349538028, test loss: 0.25487052591149095\n",
      "epoch 14516: train loss: 0.11238566392020388, test loss: 0.25487133415232155\n",
      "epoch 14517: train loss: 0.11238359308199045, test loss: 0.2548721424852514\n",
      "epoch 14518: train loss: 0.11238152243912931, test loss: 0.25487295091023526\n",
      "epoch 14519: train loss: 0.1123794519915873, test loss: 0.254873759427263\n",
      "epoch 14520: train loss: 0.11237738173933125, test loss: 0.2548745680363493\n",
      "epoch 14521: train loss: 0.11237531168232795, test loss: 0.25487537673742205\n",
      "epoch 14522: train loss: 0.1123732418205443, test loss: 0.25487618553047897\n",
      "epoch 14523: train loss: 0.11237117215394711, test loss: 0.2548769944155256\n",
      "epoch 14524: train loss: 0.11236910268250327, test loss: 0.25487780339251087\n",
      "epoch 14525: train loss: 0.11236703340617962, test loss: 0.25487861246142074\n",
      "epoch 14526: train loss: 0.11236496432494306, test loss: 0.2548794216222426\n",
      "epoch 14527: train loss: 0.11236289543876044, test loss: 0.2548802308749442\n",
      "epoch 14528: train loss: 0.11236082674759866, test loss: 0.2548810402195327\n",
      "epoch 14529: train loss: 0.11235875825142466, test loss: 0.254881849655953\n",
      "epoch 14530: train loss: 0.11235668995020529, test loss: 0.2548826591841963\n",
      "epoch 14531: train loss: 0.1123546218439075, test loss: 0.25488346880426505\n",
      "epoch 14532: train loss: 0.11235255393249818, test loss: 0.2548842785160908\n",
      "epoch 14533: train loss: 0.1123504862159443, test loss: 0.2548850883197093\n",
      "epoch 14534: train loss: 0.11234841869421278, test loss: 0.2548858982150661\n",
      "epoch 14535: train loss: 0.11234635136727057, test loss: 0.2548867082021409\n",
      "epoch 14536: train loss: 0.11234428423508462, test loss: 0.2548875182809099\n",
      "epoch 14537: train loss: 0.11234221729762187, test loss: 0.254888328451399\n",
      "epoch 14538: train loss: 0.11234015055484933, test loss: 0.2548891387135285\n",
      "epoch 14539: train loss: 0.11233808400673398, test loss: 0.25488994906731394\n",
      "epoch 14540: train loss: 0.11233601765324278, test loss: 0.25489075951270235\n",
      "epoch 14541: train loss: 0.1123339514943427, test loss: 0.25489157004972257\n",
      "epoch 14542: train loss: 0.1123318855300008, test loss: 0.25489238067830977\n",
      "epoch 14543: train loss: 0.11232981976018404, test loss: 0.254893191398464\n",
      "epoch 14544: train loss: 0.11232775418485952, test loss: 0.2548940022101502\n",
      "epoch 14545: train loss: 0.11232568880399416, test loss: 0.2548948131133668\n",
      "epoch 14546: train loss: 0.11232362361755506, test loss: 0.2548956241081012\n",
      "epoch 14547: train loss: 0.11232155862550924, test loss: 0.25489643519430144\n",
      "epoch 14548: train loss: 0.11231949382782376, test loss: 0.2548972463719742\n",
      "epoch 14549: train loss: 0.11231742922446566, test loss: 0.25489805764108625\n",
      "epoch 14550: train loss: 0.11231536481540204, test loss: 0.25489886900160674\n",
      "epoch 14551: train loss: 0.11231330060059994, test loss: 0.25489968045354217\n",
      "epoch 14552: train loss: 0.11231123658002645, test loss: 0.25490049199686104\n",
      "epoch 14553: train loss: 0.11230917275364867, test loss: 0.2549013036315491\n",
      "epoch 14554: train loss: 0.11230710912143368, test loss: 0.25490211535756413\n",
      "epoch 14555: train loss: 0.11230504568334859, test loss: 0.25490292717489604\n",
      "epoch 14556: train loss: 0.11230298243936054, test loss: 0.25490373908352937\n",
      "epoch 14557: train loss: 0.11230091938943662, test loss: 0.254904551083474\n",
      "epoch 14558: train loss: 0.11229885653354398, test loss: 0.25490536317463663\n",
      "epoch 14559: train loss: 0.11229679387164973, test loss: 0.2549061753570635\n",
      "epoch 14560: train loss: 0.11229473140372104, test loss: 0.2549069876307044\n",
      "epoch 14561: train loss: 0.11229266912972506, test loss: 0.2549077999955562\n",
      "epoch 14562: train loss: 0.1122906070496289, test loss: 0.254908612451586\n",
      "epoch 14563: train loss: 0.11228854516339982, test loss: 0.25490942499876246\n",
      "epoch 14564: train loss: 0.11228648347100494, test loss: 0.25491023763709236\n",
      "epoch 14565: train loss: 0.11228442197241145, test loss: 0.2549110503665434\n",
      "epoch 14566: train loss: 0.11228236066758654, test loss: 0.25491186318708325\n",
      "epoch 14567: train loss: 0.11228029955649742, test loss: 0.25491267609870083\n",
      "epoch 14568: train loss: 0.1122782386391113, test loss: 0.2549134891013922\n",
      "epoch 14569: train loss: 0.1122761779153954, test loss: 0.2549143021951062\n",
      "epoch 14570: train loss: 0.1122741173853169, test loss: 0.2549151153798601\n",
      "epoch 14571: train loss: 0.11227205704884309, test loss: 0.254915928655592\n",
      "epoch 14572: train loss: 0.11226999690594118, test loss: 0.25491674202231906\n",
      "epoch 14573: train loss: 0.11226793695657843, test loss: 0.2549175554800008\n",
      "epoch 14574: train loss: 0.11226587720072206, test loss: 0.2549183690286227\n",
      "epoch 14575: train loss: 0.1122638176383394, test loss: 0.25491918266816355\n",
      "epoch 14576: train loss: 0.11226175826939766, test loss: 0.25491999639859114\n",
      "epoch 14577: train loss: 0.11225969909386414, test loss: 0.254920810219914\n",
      "epoch 14578: train loss: 0.11225764011170615, test loss: 0.254921624132079\n",
      "epoch 14579: train loss: 0.11225558132289094, test loss: 0.2549224381351031\n",
      "epoch 14580: train loss: 0.11225352272738584, test loss: 0.25492325222893747\n",
      "epoch 14581: train loss: 0.11225146432515816, test loss: 0.2549240664135573\n",
      "epoch 14582: train loss: 0.11224940611617519, test loss: 0.254924880688981\n",
      "epoch 14583: train loss: 0.11224734810040435, test loss: 0.2549256950551473\n",
      "epoch 14584: train loss: 0.11224529027781284, test loss: 0.25492650951205326\n",
      "epoch 14585: train loss: 0.11224323264836808, test loss: 0.2549273240596868\n",
      "epoch 14586: train loss: 0.11224117521203741, test loss: 0.2549281386980069\n",
      "epoch 14587: train loss: 0.11223911796878816, test loss: 0.2549289534270013\n",
      "epoch 14588: train loss: 0.11223706091858776, test loss: 0.2549297682466776\n",
      "epoch 14589: train loss: 0.11223500406140353, test loss: 0.25493058315697215\n",
      "epoch 14590: train loss: 0.11223294739720284, test loss: 0.25493139815789645\n",
      "epoch 14591: train loss: 0.1122308909259531, test loss: 0.2549322132494056\n",
      "epoch 14592: train loss: 0.11222883464762173, test loss: 0.25493302843150006\n",
      "epoch 14593: train loss: 0.11222677856217611, test loss: 0.25493384370416505\n",
      "epoch 14594: train loss: 0.11222472266958365, test loss: 0.25493465906737117\n",
      "epoch 14595: train loss: 0.11222266696981176, test loss: 0.25493547452107557\n",
      "epoch 14596: train loss: 0.11222061146282789, test loss: 0.25493629006528634\n",
      "epoch 14597: train loss: 0.11221855614859948, test loss: 0.2549371056999912\n",
      "epoch 14598: train loss: 0.11221650102709395, test loss: 0.2549379214251386\n",
      "epoch 14599: train loss: 0.11221444609827877, test loss: 0.254938737240727\n",
      "epoch 14600: train loss: 0.11221239136212137, test loss: 0.2549395531467455\n",
      "epoch 14601: train loss: 0.11221033681858927, test loss: 0.2549403691431398\n",
      "epoch 14602: train loss: 0.11220828246764991, test loss: 0.2549411852299395\n",
      "epoch 14603: train loss: 0.11220622830927077, test loss: 0.2549420014070736\n",
      "epoch 14604: train loss: 0.11220417434341934, test loss: 0.2549428176745586\n",
      "epoch 14605: train loss: 0.11220212057006314, test loss: 0.2549436340323838\n",
      "epoch 14606: train loss: 0.11220006698916965, test loss: 0.2549444504804772\n",
      "epoch 14607: train loss: 0.1121980136007064, test loss: 0.2549452670188667\n",
      "epoch 14608: train loss: 0.1121959604046409, test loss: 0.25494608364751203\n",
      "epoch 14609: train loss: 0.1121939074009407, test loss: 0.2549469003663997\n",
      "epoch 14610: train loss: 0.1121918545895733, test loss: 0.2549477171754996\n",
      "epoch 14611: train loss: 0.1121898019705063, test loss: 0.25494853407481755\n",
      "epoch 14612: train loss: 0.11218774954370721, test loss: 0.2549493510643041\n",
      "epoch 14613: train loss: 0.11218569730914357, test loss: 0.2549501681439573\n",
      "epoch 14614: train loss: 0.112183645266783, test loss: 0.25495098531374544\n",
      "epoch 14615: train loss: 0.11218159341659305, test loss: 0.25495180257364564\n",
      "epoch 14616: train loss: 0.11217954175854132, test loss: 0.2549526199236482\n",
      "epoch 14617: train loss: 0.1121774902925954, test loss: 0.25495343736375026\n",
      "epoch 14618: train loss: 0.11217543901872286, test loss: 0.25495425489390977\n",
      "epoch 14619: train loss: 0.11217338793689136, test loss: 0.2549550725140875\n",
      "epoch 14620: train loss: 0.11217133704706846, test loss: 0.25495589022430953\n",
      "epoch 14621: train loss: 0.1121692863492218, test loss: 0.2549567080245249\n",
      "epoch 14622: train loss: 0.112167235843319, test loss: 0.2549575259147218\n",
      "epoch 14623: train loss: 0.11216518552932774, test loss: 0.2549583438948902\n",
      "epoch 14624: train loss: 0.11216313540721563, test loss: 0.2549591619649863\n",
      "epoch 14625: train loss: 0.11216108547695036, test loss: 0.2549599801250215\n",
      "epoch 14626: train loss: 0.11215903573849953, test loss: 0.2549607983749413\n",
      "epoch 14627: train loss: 0.11215698619183087, test loss: 0.25496161671474693\n",
      "epoch 14628: train loss: 0.112154936836912, test loss: 0.2549624351444248\n",
      "epoch 14629: train loss: 0.11215288767371064, test loss: 0.2549632536639439\n",
      "epoch 14630: train loss: 0.1121508387021945, test loss: 0.25496407227329365\n",
      "epoch 14631: train loss: 0.11214878992233125, test loss: 0.25496489097243147\n",
      "epoch 14632: train loss: 0.1121467413340886, test loss: 0.2549657097613578\n",
      "epoch 14633: train loss: 0.11214469293743427, test loss: 0.25496652864005015\n",
      "epoch 14634: train loss: 0.11214264473233598, test loss: 0.25496734760849543\n",
      "epoch 14635: train loss: 0.11214059671876146, test loss: 0.2549681666666557\n",
      "epoch 14636: train loss: 0.11213854889667849, test loss: 0.25496898581451655\n",
      "epoch 14637: train loss: 0.11213650126605472, test loss: 0.2549698050520593\n",
      "epoch 14638: train loss: 0.11213445382685804, test loss: 0.2549706243792901\n",
      "epoch 14639: train loss: 0.1121324065790561, test loss: 0.2549714437961395\n",
      "epoch 14640: train loss: 0.1121303595226167, test loss: 0.25497226330262557\n",
      "epoch 14641: train loss: 0.11212831265750763, test loss: 0.2549730828987163\n",
      "epoch 14642: train loss: 0.11212626598369667, test loss: 0.25497390258440134\n",
      "epoch 14643: train loss: 0.1121242195011516, test loss: 0.25497472235964985\n",
      "epoch 14644: train loss: 0.11212217320984022, test loss: 0.25497554222443986\n",
      "epoch 14645: train loss: 0.1121201271097304, test loss: 0.2549763621787595\n",
      "epoch 14646: train loss: 0.11211808120078987, test loss: 0.2549771822225979\n",
      "epoch 14647: train loss: 0.11211603548298649, test loss: 0.25497800235591533\n",
      "epoch 14648: train loss: 0.11211398995628809, test loss: 0.2549788225786993\n",
      "epoch 14649: train loss: 0.11211194462066253, test loss: 0.25497964289093916\n",
      "epoch 14650: train loss: 0.11210989947607762, test loss: 0.25498046329259233\n",
      "epoch 14651: train loss: 0.11210785452250124, test loss: 0.2549812837836593\n",
      "epoch 14652: train loss: 0.11210580975990124, test loss: 0.25498210436411917\n",
      "epoch 14653: train loss: 0.1121037651882455, test loss: 0.2549829250339592\n",
      "epoch 14654: train loss: 0.11210172080750187, test loss: 0.2549837457931396\n",
      "epoch 14655: train loss: 0.11209967661763827, test loss: 0.2549845666416386\n",
      "epoch 14656: train loss: 0.11209763261862257, test loss: 0.25498538757946565\n",
      "epoch 14657: train loss: 0.11209558881042268, test loss: 0.2549862086065693\n",
      "epoch 14658: train loss: 0.11209354519300652, test loss: 0.2549870297229474\n",
      "epoch 14659: train loss: 0.11209150176634196, test loss: 0.25498785092859194\n",
      "epoch 14660: train loss: 0.11208945853039698, test loss: 0.25498867222346866\n",
      "epoch 14661: train loss: 0.1120874154851395, test loss: 0.2549894936075493\n",
      "epoch 14662: train loss: 0.11208537263053742, test loss: 0.25499031508081016\n",
      "epoch 14663: train loss: 0.11208332996655869, test loss: 0.25499113664326234\n",
      "epoch 14664: train loss: 0.11208128749317132, test loss: 0.25499195829487414\n",
      "epoch 14665: train loss: 0.11207924521034324, test loss: 0.2549927800356043\n",
      "epoch 14666: train loss: 0.11207720311804241, test loss: 0.2549936018654318\n",
      "epoch 14667: train loss: 0.11207516121623681, test loss: 0.25499442378438625\n",
      "epoch 14668: train loss: 0.11207311950489444, test loss: 0.25499524579239724\n",
      "epoch 14669: train loss: 0.11207107798398327, test loss: 0.25499606788949314\n",
      "epoch 14670: train loss: 0.1120690366534713, test loss: 0.2549968900756025\n",
      "epoch 14671: train loss: 0.11206699551332658, test loss: 0.25499771235071433\n",
      "epoch 14672: train loss: 0.11206495456351705, test loss: 0.25499853471483985\n",
      "epoch 14673: train loss: 0.11206291380401082, test loss: 0.254999357167946\n",
      "epoch 14674: train loss: 0.11206087323477584, test loss: 0.2550001797100026\n",
      "epoch 14675: train loss: 0.11205883285578022, test loss: 0.25500100234099987\n",
      "epoch 14676: train loss: 0.11205679266699196, test loss: 0.2550018250609148\n",
      "epoch 14677: train loss: 0.11205475266837911, test loss: 0.25500264786972937\n",
      "epoch 14678: train loss: 0.11205271285990977, test loss: 0.25500347076743074\n",
      "epoch 14679: train loss: 0.11205067324155195, test loss: 0.2550042937539683\n",
      "epoch 14680: train loss: 0.11204863381327378, test loss: 0.25500511682936233\n",
      "epoch 14681: train loss: 0.11204659457504333, test loss: 0.2550059399935809\n",
      "epoch 14682: train loss: 0.11204455552682868, test loss: 0.2550067632465845\n",
      "epoch 14683: train loss: 0.11204251666859794, test loss: 0.2550075865884016\n",
      "epoch 14684: train loss: 0.11204047800031917, test loss: 0.25500841001896185\n",
      "epoch 14685: train loss: 0.11203843952196058, test loss: 0.25500923353826427\n",
      "epoch 14686: train loss: 0.11203640123349022, test loss: 0.2550100571462777\n",
      "epoch 14687: train loss: 0.11203436313487623, test loss: 0.25501088084301216\n",
      "epoch 14688: train loss: 0.11203232522608676, test loss: 0.2550117046284272\n",
      "epoch 14689: train loss: 0.11203028750708995, test loss: 0.25501252850250206\n",
      "epoch 14690: train loss: 0.11202824997785395, test loss: 0.2550133524652254\n",
      "epoch 14691: train loss: 0.11202621263834693, test loss: 0.25501417651658803\n",
      "epoch 14692: train loss: 0.11202417548853706, test loss: 0.2550150006565379\n",
      "epoch 14693: train loss: 0.11202213852839246, test loss: 0.2550158248850742\n",
      "epoch 14694: train loss: 0.1120201017578814, test loss: 0.2550166492021986\n",
      "epoch 14695: train loss: 0.112018065176972, test loss: 0.2550174736078587\n",
      "epoch 14696: train loss: 0.1120160287856325, test loss: 0.2550182981020433\n",
      "epoch 14697: train loss: 0.11201399258383109, test loss: 0.25501912268474336\n",
      "epoch 14698: train loss: 0.11201195657153598, test loss: 0.2550199473559186\n",
      "epoch 14699: train loss: 0.1120099207487154, test loss: 0.25502077211556684\n",
      "epoch 14700: train loss: 0.11200788511533759, test loss: 0.2550215969636796\n",
      "epoch 14701: train loss: 0.11200584967137071, test loss: 0.2550224219002252\n",
      "epoch 14702: train loss: 0.1120038144167831, test loss: 0.2550232469251622\n",
      "epoch 14703: train loss: 0.11200177935154298, test loss: 0.25502407203851246\n",
      "epoch 14704: train loss: 0.11199974447561858, test loss: 0.2550248972402246\n",
      "epoch 14705: train loss: 0.11199770978897822, test loss: 0.25502572253028744\n",
      "epoch 14706: train loss: 0.11199567529159012, test loss: 0.25502654790869067\n",
      "epoch 14707: train loss: 0.11199364098342258, test loss: 0.2550273733754152\n",
      "epoch 14708: train loss: 0.11199160686444389, test loss: 0.2550281989304186\n",
      "epoch 14709: train loss: 0.11198957293462236, test loss: 0.2550290245737119\n",
      "epoch 14710: train loss: 0.11198753919392626, test loss: 0.2550298503052451\n",
      "epoch 14711: train loss: 0.11198550564232393, test loss: 0.25503067612503666\n",
      "epoch 14712: train loss: 0.11198347227978368, test loss: 0.25503150203302705\n",
      "epoch 14713: train loss: 0.11198143910627385, test loss: 0.2550323280292257\n",
      "epoch 14714: train loss: 0.11197940612176276, test loss: 0.2550331541136025\n",
      "epoch 14715: train loss: 0.11197737332621874, test loss: 0.25503398028613744\n",
      "epoch 14716: train loss: 0.11197534071961014, test loss: 0.25503480654680855\n",
      "epoch 14717: train loss: 0.11197330830190536, test loss: 0.25503563289560766\n",
      "epoch 14718: train loss: 0.1119712760730727, test loss: 0.2550364593325035\n",
      "epoch 14719: train loss: 0.1119692440330806, test loss: 0.255037285857486\n",
      "epoch 14720: train loss: 0.11196721218189737, test loss: 0.25503811247051433\n",
      "epoch 14721: train loss: 0.11196518051949145, test loss: 0.25503893917160964\n",
      "epoch 14722: train loss: 0.11196314904583121, test loss: 0.25503976596072025\n",
      "epoch 14723: train loss: 0.11196111776088506, test loss: 0.25504059283782676\n",
      "epoch 14724: train loss: 0.1119590866646214, test loss: 0.25504141980293915\n",
      "epoch 14725: train loss: 0.11195705575700865, test loss: 0.2550422468559865\n",
      "epoch 14726: train loss: 0.11195502503801524, test loss: 0.2550430739970097\n",
      "epoch 14727: train loss: 0.1119529945076096, test loss: 0.255043901225948\n",
      "epoch 14728: train loss: 0.11195096416576016, test loss: 0.2550447285427912\n",
      "epoch 14729: train loss: 0.11194893401243537, test loss: 0.2550455559475291\n",
      "epoch 14730: train loss: 0.11194690404760371, test loss: 0.25504638344013236\n",
      "epoch 14731: train loss: 0.11194487427123363, test loss: 0.2550472110205903\n",
      "epoch 14732: train loss: 0.11194284468329353, test loss: 0.25504803868887205\n",
      "epoch 14733: train loss: 0.11194081528375202, test loss: 0.25504886644497965\n",
      "epoch 14734: train loss: 0.11193878607257748, test loss: 0.25504969428886154\n",
      "epoch 14735: train loss: 0.11193675704973842, test loss: 0.25505052222052726\n",
      "epoch 14736: train loss: 0.11193472821520338, test loss: 0.25505135023993797\n",
      "epoch 14737: train loss: 0.11193269956894081, test loss: 0.2550521783470927\n",
      "epoch 14738: train loss: 0.11193067111091928, test loss: 0.2550530065419633\n",
      "epoch 14739: train loss: 0.11192864284110726, test loss: 0.2550538348245263\n",
      "epoch 14740: train loss: 0.11192661475947333, test loss: 0.25505466319477543\n",
      "epoch 14741: train loss: 0.11192458686598598, test loss: 0.25505549165267943\n",
      "epoch 14742: train loss: 0.11192255916061378, test loss: 0.2550563201982074\n",
      "epoch 14743: train loss: 0.11192053164332527, test loss: 0.25505714883136926\n",
      "epoch 14744: train loss: 0.11191850431408902, test loss: 0.25505797755211695\n",
      "epoch 14745: train loss: 0.1119164771728736, test loss: 0.25505880636044914\n",
      "epoch 14746: train loss: 0.1119144502196476, test loss: 0.25505963525635805\n",
      "epoch 14747: train loss: 0.11191242345437954, test loss: 0.2550604642397804\n",
      "epoch 14748: train loss: 0.11191039687703806, test loss: 0.25506129331073774\n",
      "epoch 14749: train loss: 0.11190837048759175, test loss: 0.25506212246922133\n",
      "epoch 14750: train loss: 0.1119063442860092, test loss: 0.25506295171516075\n",
      "epoch 14751: train loss: 0.11190431827225904, test loss: 0.2550637810485757\n",
      "epoch 14752: train loss: 0.11190229244630988, test loss: 0.25506461046944656\n",
      "epoch 14753: train loss: 0.11190026680813032, test loss: 0.25506543997772313\n",
      "epoch 14754: train loss: 0.11189824135768904, test loss: 0.255066269573437\n",
      "epoch 14755: train loss: 0.11189621609495465, test loss: 0.2550670992565068\n",
      "epoch 14756: train loss: 0.11189419101989583, test loss: 0.2550679290269731\n",
      "epoch 14757: train loss: 0.1118921661324812, test loss: 0.25506875888476777\n",
      "epoch 14758: train loss: 0.11189014143267942, test loss: 0.2550695888299094\n",
      "epoch 14759: train loss: 0.1118881169204592, test loss: 0.25507041886235776\n",
      "epoch 14760: train loss: 0.1118860925957892, test loss: 0.2550712489820854\n",
      "epoch 14761: train loss: 0.1118840684586381, test loss: 0.25507207918912017\n",
      "epoch 14762: train loss: 0.11188204450897457, test loss: 0.2550729094833843\n",
      "epoch 14763: train loss: 0.11188002074676735, test loss: 0.25507373986487664\n",
      "epoch 14764: train loss: 0.11187799717198516, test loss: 0.25507457033360903\n",
      "epoch 14765: train loss: 0.11187597378459667, test loss: 0.25507540088952085\n",
      "epoch 14766: train loss: 0.11187395058457064, test loss: 0.25507623153261166\n",
      "epoch 14767: train loss: 0.11187192757187578, test loss: 0.2550770622628535\n",
      "epoch 14768: train loss: 0.11186990474648083, test loss: 0.2550778930802577\n",
      "epoch 14769: train loss: 0.11186788210835452, test loss: 0.2550787239847605\n",
      "epoch 14770: train loss: 0.11186585965746562, test loss: 0.25507955497638674\n",
      "epoch 14771: train loss: 0.11186383739378289, test loss: 0.2550803860550838\n",
      "epoch 14772: train loss: 0.11186181531727513, test loss: 0.25508121722083466\n",
      "epoch 14773: train loss: 0.11185979342791107, test loss: 0.25508204847363786\n",
      "epoch 14774: train loss: 0.11185777172565949, test loss: 0.25508287981348443\n",
      "epoch 14775: train loss: 0.11185575021048919, test loss: 0.25508371124030466\n",
      "epoch 14776: train loss: 0.11185372888236898, test loss: 0.25508454275413917\n",
      "epoch 14777: train loss: 0.11185170774126768, test loss: 0.2550853743549288\n",
      "epoch 14778: train loss: 0.11184968678715404, test loss: 0.255086206042654\n",
      "epoch 14779: train loss: 0.11184766601999695, test loss: 0.2550870378173258\n",
      "epoch 14780: train loss: 0.11184564543976518, test loss: 0.25508786967891267\n",
      "epoch 14781: train loss: 0.1118436250464276, test loss: 0.2550887016273676\n",
      "epoch 14782: train loss: 0.11184160483995303, test loss: 0.25508953366272036\n",
      "epoch 14783: train loss: 0.11183958482031034, test loss: 0.25509036578492134\n",
      "epoch 14784: train loss: 0.11183756498746839, test loss: 0.25509119799395136\n",
      "epoch 14785: train loss: 0.11183554534139598, test loss: 0.2550920302897904\n",
      "epoch 14786: train loss: 0.11183352588206205, test loss: 0.2550928626724296\n",
      "epoch 14787: train loss: 0.11183150660943547, test loss: 0.2550936951418489\n",
      "epoch 14788: train loss: 0.11182948752348508, test loss: 0.2550945276980303\n",
      "epoch 14789: train loss: 0.11182746862417985, test loss: 0.25509536034094416\n",
      "epoch 14790: train loss: 0.11182544991148861, test loss: 0.2550961930705908\n",
      "epoch 14791: train loss: 0.1118234313853803, test loss: 0.2550970258869205\n",
      "epoch 14792: train loss: 0.11182141304582383, test loss: 0.25509785878993485\n",
      "epoch 14793: train loss: 0.11181939489278812, test loss: 0.2550986917796333\n",
      "epoch 14794: train loss: 0.1118173769262421, test loss: 0.2550995248559478\n",
      "epoch 14795: train loss: 0.11181535914615472, test loss: 0.2551003580188982\n",
      "epoch 14796: train loss: 0.11181334155249491, test loss: 0.255101191268477\n",
      "epoch 14797: train loss: 0.11181132414523162, test loss: 0.2551020246046027\n",
      "epoch 14798: train loss: 0.11180930692433382, test loss: 0.2551028580273374\n",
      "epoch 14799: train loss: 0.11180728988977048, test loss: 0.25510369153659146\n",
      "epoch 14800: train loss: 0.11180527304151054, test loss: 0.255104525132386\n",
      "epoch 14801: train loss: 0.11180325637952304, test loss: 0.25510535881470175\n",
      "epoch 14802: train loss: 0.11180123990377691, test loss: 0.2551061925834782\n",
      "epoch 14803: train loss: 0.11179922361424116, test loss: 0.25510702643876937\n",
      "epoch 14804: train loss: 0.11179720751088483, test loss: 0.2551078603804826\n",
      "epoch 14805: train loss: 0.11179519159367692, test loss: 0.25510869440866174\n",
      "epoch 14806: train loss: 0.11179317586258641, test loss: 0.2551095285232248\n",
      "epoch 14807: train loss: 0.11179116031758234, test loss: 0.25511036272419507\n",
      "epoch 14808: train loss: 0.11178914495863378, test loss: 0.25511119701156254\n",
      "epoch 14809: train loss: 0.11178712978570973, test loss: 0.25511203138527827\n",
      "epoch 14810: train loss: 0.11178511479877924, test loss: 0.2551128658453326\n",
      "epoch 14811: train loss: 0.11178309999781139, test loss: 0.25511370039170733\n",
      "epoch 14812: train loss: 0.1117810853827752, test loss: 0.2551145350243926\n",
      "epoch 14813: train loss: 0.11177907095363979, test loss: 0.2551153697433614\n",
      "epoch 14814: train loss: 0.1117770567103742, test loss: 0.25511620454859146\n",
      "epoch 14815: train loss: 0.11177504265294753, test loss: 0.25511703944006614\n",
      "epoch 14816: train loss: 0.11177302878132886, test loss: 0.2551178744177658\n",
      "epoch 14817: train loss: 0.11177101509548734, test loss: 0.2551187094816805\n",
      "epoch 14818: train loss: 0.11176900159539199, test loss: 0.25511954463177355\n",
      "epoch 14819: train loss: 0.11176698828101195, test loss: 0.2551203798680433\n",
      "epoch 14820: train loss: 0.1117649751523164, test loss: 0.25512121519046355\n",
      "epoch 14821: train loss: 0.11176296220927438, test loss: 0.2551220505990336\n",
      "epoch 14822: train loss: 0.1117609494518551, test loss: 0.2551228860937056\n",
      "epoch 14823: train loss: 0.11175893688002767, test loss: 0.2551237216744789\n",
      "epoch 14824: train loss: 0.11175692449376122, test loss: 0.25512455734131645\n",
      "epoch 14825: train loss: 0.11175491229302492, test loss: 0.2551253930942188\n",
      "epoch 14826: train loss: 0.11175290027778796, test loss: 0.2551262289331462\n",
      "epoch 14827: train loss: 0.11175088844801948, test loss: 0.2551270648581111\n",
      "epoch 14828: train loss: 0.11174887680368868, test loss: 0.25512790086907383\n",
      "epoch 14829: train loss: 0.11174686534476473, test loss: 0.2551287369660267\n",
      "epoch 14830: train loss: 0.11174485407121687, test loss: 0.2551295731489287\n",
      "epoch 14831: train loss: 0.11174284298301423, test loss: 0.25513040941778287\n",
      "epoch 14832: train loss: 0.11174083208012606, test loss: 0.2551312457725609\n",
      "epoch 14833: train loss: 0.11173882136252153, test loss: 0.25513208221323164\n",
      "epoch 14834: train loss: 0.11173681083016994, test loss: 0.2551329187398095\n",
      "epoch 14835: train loss: 0.11173480048304046, test loss: 0.2551337553522627\n",
      "epoch 14836: train loss: 0.11173279032110234, test loss: 0.25513459205054395\n",
      "epoch 14837: train loss: 0.11173078034432482, test loss: 0.2551354288346652\n",
      "epoch 14838: train loss: 0.11172877055267719, test loss: 0.25513626570460624\n",
      "epoch 14839: train loss: 0.11172676094612864, test loss: 0.25513710266032885\n",
      "epoch 14840: train loss: 0.11172475152464852, test loss: 0.2551379397018458\n",
      "epoch 14841: train loss: 0.11172274228820601, test loss: 0.2551387768290963\n",
      "epoch 14842: train loss: 0.11172073323677047, test loss: 0.25513961404210234\n",
      "epoch 14843: train loss: 0.11171872437031116, test loss: 0.2551404513408363\n",
      "epoch 14844: train loss: 0.11171671568879736, test loss: 0.25514128872523756\n",
      "epoch 14845: train loss: 0.11171470719219836, test loss: 0.25514212619534976\n",
      "epoch 14846: train loss: 0.11171269888048353, test loss: 0.25514296375112366\n",
      "epoch 14847: train loss: 0.11171069075362214, test loss: 0.255143801392509\n",
      "epoch 14848: train loss: 0.11170868281158353, test loss: 0.2551446391195597\n",
      "epoch 14849: train loss: 0.111706675054337, test loss: 0.25514547693219514\n",
      "epoch 14850: train loss: 0.11170466748185195, test loss: 0.2551463148304178\n",
      "epoch 14851: train loss: 0.11170266009409766, test loss: 0.25514715281421957\n",
      "epoch 14852: train loss: 0.1117006528910435, test loss: 0.255147990883581\n",
      "epoch 14853: train loss: 0.11169864587265887, test loss: 0.25514882903844277\n",
      "epoch 14854: train loss: 0.1116966390389131, test loss: 0.2551496672788484\n",
      "epoch 14855: train loss: 0.11169463238977557, test loss: 0.2551505056047275\n",
      "epoch 14856: train loss: 0.1116926259252157, test loss: 0.25515134401608414\n",
      "epoch 14857: train loss: 0.11169061964520281, test loss: 0.25515218251291766\n",
      "epoch 14858: train loss: 0.11168861354970634, test loss: 0.25515302109518057\n",
      "epoch 14859: train loss: 0.1116866076386957, test loss: 0.25515385976285265\n",
      "epoch 14860: train loss: 0.11168460191214026, test loss: 0.2551546985159173\n",
      "epoch 14861: train loss: 0.11168259637000949, test loss: 0.2551555373543767\n",
      "epoch 14862: train loss: 0.11168059101227276, test loss: 0.2551563762782112\n",
      "epoch 14863: train loss: 0.11167858583889956, test loss: 0.255157215287371\n",
      "epoch 14864: train loss: 0.11167658084985929, test loss: 0.2551580543818611\n",
      "epoch 14865: train loss: 0.1116745760451214, test loss: 0.2551588935616607\n",
      "epoch 14866: train loss: 0.11167257142465538, test loss: 0.25515973282675286\n",
      "epoch 14867: train loss: 0.11167056698843063, test loss: 0.25516057217711796\n",
      "epoch 14868: train loss: 0.11166856273641665, test loss: 0.25516141161272876\n",
      "epoch 14869: train loss: 0.11166655866858292, test loss: 0.2551622511335656\n",
      "epoch 14870: train loss: 0.11166455478489894, test loss: 0.25516309073963134\n",
      "epoch 14871: train loss: 0.11166255108533417, test loss: 0.2551639304308775\n",
      "epoch 14872: train loss: 0.11166054756985812, test loss: 0.25516477020730505\n",
      "epoch 14873: train loss: 0.11165854423844028, test loss: 0.2551656100688968\n",
      "epoch 14874: train loss: 0.11165654109105018, test loss: 0.25516645001563426\n",
      "epoch 14875: train loss: 0.11165453812765735, test loss: 0.2551672900474787\n",
      "epoch 14876: train loss: 0.11165253534823126, test loss: 0.25516813016442214\n",
      "epoch 14877: train loss: 0.1116505327527415, test loss: 0.2551689703664659\n",
      "epoch 14878: train loss: 0.11164853034115758, test loss: 0.2551698106535734\n",
      "epoch 14879: train loss: 0.11164652811344906, test loss: 0.25517065102570397\n",
      "epoch 14880: train loss: 0.1116445260695855, test loss: 0.25517149148288176\n",
      "epoch 14881: train loss: 0.11164252420953646, test loss: 0.25517233202507705\n",
      "epoch 14882: train loss: 0.11164052253327149, test loss: 0.25517317265225226\n",
      "epoch 14883: train loss: 0.11163852104076019, test loss: 0.2551740133643896\n",
      "epoch 14884: train loss: 0.11163651973197211, test loss: 0.2551748541614899\n",
      "epoch 14885: train loss: 0.11163451860687688, test loss: 0.2551756950435161\n",
      "epoch 14886: train loss: 0.11163251766544409, test loss: 0.2551765360104798\n",
      "epoch 14887: train loss: 0.1116305169076433, test loss: 0.25517737706232246\n",
      "epoch 14888: train loss: 0.1116285163334442, test loss: 0.25517821819906744\n",
      "epoch 14889: train loss: 0.11162651594281633, test loss: 0.255179059420654\n",
      "epoch 14890: train loss: 0.11162451573572937, test loss: 0.2551799007270663\n",
      "epoch 14891: train loss: 0.11162251571215291, test loss: 0.25518074211832575\n",
      "epoch 14892: train loss: 0.11162051587205664, test loss: 0.2551815835943943\n",
      "epoch 14893: train loss: 0.11161851621541018, test loss: 0.255182425155245\n",
      "epoch 14894: train loss: 0.11161651674218317, test loss: 0.2551832668008471\n",
      "epoch 14895: train loss: 0.1116145174523453, test loss: 0.25518410853121465\n",
      "epoch 14896: train loss: 0.11161251834586623, test loss: 0.2551849503463093\n",
      "epoch 14897: train loss: 0.11161051942271566, test loss: 0.2551857922461237\n",
      "epoch 14898: train loss: 0.11160852068286321, test loss: 0.255186634230618\n",
      "epoch 14899: train loss: 0.11160652212627863, test loss: 0.25518747629979627\n",
      "epoch 14900: train loss: 0.11160452375293158, test loss: 0.2551883184536401\n",
      "epoch 14901: train loss: 0.11160252556279177, test loss: 0.2551891606921109\n",
      "epoch 14902: train loss: 0.11160052755582893, test loss: 0.2551900030151906\n",
      "epoch 14903: train loss: 0.11159852973201279, test loss: 0.2551908454228925\n",
      "epoch 14904: train loss: 0.11159653209131302, test loss: 0.25519168791517804\n",
      "epoch 14905: train loss: 0.11159453463369942, test loss: 0.2551925304919985\n",
      "epoch 14906: train loss: 0.11159253735914171, test loss: 0.2551933731533881\n",
      "epoch 14907: train loss: 0.1115905402676096, test loss: 0.2551942158993077\n",
      "epoch 14908: train loss: 0.11158854335907288, test loss: 0.25519505872973997\n",
      "epoch 14909: train loss: 0.1115865466335013, test loss: 0.2551959016446555\n",
      "epoch 14910: train loss: 0.11158455009086463, test loss: 0.25519674464402775\n",
      "epoch 14911: train loss: 0.11158255373113266, test loss: 0.2551975877278692\n",
      "epoch 14912: train loss: 0.11158055755427515, test loss: 0.2551984308961419\n",
      "epoch 14913: train loss: 0.11157856156026191, test loss: 0.2551992741488494\n",
      "epoch 14914: train loss: 0.11157656574906269, test loss: 0.2552001174859403\n",
      "epoch 14915: train loss: 0.11157457012064739, test loss: 0.255200960907389\n",
      "epoch 14916: train loss: 0.11157257467498573, test loss: 0.25520180441321805\n",
      "epoch 14917: train loss: 0.11157057941204755, test loss: 0.2552026480033904\n",
      "epoch 14918: train loss: 0.11156858433180272, test loss: 0.2552034916778974\n",
      "epoch 14919: train loss: 0.11156658943422101, test loss: 0.25520433543670107\n",
      "epoch 14920: train loss: 0.11156459471927228, test loss: 0.2552051792798043\n",
      "epoch 14921: train loss: 0.11156260018692642, test loss: 0.25520602320714736\n",
      "epoch 14922: train loss: 0.11156060583715323, test loss: 0.2552068672187773\n",
      "epoch 14923: train loss: 0.11155861166992259, test loss: 0.255207711314622\n",
      "epoch 14924: train loss: 0.11155661768520435, test loss: 0.25520855549467447\n",
      "epoch 14925: train loss: 0.11155462388296844, test loss: 0.25520939975891915\n",
      "epoch 14926: train loss: 0.11155263026318468, test loss: 0.2552102441073684\n",
      "epoch 14927: train loss: 0.11155063682582299, test loss: 0.25521108853996227\n",
      "epoch 14928: train loss: 0.11154864357085327, test loss: 0.2552119330566835\n",
      "epoch 14929: train loss: 0.11154665049824537, test loss: 0.25521277765753625\n",
      "epoch 14930: train loss: 0.11154465760796928, test loss: 0.25521362234250317\n",
      "epoch 14931: train loss: 0.11154266489999488, test loss: 0.25521446711154433\n",
      "epoch 14932: train loss: 0.11154067237429208, test loss: 0.2552153119646439\n",
      "epoch 14933: train loss: 0.11153868003083084, test loss: 0.25521615690181454\n",
      "epoch 14934: train loss: 0.11153668786958107, test loss: 0.25521700192299734\n",
      "epoch 14935: train loss: 0.11153469589051275, test loss: 0.25521784702820705\n",
      "epoch 14936: train loss: 0.11153270409359578, test loss: 0.25521869221738297\n",
      "epoch 14937: train loss: 0.1115307124788002, test loss: 0.2552195374905609\n",
      "epoch 14938: train loss: 0.1115287210460959, test loss: 0.2552203828477023\n",
      "epoch 14939: train loss: 0.11152672979545289, test loss: 0.2552212282887691\n",
      "epoch 14940: train loss: 0.11152473872684114, test loss: 0.2552220738137432\n",
      "epoch 14941: train loss: 0.11152274784023064, test loss: 0.25522291942262837\n",
      "epoch 14942: train loss: 0.11152075713559138, test loss: 0.25522376511541867\n",
      "epoch 14943: train loss: 0.1115187666128934, test loss: 0.2552246108920535\n",
      "epoch 14944: train loss: 0.11151677627210665, test loss: 0.2552254567525263\n",
      "epoch 14945: train loss: 0.11151478611320118, test loss: 0.255226302696841\n",
      "epoch 14946: train loss: 0.111512796136147, test loss: 0.2552271487249706\n",
      "epoch 14947: train loss: 0.11151080634091415, test loss: 0.2552279948368962\n",
      "epoch 14948: train loss: 0.11150881672747268, test loss: 0.25522884103257015\n",
      "epoch 14949: train loss: 0.11150682729579259, test loss: 0.25522968731201645\n",
      "epoch 14950: train loss: 0.11150483804584396, test loss: 0.25523053367520826\n",
      "epoch 14951: train loss: 0.11150284897759687, test loss: 0.255231380122106\n",
      "epoch 14952: train loss: 0.11150086009102134, test loss: 0.2552322266527148\n",
      "epoch 14953: train loss: 0.1114988713860875, test loss: 0.2552330732670061\n",
      "epoch 14954: train loss: 0.11149688286276536, test loss: 0.2552339199649316\n",
      "epoch 14955: train loss: 0.11149489452102504, test loss: 0.2552347667465369\n",
      "epoch 14956: train loss: 0.11149290636083663, test loss: 0.255235613611774\n",
      "epoch 14957: train loss: 0.11149091838217026, test loss: 0.25523646056059196\n",
      "epoch 14958: train loss: 0.111488930584996, test loss: 0.2552373075930186\n",
      "epoch 14959: train loss: 0.11148694296928394, test loss: 0.2552381547090139\n",
      "epoch 14960: train loss: 0.11148495553500429, test loss: 0.2552390019085606\n",
      "epoch 14961: train loss: 0.11148296828212707, test loss: 0.2552398491916537\n",
      "epoch 14962: train loss: 0.1114809812106225, test loss: 0.2552406965582531\n",
      "epoch 14963: train loss: 0.11147899432046067, test loss: 0.25524154400836435\n",
      "epoch 14964: train loss: 0.11147700761161179, test loss: 0.2552423915419697\n",
      "epoch 14965: train loss: 0.11147502108404593, test loss: 0.25524323915899877\n",
      "epoch 14966: train loss: 0.11147303473773333, test loss: 0.2552440868594994\n",
      "epoch 14967: train loss: 0.11147104857264413, test loss: 0.2552449346434422\n",
      "epoch 14968: train loss: 0.1114690625887485, test loss: 0.2552457825107685\n",
      "epoch 14969: train loss: 0.11146707678601661, test loss: 0.2552466304614831\n",
      "epoch 14970: train loss: 0.11146509116441869, test loss: 0.2552474784955897\n",
      "epoch 14971: train loss: 0.11146310572392494, test loss: 0.25524832661304964\n",
      "epoch 14972: train loss: 0.1114611204645055, test loss: 0.2552491748138256\n",
      "epoch 14973: train loss: 0.11145913538613066, test loss: 0.2552500230979425\n",
      "epoch 14974: train loss: 0.11145715048877058, test loss: 0.25525087146534164\n",
      "epoch 14975: train loss: 0.11145516577239553, test loss: 0.25525171991602574\n",
      "epoch 14976: train loss: 0.11145318123697572, test loss: 0.25525256844997984\n",
      "epoch 14977: train loss: 0.11145119688248138, test loss: 0.2552534170671863\n",
      "epoch 14978: train loss: 0.11144921270888279, test loss: 0.25525426576760524\n",
      "epoch 14979: train loss: 0.11144722871615018, test loss: 0.2552551145512437\n",
      "epoch 14980: train loss: 0.11144524490425382, test loss: 0.2552559634180621\n",
      "epoch 14981: train loss: 0.11144326127316397, test loss: 0.25525681236806486\n",
      "epoch 14982: train loss: 0.11144127782285089, test loss: 0.2552576614012043\n",
      "epoch 14983: train loss: 0.1114392945532849, test loss: 0.2552585105174945\n",
      "epoch 14984: train loss: 0.11143731146443628, test loss: 0.25525935971689717\n",
      "epoch 14985: train loss: 0.11143532855627529, test loss: 0.2552602089994076\n",
      "epoch 14986: train loss: 0.11143334582877226, test loss: 0.2552610583649959\n",
      "epoch 14987: train loss: 0.1114313632818975, test loss: 0.25526190781364727\n",
      "epoch 14988: train loss: 0.1114293809156213, test loss: 0.25526275734534415\n",
      "epoch 14989: train loss: 0.11142739872991403, test loss: 0.2552636069600917\n",
      "epoch 14990: train loss: 0.11142541672474598, test loss: 0.2552644566578085\n",
      "epoch 14991: train loss: 0.11142343490008752, test loss: 0.25526530643854256\n",
      "epoch 14992: train loss: 0.11142145325590896, test loss: 0.2552661563022547\n",
      "epoch 14993: train loss: 0.11141947179218067, test loss: 0.255267006248908\n",
      "epoch 14994: train loss: 0.11141749050887301, test loss: 0.25526785627849646\n",
      "epoch 14995: train loss: 0.11141550940595632, test loss: 0.25526870639101285\n",
      "epoch 14996: train loss: 0.11141352848340101, test loss: 0.2552695565864416\n",
      "epoch 14997: train loss: 0.11141154774117744, test loss: 0.25527040686472274\n",
      "epoch 14998: train loss: 0.11140956717925597, test loss: 0.25527125722590527\n",
      "epoch 14999: train loss: 0.11140758679760704, test loss: 0.25527210766990627\n",
      "epoch 15000: train loss: 0.11140560659620105, test loss: 0.2552729581967523\n",
      "epoch 15001: train loss: 0.11140362657500835, test loss: 0.25527380880638567\n",
      "epoch 15002: train loss: 0.11140164673399941, test loss: 0.25527465949885103\n",
      "epoch 15003: train loss: 0.11139966707314461, test loss: 0.25527551027405004\n",
      "epoch 15004: train loss: 0.11139768759241439, test loss: 0.25527636113202673\n",
      "epoch 15005: train loss: 0.1113957082917792, test loss: 0.25527721207274573\n",
      "epoch 15006: train loss: 0.11139372917120945, test loss: 0.2552780630961892\n",
      "epoch 15007: train loss: 0.11139175023067566, test loss: 0.2552789142023193\n",
      "epoch 15008: train loss: 0.11138977147014817, test loss: 0.25527976539113084\n",
      "epoch 15009: train loss: 0.11138779288959753, test loss: 0.2552806166626388\n",
      "epoch 15010: train loss: 0.11138581448899419, test loss: 0.25528146801676244\n",
      "epoch 15011: train loss: 0.1113838362683086, test loss: 0.2552823194535284\n",
      "epoch 15012: train loss: 0.11138185822751126, test loss: 0.2552831709729211\n",
      "epoch 15013: train loss: 0.11137988036657266, test loss: 0.25528402257488986\n",
      "epoch 15014: train loss: 0.11137790268546331, test loss: 0.2552848742594407\n",
      "epoch 15015: train loss: 0.11137592518415368, test loss: 0.25528572602654814\n",
      "epoch 15016: train loss: 0.11137394786261431, test loss: 0.2552865778762135\n",
      "epoch 15017: train loss: 0.11137197072081566, test loss: 0.2552874298083707\n",
      "epoch 15018: train loss: 0.11136999375872834, test loss: 0.255288281823045\n",
      "epoch 15019: train loss: 0.11136801697632284, test loss: 0.25528913392021724\n",
      "epoch 15020: train loss: 0.11136604037356967, test loss: 0.2552899860998532\n",
      "epoch 15021: train loss: 0.11136406395043942, test loss: 0.25529083836193583\n",
      "epoch 15022: train loss: 0.1113620877069026, test loss: 0.25529169070644847\n",
      "epoch 15023: train loss: 0.11136011164292983, test loss: 0.2552925431333741\n",
      "epoch 15024: train loss: 0.11135813575849159, test loss: 0.25529339564271775\n",
      "epoch 15025: train loss: 0.1113561600535585, test loss: 0.25529424823442165\n",
      "epoch 15026: train loss: 0.11135418452810114, test loss: 0.2552951009084894\n",
      "epoch 15027: train loss: 0.11135220918209007, test loss: 0.2552959536649059\n",
      "epoch 15028: train loss: 0.11135023401549594, test loss: 0.25529680650363207\n",
      "epoch 15029: train loss: 0.11134825902828928, test loss: 0.2552976594246756\n",
      "epoch 15030: train loss: 0.11134628422044072, test loss: 0.25529851242800683\n",
      "epoch 15031: train loss: 0.11134430959192092, test loss: 0.2552993655136109\n",
      "epoch 15032: train loss: 0.1113423351427004, test loss: 0.2553002186814829\n",
      "epoch 15033: train loss: 0.11134036087274987, test loss: 0.25530107193157314\n",
      "epoch 15034: train loss: 0.11133838678203994, test loss: 0.25530192526388773\n",
      "epoch 15035: train loss: 0.11133641287054125, test loss: 0.2553027786783983\n",
      "epoch 15036: train loss: 0.11133443913822445, test loss: 0.25530363217508034\n",
      "epoch 15037: train loss: 0.11133246558506016, test loss: 0.2553044857539386\n",
      "epoch 15038: train loss: 0.1113304922110191, test loss: 0.2553053394149341\n",
      "epoch 15039: train loss: 0.1113285190160719, test loss: 0.25530619315806186\n",
      "epoch 15040: train loss: 0.11132654600018924, test loss: 0.25530704698330625\n",
      "epoch 15041: train loss: 0.1113245731633418, test loss: 0.2553079008906397\n",
      "epoch 15042: train loss: 0.11132260050550025, test loss: 0.25530875488004745\n",
      "epoch 15043: train loss: 0.11132062802663534, test loss: 0.25530960895150295\n",
      "epoch 15044: train loss: 0.1113186557267177, test loss: 0.25531046310500977\n",
      "epoch 15045: train loss: 0.1113166836057181, test loss: 0.25531131734053025\n",
      "epoch 15046: train loss: 0.11131471166360722, test loss: 0.25531217165807196\n",
      "epoch 15047: train loss: 0.11131273990035578, test loss: 0.25531302605757455\n",
      "epoch 15048: train loss: 0.11131076831593453, test loss: 0.2553138805390654\n",
      "epoch 15049: train loss: 0.11130879691031423, test loss: 0.2553147351024861\n",
      "epoch 15050: train loss: 0.11130682568346556, test loss: 0.25531558974784074\n",
      "epoch 15051: train loss: 0.1113048546353593, test loss: 0.25531644447511465\n",
      "epoch 15052: train loss: 0.11130288376596621, test loss: 0.25531729928428043\n",
      "epoch 15053: train loss: 0.11130091307525705, test loss: 0.2553181541753426\n",
      "epoch 15054: train loss: 0.11129894256320258, test loss: 0.25531900914824507\n",
      "epoch 15055: train loss: 0.11129697222977356, test loss: 0.25531986420299035\n",
      "epoch 15056: train loss: 0.11129500207494085, test loss: 0.25532071933956485\n",
      "epoch 15057: train loss: 0.11129303209867515, test loss: 0.25532157455795196\n",
      "epoch 15058: train loss: 0.1112910623009473, test loss: 0.2553224298581343\n",
      "epoch 15059: train loss: 0.11128909268172812, test loss: 0.2553232852400661\n",
      "epoch 15060: train loss: 0.11128712324098837, test loss: 0.255324140703752\n",
      "epoch 15061: train loss: 0.11128515397869893, test loss: 0.2553249962491975\n",
      "epoch 15062: train loss: 0.11128318489483056, test loss: 0.25532585187634316\n",
      "epoch 15063: train loss: 0.11128121598935416, test loss: 0.25532670758519665\n",
      "epoch 15064: train loss: 0.11127924726224048, test loss: 0.2553275633757185\n",
      "epoch 15065: train loss: 0.11127727871346042, test loss: 0.25532841924791555\n",
      "epoch 15066: train loss: 0.11127531034298485, test loss: 0.25532927520177184\n",
      "epoch 15067: train loss: 0.11127334215078459, test loss: 0.2553301312372296\n",
      "epoch 15068: train loss: 0.11127137413683051, test loss: 0.2553309873543242\n",
      "epoch 15069: train loss: 0.11126940630109351, test loss: 0.25533184355300004\n",
      "epoch 15070: train loss: 0.11126743864354445, test loss: 0.25533269983323836\n",
      "epoch 15071: train loss: 0.1112654711641542, test loss: 0.25533355619505754\n",
      "epoch 15072: train loss: 0.11126350386289367, test loss: 0.2553344126384094\n",
      "epoch 15073: train loss: 0.11126153673973375, test loss: 0.25533526916327864\n",
      "epoch 15074: train loss: 0.11125956979464537, test loss: 0.25533612576965936\n",
      "epoch 15075: train loss: 0.11125760302759938, test loss: 0.25533698245751435\n",
      "epoch 15076: train loss: 0.11125563643856676, test loss: 0.25533783922685116\n",
      "epoch 15077: train loss: 0.1112536700275184, test loss: 0.2553386960776512\n",
      "epoch 15078: train loss: 0.11125170379442528, test loss: 0.255339553009859\n",
      "epoch 15079: train loss: 0.1112497377392583, test loss: 0.2553404100234999\n",
      "epoch 15080: train loss: 0.1112477718619884, test loss: 0.25534126711852745\n",
      "epoch 15081: train loss: 0.11124580616258654, test loss: 0.2553421242949463\n",
      "epoch 15082: train loss: 0.1112438406410237, test loss: 0.25534298155271895\n",
      "epoch 15083: train loss: 0.11124187529727085, test loss: 0.2553438388918528\n",
      "epoch 15084: train loss: 0.11123991013129891, test loss: 0.2553446963123094\n",
      "epoch 15085: train loss: 0.11123794514307893, test loss: 0.25534555381406393\n",
      "epoch 15086: train loss: 0.11123598033258184, test loss: 0.25534641139712194\n",
      "epoch 15087: train loss: 0.11123401569977866, test loss: 0.25534726906145755\n",
      "epoch 15088: train loss: 0.11123205124464038, test loss: 0.2553481268070207\n",
      "epoch 15089: train loss: 0.11123008696713799, test loss: 0.25534898463385175\n",
      "epoch 15090: train loss: 0.11122812286724254, test loss: 0.25534984254190346\n",
      "epoch 15091: train loss: 0.11122615894492505, test loss: 0.2553507005311585\n",
      "epoch 15092: train loss: 0.11122419520015651, test loss: 0.2553515586015914\n",
      "epoch 15093: train loss: 0.111222231632908, test loss: 0.2553524167531871\n",
      "epoch 15094: train loss: 0.1112202682431505, test loss: 0.2553532749859513\n",
      "epoch 15095: train loss: 0.11121830503085511, test loss: 0.2553541332998264\n",
      "epoch 15096: train loss: 0.11121634199599287, test loss: 0.2553549916948387\n",
      "epoch 15097: train loss: 0.11121437913853484, test loss: 0.25535585017094087\n",
      "epoch 15098: train loss: 0.11121241645845208, test loss: 0.2553567087281177\n",
      "epoch 15099: train loss: 0.11121045395571567, test loss: 0.2553575673663761\n",
      "epoch 15100: train loss: 0.11120849163029665, test loss: 0.2553584260856464\n",
      "epoch 15101: train loss: 0.1112065294821662, test loss: 0.2553592848859767\n",
      "epoch 15102: train loss: 0.11120456751129532, test loss: 0.2553601437672994\n",
      "epoch 15103: train loss: 0.11120260571765517, test loss: 0.2553610027296091\n",
      "epoch 15104: train loss: 0.11120064410121683, test loss: 0.2553618617729132\n",
      "epoch 15105: train loss: 0.1111986826619514, test loss: 0.25536272089717216\n",
      "epoch 15106: train loss: 0.11119672139983008, test loss: 0.2553635801023628\n",
      "epoch 15107: train loss: 0.11119476031482389, test loss: 0.2553644393884691\n",
      "epoch 15108: train loss: 0.11119279940690405, test loss: 0.25536529875549646\n",
      "epoch 15109: train loss: 0.11119083867604161, test loss: 0.25536615820338743\n",
      "epoch 15110: train loss: 0.11118887812220782, test loss: 0.255367017732169\n",
      "epoch 15111: train loss: 0.11118691774537376, test loss: 0.25536787734180527\n",
      "epoch 15112: train loss: 0.11118495754551065, test loss: 0.25536873703224794\n",
      "epoch 15113: train loss: 0.11118299752258959, test loss: 0.25536959680351484\n",
      "epoch 15114: train loss: 0.11118103767658181, test loss: 0.25537045665560076\n",
      "epoch 15115: train loss: 0.11117907800745847, test loss: 0.2553713165884699\n",
      "epoch 15116: train loss: 0.11117711851519074, test loss: 0.2553721766020745\n",
      "epoch 15117: train loss: 0.11117515919974984, test loss: 0.2553730366964422\n",
      "epoch 15118: train loss: 0.11117320006110697, test loss: 0.25537389687153705\n",
      "epoch 15119: train loss: 0.11117124109923333, test loss: 0.25537475712734214\n",
      "epoch 15120: train loss: 0.11116928231410014, test loss: 0.25537561746385506\n",
      "epoch 15121: train loss: 0.11116732370567861, test loss: 0.2553764778810173\n",
      "epoch 15122: train loss: 0.11116536527393997, test loss: 0.2553773383788561\n",
      "epoch 15123: train loss: 0.11116340701885548, test loss: 0.2553781989573248\n",
      "epoch 15124: train loss: 0.11116144894039635, test loss: 0.25537905961642854\n",
      "epoch 15125: train loss: 0.11115949103853381, test loss: 0.25537992035613216\n",
      "epoch 15126: train loss: 0.11115753331323919, test loss: 0.2553807811764208\n",
      "epoch 15127: train loss: 0.11115557576448368, test loss: 0.25538164207727815\n",
      "epoch 15128: train loss: 0.11115361839223854, test loss: 0.2553825030587003\n",
      "epoch 15129: train loss: 0.11115166119647511, test loss: 0.25538336412065193\n",
      "epoch 15130: train loss: 0.11114970417716463, test loss: 0.2553842252631167\n",
      "epoch 15131: train loss: 0.1111477473342784, test loss: 0.2553850864861005\n",
      "epoch 15132: train loss: 0.1111457906677877, test loss: 0.2553859477895368\n",
      "epoch 15133: train loss: 0.11114383417766384, test loss: 0.25538680917347345\n",
      "epoch 15134: train loss: 0.11114187786387815, test loss: 0.25538767063783224\n",
      "epoch 15135: train loss: 0.11113992172640189, test loss: 0.25538853218263025\n",
      "epoch 15136: train loss: 0.1111379657652064, test loss: 0.2553893938078615\n",
      "epoch 15137: train loss: 0.11113600998026305, test loss: 0.25539025551345906\n",
      "epoch 15138: train loss: 0.11113405437154313, test loss: 0.25539111729943964\n",
      "epoch 15139: train loss: 0.111132098939018, test loss: 0.2553919791658008\n",
      "epoch 15140: train loss: 0.11113014368265901, test loss: 0.2553928411124926\n",
      "epoch 15141: train loss: 0.11112818860243749, test loss: 0.2553937031395236\n",
      "epoch 15142: train loss: 0.11112623369832482, test loss: 0.2553945652468349\n",
      "epoch 15143: train loss: 0.11112427897029235, test loss: 0.2553954274344665\n",
      "epoch 15144: train loss: 0.11112232441831146, test loss: 0.2553962897023596\n",
      "epoch 15145: train loss: 0.11112037004235358, test loss: 0.25539715205049973\n",
      "epoch 15146: train loss: 0.11111841584239002, test loss: 0.2553980144788933\n",
      "epoch 15147: train loss: 0.11111646181839221, test loss: 0.2553988769875161\n",
      "epoch 15148: train loss: 0.1111145079703316, test loss: 0.25539973957633\n",
      "epoch 15149: train loss: 0.1111125542981795, test loss: 0.2554006022453214\n",
      "epoch 15150: train loss: 0.11111060080190736, test loss: 0.25540146499448624\n",
      "epoch 15151: train loss: 0.11110864748148662, test loss: 0.25540232782380945\n",
      "epoch 15152: train loss: 0.11110669433688872, test loss: 0.2554031907332756\n",
      "epoch 15153: train loss: 0.11110474136808504, test loss: 0.25540405372283936\n",
      "epoch 15154: train loss: 0.11110278857504706, test loss: 0.25540491679252786\n",
      "epoch 15155: train loss: 0.11110083595774624, test loss: 0.2554057799422831\n",
      "epoch 15156: train loss: 0.11109888351615398, test loss: 0.2554066431721126\n",
      "epoch 15157: train loss: 0.11109693125024178, test loss: 0.2554075064819806\n",
      "epoch 15158: train loss: 0.11109497915998108, test loss: 0.25540836987188276\n",
      "epoch 15159: train loss: 0.11109302724534341, test loss: 0.25540923334177196\n",
      "epoch 15160: train loss: 0.11109107550630018, test loss: 0.2554100968916985\n",
      "epoch 15161: train loss: 0.11108912394282293, test loss: 0.25541096052159323\n",
      "epoch 15162: train loss: 0.11108717255488308, test loss: 0.2554118242314311\n",
      "epoch 15163: train loss: 0.11108522134245223, test loss: 0.255412688021209\n",
      "epoch 15164: train loss: 0.1110832703055018, test loss: 0.2554135518909329\n",
      "epoch 15165: train loss: 0.11108131944400335, test loss: 0.2554144158405666\n",
      "epoch 15166: train loss: 0.11107936875792838, test loss: 0.25541527987007556\n",
      "epoch 15167: train loss: 0.11107741824724843, test loss: 0.2554161439794532\n",
      "epoch 15168: train loss: 0.111075467911935, test loss: 0.25541700816870966\n",
      "epoch 15169: train loss: 0.11107351775195969, test loss: 0.25541787243780734\n",
      "epoch 15170: train loss: 0.11107156776729397, test loss: 0.2554187367866996\n",
      "epoch 15171: train loss: 0.11106961795790944, test loss: 0.25541960121541485\n",
      "epoch 15172: train loss: 0.11106766832377765, test loss: 0.2554204657239169\n",
      "epoch 15173: train loss: 0.11106571886487018, test loss: 0.2554213303121811\n",
      "epoch 15174: train loss: 0.11106376958115856, test loss: 0.25542219498021396\n",
      "epoch 15175: train loss: 0.11106182047261441, test loss: 0.2554230597279807\n",
      "epoch 15176: train loss: 0.1110598715392093, test loss: 0.2554239245554536\n",
      "epoch 15177: train loss: 0.11105792278091481, test loss: 0.2554247894626423\n",
      "epoch 15178: train loss: 0.11105597419770255, test loss: 0.25542565444948834\n",
      "epoch 15179: train loss: 0.11105402578954411, test loss: 0.25542651951600953\n",
      "epoch 15180: train loss: 0.11105207755641114, test loss: 0.2554273846621918\n",
      "epoch 15181: train loss: 0.11105012949827522, test loss: 0.25542824988798735\n",
      "epoch 15182: train loss: 0.11104818161510797, test loss: 0.25542911519342715\n",
      "epoch 15183: train loss: 0.11104623390688104, test loss: 0.2554299805784512\n",
      "epoch 15184: train loss: 0.11104428637356607, test loss: 0.25543084604303645\n",
      "epoch 15185: train loss: 0.11104233901513469, test loss: 0.2554317115872206\n",
      "epoch 15186: train loss: 0.11104039183155855, test loss: 0.2554325772109158\n",
      "epoch 15187: train loss: 0.11103844482280933, test loss: 0.25543344291413944\n",
      "epoch 15188: train loss: 0.11103649798885869, test loss: 0.25543430869689765\n",
      "epoch 15189: train loss: 0.11103455132967827, test loss: 0.2554351745591341\n",
      "epoch 15190: train loss: 0.11103260484523975, test loss: 0.25543604050085594\n",
      "epoch 15191: train loss: 0.11103065853551486, test loss: 0.2554369065220379\n",
      "epoch 15192: train loss: 0.11102871240047524, test loss: 0.25543777262264467\n",
      "epoch 15193: train loss: 0.11102676644009261, test loss: 0.25543863880269335\n",
      "epoch 15194: train loss: 0.11102482065433861, test loss: 0.2554395050621494\n",
      "epoch 15195: train loss: 0.11102287504318507, test loss: 0.25544037140097653\n",
      "epoch 15196: train loss: 0.1110209296066036, test loss: 0.25544123781919303\n",
      "epoch 15197: train loss: 0.111018984344566, test loss: 0.25544210431676206\n",
      "epoch 15198: train loss: 0.11101703925704394, test loss: 0.2554429708936599\n",
      "epoch 15199: train loss: 0.11101509434400918, test loss: 0.2554438375498941\n",
      "epoch 15200: train loss: 0.11101314960543344, test loss: 0.2554447042854288\n",
      "epoch 15201: train loss: 0.1110112050412885, test loss: 0.2554455711002487\n",
      "epoch 15202: train loss: 0.11100926065154612, test loss: 0.25544643799432953\n",
      "epoch 15203: train loss: 0.11100731643617802, test loss: 0.25544730496766943\n",
      "epoch 15204: train loss: 0.111005372395156, test loss: 0.255448172020252\n",
      "epoch 15205: train loss: 0.11100342852845181, test loss: 0.25544903915204314\n",
      "epoch 15206: train loss: 0.11100148483603725, test loss: 0.2554499063630493\n",
      "epoch 15207: train loss: 0.11099954131788412, test loss: 0.25545077365322455\n",
      "epoch 15208: train loss: 0.11099759797396416, test loss: 0.25545164102257695\n",
      "epoch 15209: train loss: 0.11099565480424924, test loss: 0.2554525084710692\n",
      "epoch 15210: train loss: 0.11099371180871112, test loss: 0.2554533759987003\n",
      "epoch 15211: train loss: 0.1109917689873216, test loss: 0.2554542436054439\n",
      "epoch 15212: train loss: 0.11098982634005257, test loss: 0.2554551112912867\n",
      "epoch 15213: train loss: 0.11098788386687579, test loss: 0.25545597905621464\n",
      "epoch 15214: train loss: 0.11098594156776312, test loss: 0.255456846900203\n",
      "epoch 15215: train loss: 0.11098399944268639, test loss: 0.2554577148232366\n",
      "epoch 15216: train loss: 0.11098205749161744, test loss: 0.25545858282530376\n",
      "epoch 15217: train loss: 0.11098011571452815, test loss: 0.25545945090637695\n",
      "epoch 15218: train loss: 0.11097817411139035, test loss: 0.25546031906644445\n",
      "epoch 15219: train loss: 0.11097623268217592, test loss: 0.2554611873054818\n",
      "epoch 15220: train loss: 0.11097429142685672, test loss: 0.25546205562349455\n",
      "epoch 15221: train loss: 0.11097235034540465, test loss: 0.2554629240204493\n",
      "epoch 15222: train loss: 0.11097040943779156, test loss: 0.25546379249634144\n",
      "epoch 15223: train loss: 0.11096846870398938, test loss: 0.2554646610511353\n",
      "epoch 15224: train loss: 0.11096652814396997, test loss: 0.2554655296848082\n",
      "epoch 15225: train loss: 0.11096458775770525, test loss: 0.2554663983973666\n",
      "epoch 15226: train loss: 0.11096264754516717, test loss: 0.25546726718876445\n",
      "epoch 15227: train loss: 0.11096070750632758, test loss: 0.2554681360590307\n",
      "epoch 15228: train loss: 0.11095876764115843, test loss: 0.25546900500810854\n",
      "epoch 15229: train loss: 0.11095682794963166, test loss: 0.25546987403601756\n",
      "epoch 15230: train loss: 0.11095488843171922, test loss: 0.25547074314267726\n",
      "epoch 15231: train loss: 0.11095294908739302, test loss: 0.25547161232812965\n",
      "epoch 15232: train loss: 0.110951009916625, test loss: 0.25547248159233815\n",
      "epoch 15233: train loss: 0.11094907091938716, test loss: 0.2554733509352904\n",
      "epoch 15234: train loss: 0.11094713209565142, test loss: 0.25547422035696093\n",
      "epoch 15235: train loss: 0.11094519344538976, test loss: 0.25547508985732437\n",
      "epoch 15236: train loss: 0.11094325496857418, test loss: 0.2554759594363901\n",
      "epoch 15237: train loss: 0.11094131666517666, test loss: 0.25547682909412145\n",
      "epoch 15238: train loss: 0.11093937853516916, test loss: 0.2554776988304949\n",
      "epoch 15239: train loss: 0.11093744057852369, test loss: 0.2554785686455186\n",
      "epoch 15240: train loss: 0.11093550279521222, test loss: 0.2554794385391791\n",
      "epoch 15241: train loss: 0.11093356518520679, test loss: 0.2554803085114067\n",
      "epoch 15242: train loss: 0.11093162774847944, test loss: 0.2554811785622438\n",
      "epoch 15243: train loss: 0.11092969048500212, test loss: 0.2554820486916334\n",
      "epoch 15244: train loss: 0.11092775339474692, test loss: 0.25548291889958236\n",
      "epoch 15245: train loss: 0.11092581647768583, test loss: 0.25548378918606646\n",
      "epoch 15246: train loss: 0.11092387973379093, test loss: 0.2554846595510734\n",
      "epoch 15247: train loss: 0.11092194316303422, test loss: 0.2554855299945665\n",
      "epoch 15248: train loss: 0.11092000676538777, test loss: 0.25548640051654326\n",
      "epoch 15249: train loss: 0.11091807054082366, test loss: 0.2554872711170021\n",
      "epoch 15250: train loss: 0.11091613448931394, test loss: 0.2554881417959072\n",
      "epoch 15251: train loss: 0.11091419861083064, test loss: 0.2554890125532212\n",
      "epoch 15252: train loss: 0.11091226290534592, test loss: 0.25548988338896716\n",
      "epoch 15253: train loss: 0.11091032737283182, test loss: 0.2554907543031181\n",
      "epoch 15254: train loss: 0.11090839201326042, test loss: 0.25549162529563885\n",
      "epoch 15255: train loss: 0.11090645682660384, test loss: 0.2554924963665382\n",
      "epoch 15256: train loss: 0.11090452181283414, test loss: 0.25549336751577106\n",
      "epoch 15257: train loss: 0.11090258697192351, test loss: 0.25549423874333266\n",
      "epoch 15258: train loss: 0.11090065230384397, test loss: 0.25549511004921094\n",
      "epoch 15259: train loss: 0.11089871780856772, test loss: 0.2554959814333936\n",
      "epoch 15260: train loss: 0.11089678348606685, test loss: 0.25549685289586405\n",
      "epoch 15261: train loss: 0.11089484933631352, test loss: 0.2554977244365793\n",
      "epoch 15262: train loss: 0.11089291535927984, test loss: 0.255498596055557\n",
      "epoch 15263: train loss: 0.11089098155493797, test loss: 0.25549946775274074\n",
      "epoch 15264: train loss: 0.1108890479232601, test loss: 0.25550033952814927\n",
      "epoch 15265: train loss: 0.11088711446421837, test loss: 0.25550121138174814\n",
      "epoch 15266: train loss: 0.1108851811777849, test loss: 0.25550208331352375\n",
      "epoch 15267: train loss: 0.11088324806393192, test loss: 0.2555029553234745\n",
      "epoch 15268: train loss: 0.11088131512263161, test loss: 0.2555038274115528\n",
      "epoch 15269: train loss: 0.11087938235385611, test loss: 0.2555046995777688\n",
      "epoch 15270: train loss: 0.11087744975757766, test loss: 0.25550557182208566\n",
      "epoch 15271: train loss: 0.11087551733376844, test loss: 0.25550644414450213\n",
      "epoch 15272: train loss: 0.11087358508240067, test loss: 0.2555073165449942\n",
      "epoch 15273: train loss: 0.11087165300344654, test loss: 0.2555081890235477\n",
      "epoch 15274: train loss: 0.11086972109687826, test loss: 0.25550906158014036\n",
      "epoch 15275: train loss: 0.11086778936266808, test loss: 0.2555099342147786\n",
      "epoch 15276: train loss: 0.11086585780078824, test loss: 0.2555108069273844\n",
      "epoch 15277: train loss: 0.11086392641121097, test loss: 0.25551167971802097\n",
      "epoch 15278: train loss: 0.11086199519390849, test loss: 0.2555125525865992\n",
      "epoch 15279: train loss: 0.11086006414885308, test loss: 0.25551342553317086\n",
      "epoch 15280: train loss: 0.11085813327601698, test loss: 0.2555142985576579\n",
      "epoch 15281: train loss: 0.11085620257537246, test loss: 0.25551517166006854\n",
      "epoch 15282: train loss: 0.11085427204689177, test loss: 0.25551604484041174\n",
      "epoch 15283: train loss: 0.11085234169054722, test loss: 0.25551691809862975\n",
      "epoch 15284: train loss: 0.11085041150631106, test loss: 0.25551779143471115\n",
      "epoch 15285: train loss: 0.1108484814941556, test loss: 0.2555186648486742\n",
      "epoch 15286: train loss: 0.11084655165405312, test loss: 0.2555195383404513\n",
      "epoch 15287: train loss: 0.11084462198597596, test loss: 0.2555204119100739\n",
      "epoch 15288: train loss: 0.11084269248989635, test loss: 0.2555212855574746\n",
      "epoch 15289: train loss: 0.11084076316578667, test loss: 0.25552215928270405\n",
      "epoch 15290: train loss: 0.11083883401361923, test loss: 0.2555230330856855\n",
      "epoch 15291: train loss: 0.11083690503336634, test loss: 0.2555239069664151\n",
      "epoch 15292: train loss: 0.11083497622500033, test loss: 0.2555247809248926\n",
      "epoch 15293: train loss: 0.11083304758849355, test loss: 0.25552565496109336\n",
      "epoch 15294: train loss: 0.11083111912381836, test loss: 0.25552652907500345\n",
      "epoch 15295: train loss: 0.1108291908309471, test loss: 0.2555274032665892\n",
      "epoch 15296: train loss: 0.1108272627098521, test loss: 0.25552827753584845\n",
      "epoch 15297: train loss: 0.11082533476050578, test loss: 0.2555291518827896\n",
      "epoch 15298: train loss: 0.11082340698288046, test loss: 0.2555300263073235\n",
      "epoch 15299: train loss: 0.11082147937694854, test loss: 0.2555309008095129\n",
      "epoch 15300: train loss: 0.11081955194268239, test loss: 0.25553177538930305\n",
      "epoch 15301: train loss: 0.11081762468005443, test loss: 0.2555326500466791\n",
      "epoch 15302: train loss: 0.11081569758903705, test loss: 0.25553352478161906\n",
      "epoch 15303: train loss: 0.11081377066960262, test loss: 0.25553439959411867\n",
      "epoch 15304: train loss: 0.11081184392172355, test loss: 0.25553527448416696\n",
      "epoch 15305: train loss: 0.11080991734537228, test loss: 0.2555361494517171\n",
      "epoch 15306: train loss: 0.11080799094052125, test loss: 0.25553702449677923\n",
      "epoch 15307: train loss: 0.11080606470714285, test loss: 0.25553789961932794\n",
      "epoch 15308: train loss: 0.11080413864520952, test loss: 0.2555387748193414\n",
      "epoch 15309: train loss: 0.11080221275469372, test loss: 0.2555396500968053\n",
      "epoch 15310: train loss: 0.11080028703556788, test loss: 0.25554052545172995\n",
      "epoch 15311: train loss: 0.11079836148780446, test loss: 0.25554140088404587\n",
      "epoch 15312: train loss: 0.1107964361113759, test loss: 0.25554227639377536\n",
      "epoch 15313: train loss: 0.11079451090625472, test loss: 0.25554315198090527\n",
      "epoch 15314: train loss: 0.11079258587241331, test loss: 0.25554402764538847\n",
      "epoch 15315: train loss: 0.11079066100982424, test loss: 0.25554490338723507\n",
      "epoch 15316: train loss: 0.11078873631845991, test loss: 0.25554577920643273\n",
      "epoch 15317: train loss: 0.11078681179829285, test loss: 0.25554665510292407\n",
      "epoch 15318: train loss: 0.11078488744929559, test loss: 0.2555475310767415\n",
      "epoch 15319: train loss: 0.11078296327144053, test loss: 0.2555484071278262\n",
      "epoch 15320: train loss: 0.11078103926470029, test loss: 0.255549283256168\n",
      "epoch 15321: train loss: 0.11077911542904735, test loss: 0.25555015946179754\n",
      "epoch 15322: train loss: 0.11077719176445422, test loss: 0.255551035744635\n",
      "epoch 15323: train loss: 0.11077526827089343, test loss: 0.2555519121047008\n",
      "epoch 15324: train loss: 0.11077334494833753, test loss: 0.25555278854198454\n",
      "epoch 15325: train loss: 0.11077142179675903, test loss: 0.2555536650564381\n",
      "epoch 15326: train loss: 0.11076949881613052, test loss: 0.25555454164806124\n",
      "epoch 15327: train loss: 0.11076757600642452, test loss: 0.2555554183168406\n",
      "epoch 15328: train loss: 0.11076565336761358, test loss: 0.2555562950627533\n",
      "epoch 15329: train loss: 0.11076373089967032, test loss: 0.2555571718858081\n",
      "epoch 15330: train loss: 0.11076180860256728, test loss: 0.2555580487859384\n",
      "epoch 15331: train loss: 0.11075988647627703, test loss: 0.2555589257631647\n",
      "epoch 15332: train loss: 0.11075796452077218, test loss: 0.2555598028174503\n",
      "epoch 15333: train loss: 0.11075604273602525, test loss: 0.2555606799487957\n",
      "epoch 15334: train loss: 0.11075412112200894, test loss: 0.25556155715718926\n",
      "epoch 15335: train loss: 0.1107521996786958, test loss: 0.25556243444258214\n",
      "epoch 15336: train loss: 0.11075027840605843, test loss: 0.25556331180499875\n",
      "epoch 15337: train loss: 0.1107483573040695, test loss: 0.2555641892443792\n",
      "epoch 15338: train loss: 0.11074643637270157, test loss: 0.2555650667607573\n",
      "epoch 15339: train loss: 0.11074451561192727, test loss: 0.2555659443540654\n",
      "epoch 15340: train loss: 0.11074259502171929, test loss: 0.25556682202432307\n",
      "epoch 15341: train loss: 0.11074067460205024, test loss: 0.2555676997714847\n",
      "epoch 15342: train loss: 0.11073875435289277, test loss: 0.2555685775955505\n",
      "epoch 15343: train loss: 0.11073683427421949, test loss: 0.2555694554965172\n",
      "epoch 15344: train loss: 0.11073491436600313, test loss: 0.255570333474341\n",
      "epoch 15345: train loss: 0.11073299462821636, test loss: 0.25557121152904155\n",
      "epoch 15346: train loss: 0.11073107506083177, test loss: 0.2555720896605516\n",
      "epoch 15347: train loss: 0.1107291556638221, test loss: 0.25557296786889255\n",
      "epoch 15348: train loss: 0.11072723643716006, test loss: 0.2555738461540401\n",
      "epoch 15349: train loss: 0.11072531738081828, test loss: 0.25557472451597163\n",
      "epoch 15350: train loss: 0.11072339849476948, test loss: 0.2555756029546623\n",
      "epoch 15351: train loss: 0.11072147977898637, test loss: 0.25557648147011336\n",
      "epoch 15352: train loss: 0.11071956123344165, test loss: 0.25557736006229903\n",
      "epoch 15353: train loss: 0.11071764285810805, test loss: 0.2555782387312089\n",
      "epoch 15354: train loss: 0.11071572465295826, test loss: 0.2555791174768288\n",
      "epoch 15355: train loss: 0.11071380661796507, test loss: 0.25557999629913725\n",
      "epoch 15356: train loss: 0.11071188875310117, test loss: 0.2555808751981106\n",
      "epoch 15357: train loss: 0.1107099710583393, test loss: 0.25558175417373635\n",
      "epoch 15358: train loss: 0.11070805353365223, test loss: 0.25558263322601316\n",
      "epoch 15359: train loss: 0.11070613617901269, test loss: 0.2555835123548956\n",
      "epoch 15360: train loss: 0.11070421899439346, test loss: 0.25558439156039364\n",
      "epoch 15361: train loss: 0.11070230197976727, test loss: 0.25558527084246296\n",
      "epoch 15362: train loss: 0.11070038513510695, test loss: 0.2555861502011001\n",
      "epoch 15363: train loss: 0.11069846846038522, test loss: 0.2555870296363181\n",
      "epoch 15364: train loss: 0.11069655195557492, test loss: 0.2555879091480467\n",
      "epoch 15365: train loss: 0.11069463562064877, test loss: 0.2555887887363092\n",
      "epoch 15366: train loss: 0.11069271945557965, test loss: 0.255589668401092\n",
      "epoch 15367: train loss: 0.11069080346034027, test loss: 0.2555905481423504\n",
      "epoch 15368: train loss: 0.11068888763490353, test loss: 0.2555914279600826\n",
      "epoch 15369: train loss: 0.1106869719792422, test loss: 0.2555923078542785\n",
      "epoch 15370: train loss: 0.11068505649332909, test loss: 0.25559318782490176\n",
      "epoch 15371: train loss: 0.11068314117713708, test loss: 0.25559406787195227\n",
      "epoch 15372: train loss: 0.11068122603063896, test loss: 0.25559494799539545\n",
      "epoch 15373: train loss: 0.11067931105380759, test loss: 0.25559582819524285\n",
      "epoch 15374: train loss: 0.11067739624661577, test loss: 0.2555967084714589\n",
      "epoch 15375: train loss: 0.11067548160903641, test loss: 0.25559758882404293\n",
      "epoch 15376: train loss: 0.11067356714104236, test loss: 0.25559846925294866\n",
      "epoch 15377: train loss: 0.11067165284260644, test loss: 0.25559934975821175\n",
      "epoch 15378: train loss: 0.11066973871370157, test loss: 0.255600230339738\n",
      "epoch 15379: train loss: 0.11066782475430063, test loss: 0.2556011109975747\n",
      "epoch 15380: train loss: 0.11066591096437646, test loss: 0.25560199173168635\n",
      "epoch 15381: train loss: 0.11066399734390199, test loss: 0.2556028725420725\n",
      "epoch 15382: train loss: 0.11066208389285011, test loss: 0.2556037534286653\n",
      "epoch 15383: train loss: 0.1106601706111937, test loss: 0.2556046343915195\n",
      "epoch 15384: train loss: 0.11065825749890568, test loss: 0.25560551543054477\n",
      "epoch 15385: train loss: 0.11065634455595896, test loss: 0.2556063965457859\n",
      "epoch 15386: train loss: 0.11065443178232648, test loss: 0.2556072777372085\n",
      "epoch 15387: train loss: 0.11065251917798115, test loss: 0.2556081590047889\n",
      "epoch 15388: train loss: 0.11065060674289592, test loss: 0.255609040348483\n",
      "epoch 15389: train loss: 0.11064869447704369, test loss: 0.25560992176833514\n",
      "epoch 15390: train loss: 0.11064678238039746, test loss: 0.2556108032642863\n",
      "epoch 15391: train loss: 0.11064487045293012, test loss: 0.25561168483631597\n",
      "epoch 15392: train loss: 0.11064295869461467, test loss: 0.2556125664844455\n",
      "epoch 15393: train loss: 0.11064104710542409, test loss: 0.2556134482086172\n",
      "epoch 15394: train loss: 0.11063913568533128, test loss: 0.25561433000885353\n",
      "epoch 15395: train loss: 0.11063722443430929, test loss: 0.2556152118850992\n",
      "epoch 15396: train loss: 0.11063531335233108, test loss: 0.25561609383735195\n",
      "epoch 15397: train loss: 0.1106334024393696, test loss: 0.2556169758656343\n",
      "epoch 15398: train loss: 0.11063149169539788, test loss: 0.2556178579698576\n",
      "epoch 15399: train loss: 0.11062958112038894, test loss: 0.2556187401500641\n",
      "epoch 15400: train loss: 0.11062767071431574, test loss: 0.2556196224061965\n",
      "epoch 15401: train loss: 0.11062576047715134, test loss: 0.25562050473828024\n",
      "epoch 15402: train loss: 0.1106238504088687, test loss: 0.2556213871462558\n",
      "epoch 15403: train loss: 0.1106219405094409, test loss: 0.2556222696301471\n",
      "epoch 15404: train loss: 0.11062003077884093, test loss: 0.2556231521899075\n",
      "epoch 15405: train loss: 0.11061812121704188, test loss: 0.2556240348255496\n",
      "epoch 15406: train loss: 0.11061621182401674, test loss: 0.2556249175370256\n",
      "epoch 15407: train loss: 0.11061430259973858, test loss: 0.25562580032431365\n",
      "epoch 15408: train loss: 0.11061239354418047, test loss: 0.2556266831874381\n",
      "epoch 15409: train loss: 0.11061048465731545, test loss: 0.25562756612635024\n",
      "epoch 15410: train loss: 0.11060857593911662, test loss: 0.25562844914106386\n",
      "epoch 15411: train loss: 0.11060666738955699, test loss: 0.2556293322315088\n",
      "epoch 15412: train loss: 0.11060475900860971, test loss: 0.2556302153977326\n",
      "epoch 15413: train loss: 0.11060285079624782, test loss: 0.2556310986396989\n",
      "epoch 15414: train loss: 0.11060094275244445, test loss: 0.2556319819573627\n",
      "epoch 15415: train loss: 0.11059903487717265, test loss: 0.25563286535072444\n",
      "epoch 15416: train loss: 0.1105971271704056, test loss: 0.2556337488197729\n",
      "epoch 15417: train loss: 0.11059521963211633, test loss: 0.2556346323644846\n",
      "epoch 15418: train loss: 0.11059331226227802, test loss: 0.25563551598486056\n",
      "epoch 15419: train loss: 0.11059140506086372, test loss: 0.25563639968086604\n",
      "epoch 15420: train loss: 0.11058949802784664, test loss: 0.2556372834524662\n",
      "epoch 15421: train loss: 0.11058759116319986, test loss: 0.25563816729970734\n",
      "epoch 15422: train loss: 0.11058568446689655, test loss: 0.2556390512225213\n",
      "epoch 15423: train loss: 0.11058377793890987, test loss: 0.2556399352208852\n",
      "epoch 15424: train loss: 0.11058187157921293, test loss: 0.2556408192948329\n",
      "epoch 15425: train loss: 0.11057996538777892, test loss: 0.2556417034442858\n",
      "epoch 15426: train loss: 0.11057805936458098, test loss: 0.25564258766929965\n",
      "epoch 15427: train loss: 0.11057615350959232, test loss: 0.25564347196977394\n",
      "epoch 15428: train loss: 0.11057424782278609, test loss: 0.2556443563457516\n",
      "epoch 15429: train loss: 0.11057234230413548, test loss: 0.25564524079722317\n",
      "epoch 15430: train loss: 0.11057043695361368, test loss: 0.25564612532413206\n",
      "epoch 15431: train loss: 0.11056853177119393, test loss: 0.2556470099264779\n",
      "epoch 15432: train loss: 0.11056662675684933, test loss: 0.2556478946042495\n",
      "epoch 15433: train loss: 0.11056472191055317, test loss: 0.2556487793574355\n",
      "epoch 15434: train loss: 0.11056281723227865, test loss: 0.25564966418602475\n",
      "epoch 15435: train loss: 0.110560912721999, test loss: 0.25565054908996165\n",
      "epoch 15436: train loss: 0.11055900837968742, test loss: 0.25565143406924434\n",
      "epoch 15437: train loss: 0.11055710420531716, test loss: 0.2556523191239091\n",
      "epoch 15438: train loss: 0.11055520019886142, test loss: 0.2556532042538746\n",
      "epoch 15439: train loss: 0.11055329636029353, test loss: 0.2556540894591658\n",
      "epoch 15440: train loss: 0.11055139268958665, test loss: 0.25565497473972504\n",
      "epoch 15441: train loss: 0.11054948918671409, test loss: 0.25565586009558605\n",
      "epoch 15442: train loss: 0.11054758585164909, test loss: 0.25565674552668244\n",
      "epoch 15443: train loss: 0.11054568268436495, test loss: 0.255657631033036\n",
      "epoch 15444: train loss: 0.11054377968483488, test loss: 0.25565851661463546\n",
      "epoch 15445: train loss: 0.11054187685303223, test loss: 0.25565940227141437\n",
      "epoch 15446: train loss: 0.11053997418893025, test loss: 0.2556602880034169\n",
      "epoch 15447: train loss: 0.11053807169250227, test loss: 0.25566117381057657\n",
      "epoch 15448: train loss: 0.11053616936372157, test loss: 0.25566205969292677\n",
      "epoch 15449: train loss: 0.11053426720256143, test loss: 0.25566294565040054\n",
      "epoch 15450: train loss: 0.11053236520899518, test loss: 0.25566383168301954\n",
      "epoch 15451: train loss: 0.11053046338299616, test loss: 0.25566471779075123\n",
      "epoch 15452: train loss: 0.11052856172453765, test loss: 0.2556656039735612\n",
      "epoch 15453: train loss: 0.110526660233593, test loss: 0.2556664902314726\n",
      "epoch 15454: train loss: 0.11052475891013558, test loss: 0.25566737656445293\n",
      "epoch 15455: train loss: 0.11052285775413867, test loss: 0.25566826297247763\n",
      "epoch 15456: train loss: 0.11052095676557566, test loss: 0.2556691494555384\n",
      "epoch 15457: train loss: 0.11051905594441992, test loss: 0.25567003601361127\n",
      "epoch 15458: train loss: 0.11051715529064474, test loss: 0.25567092264668484\n",
      "epoch 15459: train loss: 0.11051525480422354, test loss: 0.25567180935474987\n",
      "epoch 15460: train loss: 0.1105133544851297, test loss: 0.2556726961377936\n",
      "epoch 15461: train loss: 0.11051145433333655, test loss: 0.2556735829957607\n",
      "epoch 15462: train loss: 0.11050955434881754, test loss: 0.2556744699286754\n",
      "epoch 15463: train loss: 0.110507654531546, test loss: 0.2556753569365242\n",
      "epoch 15464: train loss: 0.11050575488149533, test loss: 0.2556762440192648\n",
      "epoch 15465: train loss: 0.11050385539863897, test loss: 0.2556771311769188\n",
      "epoch 15466: train loss: 0.11050195608295033, test loss: 0.2556780184094303\n",
      "epoch 15467: train loss: 0.11050005693440276, test loss: 0.2556789057167773\n",
      "epoch 15468: train loss: 0.11049815795296977, test loss: 0.2556797930989719\n",
      "epoch 15469: train loss: 0.11049625913862471, test loss: 0.2556806805560023\n",
      "epoch 15470: train loss: 0.11049436049134105, test loss: 0.2556815680878591\n",
      "epoch 15471: train loss: 0.11049246201109224, test loss: 0.25568245569446246\n",
      "epoch 15472: train loss: 0.11049056369785168, test loss: 0.2556833433758808\n",
      "epoch 15473: train loss: 0.11048866555159285, test loss: 0.2556842311320359\n",
      "epoch 15474: train loss: 0.11048676757228922, test loss: 0.2556851189629509\n",
      "epoch 15475: train loss: 0.1104848697599142, test loss: 0.2556860068685821\n",
      "epoch 15476: train loss: 0.11048297211444132, test loss: 0.2556868948489045\n",
      "epoch 15477: train loss: 0.11048107463584403, test loss: 0.2556877829039551\n",
      "epoch 15478: train loss: 0.1104791773240958, test loss: 0.25568867103366666\n",
      "epoch 15479: train loss: 0.11047728017917013, test loss: 0.2556895592380611\n",
      "epoch 15480: train loss: 0.11047538320104047, test loss: 0.2556904475170718\n",
      "epoch 15481: train loss: 0.1104734863896804, test loss: 0.2556913358707444\n",
      "epoch 15482: train loss: 0.11047158974506335, test loss: 0.25569222429902416\n",
      "epoch 15483: train loss: 0.11046969326716287, test loss: 0.2556931128018753\n",
      "epoch 15484: train loss: 0.11046779695595246, test loss: 0.25569400137932347\n",
      "epoch 15485: train loss: 0.11046590081140564, test loss: 0.2556948900313572\n",
      "epoch 15486: train loss: 0.11046400483349597, test loss: 0.2556957787579084\n",
      "epoch 15487: train loss: 0.11046210902219694, test loss: 0.25569666755903564\n",
      "epoch 15488: train loss: 0.11046021337748213, test loss: 0.25569755643464914\n",
      "epoch 15489: train loss: 0.11045831789932505, test loss: 0.25569844538478187\n",
      "epoch 15490: train loss: 0.11045642258769928, test loss: 0.25569933440938003\n",
      "epoch 15491: train loss: 0.11045452744257837, test loss: 0.25570022350846583\n",
      "epoch 15492: train loss: 0.11045263246393587, test loss: 0.2557011126819948\n",
      "epoch 15493: train loss: 0.11045073765174537, test loss: 0.255702001929967\n",
      "epoch 15494: train loss: 0.11044884300598042, test loss: 0.2557028912523628\n",
      "epoch 15495: train loss: 0.11044694852661464, test loss: 0.2557037806491697\n",
      "epoch 15496: train loss: 0.11044505421362161, test loss: 0.2557046701203667\n",
      "epoch 15497: train loss: 0.1104431600669749, test loss: 0.25570555966593145\n",
      "epoch 15498: train loss: 0.11044126608664812, test loss: 0.2557064492858435\n",
      "epoch 15499: train loss: 0.11043937227261487, test loss: 0.2557073389801008\n",
      "epoch 15500: train loss: 0.11043747862484878, test loss: 0.25570822874868426\n",
      "epoch 15501: train loss: 0.11043558514332347, test loss: 0.25570911859159345\n",
      "epoch 15502: train loss: 0.11043369182801253, test loss: 0.255710008508796\n",
      "epoch 15503: train loss: 0.11043179867888962, test loss: 0.2557108985002464\n",
      "epoch 15504: train loss: 0.11042990569592838, test loss: 0.2557117885659811\n",
      "epoch 15505: train loss: 0.11042801287910242, test loss: 0.2557126787059666\n",
      "epoch 15506: train loss: 0.11042612022838544, test loss: 0.2557135689201911\n",
      "epoch 15507: train loss: 0.11042422774375103, test loss: 0.2557144592085994\n",
      "epoch 15508: train loss: 0.11042233542517288, test loss: 0.2557153495712161\n",
      "epoch 15509: train loss: 0.11042044327262468, test loss: 0.25571624000803056\n",
      "epoch 15510: train loss: 0.11041855128608008, test loss: 0.25571713051899786\n",
      "epoch 15511: train loss: 0.11041665946551273, test loss: 0.2557180211041203\n",
      "epoch 15512: train loss: 0.11041476781089635, test loss: 0.2557189117633738\n",
      "epoch 15513: train loss: 0.11041287632220463, test loss: 0.2557198024967379\n",
      "epoch 15514: train loss: 0.11041098499941125, test loss: 0.2557206933042145\n",
      "epoch 15515: train loss: 0.11040909384248991, test loss: 0.2557215841857587\n",
      "epoch 15516: train loss: 0.11040720285141431, test loss: 0.2557224751413931\n",
      "epoch 15517: train loss: 0.1104053120261582, test loss: 0.25572336617108654\n",
      "epoch 15518: train loss: 0.11040342136669526, test loss: 0.25572425727479403\n",
      "epoch 15519: train loss: 0.11040153087299924, test loss: 0.25572514845253885\n",
      "epoch 15520: train loss: 0.11039964054504386, test loss: 0.2557260397042996\n",
      "epoch 15521: train loss: 0.11039775038280286, test loss: 0.2557269310300329\n",
      "epoch 15522: train loss: 0.11039586038624996, test loss: 0.25572782242973885\n",
      "epoch 15523: train loss: 0.11039397055535898, test loss: 0.25572871390341817\n",
      "epoch 15524: train loss: 0.11039208089010355, test loss: 0.2557296054510409\n",
      "epoch 15525: train loss: 0.11039019139045755, test loss: 0.25573049707260354\n",
      "epoch 15526: train loss: 0.1103883020563947, test loss: 0.2557313887680319\n",
      "epoch 15527: train loss: 0.11038641288788878, test loss: 0.25573228053739355\n",
      "epoch 15528: train loss: 0.11038452388491356, test loss: 0.2557331723806315\n",
      "epoch 15529: train loss: 0.11038263504744282, test loss: 0.2557340642977039\n",
      "epoch 15530: train loss: 0.11038074637545034, test loss: 0.25573495628865694\n",
      "epoch 15531: train loss: 0.11037885786890994, test loss: 0.2557358483534235\n",
      "epoch 15532: train loss: 0.11037696952779542, test loss: 0.25573674049201606\n",
      "epoch 15533: train loss: 0.1103750813520806, test loss: 0.25573763270441235\n",
      "epoch 15534: train loss: 0.11037319334173926, test loss: 0.2557385249905817\n",
      "epoch 15535: train loss: 0.11037130549674522, test loss: 0.25573941735049965\n",
      "epoch 15536: train loss: 0.11036941781707232, test loss: 0.25574030978420537\n",
      "epoch 15537: train loss: 0.11036753030269442, test loss: 0.25574120229162917\n",
      "epoch 15538: train loss: 0.11036564295358532, test loss: 0.25574209487279503\n",
      "epoch 15539: train loss: 0.11036375576971888, test loss: 0.2557429875276381\n",
      "epoch 15540: train loss: 0.11036186875106893, test loss: 0.2557438802561797\n",
      "epoch 15541: train loss: 0.11035998189760936, test loss: 0.25574477305837845\n",
      "epoch 15542: train loss: 0.11035809520931399, test loss: 0.2557456659342573\n",
      "epoch 15543: train loss: 0.1103562086861567, test loss: 0.25574655888377223\n",
      "epoch 15544: train loss: 0.11035432232811139, test loss: 0.25574745190690257\n",
      "epoch 15545: train loss: 0.11035243613515189, test loss: 0.2557483450036592\n",
      "epoch 15546: train loss: 0.11035055010725214, test loss: 0.255749238174\n",
      "epoch 15547: train loss: 0.11034866424438597, test loss: 0.25575013141792646\n",
      "epoch 15548: train loss: 0.11034677854652734, test loss: 0.2557510247353811\n",
      "epoch 15549: train loss: 0.1103448930136501, test loss: 0.255751918126413\n",
      "epoch 15550: train loss: 0.11034300764572819, test loss: 0.25575281159096513\n",
      "epoch 15551: train loss: 0.11034112244273551, test loss: 0.25575370512904044\n",
      "epoch 15552: train loss: 0.11033923740464598, test loss: 0.25575459874061773\n",
      "epoch 15553: train loss: 0.11033735253143355, test loss: 0.25575549242565093\n",
      "epoch 15554: train loss: 0.1103354678230721, test loss: 0.2557563861841669\n",
      "epoch 15555: train loss: 0.11033358327953562, test loss: 0.25575728001614245\n",
      "epoch 15556: train loss: 0.11033169890079801, test loss: 0.25575817392156913\n",
      "epoch 15557: train loss: 0.11032981468683324, test loss: 0.25575906790038\n",
      "epoch 15558: train loss: 0.11032793063761527, test loss: 0.2557599619526097\n",
      "epoch 15559: train loss: 0.11032604675311809, test loss: 0.2557608560782378\n",
      "epoch 15560: train loss: 0.11032416303331559, test loss: 0.2557617502772214\n",
      "epoch 15561: train loss: 0.1103222794781818, test loss: 0.2557626445495707\n",
      "epoch 15562: train loss: 0.11032039608769068, test loss: 0.2557635388952445\n",
      "epoch 15563: train loss: 0.11031851286181622, test loss: 0.2557644333142556\n",
      "epoch 15564: train loss: 0.11031662980053242, test loss: 0.25576532780659295\n",
      "epoch 15565: train loss: 0.11031474690381327, test loss: 0.25576622237219027\n",
      "epoch 15566: train loss: 0.11031286417163272, test loss: 0.25576711701109495\n",
      "epoch 15567: train loss: 0.11031098160396485, test loss: 0.25576801172324054\n",
      "epoch 15568: train loss: 0.11030909920078366, test loss: 0.2557689065086517\n",
      "epoch 15569: train loss: 0.11030721696206318, test loss: 0.2557698013672843\n",
      "epoch 15570: train loss: 0.11030533488777737, test loss: 0.25577069629911703\n",
      "epoch 15571: train loss: 0.11030345297790031, test loss: 0.2557715913041745\n",
      "epoch 15572: train loss: 0.11030157123240607, test loss: 0.25577248638239075\n",
      "epoch 15573: train loss: 0.11029968965126859, test loss: 0.2557733815337893\n",
      "epoch 15574: train loss: 0.11029780823446203, test loss: 0.2557742767583264\n",
      "epoch 15575: train loss: 0.11029592698196038, test loss: 0.2557751720560047\n",
      "epoch 15576: train loss: 0.11029404589373769, test loss: 0.2557760674268252\n",
      "epoch 15577: train loss: 0.1102921649697681, test loss: 0.2557769628707337\n",
      "epoch 15578: train loss: 0.11029028421002561, test loss: 0.2557778583877208\n",
      "epoch 15579: train loss: 0.11028840361448435, test loss: 0.2557787539777866\n",
      "epoch 15580: train loss: 0.11028652318311832, test loss: 0.2557796496409223\n",
      "epoch 15581: train loss: 0.11028464291590172, test loss: 0.2557805453771077\n",
      "epoch 15582: train loss: 0.11028276281280858, test loss: 0.255781441186288\n",
      "epoch 15583: train loss: 0.11028088287381302, test loss: 0.2557823370684846\n",
      "epoch 15584: train loss: 0.1102790030988891, test loss: 0.2557832330236927\n",
      "epoch 15585: train loss: 0.11027712348801103, test loss: 0.2557841290518887\n",
      "epoch 15586: train loss: 0.11027524404115283, test loss: 0.2557850251530175\n",
      "epoch 15587: train loss: 0.11027336475828868, test loss: 0.25578592132710426\n",
      "epoch 15588: train loss: 0.11027148563939271, test loss: 0.2557868175741398\n",
      "epoch 15589: train loss: 0.11026960668443903, test loss: 0.25578771389407995\n",
      "epoch 15590: train loss: 0.11026772789340178, test loss: 0.25578861028689226\n",
      "epoch 15591: train loss: 0.11026584926625514, test loss: 0.25578950675262596\n",
      "epoch 15592: train loss: 0.11026397080297322, test loss: 0.2557904032912229\n",
      "epoch 15593: train loss: 0.11026209250353021, test loss: 0.25579129990266575\n",
      "epoch 15594: train loss: 0.11026021436790029, test loss: 0.25579219658695473\n",
      "epoch 15595: train loss: 0.11025833639605759, test loss: 0.2557930933440702\n",
      "epoch 15596: train loss: 0.11025645858797634, test loss: 0.2557939901739779\n",
      "epoch 15597: train loss: 0.11025458094363066, test loss: 0.2557948870766802\n",
      "epoch 15598: train loss: 0.11025270346299479, test loss: 0.25579578405214715\n",
      "epoch 15599: train loss: 0.11025082614604287, test loss: 0.2557966811004021\n",
      "epoch 15600: train loss: 0.11024894899274916, test loss: 0.2557975782213783\n",
      "epoch 15601: train loss: 0.11024707200308782, test loss: 0.2557984754151018\n",
      "epoch 15602: train loss: 0.11024519517703307, test loss: 0.2557993726815285\n",
      "epoch 15603: train loss: 0.11024331851455915, test loss: 0.25580027002064815\n",
      "epoch 15604: train loss: 0.11024144201564029, test loss: 0.2558011674324409\n",
      "epoch 15605: train loss: 0.11023956568025067, test loss: 0.2558020649169218\n",
      "epoch 15606: train loss: 0.11023768950836459, test loss: 0.2558029624740456\n",
      "epoch 15607: train loss: 0.1102358134999562, test loss: 0.25580386010379125\n",
      "epoch 15608: train loss: 0.11023393765499984, test loss: 0.2558047578061618\n",
      "epoch 15609: train loss: 0.11023206197346971, test loss: 0.2558056555811479\n",
      "epoch 15610: train loss: 0.11023018645534007, test loss: 0.25580655342870573\n",
      "epoch 15611: train loss: 0.1102283111005852, test loss: 0.2558074513488502\n",
      "epoch 15612: train loss: 0.11022643590917937, test loss: 0.25580834934153546\n",
      "epoch 15613: train loss: 0.11022456088109685, test loss: 0.2558092474067889\n",
      "epoch 15614: train loss: 0.1102226860163119, test loss: 0.2558101455445322\n",
      "epoch 15615: train loss: 0.11022081131479883, test loss: 0.25581104375481273\n",
      "epoch 15616: train loss: 0.11021893677653193, test loss: 0.2558119420375754\n",
      "epoch 15617: train loss: 0.1102170624014855, test loss: 0.2558128403928244\n",
      "epoch 15618: train loss: 0.11021518818963383, test loss: 0.25581373882052516\n",
      "epoch 15619: train loss: 0.11021331414095123, test loss: 0.255814637320682\n",
      "epoch 15620: train loss: 0.11021144025541205, test loss: 0.2558155358932855\n",
      "epoch 15621: train loss: 0.11020956653299056, test loss: 0.2558164345382911\n",
      "epoch 15622: train loss: 0.11020769297366112, test loss: 0.25581733325569156\n",
      "epoch 15623: train loss: 0.11020581957739806, test loss: 0.25581823204548804\n",
      "epoch 15624: train loss: 0.1102039463441757, test loss: 0.25581913090763586\n",
      "epoch 15625: train loss: 0.11020207327396843, test loss: 0.25582002984216234\n",
      "epoch 15626: train loss: 0.11020020036675053, test loss: 0.2558209288490359\n",
      "epoch 15627: train loss: 0.11019832762249643, test loss: 0.25582182792820085\n",
      "epoch 15628: train loss: 0.11019645504118043, test loss: 0.25582272707968223\n",
      "epoch 15629: train loss: 0.11019458262277693, test loss: 0.2558236263034587\n",
      "epoch 15630: train loss: 0.1101927103672603, test loss: 0.2558245255995358\n",
      "epoch 15631: train loss: 0.1101908382746049, test loss: 0.25582542496784455\n",
      "epoch 15632: train loss: 0.11018896634478514, test loss: 0.2558263244084113\n",
      "epoch 15633: train loss: 0.1101870945777754, test loss: 0.25582722392119295\n",
      "epoch 15634: train loss: 0.11018522297355006, test loss: 0.25582812350621464\n",
      "epoch 15635: train loss: 0.11018335153208354, test loss: 0.2558290231634108\n",
      "epoch 15636: train loss: 0.11018148025335027, test loss: 0.25582992289280526\n",
      "epoch 15637: train loss: 0.11017960913732462, test loss: 0.25583082269437907\n",
      "epoch 15638: train loss: 0.11017773818398101, test loss: 0.25583172256807707\n",
      "epoch 15639: train loss: 0.11017586739329388, test loss: 0.2558326225139372\n",
      "epoch 15640: train loss: 0.1101739967652377, test loss: 0.25583352253191377\n",
      "epoch 15641: train loss: 0.11017212629978683, test loss: 0.25583442262197664\n",
      "epoch 15642: train loss: 0.11017025599691575, test loss: 0.2558353227841521\n",
      "epoch 15643: train loss: 0.1101683858565989, test loss: 0.25583622301840775\n",
      "epoch 15644: train loss: 0.11016651587881075, test loss: 0.25583712332470027\n",
      "epoch 15645: train loss: 0.11016464606352576, test loss: 0.2558380237030427\n",
      "epoch 15646: train loss: 0.11016277641071838, test loss: 0.2558389241534165\n",
      "epoch 15647: train loss: 0.11016090692036308, test loss: 0.25583982467582383\n",
      "epoch 15648: train loss: 0.11015903759243433, test loss: 0.25584072527019786\n",
      "epoch 15649: train loss: 0.11015716842690663, test loss: 0.2558416259365884\n",
      "epoch 15650: train loss: 0.11015529942375445, test loss: 0.25584252667492874\n",
      "epoch 15651: train loss: 0.1101534305829523, test loss: 0.25584342748522054\n",
      "epoch 15652: train loss: 0.11015156190447467, test loss: 0.25584432836744514\n",
      "epoch 15653: train loss: 0.11014969338829607, test loss: 0.255845229321581\n",
      "epoch 15654: train loss: 0.11014782503439101, test loss: 0.25584613034765574\n",
      "epoch 15655: train loss: 0.11014595684273397, test loss: 0.25584703144558985\n",
      "epoch 15656: train loss: 0.11014408881329953, test loss: 0.25584793261541044\n",
      "epoch 15657: train loss: 0.11014222094606219, test loss: 0.2558488338570854\n",
      "epoch 15658: train loss: 0.11014035324099648, test loss: 0.25584973517060566\n",
      "epoch 15659: train loss: 0.11013848569807694, test loss: 0.25585063655595197\n",
      "epoch 15660: train loss: 0.11013661831727813, test loss: 0.2558515380131151\n",
      "epoch 15661: train loss: 0.11013475109857458, test loss: 0.2558524395420637\n",
      "epoch 15662: train loss: 0.11013288404194084, test loss: 0.2558533411428227\n",
      "epoch 15663: train loss: 0.1101310171473515, test loss: 0.2558542428153281\n",
      "epoch 15664: train loss: 0.11012915041478111, test loss: 0.255855144559569\n",
      "epoch 15665: train loss: 0.11012728384420424, test loss: 0.25585604637558423\n",
      "epoch 15666: train loss: 0.1101254174355955, test loss: 0.2558569482632844\n",
      "epoch 15667: train loss: 0.11012355118892941, test loss: 0.2558578502227069\n",
      "epoch 15668: train loss: 0.11012168510418062, test loss: 0.2558587522538439\n",
      "epoch 15669: train loss: 0.1101198191813237, test loss: 0.25585965435662783\n",
      "epoch 15670: train loss: 0.11011795342033327, test loss: 0.2558605565310515\n",
      "epoch 15671: train loss: 0.1101160878211839, test loss: 0.25586145877713945\n",
      "epoch 15672: train loss: 0.11011422238385025, test loss: 0.2558623610948741\n",
      "epoch 15673: train loss: 0.1101123571083069, test loss: 0.25586326348418664\n",
      "epoch 15674: train loss: 0.11011049199452849, test loss: 0.2558641659451294\n",
      "epoch 15675: train loss: 0.11010862704248964, test loss: 0.25586506847763374\n",
      "epoch 15676: train loss: 0.11010676225216502, test loss: 0.2558659710816903\n",
      "epoch 15677: train loss: 0.11010489762352922, test loss: 0.2558668737573293\n",
      "epoch 15678: train loss: 0.11010303315655692, test loss: 0.255867776504492\n",
      "epoch 15679: train loss: 0.11010116885122276, test loss: 0.2558686793231842\n",
      "epoch 15680: train loss: 0.11009930470750141, test loss: 0.25586958221337375\n",
      "epoch 15681: train loss: 0.1100974407253675, test loss: 0.25587048517506333\n",
      "epoch 15682: train loss: 0.11009557690479578, test loss: 0.25587138820821065\n",
      "epoch 15683: train loss: 0.11009371324576084, test loss: 0.25587229131281797\n",
      "epoch 15684: train loss: 0.11009184974823738, test loss: 0.25587319448886625\n",
      "epoch 15685: train loss: 0.11008998641220011, test loss: 0.25587409773637\n",
      "epoch 15686: train loss: 0.11008812323762368, test loss: 0.2558750010552626\n",
      "epoch 15687: train loss: 0.11008626022448287, test loss: 0.2558759044455605\n",
      "epoch 15688: train loss: 0.11008439737275232, test loss: 0.2558768079072418\n",
      "epoch 15689: train loss: 0.11008253468240671, test loss: 0.2558777114402985\n",
      "epoch 15690: train loss: 0.11008067215342084, test loss: 0.25587861504468973\n",
      "epoch 15691: train loss: 0.11007880978576935, test loss: 0.2558795187204403\n",
      "epoch 15692: train loss: 0.11007694757942701, test loss: 0.25588042246747206\n",
      "epoch 15693: train loss: 0.11007508553436854, test loss: 0.25588132628585897\n",
      "epoch 15694: train loss: 0.11007322365056871, test loss: 0.25588223017549944\n",
      "epoch 15695: train loss: 0.1100713619280022, test loss: 0.2558831341364441\n",
      "epoch 15696: train loss: 0.1100695003666438, test loss: 0.25588403816863703\n",
      "epoch 15697: train loss: 0.11006763896646826, test loss: 0.2558849422720826\n",
      "epoch 15698: train loss: 0.11006577772745033, test loss: 0.2558858464467372\n",
      "epoch 15699: train loss: 0.11006391664956477, test loss: 0.2558867506926291\n",
      "epoch 15700: train loss: 0.1100620557327864, test loss: 0.2558876550097132\n",
      "epoch 15701: train loss: 0.11006019497708992, test loss: 0.2558885593979593\n",
      "epoch 15702: train loss: 0.11005833438245016, test loss: 0.255889463857394\n",
      "epoch 15703: train loss: 0.11005647394884192, test loss: 0.25589036838799833\n",
      "epoch 15704: train loss: 0.11005461367623993, test loss: 0.2558912729897048\n",
      "epoch 15705: train loss: 0.11005275356461906, test loss: 0.25589217766256417\n",
      "epoch 15706: train loss: 0.11005089361395408, test loss: 0.25589308240653397\n",
      "epoch 15707: train loss: 0.1100490338242198, test loss: 0.2558939872215696\n",
      "epoch 15708: train loss: 0.11004717419539105, test loss: 0.25589489210769906\n",
      "epoch 15709: train loss: 0.11004531472744265, test loss: 0.2558957970648921\n",
      "epoch 15710: train loss: 0.1100434554203494, test loss: 0.2558967020931276\n",
      "epoch 15711: train loss: 0.11004159627408619, test loss: 0.2558976071923973\n",
      "epoch 15712: train loss: 0.11003973728862779, test loss: 0.25589851236269395\n",
      "epoch 15713: train loss: 0.1100378784639491, test loss: 0.25589941760396345\n",
      "epoch 15714: train loss: 0.11003601980002492, test loss: 0.25590032291624387\n",
      "epoch 15715: train loss: 0.11003416129683014, test loss: 0.25590122829949236\n",
      "epoch 15716: train loss: 0.11003230295433962, test loss: 0.25590213375367715\n",
      "epoch 15717: train loss: 0.11003044477252821, test loss: 0.25590303927882635\n",
      "epoch 15718: train loss: 0.1100285867513708, test loss: 0.2559039448748965\n",
      "epoch 15719: train loss: 0.11002672889084225, test loss: 0.2559048505418676\n",
      "epoch 15720: train loss: 0.11002487119091744, test loss: 0.2559057562797337\n",
      "epoch 15721: train loss: 0.1100230136515713, test loss: 0.2559066620884726\n",
      "epoch 15722: train loss: 0.1100211562727787, test loss: 0.2559075679681013\n",
      "epoch 15723: train loss: 0.11001929905451452, test loss: 0.2559084739185411\n",
      "epoch 15724: train loss: 0.11001744199675369, test loss: 0.2559093799398438\n",
      "epoch 15725: train loss: 0.1100155850994711, test loss: 0.25591028603196536\n",
      "epoch 15726: train loss: 0.11001372836264167, test loss: 0.2559111921948747\n",
      "epoch 15727: train loss: 0.11001187178624035, test loss: 0.2559120984285776\n",
      "epoch 15728: train loss: 0.11001001537024205, test loss: 0.25591300473307554\n",
      "epoch 15729: train loss: 0.1100081591146217, test loss: 0.2559139111083028\n",
      "epoch 15730: train loss: 0.11000630301935424, test loss: 0.2559148175542881\n",
      "epoch 15731: train loss: 0.11000444708441462, test loss: 0.2559157240709876\n",
      "epoch 15732: train loss: 0.1100025913097778, test loss: 0.2559166306584274\n",
      "epoch 15733: train loss: 0.11000073569541868, test loss: 0.25591753731653166\n",
      "epoch 15734: train loss: 0.10999888024131231, test loss: 0.2559184440453505\n",
      "epoch 15735: train loss: 0.1099970249474336, test loss: 0.2559193508448178\n",
      "epoch 15736: train loss: 0.10999516981375754, test loss: 0.2559202577149376\n",
      "epoch 15737: train loss: 0.10999331484025911, test loss: 0.25592116465569026\n",
      "epoch 15738: train loss: 0.10999146002691326, test loss: 0.25592207166706726\n",
      "epoch 15739: train loss: 0.10998960537369502, test loss: 0.25592297874905157\n",
      "epoch 15740: train loss: 0.10998775088057937, test loss: 0.2559238859016443\n",
      "epoch 15741: train loss: 0.1099858965475413, test loss: 0.25592479312480576\n",
      "epoch 15742: train loss: 0.10998404237455589, test loss: 0.25592570041851376\n",
      "epoch 15743: train loss: 0.10998218836159802, test loss: 0.2559266077827623\n",
      "epoch 15744: train loss: 0.10998033450864281, test loss: 0.25592751521757917\n",
      "epoch 15745: train loss: 0.10997848081566523, test loss: 0.2559284227228741\n",
      "epoch 15746: train loss: 0.10997662728264036, test loss: 0.25592933029869835\n",
      "epoch 15747: train loss: 0.10997477390954316, test loss: 0.2559302379449856\n",
      "epoch 15748: train loss: 0.10997292069634873, test loss: 0.25593114566175207\n",
      "epoch 15749: train loss: 0.10997106764303209, test loss: 0.2559320534489787\n",
      "epoch 15750: train loss: 0.10996921474956832, test loss: 0.2559329613066446\n",
      "epoch 15751: train loss: 0.10996736201593243, test loss: 0.2559338692347326\n",
      "epoch 15752: train loss: 0.1099655094420995, test loss: 0.25593477723324587\n",
      "epoch 15753: train loss: 0.10996365702804463, test loss: 0.25593568530213007\n",
      "epoch 15754: train loss: 0.10996180477374284, test loss: 0.25593659344141323\n",
      "epoch 15755: train loss: 0.10995995267916925, test loss: 0.2559375016510285\n",
      "epoch 15756: train loss: 0.1099581007442989, test loss: 0.25593840993101663\n",
      "epoch 15757: train loss: 0.10995624896910694, test loss: 0.25593931828137034\n",
      "epoch 15758: train loss: 0.10995439735356839, test loss: 0.2559402267019979\n",
      "epoch 15759: train loss: 0.10995254589765842, test loss: 0.2559411351929638\n",
      "epoch 15760: train loss: 0.10995069460135211, test loss: 0.2559420437542027\n",
      "epoch 15761: train loss: 0.10994884346462457, test loss: 0.2559429523857047\n",
      "epoch 15762: train loss: 0.10994699248745089, test loss: 0.2559438610874772\n",
      "epoch 15763: train loss: 0.10994514166980622, test loss: 0.25594476985949854\n",
      "epoch 15764: train loss: 0.10994329101166571, test loss: 0.25594567870175133\n",
      "epoch 15765: train loss: 0.10994144051300445, test loss: 0.25594658761421507\n",
      "epoch 15766: train loss: 0.10993959017379762, test loss: 0.25594749659688304\n",
      "epoch 15767: train loss: 0.1099377399940203, test loss: 0.2559484056497133\n",
      "epoch 15768: train loss: 0.10993588997364773, test loss: 0.25594931477273375\n",
      "epoch 15769: train loss: 0.10993404011265501, test loss: 0.2559502239659242\n",
      "epoch 15770: train loss: 0.10993219041101727, test loss: 0.2559511332292434\n",
      "epoch 15771: train loss: 0.10993034086870977, test loss: 0.25595204256265935\n",
      "epoch 15772: train loss: 0.10992849148570759, test loss: 0.25595295196622425\n",
      "epoch 15773: train loss: 0.109926642261986, test loss: 0.25595386143984866\n",
      "epoch 15774: train loss: 0.10992479319752006, test loss: 0.25595477098357206\n",
      "epoch 15775: train loss: 0.10992294429228508, test loss: 0.255955680597364\n",
      "epoch 15776: train loss: 0.10992109554625619, test loss: 0.25595659028120527\n",
      "epoch 15777: train loss: 0.10991924695940859, test loss: 0.2559575000350521\n",
      "epoch 15778: train loss: 0.1099173985317175, test loss: 0.25595840985893437\n",
      "epoch 15779: train loss: 0.10991555026315815, test loss: 0.25595931975284547\n",
      "epoch 15780: train loss: 0.10991370215370572, test loss: 0.25596022971670573\n",
      "epoch 15781: train loss: 0.10991185420333544, test loss: 0.25596113975059126\n",
      "epoch 15782: train loss: 0.10991000641202256, test loss: 0.2559620498543895\n",
      "epoch 15783: train loss: 0.10990815877974228, test loss: 0.25596296002816293\n",
      "epoch 15784: train loss: 0.10990631130646986, test loss: 0.2559638702718455\n",
      "epoch 15785: train loss: 0.10990446399218054, test loss: 0.255964780585442\n",
      "epoch 15786: train loss: 0.10990261683684958, test loss: 0.2559656909689581\n",
      "epoch 15787: train loss: 0.1099007698404522, test loss: 0.2559666014223394\n",
      "epoch 15788: train loss: 0.10989892300296371, test loss: 0.2559675119456125\n",
      "epoch 15789: train loss: 0.10989707632435934, test loss: 0.2559684225387134\n",
      "epoch 15790: train loss: 0.10989522980461437, test loss: 0.25596933320165777\n",
      "epoch 15791: train loss: 0.10989338344370408, test loss: 0.2559702439344503\n",
      "epoch 15792: train loss: 0.10989153724160375, test loss: 0.2559711547370246\n",
      "epoch 15793: train loss: 0.10988969119828866, test loss: 0.25597206560941027\n",
      "epoch 15794: train loss: 0.1098878453137341, test loss: 0.25597297655158735\n",
      "epoch 15795: train loss: 0.1098859995879154, test loss: 0.25597388756350165\n",
      "epoch 15796: train loss: 0.10988415402080783, test loss: 0.25597479864520584\n",
      "epoch 15797: train loss: 0.10988230861238671, test loss: 0.2559757097966099\n",
      "epoch 15798: train loss: 0.10988046336262737, test loss: 0.25597662101775465\n",
      "epoch 15799: train loss: 0.10987861827150511, test loss: 0.25597753230857384\n",
      "epoch 15800: train loss: 0.10987677333899526, test loss: 0.2559784436691188\n",
      "epoch 15801: train loss: 0.10987492856507317, test loss: 0.2559793550993247\n",
      "epoch 15802: train loss: 0.10987308394971416, test loss: 0.2559802665991845\n",
      "epoch 15803: train loss: 0.10987123949289357, test loss: 0.2559811781687022\n",
      "epoch 15804: train loss: 0.10986939519458673, test loss: 0.25598208980785936\n",
      "epoch 15805: train loss: 0.10986755105476906, test loss: 0.25598300151660264\n",
      "epoch 15806: train loss: 0.10986570707341585, test loss: 0.255983913294984\n",
      "epoch 15807: train loss: 0.1098638632505025, test loss: 0.25598482514291276\n",
      "epoch 15808: train loss: 0.10986201958600436, test loss: 0.2559857370604424\n",
      "epoch 15809: train loss: 0.1098601760798968, test loss: 0.2559866490475182\n",
      "epoch 15810: train loss: 0.10985833273215523, test loss: 0.2559875611041455\n",
      "epoch 15811: train loss: 0.10985648954275504, test loss: 0.25598847323028184\n",
      "epoch 15812: train loss: 0.10985464651167158, test loss: 0.255989385425944\n",
      "epoch 15813: train loss: 0.10985280363888027, test loss: 0.25599029769109016\n",
      "epoch 15814: train loss: 0.1098509609243565, test loss: 0.2559912100257241\n",
      "epoch 15815: train loss: 0.10984911836807573, test loss: 0.255992122429829\n",
      "epoch 15816: train loss: 0.10984727597001331, test loss: 0.2559930349033966\n",
      "epoch 15817: train loss: 0.10984543373014467, test loss: 0.25599394744638637\n",
      "epoch 15818: train loss: 0.10984359164844527, test loss: 0.2559948600588015\n",
      "epoch 15819: train loss: 0.10984174972489051, test loss: 0.2559957727406126\n",
      "epoch 15820: train loss: 0.10983990795945582, test loss: 0.2559966854918487\n",
      "epoch 15821: train loss: 0.10983806635211664, test loss: 0.25599759831244334\n",
      "epoch 15822: train loss: 0.10983622490284846, test loss: 0.25599851120240225\n",
      "epoch 15823: train loss: 0.10983438361162669, test loss: 0.25599942416169275\n",
      "epoch 15824: train loss: 0.1098325424784268, test loss: 0.2560003371903482\n",
      "epoch 15825: train loss: 0.10983070150322423, test loss: 0.25600125028832305\n",
      "epoch 15826: train loss: 0.10982886068599448, test loss: 0.2560021634555629\n",
      "epoch 15827: train loss: 0.10982702002671299, test loss: 0.2560030766921224\n",
      "epoch 15828: train loss: 0.10982517952535527, test loss: 0.25600398999794727\n",
      "epoch 15829: train loss: 0.1098233391818968, test loss: 0.2560049033730421\n",
      "epoch 15830: train loss: 0.10982149899631306, test loss: 0.2560058168173645\n",
      "epoch 15831: train loss: 0.10981965896857954, test loss: 0.2560067303309435\n",
      "epoch 15832: train loss: 0.10981781909867173, test loss: 0.25600764391371306\n",
      "epoch 15833: train loss: 0.10981597938656516, test loss: 0.2560085575656923\n",
      "epoch 15834: train loss: 0.10981413983223537, test loss: 0.25600947128686025\n",
      "epoch 15835: train loss: 0.1098123004356578, test loss: 0.25601038507718776\n",
      "epoch 15836: train loss: 0.10981046119680801, test loss: 0.2560112989366805\n",
      "epoch 15837: train loss: 0.10980862211566154, test loss: 0.2560122128653191\n",
      "epoch 15838: train loss: 0.10980678319219393, test loss: 0.25601312686307454\n",
      "epoch 15839: train loss: 0.10980494442638068, test loss: 0.25601404092995134\n",
      "epoch 15840: train loss: 0.10980310581819737, test loss: 0.2560149550659079\n",
      "epoch 15841: train loss: 0.10980126736761951, test loss: 0.2560158692709608\n",
      "epoch 15842: train loss: 0.1097994290746227, test loss: 0.25601678354509305\n",
      "epoch 15843: train loss: 0.10979759093918247, test loss: 0.2560176978882374\n",
      "epoch 15844: train loss: 0.10979575296127438, test loss: 0.25601861230045947\n",
      "epoch 15845: train loss: 0.10979391514087401, test loss: 0.2560195267816932\n",
      "epoch 15846: train loss: 0.10979207747795694, test loss: 0.25602044133192053\n",
      "epoch 15847: train loss: 0.10979023997249877, test loss: 0.2560213559511699\n",
      "epoch 15848: train loss: 0.10978840262447505, test loss: 0.25602227063936517\n",
      "epoch 15849: train loss: 0.10978656543386141, test loss: 0.2560231853965583\n",
      "epoch 15850: train loss: 0.1097847284006334, test loss: 0.2560241002226833\n",
      "epoch 15851: train loss: 0.10978289152476668, test loss: 0.25602501511773446\n",
      "epoch 15852: train loss: 0.1097810548062368, test loss: 0.2560259300817042\n",
      "epoch 15853: train loss: 0.10977921824501942, test loss: 0.2560268451145995\n",
      "epoch 15854: train loss: 0.10977738184109014, test loss: 0.2560277602163902\n",
      "epoch 15855: train loss: 0.10977554559442457, test loss: 0.2560286753870216\n",
      "epoch 15856: train loss: 0.10977370950499836, test loss: 0.2560295906265359\n",
      "epoch 15857: train loss: 0.10977187357278716, test loss: 0.2560305059348908\n",
      "epoch 15858: train loss: 0.10977003779776658, test loss: 0.25603142131209183\n",
      "epoch 15859: train loss: 0.10976820217991225, test loss: 0.2560323367580849\n",
      "epoch 15860: train loss: 0.10976636671919991, test loss: 0.2560332522728875\n",
      "epoch 15861: train loss: 0.1097645314156051, test loss: 0.25603416785649413\n",
      "epoch 15862: train loss: 0.10976269626910358, test loss: 0.25603508350886167\n",
      "epoch 15863: train loss: 0.10976086127967094, test loss: 0.2560359992300098\n",
      "epoch 15864: train loss: 0.10975902644728291, test loss: 0.25603691501985787\n",
      "epoch 15865: train loss: 0.10975719177191515, test loss: 0.25603783087846244\n",
      "epoch 15866: train loss: 0.10975535725354334, test loss: 0.2560387468057791\n",
      "epoch 15867: train loss: 0.10975352289214316, test loss: 0.25603966280180224\n",
      "epoch 15868: train loss: 0.10975168868769032, test loss: 0.256040578866491\n",
      "epoch 15869: train loss: 0.1097498546401605, test loss: 0.25604149499984996\n",
      "epoch 15870: train loss: 0.10974802074952944, test loss: 0.25604241120187476\n",
      "epoch 15871: train loss: 0.10974618701577282, test loss: 0.25604332747254477\n",
      "epoch 15872: train loss: 0.10974435343886636, test loss: 0.25604424381184426\n",
      "epoch 15873: train loss: 0.1097425200187858, test loss: 0.2560451602197417\n",
      "epoch 15874: train loss: 0.10974068675550684, test loss: 0.25604607669624435\n",
      "epoch 15875: train loss: 0.10973885364900525, test loss: 0.25604699324133323\n",
      "epoch 15876: train loss: 0.10973702069925674, test loss: 0.25604790985495324\n",
      "epoch 15877: train loss: 0.10973518790623706, test loss: 0.25604882653715\n",
      "epoch 15878: train loss: 0.10973335526992195, test loss: 0.25604974328790187\n",
      "epoch 15879: train loss: 0.10973152279028714, test loss: 0.25605066010716804\n",
      "epoch 15880: train loss: 0.10972969046730842, test loss: 0.2560515769949317\n",
      "epoch 15881: train loss: 0.10972785830096157, test loss: 0.25605249395118546\n",
      "epoch 15882: train loss: 0.10972602629122234, test loss: 0.25605341097593515\n",
      "epoch 15883: train loss: 0.10972419443806651, test loss: 0.25605432806916506\n",
      "epoch 15884: train loss: 0.10972236274146983, test loss: 0.2560552452308185\n",
      "epoch 15885: train loss: 0.10972053120140812, test loss: 0.2560561624609029\n",
      "epoch 15886: train loss: 0.10971869981785716, test loss: 0.25605707975941266\n",
      "epoch 15887: train loss: 0.10971686859079276, test loss: 0.25605799712634136\n",
      "epoch 15888: train loss: 0.10971503752019068, test loss: 0.25605891456165936\n",
      "epoch 15889: train loss: 0.10971320660602679, test loss: 0.2560598320653493\n",
      "epoch 15890: train loss: 0.10971137584827684, test loss: 0.2560607496373914\n",
      "epoch 15891: train loss: 0.10970954524691667, test loss: 0.2560616672778177\n",
      "epoch 15892: train loss: 0.10970771480192214, test loss: 0.25606258498653767\n",
      "epoch 15893: train loss: 0.10970588451326903, test loss: 0.2560635027635944\n",
      "epoch 15894: train loss: 0.10970405438093317, test loss: 0.25606442060894347\n",
      "epoch 15895: train loss: 0.10970222440489046, test loss: 0.25606533852259444\n",
      "epoch 15896: train loss: 0.10970039458511666, test loss: 0.25606625650452647\n",
      "epoch 15897: train loss: 0.1096985649215877, test loss: 0.256067174554712\n",
      "epoch 15898: train loss: 0.10969673541427935, test loss: 0.25606809267314484\n",
      "epoch 15899: train loss: 0.10969490606316754, test loss: 0.2560690108598068\n",
      "epoch 15900: train loss: 0.1096930768682281, test loss: 0.2560699291146788\n",
      "epoch 15901: train loss: 0.10969124782943694, test loss: 0.2560708474377698\n",
      "epoch 15902: train loss: 0.10968941894676988, test loss: 0.25607176582902397\n",
      "epoch 15903: train loss: 0.10968759022020286, test loss: 0.2560726842884847\n",
      "epoch 15904: train loss: 0.10968576164971172, test loss: 0.25607360281608593\n",
      "epoch 15905: train loss: 0.10968393323527234, test loss: 0.25607452141183373\n",
      "epoch 15906: train loss: 0.10968210497686069, test loss: 0.25607544007569844\n",
      "epoch 15907: train loss: 0.10968027687445261, test loss: 0.2560763588076867\n",
      "epoch 15908: train loss: 0.10967844892802404, test loss: 0.25607727760778043\n",
      "epoch 15909: train loss: 0.10967662113755085, test loss: 0.2560781964759499\n",
      "epoch 15910: train loss: 0.109674793503009, test loss: 0.2560791154121903\n",
      "epoch 15911: train loss: 0.1096729660243744, test loss: 0.25608003441650723\n",
      "epoch 15912: train loss: 0.10967113870162297, test loss: 0.25608095348883575\n",
      "epoch 15913: train loss: 0.1096693115347307, test loss: 0.25608187262921683\n",
      "epoch 15914: train loss: 0.10966748452367345, test loss: 0.2560827918376091\n",
      "epoch 15915: train loss: 0.10966565766842719, test loss: 0.2560837111139975\n",
      "epoch 15916: train loss: 0.1096638309689679, test loss: 0.2560846304583485\n",
      "epoch 15917: train loss: 0.10966200442527149, test loss: 0.2560855498706837\n",
      "epoch 15918: train loss: 0.10966017803731397, test loss: 0.25608646935098456\n",
      "epoch 15919: train loss: 0.10965835180507127, test loss: 0.25608738889920885\n",
      "epoch 15920: train loss: 0.10965652572851939, test loss: 0.2560883085153881\n",
      "epoch 15921: train loss: 0.10965469980763426, test loss: 0.2560892281994438\n",
      "epoch 15922: train loss: 0.10965287404239192, test loss: 0.2560901479514186\n",
      "epoch 15923: train loss: 0.1096510484327683, test loss: 0.2560910677712714\n",
      "epoch 15924: train loss: 0.10964922297873947, test loss: 0.25609198765898356\n",
      "epoch 15925: train loss: 0.10964739768028135, test loss: 0.2560929076145516\n",
      "epoch 15926: train loss: 0.10964557253736996, test loss: 0.2560938276379803\n",
      "epoch 15927: train loss: 0.10964374754998135, test loss: 0.25609474772922947\n",
      "epoch 15928: train loss: 0.10964192271809148, test loss: 0.25609566788825644\n",
      "epoch 15929: train loss: 0.10964009804167642, test loss: 0.2560965881151156\n",
      "epoch 15930: train loss: 0.10963827352071213, test loss: 0.2560975084097185\n",
      "epoch 15931: train loss: 0.10963644915517472, test loss: 0.25609842877214417\n",
      "epoch 15932: train loss: 0.10963462494504014, test loss: 0.25609934920227784\n",
      "epoch 15933: train loss: 0.10963280089028452, test loss: 0.2561002697001502\n",
      "epoch 15934: train loss: 0.10963097699088382, test loss: 0.25610119026576916\n",
      "epoch 15935: train loss: 0.10962915324681416, test loss: 0.2561021108990668\n",
      "epoch 15936: train loss: 0.10962732965805154, test loss: 0.25610303160008924\n",
      "epoch 15937: train loss: 0.10962550622457207, test loss: 0.2561039523687673\n",
      "epoch 15938: train loss: 0.10962368294635176, test loss: 0.2561048732051225\n",
      "epoch 15939: train loss: 0.10962185982336674, test loss: 0.25610579410913575\n",
      "epoch 15940: train loss: 0.10962003685559306, test loss: 0.25610671508079164\n",
      "epoch 15941: train loss: 0.10961821404300677, test loss: 0.25610763612005955\n",
      "epoch 15942: train loss: 0.10961639138558402, test loss: 0.2561085572269085\n",
      "epoch 15943: train loss: 0.10961456888330087, test loss: 0.2561094784013969\n",
      "epoch 15944: train loss: 0.10961274653613341, test loss: 0.2561103996434332\n",
      "epoch 15945: train loss: 0.10961092434405774, test loss: 0.25611132095305905\n",
      "epoch 15946: train loss: 0.10960910230705, test loss: 0.2561122423302109\n",
      "epoch 15947: train loss: 0.10960728042508629, test loss: 0.2561131637749076\n",
      "epoch 15948: train loss: 0.10960545869814271, test loss: 0.25611408528713037\n",
      "epoch 15949: train loss: 0.10960363712619543, test loss: 0.25611500686686406\n",
      "epoch 15950: train loss: 0.10960181570922051, test loss: 0.2561159285140657\n",
      "epoch 15951: train loss: 0.10959999444719415, test loss: 0.25611685022876624\n",
      "epoch 15952: train loss: 0.10959817334009245, test loss: 0.2561177720109253\n",
      "epoch 15953: train loss: 0.10959635238789156, test loss: 0.2561186938605368\n",
      "epoch 15954: train loss: 0.10959453159056765, test loss: 0.25611961577758424\n",
      "epoch 15955: train loss: 0.10959271094809686, test loss: 0.25612053776205\n",
      "epoch 15956: train loss: 0.10959089046045536, test loss: 0.25612145981392914\n",
      "epoch 15957: train loss: 0.1095890701276193, test loss: 0.25612238193319237\n",
      "epoch 15958: train loss: 0.10958724994956487, test loss: 0.2561233041198216\n",
      "epoch 15959: train loss: 0.10958542992626821, test loss: 0.25612422637384946\n",
      "epoch 15960: train loss: 0.10958361005770556, test loss: 0.2561251486951976\n",
      "epoch 15961: train loss: 0.10958179034385308, test loss: 0.2561260710838976\n",
      "epoch 15962: train loss: 0.10957997078468694, test loss: 0.256126993539896\n",
      "epoch 15963: train loss: 0.10957815138018337, test loss: 0.25612791606321056\n",
      "epoch 15964: train loss: 0.10957633213031853, test loss: 0.25612883865381375\n",
      "epoch 15965: train loss: 0.10957451303506868, test loss: 0.25612976131170084\n",
      "epoch 15966: train loss: 0.10957269409441, test loss: 0.2561306840368643\n",
      "epoch 15967: train loss: 0.10957087530831874, test loss: 0.2561316068292421\n",
      "epoch 15968: train loss: 0.1095690566767711, test loss: 0.25613252968887507\n",
      "epoch 15969: train loss: 0.10956723819974329, test loss: 0.2561334526157345\n",
      "epoch 15970: train loss: 0.10956541987721156, test loss: 0.2561343756097797\n",
      "epoch 15971: train loss: 0.10956360170915218, test loss: 0.2561352986710175\n",
      "epoch 15972: train loss: 0.10956178369554134, test loss: 0.25613622179944373\n",
      "epoch 15973: train loss: 0.10955996583635534, test loss: 0.25613714499503987\n",
      "epoch 15974: train loss: 0.1095581481315704, test loss: 0.2561380682577651\n",
      "epoch 15975: train loss: 0.1095563305811628, test loss: 0.25613899158762743\n",
      "epoch 15976: train loss: 0.10955451318510878, test loss: 0.25613991498460964\n",
      "epoch 15977: train loss: 0.10955269594338464, test loss: 0.25614083844871843\n",
      "epoch 15978: train loss: 0.10955087885596664, test loss: 0.2561417619799011\n",
      "epoch 15979: train loss: 0.10954906192283108, test loss: 0.2561426855781644\n",
      "epoch 15980: train loss: 0.1095472451439542, test loss: 0.2561436092434795\n",
      "epoch 15981: train loss: 0.10954542851931234, test loss: 0.2561445329758411\n",
      "epoch 15982: train loss: 0.10954361204888176, test loss: 0.2561454567752329\n",
      "epoch 15983: train loss: 0.10954179573263881, test loss: 0.2561463806416615\n",
      "epoch 15984: train loss: 0.10953997957055975, test loss: 0.2561473045751106\n",
      "epoch 15985: train loss: 0.1095381635626209, test loss: 0.2561482285755273\n",
      "epoch 15986: train loss: 0.10953634770879858, test loss: 0.25614915264290483\n",
      "epoch 15987: train loss: 0.10953453200906914, test loss: 0.25615007677726576\n",
      "epoch 15988: train loss: 0.10953271646340887, test loss: 0.25615100097856613\n",
      "epoch 15989: train loss: 0.10953090107179414, test loss: 0.25615192524681474\n",
      "epoch 15990: train loss: 0.10952908583420122, test loss: 0.2561528495819583\n",
      "epoch 15991: train loss: 0.10952727075060655, test loss: 0.25615377398402767\n",
      "epoch 15992: train loss: 0.10952545582098638, test loss: 0.2561546984529945\n",
      "epoch 15993: train loss: 0.10952364104531714, test loss: 0.2561556229888054\n",
      "epoch 15994: train loss: 0.10952182642357515, test loss: 0.25615654759151707\n",
      "epoch 15995: train loss: 0.10952001195573677, test loss: 0.25615747226106217\n",
      "epoch 15996: train loss: 0.10951819764177839, test loss: 0.2561583969974385\n",
      "epoch 15997: train loss: 0.1095163834816764, test loss: 0.2561593218006148\n",
      "epoch 15998: train loss: 0.10951456947540711, test loss: 0.2561602466706246\n",
      "epoch 15999: train loss: 0.10951275562294699, test loss: 0.2561611716074144\n",
      "epoch 16000: train loss: 0.10951094192427237, test loss: 0.2561620966109911\n",
      "epoch 16001: train loss: 0.10950912837935967, test loss: 0.2561630216813139\n",
      "epoch 16002: train loss: 0.10950731498818528, test loss: 0.25616394681838967\n",
      "epoch 16003: train loss: 0.10950550175072561, test loss: 0.2561648720222034\n",
      "epoch 16004: train loss: 0.10950368866695706, test loss: 0.25616579729273736\n",
      "epoch 16005: train loss: 0.10950187573685607, test loss: 0.25616672262998613\n",
      "epoch 16006: train loss: 0.10950006296039902, test loss: 0.25616764803393477\n",
      "epoch 16007: train loss: 0.10949825033756241, test loss: 0.25616857350452876\n",
      "epoch 16008: train loss: 0.10949643786832258, test loss: 0.2561694990418118\n",
      "epoch 16009: train loss: 0.10949462555265602, test loss: 0.2561704246457199\n",
      "epoch 16010: train loss: 0.10949281339053915, test loss: 0.2561713503162843\n",
      "epoch 16011: train loss: 0.10949100138194844, test loss: 0.256172276053453\n",
      "epoch 16012: train loss: 0.10948918952686032, test loss: 0.25617320185724385\n",
      "epoch 16013: train loss: 0.10948737782525127, test loss: 0.25617412772761755\n",
      "epoch 16014: train loss: 0.10948556627709773, test loss: 0.25617505366456816\n",
      "epoch 16015: train loss: 0.10948375488237616, test loss: 0.256175979668092\n",
      "epoch 16016: train loss: 0.10948194364106305, test loss: 0.2561769057381491\n",
      "epoch 16017: train loss: 0.10948013255313488, test loss: 0.2561778318747699\n",
      "epoch 16018: train loss: 0.10947832161856813, test loss: 0.2561787580778781\n",
      "epoch 16019: train loss: 0.10947651083733927, test loss: 0.25617968434751714\n",
      "epoch 16020: train loss: 0.1094747002094248, test loss: 0.25618061068364634\n",
      "epoch 16021: train loss: 0.10947288973480124, test loss: 0.2561815370862505\n",
      "epoch 16022: train loss: 0.10947107941344507, test loss: 0.25618246355532265\n",
      "epoch 16023: train loss: 0.10946926924533279, test loss: 0.25618339009083513\n",
      "epoch 16024: train loss: 0.10946745923044096, test loss: 0.25618431669279723\n",
      "epoch 16025: train loss: 0.10946564936874605, test loss: 0.25618524336115345\n",
      "epoch 16026: train loss: 0.10946383966022459, test loss: 0.25618617009595\n",
      "epoch 16027: train loss: 0.10946203010485313, test loss: 0.25618709689712177\n",
      "epoch 16028: train loss: 0.1094602207026082, test loss: 0.2561880237647\n",
      "epoch 16029: train loss: 0.10945841145346633, test loss: 0.2561889506986067\n",
      "epoch 16030: train loss: 0.10945660235740405, test loss: 0.2561898776988882\n",
      "epoch 16031: train loss: 0.10945479341439794, test loss: 0.2561908047655019\n",
      "epoch 16032: train loss: 0.10945298462442456, test loss: 0.2561917318984447\n",
      "epoch 16033: train loss: 0.10945117598746039, test loss: 0.2561926590976988\n",
      "epoch 16034: train loss: 0.10944936750348208, test loss: 0.25619358636323825\n",
      "epoch 16035: train loss: 0.1094475591724662, test loss: 0.2561945136950562\n",
      "epoch 16036: train loss: 0.10944575099438925, test loss: 0.2561954410931364\n",
      "epoch 16037: train loss: 0.1094439429692279, test loss: 0.25619636855747663\n",
      "epoch 16038: train loss: 0.10944213509695866, test loss: 0.2561972960880588\n",
      "epoch 16039: train loss: 0.10944032737755817, test loss: 0.2561982236848913\n",
      "epoch 16040: train loss: 0.10943851981100301, test loss: 0.25619915134789517\n",
      "epoch 16041: train loss: 0.10943671239726978, test loss: 0.2562000790770938\n",
      "epoch 16042: train loss: 0.10943490513633508, test loss: 0.2562010068724812\n",
      "epoch 16043: train loss: 0.10943309802817552, test loss: 0.2562019347340533\n",
      "epoch 16044: train loss: 0.10943129107276771, test loss: 0.25620286266176984\n",
      "epoch 16045: train loss: 0.1094294842700883, test loss: 0.2562037906556141\n",
      "epoch 16046: train loss: 0.1094276776201139, test loss: 0.2562047187156067\n",
      "epoch 16047: train loss: 0.10942587112282114, test loss: 0.2562056468417063\n",
      "epoch 16048: train loss: 0.10942406477818664, test loss: 0.25620657503388505\n",
      "epoch 16049: train loss: 0.10942225858618705, test loss: 0.256207503292163\n",
      "epoch 16050: train loss: 0.10942045254679904, test loss: 0.2562084316164989\n",
      "epoch 16051: train loss: 0.10941864665999923, test loss: 0.25620936000691474\n",
      "epoch 16052: train loss: 0.10941684092576427, test loss: 0.2562102884633678\n",
      "epoch 16053: train loss: 0.1094150353440709, test loss: 0.25621121698583144\n",
      "epoch 16054: train loss: 0.10941322991489567, test loss: 0.25621214557429967\n",
      "epoch 16055: train loss: 0.10941142463821532, test loss: 0.25621307422878253\n",
      "epoch 16056: train loss: 0.10940961951400655, test loss: 0.2562140029492501\n",
      "epoch 16057: train loss: 0.10940781454224596, test loss: 0.256214931735676\n",
      "epoch 16058: train loss: 0.10940600972291031, test loss: 0.2562158605880903\n",
      "epoch 16059: train loss: 0.10940420505597627, test loss: 0.25621678950643023\n",
      "epoch 16060: train loss: 0.10940240054142056, test loss: 0.2562177184906898\n",
      "epoch 16061: train loss: 0.1094005961792198, test loss: 0.25621864754086654\n",
      "epoch 16062: train loss: 0.10939879196935079, test loss: 0.25621957665693074\n",
      "epoch 16063: train loss: 0.10939698791179019, test loss: 0.2562205058389182\n",
      "epoch 16064: train loss: 0.10939518400651477, test loss: 0.25622143508675943\n",
      "epoch 16065: train loss: 0.10939338025350118, test loss: 0.2562223644004659\n",
      "epoch 16066: train loss: 0.10939157665272621, test loss: 0.25622329377999586\n",
      "epoch 16067: train loss: 0.10938977320416654, test loss: 0.256224223225382\n",
      "epoch 16068: train loss: 0.10938796990779895, test loss: 0.2562251527365737\n",
      "epoch 16069: train loss: 0.10938616676360016, test loss: 0.25622608231356375\n",
      "epoch 16070: train loss: 0.10938436377154692, test loss: 0.25622701195634984\n",
      "epoch 16071: train loss: 0.109382560931616, test loss: 0.2562279416649153\n",
      "epoch 16072: train loss: 0.10938075824378415, test loss: 0.25622887143924544\n",
      "epoch 16073: train loss: 0.10937895570802812, test loss: 0.2562298012793118\n",
      "epoch 16074: train loss: 0.10937715332432471, test loss: 0.25623073118510914\n",
      "epoch 16075: train loss: 0.10937535109265065, test loss: 0.2562316611566098\n",
      "epoch 16076: train loss: 0.10937354901298275, test loss: 0.2562325911938479\n",
      "epoch 16077: train loss: 0.10937174708529777, test loss: 0.25623352129675625\n",
      "epoch 16078: train loss: 0.10936994530957253, test loss: 0.2562344514653443\n",
      "epoch 16079: train loss: 0.10936814368578382, test loss: 0.2562353816996227\n",
      "epoch 16080: train loss: 0.1093663422139084, test loss: 0.25623631199952307\n",
      "epoch 16081: train loss: 0.1093645408939231, test loss: 0.256237242365055\n",
      "epoch 16082: train loss: 0.10936273972580474, test loss: 0.2562381727962032\n",
      "epoch 16083: train loss: 0.10936093870953009, test loss: 0.25623910329297644\n",
      "epoch 16084: train loss: 0.10935913784507602, test loss: 0.25624003385534605\n",
      "epoch 16085: train loss: 0.10935733713241932, test loss: 0.2562409644832837\n",
      "epoch 16086: train loss: 0.10935553657153683, test loss: 0.256241895176785\n",
      "epoch 16087: train loss: 0.1093537361624054, test loss: 0.2562428259358362\n",
      "epoch 16088: train loss: 0.10935193590500183, test loss: 0.2562437567604441\n",
      "epoch 16089: train loss: 0.10935013579930303, test loss: 0.2562446876505558\n",
      "epoch 16090: train loss: 0.10934833584528576, test loss: 0.25624561860616885\n",
      "epoch 16091: train loss: 0.10934653604292696, test loss: 0.25624654962727894\n",
      "epoch 16092: train loss: 0.10934473639220343, test loss: 0.256247480713895\n",
      "epoch 16093: train loss: 0.10934293689309207, test loss: 0.25624841186596403\n",
      "epoch 16094: train loss: 0.10934113754556972, test loss: 0.25624934308348224\n",
      "epoch 16095: train loss: 0.10933933834961328, test loss: 0.2562502743664597\n",
      "epoch 16096: train loss: 0.10933753930519959, test loss: 0.2562512057148425\n",
      "epoch 16097: train loss: 0.10933574041230561, test loss: 0.256252137128639\n",
      "epoch 16098: train loss: 0.10933394167090814, test loss: 0.25625306860784713\n",
      "epoch 16099: train loss: 0.1093321430809841, test loss: 0.2562540001524387\n",
      "epoch 16100: train loss: 0.10933034464251046, test loss: 0.2562549317623964\n",
      "epoch 16101: train loss: 0.10932854635546403, test loss: 0.2562558634376921\n",
      "epoch 16102: train loss: 0.10932674821982177, test loss: 0.25625679517834843\n",
      "epoch 16103: train loss: 0.1093249502355606, test loss: 0.25625772698432503\n",
      "epoch 16104: train loss: 0.1093231524026574, test loss: 0.2562586588556175\n",
      "epoch 16105: train loss: 0.10932135472108911, test loss: 0.2562595907922226\n",
      "epoch 16106: train loss: 0.10931955719083268, test loss: 0.2562605227940998\n",
      "epoch 16107: train loss: 0.10931775981186503, test loss: 0.2562614548612709\n",
      "epoch 16108: train loss: 0.1093159625841631, test loss: 0.2562623869936828\n",
      "epoch 16109: train loss: 0.10931416550770383, test loss: 0.25626331919134315\n",
      "epoch 16110: train loss: 0.10931236858246418, test loss: 0.25626425145422627\n",
      "epoch 16111: train loss: 0.10931057180842109, test loss: 0.2562651837823283\n",
      "epoch 16112: train loss: 0.10930877518555153, test loss: 0.2562661161756581\n",
      "epoch 16113: train loss: 0.10930697871383249, test loss: 0.2562670486341616\n",
      "epoch 16114: train loss: 0.10930518239324089, test loss: 0.2562679811578367\n",
      "epoch 16115: train loss: 0.10930338622375374, test loss: 0.2562689137466675\n",
      "epoch 16116: train loss: 0.10930159020534798, test loss: 0.25626984640067685\n",
      "epoch 16117: train loss: 0.10929979433800063, test loss: 0.256270779119784\n",
      "epoch 16118: train loss: 0.1092979986216887, test loss: 0.25627171190402565\n",
      "epoch 16119: train loss: 0.10929620305638914, test loss: 0.25627264475337164\n",
      "epoch 16120: train loss: 0.10929440764207893, test loss: 0.2562735776678216\n",
      "epoch 16121: train loss: 0.10929261237873515, test loss: 0.2562745106473573\n",
      "epoch 16122: train loss: 0.10929081726633477, test loss: 0.2562754436919264\n",
      "epoch 16123: train loss: 0.10928902230485478, test loss: 0.25627637680157683\n",
      "epoch 16124: train loss: 0.10928722749427223, test loss: 0.25627730997624226\n",
      "epoch 16125: train loss: 0.10928543283456417, test loss: 0.25627824321594406\n",
      "epoch 16126: train loss: 0.10928363832570756, test loss: 0.25627917652065524\n",
      "epoch 16127: train loss: 0.10928184396767948, test loss: 0.2562801098903595\n",
      "epoch 16128: train loss: 0.109280049760457, test loss: 0.2562810433250537\n",
      "epoch 16129: train loss: 0.10927825570401707, test loss: 0.25628197682471004\n",
      "epoch 16130: train loss: 0.10927646179833683, test loss: 0.2562829103893137\n",
      "epoch 16131: train loss: 0.10927466804339328, test loss: 0.2562838440188606\n",
      "epoch 16132: train loss: 0.1092728744391635, test loss: 0.25628477771334723\n",
      "epoch 16133: train loss: 0.10927108098562456, test loss: 0.25628571147272305\n",
      "epoch 16134: train loss: 0.10926928768275353, test loss: 0.25628664529700734\n",
      "epoch 16135: train loss: 0.1092674945305275, test loss: 0.25628757918618533\n",
      "epoch 16136: train loss: 0.1092657015289235, test loss: 0.25628851314024337\n",
      "epoch 16137: train loss: 0.10926390867791864, test loss: 0.2562894471591507\n",
      "epoch 16138: train loss: 0.10926211597749003, test loss: 0.25629038124290743\n",
      "epoch 16139: train loss: 0.10926032342761474, test loss: 0.2562913153914828\n",
      "epoch 16140: train loss: 0.10925853102826988, test loss: 0.2562922496048753\n",
      "epoch 16141: train loss: 0.10925673877943252, test loss: 0.25629318388306827\n",
      "epoch 16142: train loss: 0.10925494668107982, test loss: 0.2562941182260733\n",
      "epoch 16143: train loss: 0.10925315473318886, test loss: 0.25629505263383695\n",
      "epoch 16144: train loss: 0.1092513629357368, test loss: 0.25629598710634227\n",
      "epoch 16145: train loss: 0.10924957128870069, test loss: 0.2562969216436257\n",
      "epoch 16146: train loss: 0.10924777979205774, test loss: 0.25629785624562035\n",
      "epoch 16147: train loss: 0.10924598844578505, test loss: 0.2562987909123376\n",
      "epoch 16148: train loss: 0.10924419724985977, test loss: 0.25629972564378556\n",
      "epoch 16149: train loss: 0.109242406204259, test loss: 0.25630066043989946\n",
      "epoch 16150: train loss: 0.10924061530895994, test loss: 0.2563015953007137\n",
      "epoch 16151: train loss: 0.10923882456393973, test loss: 0.2563025302261761\n",
      "epoch 16152: train loss: 0.10923703396917549, test loss: 0.25630346521629627\n",
      "epoch 16153: train loss: 0.10923524352464446, test loss: 0.25630440027105816\n",
      "epoch 16154: train loss: 0.10923345323032374, test loss: 0.25630533539043393\n",
      "epoch 16155: train loss: 0.10923166308619052, test loss: 0.25630627057444716\n",
      "epoch 16156: train loss: 0.10922987309222201, test loss: 0.25630720582301775\n",
      "epoch 16157: train loss: 0.10922808324839536, test loss: 0.2563081411361956\n",
      "epoch 16158: train loss: 0.10922629355468778, test loss: 0.25630907651393914\n",
      "epoch 16159: train loss: 0.10922450401107646, test loss: 0.25631001195621983\n",
      "epoch 16160: train loss: 0.10922271461753855, test loss: 0.2563109474630627\n",
      "epoch 16161: train loss: 0.10922092537405134, test loss: 0.2563118830344245\n",
      "epoch 16162: train loss: 0.10921913628059197, test loss: 0.256312818670303\n",
      "epoch 16163: train loss: 0.1092173473371377, test loss: 0.2563137543706968\n",
      "epoch 16164: train loss: 0.10921555854366571, test loss: 0.25631469013556524\n",
      "epoch 16165: train loss: 0.10921376990015325, test loss: 0.25631562596490576\n",
      "epoch 16166: train loss: 0.10921198140657752, test loss: 0.25631656185871415\n",
      "epoch 16167: train loss: 0.1092101930629158, test loss: 0.25631749781693935\n",
      "epoch 16168: train loss: 0.10920840486914525, test loss: 0.25631843383962866\n",
      "epoch 16169: train loss: 0.1092066168252432, test loss: 0.2563193699267167\n",
      "epoch 16170: train loss: 0.10920482893118684, test loss: 0.25632030607823875\n",
      "epoch 16171: train loss: 0.10920304118695345, test loss: 0.2563212422941152\n",
      "epoch 16172: train loss: 0.10920125359252028, test loss: 0.25632217857438355\n",
      "epoch 16173: train loss: 0.10919946614786458, test loss: 0.25632311491901527\n",
      "epoch 16174: train loss: 0.10919767885296362, test loss: 0.2563240513279941\n",
      "epoch 16175: train loss: 0.1091958917077947, test loss: 0.2563249878013313\n",
      "epoch 16176: train loss: 0.10919410471233507, test loss: 0.25632592433894913\n",
      "epoch 16177: train loss: 0.10919231786656203, test loss: 0.2563268609409195\n",
      "epoch 16178: train loss: 0.10919053117045284, test loss: 0.2563277976071646\n",
      "epoch 16179: train loss: 0.10918874462398481, test loss: 0.25632873433768316\n",
      "epoch 16180: train loss: 0.10918695822713526, test loss: 0.2563296711324843\n",
      "epoch 16181: train loss: 0.10918517197988145, test loss: 0.25633060799151475\n",
      "epoch 16182: train loss: 0.10918338588220071, test loss: 0.25633154491482374\n",
      "epoch 16183: train loss: 0.10918159993407034, test loss: 0.2563324819023453\n",
      "epoch 16184: train loss: 0.10917981413546766, test loss: 0.2563334189540767\n",
      "epoch 16185: train loss: 0.10917802848637001, test loss: 0.256334356069991\n",
      "epoch 16186: train loss: 0.1091762429867547, test loss: 0.25633529325010995\n",
      "epoch 16187: train loss: 0.10917445763659904, test loss: 0.2563362304944066\n",
      "epoch 16188: train loss: 0.10917267243588041, test loss: 0.25633716780284105\n",
      "epoch 16189: train loss: 0.10917088738457612, test loss: 0.2563381051754367\n",
      "epoch 16190: train loss: 0.10916910248266351, test loss: 0.2563390426121638\n",
      "epoch 16191: train loss: 0.10916731773011998, test loss: 0.2563399801130094\n",
      "epoch 16192: train loss: 0.10916553312692284, test loss: 0.25634091767797057\n",
      "epoch 16193: train loss: 0.10916374867304947, test loss: 0.256341855306981\n",
      "epoch 16194: train loss: 0.10916196436847724, test loss: 0.25634279300010326\n",
      "epoch 16195: train loss: 0.10916018021318347, test loss: 0.2563437307572968\n",
      "epoch 16196: train loss: 0.10915839620714561, test loss: 0.2563446685785335\n",
      "epoch 16197: train loss: 0.10915661235034099, test loss: 0.25634560646378496\n",
      "epoch 16198: train loss: 0.109154828642747, test loss: 0.2563465444130766\n",
      "epoch 16199: train loss: 0.10915304508434107, test loss: 0.2563474824263528\n",
      "epoch 16200: train loss: 0.10915126167510056, test loss: 0.25634842050366496\n",
      "epoch 16201: train loss: 0.10914947841500286, test loss: 0.25634935864494546\n",
      "epoch 16202: train loss: 0.10914769530402539, test loss: 0.256350296850167\n",
      "epoch 16203: train loss: 0.10914591234214557, test loss: 0.25635123511937846\n",
      "epoch 16204: train loss: 0.10914412952934079, test loss: 0.2563521734525148\n",
      "epoch 16205: train loss: 0.10914234686558853, test loss: 0.2563531118495728\n",
      "epoch 16206: train loss: 0.10914056435086611, test loss: 0.2563540503105767\n",
      "epoch 16207: train loss: 0.10913878198515105, test loss: 0.2563549888354486\n",
      "epoch 16208: train loss: 0.10913699976842076, test loss: 0.25635592742421026\n",
      "epoch 16209: train loss: 0.10913521770065264, test loss: 0.25635686607684804\n",
      "epoch 16210: train loss: 0.10913343578182419, test loss: 0.2563578047933593\n",
      "epoch 16211: train loss: 0.1091316540119128, test loss: 0.2563587435736921\n",
      "epoch 16212: train loss: 0.10912987239089599, test loss: 0.2563596824178686\n",
      "epoch 16213: train loss: 0.10912809091875117, test loss: 0.25636062132588805\n",
      "epoch 16214: train loss: 0.10912630959545583, test loss: 0.2563615602976828\n",
      "epoch 16215: train loss: 0.10912452842098741, test loss: 0.2563624993332782\n",
      "epoch 16216: train loss: 0.1091227473953234, test loss: 0.25636343843264664\n",
      "epoch 16217: train loss: 0.10912096651844129, test loss: 0.256364377595798\n",
      "epoch 16218: train loss: 0.10911918579031855, test loss: 0.256365316822706\n",
      "epoch 16219: train loss: 0.10911740521093266, test loss: 0.25636625611332975\n",
      "epoch 16220: train loss: 0.10911562478026113, test loss: 0.25636719546768\n",
      "epoch 16221: train loss: 0.10911384449828145, test loss: 0.25636813488574156\n",
      "epoch 16222: train loss: 0.1091120643649711, test loss: 0.2563690743675017\n",
      "epoch 16223: train loss: 0.10911028438030762, test loss: 0.2563700139129568\n",
      "epoch 16224: train loss: 0.10910850454426851, test loss: 0.2563709535220664\n",
      "epoch 16225: train loss: 0.10910672485683129, test loss: 0.25637189319485576\n",
      "epoch 16226: train loss: 0.10910494531797348, test loss: 0.25637283293127133\n",
      "epoch 16227: train loss: 0.10910316592767255, test loss: 0.2563737727313233\n",
      "epoch 16228: train loss: 0.10910138668590616, test loss: 0.2563747125949856\n",
      "epoch 16229: train loss: 0.10909960759265173, test loss: 0.2563756525222422\n",
      "epoch 16230: train loss: 0.10909782864788685, test loss: 0.2563765925131056\n",
      "epoch 16231: train loss: 0.10909604985158904, test loss: 0.25637753256753487\n",
      "epoch 16232: train loss: 0.1090942712037359, test loss: 0.25637847268551506\n",
      "epoch 16233: train loss: 0.10909249270430493, test loss: 0.25637941286707017\n",
      "epoch 16234: train loss: 0.10909071435327374, test loss: 0.25638035311213486\n",
      "epoch 16235: train loss: 0.10908893615061985, test loss: 0.25638129342073296\n",
      "epoch 16236: train loss: 0.10908715809632084, test loss: 0.25638223379286174\n",
      "epoch 16237: train loss: 0.10908538019035433, test loss: 0.25638317422845786\n",
      "epoch 16238: train loss: 0.10908360243269782, test loss: 0.25638411472754263\n",
      "epoch 16239: train loss: 0.10908182482332897, test loss: 0.25638505529007805\n",
      "epoch 16240: train loss: 0.10908004736222533, test loss: 0.2563859959160994\n",
      "epoch 16241: train loss: 0.10907827004936452, test loss: 0.2563869366055663\n",
      "epoch 16242: train loss: 0.1090764928847241, test loss: 0.25638787735841484\n",
      "epoch 16243: train loss: 0.10907471586828171, test loss: 0.2563888181747189\n",
      "epoch 16244: train loss: 0.10907293900001493, test loss: 0.2563897590544129\n",
      "epoch 16245: train loss: 0.10907116227990139, test loss: 0.2563906999974952\n",
      "epoch 16246: train loss: 0.10906938570791871, test loss: 0.25639164100393935\n",
      "epoch 16247: train loss: 0.10906760928404452, test loss: 0.2563925820737431\n",
      "epoch 16248: train loss: 0.10906583300825641, test loss: 0.2563935232069056\n",
      "epoch 16249: train loss: 0.10906405688053206, test loss: 0.2563944644034102\n",
      "epoch 16250: train loss: 0.10906228090084909, test loss: 0.25639540566320734\n",
      "epoch 16251: train loss: 0.10906050506918515, test loss: 0.2563963469863446\n",
      "epoch 16252: train loss: 0.10905872938551785, test loss: 0.2563972883727438\n",
      "epoch 16253: train loss: 0.10905695384982489, test loss: 0.25639822982242905\n",
      "epoch 16254: train loss: 0.1090551784620839, test loss: 0.2563991713354113\n",
      "epoch 16255: train loss: 0.10905340322227254, test loss: 0.25640011291161374\n",
      "epoch 16256: train loss: 0.10905162813036849, test loss: 0.256401054551047\n",
      "epoch 16257: train loss: 0.10904985318634942, test loss: 0.25640199625374666\n",
      "epoch 16258: train loss: 0.10904807839019302, test loss: 0.2564029380196223\n",
      "epoch 16259: train loss: 0.10904630374187693, test loss: 0.2564038798487116\n",
      "epoch 16260: train loss: 0.10904452924137885, test loss: 0.25640482174098667\n",
      "epoch 16261: train loss: 0.1090427548886765, test loss: 0.25640576369643375\n",
      "epoch 16262: train loss: 0.10904098068374755, test loss: 0.2564067057150504\n",
      "epoch 16263: train loss: 0.10903920662656973, test loss: 0.2564076477968109\n",
      "epoch 16264: train loss: 0.10903743271712069, test loss: 0.2564085899416883\n",
      "epoch 16265: train loss: 0.10903565895537817, test loss: 0.256409532149679\n",
      "epoch 16266: train loss: 0.10903388534131991, test loss: 0.2564104744207839\n",
      "epoch 16267: train loss: 0.1090321118749236, test loss: 0.2564114167549749\n",
      "epoch 16268: train loss: 0.10903033855616695, test loss: 0.2564123591522646\n",
      "epoch 16269: train loss: 0.10902856538502771, test loss: 0.25641330161262343\n",
      "epoch 16270: train loss: 0.10902679236148362, test loss: 0.2564142441360011\n",
      "epoch 16271: train loss: 0.10902501948551241, test loss: 0.2564151867224586\n",
      "epoch 16272: train loss: 0.10902324675709181, test loss: 0.25641612937191965\n",
      "epoch 16273: train loss: 0.10902147417619962, test loss: 0.256417072084394\n",
      "epoch 16274: train loss: 0.10901970174281353, test loss: 0.2564180148598561\n",
      "epoch 16275: train loss: 0.10901792945691131, test loss: 0.2564189576983162\n",
      "epoch 16276: train loss: 0.10901615731847075, test loss: 0.2564199005997487\n",
      "epoch 16277: train loss: 0.1090143853274696, test loss: 0.25642084356412503\n",
      "epoch 16278: train loss: 0.10901261348388566, test loss: 0.25642178659144466\n",
      "epoch 16279: train loss: 0.10901084178769663, test loss: 0.2564227296817062\n",
      "epoch 16280: train loss: 0.10900907023888039, test loss: 0.25642367283489703\n",
      "epoch 16281: train loss: 0.10900729883741465, test loss: 0.2564246160509874\n",
      "epoch 16282: train loss: 0.10900552758327724, test loss: 0.2564255593299778\n",
      "epoch 16283: train loss: 0.10900375647644595, test loss: 0.256426502671816\n",
      "epoch 16284: train loss: 0.10900198551689858, test loss: 0.2564274460765513\n",
      "epoch 16285: train loss: 0.10900021470461294, test loss: 0.25642838954410546\n",
      "epoch 16286: train loss: 0.10899844403956681, test loss: 0.2564293330745295\n",
      "epoch 16287: train loss: 0.10899667352173804, test loss: 0.2564302766677833\n",
      "epoch 16288: train loss: 0.10899490315110444, test loss: 0.25643122032381416\n",
      "epoch 16289: train loss: 0.10899313292764383, test loss: 0.2564321640426714\n",
      "epoch 16290: train loss: 0.10899136285133405, test loss: 0.2564331078243034\n",
      "epoch 16291: train loss: 0.10898959292215295, test loss: 0.2564340516687097\n",
      "epoch 16292: train loss: 0.10898782314007831, test loss: 0.25643499557586297\n",
      "epoch 16293: train loss: 0.10898605350508803, test loss: 0.2564359395457861\n",
      "epoch 16294: train loss: 0.10898428401715991, test loss: 0.25643688357840405\n",
      "epoch 16295: train loss: 0.10898251467627183, test loss: 0.25643782767377693\n",
      "epoch 16296: train loss: 0.10898074548240168, test loss: 0.25643877183182867\n",
      "epoch 16297: train loss: 0.10897897643552729, test loss: 0.25643971605259586\n",
      "epoch 16298: train loss: 0.10897720753562652, test loss: 0.2564406603360383\n",
      "epoch 16299: train loss: 0.10897543878267722, test loss: 0.2564416046821308\n",
      "epoch 16300: train loss: 0.10897367017665735, test loss: 0.2564425490908719\n",
      "epoch 16301: train loss: 0.1089719017175447, test loss: 0.2564434935622712\n",
      "epoch 16302: train loss: 0.10897013340531723, test loss: 0.2564444380963048\n",
      "epoch 16303: train loss: 0.10896836523995279, test loss: 0.25644538269293177\n",
      "epoch 16304: train loss: 0.10896659722142928, test loss: 0.2564463273521511\n",
      "epoch 16305: train loss: 0.10896482934972461, test loss: 0.2564472720739747\n",
      "epoch 16306: train loss: 0.10896306162481667, test loss: 0.25644821685835206\n",
      "epoch 16307: train loss: 0.10896129404668338, test loss: 0.256449161705305\n",
      "epoch 16308: train loss: 0.10895952661530267, test loss: 0.2564501066147814\n",
      "epoch 16309: train loss: 0.10895775933065244, test loss: 0.2564510515868089\n",
      "epoch 16310: train loss: 0.10895599219271061, test loss: 0.2564519966213703\n",
      "epoch 16311: train loss: 0.10895422520145512, test loss: 0.25645294171841454\n",
      "epoch 16312: train loss: 0.10895245835686392, test loss: 0.25645388687795373\n",
      "epoch 16313: train loss: 0.10895069165891492, test loss: 0.2564548320999881\n",
      "epoch 16314: train loss: 0.10894892510758608, test loss: 0.2564557773844766\n",
      "epoch 16315: train loss: 0.10894715870285535, test loss: 0.2564567227314173\n",
      "epoch 16316: train loss: 0.10894539244470065, test loss: 0.2564576681408127\n",
      "epoch 16317: train loss: 0.10894362633309998, test loss: 0.2564586136126073\n",
      "epoch 16318: train loss: 0.10894186036803127, test loss: 0.25645955914685287\n",
      "epoch 16319: train loss: 0.10894009454947254, test loss: 0.25646050474347154\n",
      "epoch 16320: train loss: 0.10893832887740171, test loss: 0.2564614504024882\n",
      "epoch 16321: train loss: 0.10893656335179674, test loss: 0.25646239612386246\n",
      "epoch 16322: train loss: 0.10893479797263568, test loss: 0.2564633419076203\n",
      "epoch 16323: train loss: 0.10893303273989648, test loss: 0.25646428775370983\n",
      "epoch 16324: train loss: 0.1089312676535571, test loss: 0.2564652336621541\n",
      "epoch 16325: train loss: 0.1089295027135956, test loss: 0.2564661796328906\n",
      "epoch 16326: train loss: 0.10892773791998991, test loss: 0.2564671256659547\n",
      "epoch 16327: train loss: 0.10892597327271812, test loss: 0.2564680717612967\n",
      "epoch 16328: train loss: 0.10892420877175817, test loss: 0.25646901791893817\n",
      "epoch 16329: train loss: 0.10892244441708807, test loss: 0.2564699641388428\n",
      "epoch 16330: train loss: 0.1089206802086859, test loss: 0.2564709104209942\n",
      "epoch 16331: train loss: 0.10891891614652958, test loss: 0.256471856765395\n",
      "epoch 16332: train loss: 0.10891715223059727, test loss: 0.2564728031720297\n",
      "epoch 16333: train loss: 0.10891538846086693, test loss: 0.2564737496408711\n",
      "epoch 16334: train loss: 0.10891362483731658, test loss: 0.256474696171907\n",
      "epoch 16335: train loss: 0.10891186135992431, test loss: 0.2564756427651363\n",
      "epoch 16336: train loss: 0.10891009802866812, test loss: 0.2564765894205705\n",
      "epoch 16337: train loss: 0.10890833484352612, test loss: 0.2564775361381331\n",
      "epoch 16338: train loss: 0.10890657180447633, test loss: 0.2564784829178733\n",
      "epoch 16339: train loss: 0.1089048089114968, test loss: 0.25647942975971383\n",
      "epoch 16340: train loss: 0.10890304616456564, test loss: 0.2564803766637062\n",
      "epoch 16341: train loss: 0.10890128356366087, test loss: 0.25648132362981074\n",
      "epoch 16342: train loss: 0.1088995211087606, test loss: 0.25648227065798823\n",
      "epoch 16343: train loss: 0.1088977587998429, test loss: 0.2564832177482634\n",
      "epoch 16344: train loss: 0.10889599663688586, test loss: 0.25648416490062337\n",
      "epoch 16345: train loss: 0.10889423461986754, test loss: 0.25648511211502845\n",
      "epoch 16346: train loss: 0.10889247274876607, test loss: 0.25648605939147767\n",
      "epoch 16347: train loss: 0.10889071102355953, test loss: 0.2564870067299593\n",
      "epoch 16348: train loss: 0.10888894944422604, test loss: 0.25648795413044534\n",
      "epoch 16349: train loss: 0.10888718801074371, test loss: 0.2564889015929607\n",
      "epoch 16350: train loss: 0.10888542672309061, test loss: 0.2564898491174544\n",
      "epoch 16351: train loss: 0.10888366558124492, test loss: 0.2564907967039267\n",
      "epoch 16352: train loss: 0.10888190458518472, test loss: 0.25649174435237476\n",
      "epoch 16353: train loss: 0.10888014373488815, test loss: 0.25649269206277486\n",
      "epoch 16354: train loss: 0.10887838303033334, test loss: 0.2564936398350983\n",
      "epoch 16355: train loss: 0.10887662247149844, test loss: 0.2564945876693454\n",
      "epoch 16356: train loss: 0.10887486205836155, test loss: 0.2564955355655175\n",
      "epoch 16357: train loss: 0.10887310179090087, test loss: 0.25649648352358556\n",
      "epoch 16358: train loss: 0.10887134166909451, test loss: 0.2564974315435633\n",
      "epoch 16359: train loss: 0.10886958169292064, test loss: 0.2564983796253987\n",
      "epoch 16360: train loss: 0.10886782186235745, test loss: 0.2564993277690792\n",
      "epoch 16361: train loss: 0.10886606217738304, test loss: 0.25650027597462854\n",
      "epoch 16362: train loss: 0.10886430263797564, test loss: 0.2565012242420087\n",
      "epoch 16363: train loss: 0.10886254324411336, test loss: 0.2565021725711938\n",
      "epoch 16364: train loss: 0.10886078399577447, test loss: 0.25650312096219513\n",
      "epoch 16365: train loss: 0.10885902489293706, test loss: 0.2565040694149872\n",
      "epoch 16366: train loss: 0.10885726593557939, test loss: 0.2565050179295828\n",
      "epoch 16367: train loss: 0.10885550712367958, test loss: 0.2565059665059299\n",
      "epoch 16368: train loss: 0.10885374845721589, test loss: 0.2565069151440535\n",
      "epoch 16369: train loss: 0.10885198993616649, test loss: 0.25650786384390173\n",
      "epoch 16370: train loss: 0.1088502315605096, test loss: 0.25650881260549846\n",
      "epoch 16371: train loss: 0.10884847333022345, test loss: 0.25650976142876947\n",
      "epoch 16372: train loss: 0.1088467152452862, test loss: 0.2565107103137903\n",
      "epoch 16373: train loss: 0.10884495730567612, test loss: 0.2565116592604833\n",
      "epoch 16374: train loss: 0.1088431995113714, test loss: 0.25651260826884703\n",
      "epoch 16375: train loss: 0.10884144186235031, test loss: 0.25651355733888254\n",
      "epoch 16376: train loss: 0.10883968435859107, test loss: 0.2565145064705627\n",
      "epoch 16377: train loss: 0.10883792700007192, test loss: 0.25651545566390077\n",
      "epoch 16378: train loss: 0.10883616978677103, test loss: 0.25651640491887084\n",
      "epoch 16379: train loss: 0.1088344127186668, test loss: 0.25651735423543287\n",
      "epoch 16380: train loss: 0.10883265579573735, test loss: 0.2565183036135884\n",
      "epoch 16381: train loss: 0.10883089901796097, test loss: 0.2565192530533367\n",
      "epoch 16382: train loss: 0.10882914238531598, test loss: 0.2565202025546508\n",
      "epoch 16383: train loss: 0.10882738589778057, test loss: 0.2565211521175435\n",
      "epoch 16384: train loss: 0.10882562955533306, test loss: 0.25652210174196405\n",
      "epoch 16385: train loss: 0.10882387335795173, test loss: 0.2565230514279243\n",
      "epoch 16386: train loss: 0.10882211730561483, test loss: 0.2565240011754246\n",
      "epoch 16387: train loss: 0.10882036139830065, test loss: 0.2565249509844252\n",
      "epoch 16388: train loss: 0.10881860563598751, test loss: 0.25652590085490057\n",
      "epoch 16389: train loss: 0.10881685001865365, test loss: 0.25652685078687776\n",
      "epoch 16390: train loss: 0.10881509454627744, test loss: 0.25652780078032794\n",
      "epoch 16391: train loss: 0.10881333921883714, test loss: 0.2565287508352153\n",
      "epoch 16392: train loss: 0.10881158403631107, test loss: 0.2565297009515633\n",
      "epoch 16393: train loss: 0.10880982899867753, test loss: 0.2565306511293338\n",
      "epoch 16394: train loss: 0.10880807410591486, test loss: 0.25653160136852626\n",
      "epoch 16395: train loss: 0.10880631935800139, test loss: 0.2565325516691165\n",
      "epoch 16396: train loss: 0.10880456475491539, test loss: 0.25653350203110337\n",
      "epoch 16397: train loss: 0.10880281029663524, test loss: 0.25653445245444756\n",
      "epoch 16398: train loss: 0.10880105598313929, test loss: 0.25653540293918686\n",
      "epoch 16399: train loss: 0.10879930181440585, test loss: 0.2565363534852725\n",
      "epoch 16400: train loss: 0.10879754779041327, test loss: 0.2565373040926891\n",
      "epoch 16401: train loss: 0.10879579391113989, test loss: 0.25653825476142456\n",
      "epoch 16402: train loss: 0.10879404017656409, test loss: 0.25653920549147896\n",
      "epoch 16403: train loss: 0.10879228658666423, test loss: 0.25654015628285187\n",
      "epoch 16404: train loss: 0.10879053314141868, test loss: 0.2565411071354927\n",
      "epoch 16405: train loss: 0.10878877984080575, test loss: 0.2565420580494264\n",
      "epoch 16406: train loss: 0.1087870266848039, test loss: 0.2565430090246001\n",
      "epoch 16407: train loss: 0.10878527367339144, test loss: 0.2565439600610162\n",
      "epoch 16408: train loss: 0.1087835208065468, test loss: 0.25654491115869926\n",
      "epoch 16409: train loss: 0.10878176808424833, test loss: 0.25654586231757265\n",
      "epoch 16410: train loss: 0.10878001550647444, test loss: 0.2565468135376863\n",
      "epoch 16411: train loss: 0.10877826307320353, test loss: 0.25654776481896535\n",
      "epoch 16412: train loss: 0.10877651078441397, test loss: 0.25654871616145913\n",
      "epoch 16413: train loss: 0.10877475864008422, test loss: 0.25654966756509107\n",
      "epoch 16414: train loss: 0.10877300664019267, test loss: 0.25655061902991394\n",
      "epoch 16415: train loss: 0.1087712547847177, test loss: 0.256551570555836\n",
      "epoch 16416: train loss: 0.10876950307363777, test loss: 0.25655252214294966\n",
      "epoch 16417: train loss: 0.10876775150693127, test loss: 0.2565534737911234\n",
      "epoch 16418: train loss: 0.10876600008457668, test loss: 0.2565544255004617\n",
      "epoch 16419: train loss: 0.10876424880655239, test loss: 0.2565553772708364\n",
      "epoch 16420: train loss: 0.10876249767283686, test loss: 0.25655632910233783\n",
      "epoch 16421: train loss: 0.10876074668340852, test loss: 0.25655728099487524\n",
      "epoch 16422: train loss: 0.10875899583824583, test loss: 0.2565582329484727\n",
      "epoch 16423: train loss: 0.10875724513732722, test loss: 0.2565591849631352\n",
      "epoch 16424: train loss: 0.10875549458063116, test loss: 0.25656013703880703\n",
      "epoch 16425: train loss: 0.1087537441681361, test loss: 0.2565610891754897\n",
      "epoch 16426: train loss: 0.10875199389982054, test loss: 0.2565620413731843\n",
      "epoch 16427: train loss: 0.10875024377566293, test loss: 0.2565629936318638\n",
      "epoch 16428: train loss: 0.10874849379564172, test loss: 0.2565639459515295\n",
      "epoch 16429: train loss: 0.10874674395973544, test loss: 0.25656489833214136\n",
      "epoch 16430: train loss: 0.10874499426792252, test loss: 0.25656585077371474\n",
      "epoch 16431: train loss: 0.10874324472018147, test loss: 0.2565668032762228\n",
      "epoch 16432: train loss: 0.10874149531649081, test loss: 0.256567755839653\n",
      "epoch 16433: train loss: 0.108739746056829, test loss: 0.25656870846401836\n",
      "epoch 16434: train loss: 0.10873799694117454, test loss: 0.25656966114926777\n",
      "epoch 16435: train loss: 0.10873624796950601, test loss: 0.25657061389538854\n",
      "epoch 16436: train loss: 0.10873449914180182, test loss: 0.2565715667024199\n",
      "epoch 16437: train loss: 0.10873275045804054, test loss: 0.25657251957028465\n",
      "epoch 16438: train loss: 0.10873100191820068, test loss: 0.25657347249899515\n",
      "epoch 16439: train loss: 0.10872925352226077, test loss: 0.25657442548856635\n",
      "epoch 16440: train loss: 0.10872750527019935, test loss: 0.2565753785389458\n",
      "epoch 16441: train loss: 0.10872575716199494, test loss: 0.25657633165012006\n",
      "epoch 16442: train loss: 0.10872400919762606, test loss: 0.2565772848221174\n",
      "epoch 16443: train loss: 0.1087222613770713, test loss: 0.25657823805487157\n",
      "epoch 16444: train loss: 0.10872051370030916, test loss: 0.25657919134841023\n",
      "epoch 16445: train loss: 0.10871876616731824, test loss: 0.25658014470269325\n",
      "epoch 16446: train loss: 0.10871701877807707, test loss: 0.25658109811775026\n",
      "epoch 16447: train loss: 0.10871527153256419, test loss: 0.25658205159352615\n",
      "epoch 16448: train loss: 0.10871352443075821, test loss: 0.25658300513002447\n",
      "epoch 16449: train loss: 0.10871177747263766, test loss: 0.2565839587272168\n",
      "epoch 16450: train loss: 0.10871003065818119, test loss: 0.2565849123851062\n",
      "epoch 16451: train loss: 0.10870828398736726, test loss: 0.25658586610367967\n",
      "epoch 16452: train loss: 0.10870653746017457, test loss: 0.25658681988292253\n",
      "epoch 16453: train loss: 0.10870479107658165, test loss: 0.2565877737228384\n",
      "epoch 16454: train loss: 0.10870304483656708, test loss: 0.2565887276233472\n",
      "epoch 16455: train loss: 0.10870129874010952, test loss: 0.2565896815845299\n",
      "epoch 16456: train loss: 0.1086995527871875, test loss: 0.25659063560630757\n",
      "epoch 16457: train loss: 0.1086978069777797, test loss: 0.2565915896887327\n",
      "epoch 16458: train loss: 0.10869606131186467, test loss: 0.25659254383170255\n",
      "epoch 16459: train loss: 0.10869431578942106, test loss: 0.25659349803525666\n",
      "epoch 16460: train loss: 0.10869257041042749, test loss: 0.25659445229938255\n",
      "epoch 16461: train loss: 0.10869082517486256, test loss: 0.25659540662408004\n",
      "epoch 16462: train loss: 0.1086890800827049, test loss: 0.25659636100929917\n",
      "epoch 16463: train loss: 0.10868733513393324, test loss: 0.2565973154550403\n",
      "epoch 16464: train loss: 0.10868559032852607, test loss: 0.25659826996128937\n",
      "epoch 16465: train loss: 0.10868384566646214, test loss: 0.25659922452804834\n",
      "epoch 16466: train loss: 0.10868210114772005, test loss: 0.2566001791553323\n",
      "epoch 16467: train loss: 0.10868035677227848, test loss: 0.25660113384306127\n",
      "epoch 16468: train loss: 0.10867861254011606, test loss: 0.2566020885912505\n",
      "epoch 16469: train loss: 0.1086768684512115, test loss: 0.2566030433998744\n",
      "epoch 16470: train loss: 0.1086751245055434, test loss: 0.25660399826895847\n",
      "epoch 16471: train loss: 0.10867338070309046, test loss: 0.2566049531984798\n",
      "epoch 16472: train loss: 0.10867163704383138, test loss: 0.2566059081883856\n",
      "epoch 16473: train loss: 0.10866989352774482, test loss: 0.25660686323870247\n",
      "epoch 16474: train loss: 0.10866815015480945, test loss: 0.2566078183494051\n",
      "epoch 16475: train loss: 0.10866640692500397, test loss: 0.25660877352049566\n",
      "epoch 16476: train loss: 0.1086646638383071, test loss: 0.256609728751922\n",
      "epoch 16477: train loss: 0.1086629208946975, test loss: 0.2566106840437103\n",
      "epoch 16478: train loss: 0.10866117809415389, test loss: 0.25661163939583514\n",
      "epoch 16479: train loss: 0.10865943543665497, test loss: 0.25661259480828513\n",
      "epoch 16480: train loss: 0.10865769292217947, test loss: 0.25661355028103494\n",
      "epoch 16481: train loss: 0.1086559505507061, test loss: 0.2566145058140993\n",
      "epoch 16482: train loss: 0.10865420832221355, test loss: 0.25661546140745173\n",
      "epoch 16483: train loss: 0.10865246623668061, test loss: 0.2566164170610541\n",
      "epoch 16484: train loss: 0.10865072429408591, test loss: 0.2566173727749073\n",
      "epoch 16485: train loss: 0.10864898249440831, test loss: 0.25661832854903865\n",
      "epoch 16486: train loss: 0.10864724083762642, test loss: 0.2566192843833974\n",
      "epoch 16487: train loss: 0.1086454993237191, test loss: 0.25662024027795705\n",
      "epoch 16488: train loss: 0.10864375795266502, test loss: 0.25662119623274504\n",
      "epoch 16489: train loss: 0.10864201672444296, test loss: 0.25662215224770996\n",
      "epoch 16490: train loss: 0.10864027563903166, test loss: 0.25662310832288027\n",
      "epoch 16491: train loss: 0.1086385346964099, test loss: 0.2566240644582029\n",
      "epoch 16492: train loss: 0.10863679389655644, test loss: 0.25662502065369264\n",
      "epoch 16493: train loss: 0.1086350532394501, test loss: 0.25662597690929806\n",
      "epoch 16494: train loss: 0.10863331272506954, test loss: 0.2566269332250861\n",
      "epoch 16495: train loss: 0.10863157235339366, test loss: 0.256627889600952\n",
      "epoch 16496: train loss: 0.10862983212440114, test loss: 0.25662884603693664\n",
      "epoch 16497: train loss: 0.10862809203807085, test loss: 0.2566298025330417\n",
      "epoch 16498: train loss: 0.10862635209438153, test loss: 0.25663075908918875\n",
      "epoch 16499: train loss: 0.10862461229331201, test loss: 0.25663171570543175\n",
      "epoch 16500: train loss: 0.10862287263484109, test loss: 0.2566326723817199\n",
      "epoch 16501: train loss: 0.10862113311894755, test loss: 0.25663362911805265\n",
      "epoch 16502: train loss: 0.10861939374561025, test loss: 0.2566345859144061\n",
      "epoch 16503: train loss: 0.10861765451480793, test loss: 0.25663554277080836\n",
      "epoch 16504: train loss: 0.1086159154265195, test loss: 0.25663649968718\n",
      "epoch 16505: train loss: 0.10861417648072369, test loss: 0.2566374566635613\n",
      "epoch 16506: train loss: 0.10861243767739942, test loss: 0.2566384136999036\n",
      "epoch 16507: train loss: 0.10861069901652547, test loss: 0.2566393707962456\n",
      "epoch 16508: train loss: 0.10860896049808069, test loss: 0.25664032795252323\n",
      "epoch 16509: train loss: 0.1086072221220439, test loss: 0.2566412851687366\n",
      "epoch 16510: train loss: 0.10860548388839397, test loss: 0.25664224244488887\n",
      "epoch 16511: train loss: 0.10860374579710977, test loss: 0.2566431997809545\n",
      "epoch 16512: train loss: 0.10860200784817015, test loss: 0.25664415717693295\n",
      "epoch 16513: train loss: 0.10860027004155391, test loss: 0.25664511463278794\n",
      "epoch 16514: train loss: 0.10859853237724, test loss: 0.25664607214853336\n",
      "epoch 16515: train loss: 0.10859679485520722, test loss: 0.25664702972414366\n",
      "epoch 16516: train loss: 0.10859505747543449, test loss: 0.2566479873595819\n",
      "epoch 16517: train loss: 0.10859332023790069, test loss: 0.25664894505490043\n",
      "epoch 16518: train loss: 0.10859158314258469, test loss: 0.2566499028100233\n",
      "epoch 16519: train loss: 0.10858984618946532, test loss: 0.25665086062494963\n",
      "epoch 16520: train loss: 0.10858810937852155, test loss: 0.2566518184997091\n",
      "epoch 16521: train loss: 0.10858637270973227, test loss: 0.25665277643424955\n",
      "epoch 16522: train loss: 0.10858463618307634, test loss: 0.25665373442854633\n",
      "epoch 16523: train loss: 0.1085828997985327, test loss: 0.2566546924826272\n",
      "epoch 16524: train loss: 0.10858116355608026, test loss: 0.256655650596466\n",
      "epoch 16525: train loss: 0.1085794274556979, test loss: 0.25665660877002533\n",
      "epoch 16526: train loss: 0.10857769149736454, test loss: 0.2566575670033337\n",
      "epoch 16527: train loss: 0.10857595568105914, test loss: 0.25665852529633965\n",
      "epoch 16528: train loss: 0.10857422000676059, test loss: 0.2566594836490437\n",
      "epoch 16529: train loss: 0.10857248447444787, test loss: 0.25666044206144795\n",
      "epoch 16530: train loss: 0.10857074908409987, test loss: 0.25666140053352743\n",
      "epoch 16531: train loss: 0.10856901383569557, test loss: 0.25666235906528334\n",
      "epoch 16532: train loss: 0.10856727872921387, test loss: 0.2566633176566504\n",
      "epoch 16533: train loss: 0.10856554376463377, test loss: 0.2566642763076849\n",
      "epoch 16534: train loss: 0.10856380894193418, test loss: 0.25666523501834704\n",
      "epoch 16535: train loss: 0.10856207426109409, test loss: 0.2566661937886264\n",
      "epoch 16536: train loss: 0.10856033972209243, test loss: 0.25666715261849693\n",
      "epoch 16537: train loss: 0.10855860532490819, test loss: 0.25666811150796137\n",
      "epoch 16538: train loss: 0.10855687106952036, test loss: 0.25666907045699333\n",
      "epoch 16539: train loss: 0.10855513695590786, test loss: 0.2566700294655961\n",
      "epoch 16540: train loss: 0.10855340298404975, test loss: 0.2566709885337552\n",
      "epoch 16541: train loss: 0.10855166915392495, test loss: 0.25667194766142354\n",
      "epoch 16542: train loss: 0.10854993546551248, test loss: 0.25667290684865224\n",
      "epoch 16543: train loss: 0.10854820191879132, test loss: 0.2566738660953773\n",
      "epoch 16544: train loss: 0.10854646851374046, test loss: 0.25667482540158837\n",
      "epoch 16545: train loss: 0.10854473525033896, test loss: 0.2566757847673138\n",
      "epoch 16546: train loss: 0.10854300212856575, test loss: 0.2566767441924993\n",
      "epoch 16547: train loss: 0.10854126914839987, test loss: 0.25667770367716286\n",
      "epoch 16548: train loss: 0.10853953630982037, test loss: 0.25667866322126587\n",
      "epoch 16549: train loss: 0.10853780361280625, test loss: 0.2566796228248083\n",
      "epoch 16550: train loss: 0.10853607105733651, test loss: 0.25668058248778175\n",
      "epoch 16551: train loss: 0.10853433864339022, test loss: 0.2566815422101578\n",
      "epoch 16552: train loss: 0.1085326063709464, test loss: 0.25668250199194015\n",
      "epoch 16553: train loss: 0.10853087423998409, test loss: 0.2566834618330908\n",
      "epoch 16554: train loss: 0.1085291422504823, test loss: 0.2566844217336375\n",
      "epoch 16555: train loss: 0.10852741040242013, test loss: 0.25668538169354294\n",
      "epoch 16556: train loss: 0.10852567869577659, test loss: 0.2566863417127812\n",
      "epoch 16557: train loss: 0.10852394713053073, test loss: 0.25668730179136917\n",
      "epoch 16558: train loss: 0.10852221570666166, test loss: 0.25668826192927924\n",
      "epoch 16559: train loss: 0.10852048442414844, test loss: 0.25668922212651635\n",
      "epoch 16560: train loss: 0.1085187532829701, test loss: 0.25669018238305397\n",
      "epoch 16561: train loss: 0.10851702228310575, test loss: 0.2566911426988675\n",
      "epoch 16562: train loss: 0.10851529142453444, test loss: 0.2566921030739461\n",
      "epoch 16563: train loss: 0.10851356070723525, test loss: 0.2566930635082906\n",
      "epoch 16564: train loss: 0.10851183013118731, test loss: 0.2566940240019035\n",
      "epoch 16565: train loss: 0.10851009969636966, test loss: 0.2566949845547336\n",
      "epoch 16566: train loss: 0.10850836940276146, test loss: 0.25669594516678346\n",
      "epoch 16567: train loss: 0.10850663925034172, test loss: 0.2566969058380806\n",
      "epoch 16568: train loss: 0.10850490923908965, test loss: 0.25669786656854754\n",
      "epoch 16569: train loss: 0.10850317936898426, test loss: 0.2566988273581981\n",
      "epoch 16570: train loss: 0.10850144964000474, test loss: 0.2566997882070365\n",
      "epoch 16571: train loss: 0.10849972005213016, test loss: 0.25670074911503754\n",
      "epoch 16572: train loss: 0.10849799060533968, test loss: 0.25670171008217496\n",
      "epoch 16573: train loss: 0.1084962612996124, test loss: 0.2567026711084792\n",
      "epoch 16574: train loss: 0.10849453213492746, test loss: 0.25670363219388515\n",
      "epoch 16575: train loss: 0.108492803111264, test loss: 0.25670459333839374\n",
      "epoch 16576: train loss: 0.10849107422860114, test loss: 0.25670555454200816\n",
      "epoch 16577: train loss: 0.10848934548691806, test loss: 0.25670651580473114\n",
      "epoch 16578: train loss: 0.10848761688619389, test loss: 0.2567074771265092\n",
      "epoch 16579: train loss: 0.10848588842640777, test loss: 0.25670843850734676\n",
      "epoch 16580: train loss: 0.10848416010753889, test loss: 0.25670939994724473\n",
      "epoch 16581: train loss: 0.10848243192956637, test loss: 0.25671036144619086\n",
      "epoch 16582: train loss: 0.10848070389246944, test loss: 0.25671132300413446\n",
      "epoch 16583: train loss: 0.1084789759962272, test loss: 0.2567122846211062\n",
      "epoch 16584: train loss: 0.10847724824081889, test loss: 0.2567132462970798\n",
      "epoch 16585: train loss: 0.10847552062622362, test loss: 0.2567142080320563\n",
      "epoch 16586: train loss: 0.10847379315242066, test loss: 0.25671516982597353\n",
      "epoch 16587: train loss: 0.10847206581938913, test loss: 0.2567161316788592\n",
      "epoch 16588: train loss: 0.10847033862710824, test loss: 0.2567170935907289\n",
      "epoch 16589: train loss: 0.10846861157555718, test loss: 0.25671805556151817\n",
      "epoch 16590: train loss: 0.1084668846647152, test loss: 0.2567190175912147\n",
      "epoch 16591: train loss: 0.10846515789456142, test loss: 0.2567199796798352\n",
      "epoch 16592: train loss: 0.10846343126507514, test loss: 0.25672094182735533\n",
      "epoch 16593: train loss: 0.10846170477623554, test loss: 0.256721904033764\n",
      "epoch 16594: train loss: 0.10845997842802182, test loss: 0.25672286629906227\n",
      "epoch 16595: train loss: 0.10845825222041323, test loss: 0.2567238286232269\n",
      "epoch 16596: train loss: 0.108456526153389, test loss: 0.25672479100623247\n",
      "epoch 16597: train loss: 0.10845480022692833, test loss: 0.2567257534480556\n",
      "epoch 16598: train loss: 0.10845307444101049, test loss: 0.25672671594873697\n",
      "epoch 16599: train loss: 0.1084513487956147, test loss: 0.2567276785081994\n",
      "epoch 16600: train loss: 0.10844962329072022, test loss: 0.25672864112650035\n",
      "epoch 16601: train loss: 0.1084478979263063, test loss: 0.25672960380355975\n",
      "epoch 16602: train loss: 0.10844617270235218, test loss: 0.2567305665394067\n",
      "epoch 16603: train loss: 0.10844444761883715, test loss: 0.25673152933403126\n",
      "epoch 16604: train loss: 0.10844272267574045, test loss: 0.25673249218738137\n",
      "epoch 16605: train loss: 0.10844099787304134, test loss: 0.25673345509948664\n",
      "epoch 16606: train loss: 0.10843927321071911, test loss: 0.25673441807032277\n",
      "epoch 16607: train loss: 0.10843754868875301, test loss: 0.25673538109986543\n",
      "epoch 16608: train loss: 0.10843582430712236, test loss: 0.2567363441881031\n",
      "epoch 16609: train loss: 0.10843410006580644, test loss: 0.25673730733503763\n",
      "epoch 16610: train loss: 0.10843237596478451, test loss: 0.2567382705406465\n",
      "epoch 16611: train loss: 0.10843065200403586, test loss: 0.2567392338049304\n",
      "epoch 16612: train loss: 0.10842892818353983, test loss: 0.25674019712786517\n",
      "epoch 16613: train loss: 0.10842720450327568, test loss: 0.25674116050944035\n",
      "epoch 16614: train loss: 0.10842548096322276, test loss: 0.2567421239496323\n",
      "epoch 16615: train loss: 0.10842375756336034, test loss: 0.25674308744844176\n",
      "epoch 16616: train loss: 0.10842203430366776, test loss: 0.25674405100587316\n",
      "epoch 16617: train loss: 0.10842031118412436, test loss: 0.2567450146218741\n",
      "epoch 16618: train loss: 0.1084185882047094, test loss: 0.25674597829645995\n",
      "epoch 16619: train loss: 0.10841686536540228, test loss: 0.2567469420296067\n",
      "epoch 16620: train loss: 0.10841514266618227, test loss: 0.2567479058213184\n",
      "epoch 16621: train loss: 0.10841342010702876, test loss: 0.2567488696715698\n",
      "epoch 16622: train loss: 0.10841169768792104, test loss: 0.2567498335803355\n",
      "epoch 16623: train loss: 0.10840997540883851, test loss: 0.2567507975476332\n",
      "epoch 16624: train loss: 0.1084082532697605, test loss: 0.2567517615734371\n",
      "epoch 16625: train loss: 0.10840653127066638, test loss: 0.2567527256577241\n",
      "epoch 16626: train loss: 0.10840480941153544, test loss: 0.25675368980048274\n",
      "epoch 16627: train loss: 0.10840308769234713, test loss: 0.25675465400171527\n",
      "epoch 16628: train loss: 0.10840136611308077, test loss: 0.2567556182614394\n",
      "epoch 16629: train loss: 0.10839964467371577, test loss: 0.2567565825795616\n",
      "epoch 16630: train loss: 0.10839792337423143, test loss: 0.25675754695613995\n",
      "epoch 16631: train loss: 0.10839620221460722, test loss: 0.25675851139109485\n",
      "epoch 16632: train loss: 0.10839448119482245, test loss: 0.25675947588449727\n",
      "epoch 16633: train loss: 0.1083927603148566, test loss: 0.25676044043626944\n",
      "epoch 16634: train loss: 0.108391039574689, test loss: 0.2567614050464403\n",
      "epoch 16635: train loss: 0.10838931897429906, test loss: 0.2567623697149847\n",
      "epoch 16636: train loss: 0.10838759851366614, test loss: 0.2567633344418672\n",
      "epoch 16637: train loss: 0.10838587819276974, test loss: 0.25676429922708904\n",
      "epoch 16638: train loss: 0.10838415801158918, test loss: 0.2567652640706669\n",
      "epoch 16639: train loss: 0.10838243797010394, test loss: 0.25676622897253637\n",
      "epoch 16640: train loss: 0.1083807180682934, test loss: 0.25676719393274\n",
      "epoch 16641: train loss: 0.10837899830613702, test loss: 0.2567681589512407\n",
      "epoch 16642: train loss: 0.10837727868361421, test loss: 0.2567691240280002\n",
      "epoch 16643: train loss: 0.1083755592007044, test loss: 0.2567700891630355\n",
      "epoch 16644: train loss: 0.10837383985738702, test loss: 0.2567710543563345\n",
      "epoch 16645: train loss: 0.10837212065364152, test loss: 0.2567720196078748\n",
      "epoch 16646: train loss: 0.10837040158944738, test loss: 0.2567729849176459\n",
      "epoch 16647: train loss: 0.108368682664784, test loss: 0.25677395028564814\n",
      "epoch 16648: train loss: 0.10836696387963084, test loss: 0.25677491571186123\n",
      "epoch 16649: train loss: 0.10836524523396736, test loss: 0.2567758811962716\n",
      "epoch 16650: train loss: 0.10836352672777305, test loss: 0.2567768467388546\n",
      "epoch 16651: train loss: 0.10836180836102738, test loss: 0.2567778123396161\n",
      "epoch 16652: train loss: 0.10836009013370976, test loss: 0.25677877799855614\n",
      "epoch 16653: train loss: 0.10835837204579976, test loss: 0.2567797437156261\n",
      "epoch 16654: train loss: 0.10835665409727678, test loss: 0.25678070949081383\n",
      "epoch 16655: train loss: 0.10835493628812035, test loss: 0.2567816753241484\n",
      "epoch 16656: train loss: 0.10835321861830995, test loss: 0.25678264121558025\n",
      "epoch 16657: train loss: 0.10835150108782504, test loss: 0.25678360716513926\n",
      "epoch 16658: train loss: 0.10834978369664518, test loss: 0.256784573172774\n",
      "epoch 16659: train loss: 0.1083480664447498, test loss: 0.2567855392384751\n",
      "epoch 16660: train loss: 0.10834634933211848, test loss: 0.2567865053622028\n",
      "epoch 16661: train loss: 0.10834463235873068, test loss: 0.2567874715440158\n",
      "epoch 16662: train loss: 0.10834291552456594, test loss: 0.256788437783875\n",
      "epoch 16663: train loss: 0.10834119882960376, test loss: 0.2567894040817588\n",
      "epoch 16664: train loss: 0.10833948227382369, test loss: 0.2567903704376555\n",
      "epoch 16665: train loss: 0.10833776585720521, test loss: 0.25679133685152616\n",
      "epoch 16666: train loss: 0.10833604957972788, test loss: 0.2567923033234036\n",
      "epoch 16667: train loss: 0.10833433344137128, test loss: 0.25679326985326106\n",
      "epoch 16668: train loss: 0.10833261744211488, test loss: 0.25679423644107635\n",
      "epoch 16669: train loss: 0.10833090158193828, test loss: 0.2567952030868521\n",
      "epoch 16670: train loss: 0.10832918586082098, test loss: 0.2567961697905503\n",
      "epoch 16671: train loss: 0.1083274702787426, test loss: 0.2567971365522023\n",
      "epoch 16672: train loss: 0.10832575483568259, test loss: 0.2567981033717567\n",
      "epoch 16673: train loss: 0.10832403953162062, test loss: 0.2567990702492159\n",
      "epoch 16674: train loss: 0.10832232436653623, test loss: 0.2568000371845433\n",
      "epoch 16675: train loss: 0.10832060934040894, test loss: 0.25680100417779544\n",
      "epoch 16676: train loss: 0.10831889445321838, test loss: 0.25680197122886694\n",
      "epoch 16677: train loss: 0.10831717970494413, test loss: 0.2568029383378026\n",
      "epoch 16678: train loss: 0.10831546509556572, test loss: 0.25680390550460513\n",
      "epoch 16679: train loss: 0.1083137506250628, test loss: 0.25680487272922314\n",
      "epoch 16680: train loss: 0.10831203629341492, test loss: 0.2568058400116615\n",
      "epoch 16681: train loss: 0.1083103221006017, test loss: 0.25680680735189365\n",
      "epoch 16682: train loss: 0.10830860804660274, test loss: 0.25680777474993854\n",
      "epoch 16683: train loss: 0.10830689413139762, test loss: 0.256808742205772\n",
      "epoch 16684: train loss: 0.10830518035496597, test loss: 0.2568097097193424\n",
      "epoch 16685: train loss: 0.10830346671728742, test loss: 0.2568106772906927\n",
      "epoch 16686: train loss: 0.10830175321834154, test loss: 0.2568116449197748\n",
      "epoch 16687: train loss: 0.10830003985810802, test loss: 0.2568126126065892\n",
      "epoch 16688: train loss: 0.10829832663656642, test loss: 0.25681358035114155\n",
      "epoch 16689: train loss: 0.10829661355369641, test loss: 0.2568145481533922\n",
      "epoch 16690: train loss: 0.10829490060947759, test loss: 0.2568155160133181\n",
      "epoch 16691: train loss: 0.10829318780388966, test loss: 0.2568164839309508\n",
      "epoch 16692: train loss: 0.10829147513691222, test loss: 0.2568174519062518\n",
      "epoch 16693: train loss: 0.1082897626085249, test loss: 0.2568184199392253\n",
      "epoch 16694: train loss: 0.1082880502187074, test loss: 0.256819388029819\n",
      "epoch 16695: train loss: 0.10828633796743933, test loss: 0.2568203561780643\n",
      "epoch 16696: train loss: 0.1082846258547004, test loss: 0.256821324383925\n",
      "epoch 16697: train loss: 0.10828291388047023, test loss: 0.2568222926474287\n",
      "epoch 16698: train loss: 0.10828120204472853, test loss: 0.25682326096847335\n",
      "epoch 16699: train loss: 0.10827949034745493, test loss: 0.256824229347155\n",
      "epoch 16700: train loss: 0.10827777878862914, test loss: 0.25682519778339685\n",
      "epoch 16701: train loss: 0.10827606736823084, test loss: 0.25682616627720234\n",
      "epoch 16702: train loss: 0.1082743560862397, test loss: 0.256827134828548\n",
      "epoch 16703: train loss: 0.10827264494263542, test loss: 0.25682810343742135\n",
      "epoch 16704: train loss: 0.10827093393739767, test loss: 0.2568290721038564\n",
      "epoch 16705: train loss: 0.10826922307050621, test loss: 0.2568300408277731\n",
      "epoch 16706: train loss: 0.10826751234194067, test loss: 0.2568310096092297\n",
      "epoch 16707: train loss: 0.10826580175168082, test loss: 0.2568319784481349\n",
      "epoch 16708: train loss: 0.10826409129970635, test loss: 0.25683294734454637\n",
      "epoch 16709: train loss: 0.10826238098599694, test loss: 0.25683391629841235\n",
      "epoch 16710: train loss: 0.10826067081053235, test loss: 0.2568348853097238\n",
      "epoch 16711: train loss: 0.1082589607732923, test loss: 0.2568358543784857\n",
      "epoch 16712: train loss: 0.10825725087425654, test loss: 0.2568368235046703\n",
      "epoch 16713: train loss: 0.10825554111340477, test loss: 0.25683779268827167\n",
      "epoch 16714: train loss: 0.10825383149071673, test loss: 0.2568387619292911\n",
      "epoch 16715: train loss: 0.10825212200617215, test loss: 0.25683973122770515\n",
      "epoch 16716: train loss: 0.10825041265975081, test loss: 0.2568407005834772\n",
      "epoch 16717: train loss: 0.10824870345143242, test loss: 0.25684166999665214\n",
      "epoch 16718: train loss: 0.1082469943811968, test loss: 0.2568426394671646\n",
      "epoch 16719: train loss: 0.10824528544902362, test loss: 0.2568436089950042\n",
      "epoch 16720: train loss: 0.1082435766548927, test loss: 0.25684457858021753\n",
      "epoch 16721: train loss: 0.10824186799878381, test loss: 0.2568455482227103\n",
      "epoch 16722: train loss: 0.10824015948067672, test loss: 0.2568465179225298\n",
      "epoch 16723: train loss: 0.10823845110055115, test loss: 0.2568474876796628\n",
      "epoch 16724: train loss: 0.10823674285838696, test loss: 0.2568484574940474\n",
      "epoch 16725: train loss: 0.10823503475416388, test loss: 0.25684942736572675\n",
      "epoch 16726: train loss: 0.1082333267878617, test loss: 0.2568503972946644\n",
      "epoch 16727: train loss: 0.10823161895946022, test loss: 0.2568513672808377\n",
      "epoch 16728: train loss: 0.10822991126893929, test loss: 0.256852337324262\n",
      "epoch 16729: train loss: 0.10822820371627863, test loss: 0.2568533074249022\n",
      "epoch 16730: train loss: 0.10822649630145807, test loss: 0.25685427758275897\n",
      "epoch 16731: train loss: 0.10822478902445745, test loss: 0.2568552477978132\n",
      "epoch 16732: train loss: 0.10822308188525655, test loss: 0.2568562180700373\n",
      "epoch 16733: train loss: 0.1082213748838352, test loss: 0.2568571883994509\n",
      "epoch 16734: train loss: 0.10821966802017322, test loss: 0.25685815878605794\n",
      "epoch 16735: train loss: 0.10821796129425044, test loss: 0.25685912922978\n",
      "epoch 16736: train loss: 0.10821625470604669, test loss: 0.2568600997306605\n",
      "epoch 16737: train loss: 0.1082145482555418, test loss: 0.2568610702886507\n",
      "epoch 16738: train loss: 0.10821284194271563, test loss: 0.2568620409037799\n",
      "epoch 16739: train loss: 0.10821113576754797, test loss: 0.25686301157599867\n",
      "epoch 16740: train loss: 0.10820942973001872, test loss: 0.2568639823052966\n",
      "epoch 16741: train loss: 0.10820772383010771, test loss: 0.2568649530916902\n",
      "epoch 16742: train loss: 0.10820601806779478, test loss: 0.25686592393515895\n",
      "epoch 16743: train loss: 0.10820431244305985, test loss: 0.2568668948356778\n",
      "epoch 16744: train loss: 0.10820260695588274, test loss: 0.2568678657932222\n",
      "epoch 16745: train loss: 0.10820090160624328, test loss: 0.2568688368078246\n",
      "epoch 16746: train loss: 0.10819919639412141, test loss: 0.25686980787944796\n",
      "epoch 16747: train loss: 0.10819749131949699, test loss: 0.25687077900806904\n",
      "epoch 16748: train loss: 0.10819578638234989, test loss: 0.2568717501936782\n",
      "epoch 16749: train loss: 0.10819408158265997, test loss: 0.25687272143627815\n",
      "epoch 16750: train loss: 0.10819237692040717, test loss: 0.25687369273584687\n",
      "epoch 16751: train loss: 0.10819067239557136, test loss: 0.2568746640923735\n",
      "epoch 16752: train loss: 0.10818896800813242, test loss: 0.2568756355058353\n",
      "epoch 16753: train loss: 0.10818726375807028, test loss: 0.25687660697626497\n",
      "epoch 16754: train loss: 0.10818555964536485, test loss: 0.2568775785036094\n",
      "epoch 16755: train loss: 0.10818385566999603, test loss: 0.2568785500878603\n",
      "epoch 16756: train loss: 0.1081821518319437, test loss: 0.25687952172899314\n",
      "epoch 16757: train loss: 0.10818044813118782, test loss: 0.25688049342704117\n",
      "epoch 16758: train loss: 0.10817874456770829, test loss: 0.2568814651819389\n",
      "epoch 16759: train loss: 0.10817704114148509, test loss: 0.2568824369937047\n",
      "epoch 16760: train loss: 0.10817533785249807, test loss: 0.2568834088623417\n",
      "epoch 16761: train loss: 0.10817363470072722, test loss: 0.25688438078780024\n",
      "epoch 16762: train loss: 0.10817193168615248, test loss: 0.256885352770112\n",
      "epoch 16763: train loss: 0.10817022880875378, test loss: 0.2568863248092245\n",
      "epoch 16764: train loss: 0.10816852606851106, test loss: 0.256887296905157\n",
      "epoch 16765: train loss: 0.10816682346540427, test loss: 0.2568882690578587\n",
      "epoch 16766: train loss: 0.1081651209994134, test loss: 0.256889241267347\n",
      "epoch 16767: train loss: 0.10816341867051836, test loss: 0.25689021353359875\n",
      "epoch 16768: train loss: 0.10816171647869915, test loss: 0.25689118585661924\n",
      "epoch 16769: train loss: 0.10816001442393576, test loss: 0.25689215823637057\n",
      "epoch 16770: train loss: 0.10815831250620811, test loss: 0.25689313067285624\n",
      "epoch 16771: train loss: 0.10815661072549622, test loss: 0.2568941031660824\n",
      "epoch 16772: train loss: 0.10815490908178005, test loss: 0.25689507571598236\n",
      "epoch 16773: train loss: 0.10815320757503956, test loss: 0.2568960483226169\n",
      "epoch 16774: train loss: 0.1081515062052548, test loss: 0.25689702098590705\n",
      "epoch 16775: train loss: 0.10814980497240573, test loss: 0.2568979937058988\n",
      "epoch 16776: train loss: 0.10814810387647235, test loss: 0.2568989664825134\n",
      "epoch 16777: train loss: 0.10814640291743467, test loss: 0.25689993931579785\n",
      "epoch 16778: train loss: 0.10814470209527269, test loss: 0.2569009122057269\n",
      "epoch 16779: train loss: 0.10814300140996641, test loss: 0.2569018851522508\n",
      "epoch 16780: train loss: 0.10814130086149586, test loss: 0.25690285815541564\n",
      "epoch 16781: train loss: 0.10813960044984107, test loss: 0.2569038312151697\n",
      "epoch 16782: train loss: 0.10813790017498207, test loss: 0.25690480433151913\n",
      "epoch 16783: train loss: 0.10813620003689882, test loss: 0.2569057775044664\n",
      "epoch 16784: train loss: 0.1081345000355714, test loss: 0.2569067507339469\n",
      "epoch 16785: train loss: 0.10813280017097987, test loss: 0.25690772401998047\n",
      "epoch 16786: train loss: 0.10813110044310426, test loss: 0.25690869736258465\n",
      "epoch 16787: train loss: 0.10812940085192459, test loss: 0.256909670761694\n",
      "epoch 16788: train loss: 0.10812770139742088, test loss: 0.2569106442173284\n",
      "epoch 16789: train loss: 0.10812600207957326, test loss: 0.2569116177294638\n",
      "epoch 16790: train loss: 0.10812430289836172, test loss: 0.25691259129810506\n",
      "epoch 16791: train loss: 0.10812260385376636, test loss: 0.2569135649232282\n",
      "epoch 16792: train loss: 0.10812090494576725, test loss: 0.2569145386048244\n",
      "epoch 16793: train loss: 0.10811920617434442, test loss: 0.25691551234285637\n",
      "epoch 16794: train loss: 0.10811750753947795, test loss: 0.2569164861373573\n",
      "epoch 16795: train loss: 0.10811580904114795, test loss: 0.2569174599882761\n",
      "epoch 16796: train loss: 0.10811411067933449, test loss: 0.2569184338956442\n",
      "epoch 16797: train loss: 0.10811241245401765, test loss: 0.2569194078594112\n",
      "epoch 16798: train loss: 0.10811071436517752, test loss: 0.25692038187956784\n",
      "epoch 16799: train loss: 0.10810901641279419, test loss: 0.2569213559561055\n",
      "epoch 16800: train loss: 0.10810731859684776, test loss: 0.2569223300890281\n",
      "epoch 16801: train loss: 0.10810562091731832, test loss: 0.2569233042783129\n",
      "epoch 16802: train loss: 0.108103923374186, test loss: 0.2569242785239511\n",
      "epoch 16803: train loss: 0.10810222596743092, test loss: 0.25692525282594614\n",
      "epoch 16804: train loss: 0.10810052869703317, test loss: 0.25692622718424896\n",
      "epoch 16805: train loss: 0.10809883156297286, test loss: 0.25692720159887716\n",
      "epoch 16806: train loss: 0.10809713456523012, test loss: 0.25692817606979296\n",
      "epoch 16807: train loss: 0.10809543770378509, test loss: 0.25692915059700216\n",
      "epoch 16808: train loss: 0.10809374097861792, test loss: 0.2569301251805232\n",
      "epoch 16809: train loss: 0.1080920443897087, test loss: 0.2569310998202907\n",
      "epoch 16810: train loss: 0.10809034793703759, test loss: 0.25693207451632494\n",
      "epoch 16811: train loss: 0.10808865162058474, test loss: 0.25693304926857424\n",
      "epoch 16812: train loss: 0.10808695544033028, test loss: 0.256934024077084\n",
      "epoch 16813: train loss: 0.10808525939625441, test loss: 0.2569349989418045\n",
      "epoch 16814: train loss: 0.10808356348833721, test loss: 0.2569359738627404\n",
      "epoch 16815: train loss: 0.10808186771655892, test loss: 0.2569369488398833\n",
      "epoch 16816: train loss: 0.10808017208089965, test loss: 0.2569379238732097\n",
      "epoch 16817: train loss: 0.10807847658133957, test loss: 0.2569388989626956\n",
      "epoch 16818: train loss: 0.1080767812178589, test loss: 0.2569398741083603\n",
      "epoch 16819: train loss: 0.10807508599043775, test loss: 0.25694084931018196\n",
      "epoch 16820: train loss: 0.10807339089905632, test loss: 0.2569418245680953\n",
      "epoch 16821: train loss: 0.10807169594369484, test loss: 0.25694279988218727\n",
      "epoch 16822: train loss: 0.10807000112433349, test loss: 0.25694377525238116\n",
      "epoch 16823: train loss: 0.1080683064409524, test loss: 0.2569447506786679\n",
      "epoch 16824: train loss: 0.10806661189353182, test loss: 0.2569457261610506\n",
      "epoch 16825: train loss: 0.10806491748205196, test loss: 0.2569467016995226\n",
      "epoch 16826: train loss: 0.108063223206493, test loss: 0.2569476772940589\n",
      "epoch 16827: train loss: 0.10806152906683514, test loss: 0.25694865294463887\n",
      "epoch 16828: train loss: 0.1080598350630586, test loss: 0.2569496286512785\n",
      "epoch 16829: train loss: 0.10805814119514366, test loss: 0.2569506044139165\n",
      "epoch 16830: train loss: 0.10805644746307046, test loss: 0.25695158023260994\n",
      "epoch 16831: train loss: 0.10805475386681924, test loss: 0.25695255610731094\n",
      "epoch 16832: train loss: 0.10805306040637028, test loss: 0.25695353203799376\n",
      "epoch 16833: train loss: 0.10805136708170375, test loss: 0.2569545080246952\n",
      "epoch 16834: train loss: 0.10804967389279994, test loss: 0.25695548406734753\n",
      "epoch 16835: train loss: 0.10804798083963905, test loss: 0.25695646016594353\n",
      "epoch 16836: train loss: 0.10804628792220139, test loss: 0.2569574363205154\n",
      "epoch 16837: train loss: 0.10804459514046715, test loss: 0.25695841253099816\n",
      "epoch 16838: train loss: 0.1080429024944166, test loss: 0.25695938879745234\n",
      "epoch 16839: train loss: 0.10804120998402998, test loss: 0.2569603651197739\n",
      "epoch 16840: train loss: 0.10803951760928761, test loss: 0.25696134149803407\n",
      "epoch 16841: train loss: 0.10803782537016973, test loss: 0.2569623179321856\n",
      "epoch 16842: train loss: 0.10803613326665658, test loss: 0.2569632944221883\n",
      "epoch 16843: train loss: 0.1080344412987285, test loss: 0.25696427096807795\n",
      "epoch 16844: train loss: 0.10803274946636571, test loss: 0.25696524756983075\n",
      "epoch 16845: train loss: 0.10803105776954851, test loss: 0.2569662242274107\n",
      "epoch 16846: train loss: 0.10802936620825719, test loss: 0.2569672009408218\n",
      "epoch 16847: train loss: 0.10802767478247205, test loss: 0.2569681777100697\n",
      "epoch 16848: train loss: 0.1080259834921734, test loss: 0.2569691545351173\n",
      "epoch 16849: train loss: 0.10802429233734148, test loss: 0.2569701314159571\n",
      "epoch 16850: train loss: 0.10802260131795666, test loss: 0.25697110835259357\n",
      "epoch 16851: train loss: 0.10802091043399922, test loss: 0.25697208534500243\n",
      "epoch 16852: train loss: 0.10801921968544949, test loss: 0.25697306239317785\n",
      "epoch 16853: train loss: 0.10801752907228775, test loss: 0.25697403949709485\n",
      "epoch 16854: train loss: 0.10801583859449437, test loss: 0.2569750166567599\n",
      "epoch 16855: train loss: 0.10801414825204962, test loss: 0.25697599387216363\n",
      "epoch 16856: train loss: 0.10801245804493387, test loss: 0.25697697114324136\n",
      "epoch 16857: train loss: 0.10801076797312743, test loss: 0.25697794847005545\n",
      "epoch 16858: train loss: 0.10800907803661065, test loss: 0.2569789258525533\n",
      "epoch 16859: train loss: 0.10800738823536388, test loss: 0.25697990329072856\n",
      "epoch 16860: train loss: 0.10800569856936743, test loss: 0.25698088078458425\n",
      "epoch 16861: train loss: 0.10800400903860169, test loss: 0.25698185833409765\n",
      "epoch 16862: train loss: 0.10800231964304696, test loss: 0.25698283593923554\n",
      "epoch 16863: train loss: 0.10800063038268366, test loss: 0.2569838136000133\n",
      "epoch 16864: train loss: 0.10799894125749213, test loss: 0.2569847913164387\n",
      "epoch 16865: train loss: 0.10799725226745271, test loss: 0.25698576908843196\n",
      "epoch 16866: train loss: 0.1079955634125458, test loss: 0.2569867469160412\n",
      "epoch 16867: train loss: 0.10799387469275175, test loss: 0.25698772479927096\n",
      "epoch 16868: train loss: 0.10799218610805095, test loss: 0.2569887027380295\n",
      "epoch 16869: train loss: 0.10799049765842379, test loss: 0.25698968073236317\n",
      "epoch 16870: train loss: 0.10798880934385066, test loss: 0.2569906587822646\n",
      "epoch 16871: train loss: 0.1079871211643119, test loss: 0.2569916368877114\n",
      "epoch 16872: train loss: 0.10798543311978798, test loss: 0.256992615048651\n",
      "epoch 16873: train loss: 0.10798374521025923, test loss: 0.256993593265146\n",
      "epoch 16874: train loss: 0.10798205743570609, test loss: 0.25699457153711963\n",
      "epoch 16875: train loss: 0.10798036979610895, test loss: 0.2569955498645882\n",
      "epoch 16876: train loss: 0.10797868229144825, test loss: 0.2569965282475311\n",
      "epoch 16877: train loss: 0.10797699492170436, test loss: 0.25699750668596705\n",
      "epoch 16878: train loss: 0.10797530768685772, test loss: 0.25699848517984764\n",
      "epoch 16879: train loss: 0.10797362058688875, test loss: 0.2569994637292025\n",
      "epoch 16880: train loss: 0.10797193362177789, test loss: 0.2570004423339688\n",
      "epoch 16881: train loss: 0.10797024679150556, test loss: 0.2570014209941543\n",
      "epoch 16882: train loss: 0.10796856009605219, test loss: 0.2570023997097469\n",
      "epoch 16883: train loss: 0.10796687353539822, test loss: 0.25700337848074134\n",
      "epoch 16884: train loss: 0.10796518710952409, test loss: 0.2570043573071557\n",
      "epoch 16885: train loss: 0.10796350081841027, test loss: 0.2570053361889246\n",
      "epoch 16886: train loss: 0.10796181466203718, test loss: 0.2570063151260405\n",
      "epoch 16887: train loss: 0.10796012864038528, test loss: 0.2570072941185383\n",
      "epoch 16888: train loss: 0.10795844275343501, test loss: 0.25700827316637886\n",
      "epoch 16889: train loss: 0.10795675700116691, test loss: 0.2570092522695422\n",
      "epoch 16890: train loss: 0.10795507138356139, test loss: 0.2570102314280037\n",
      "epoch 16891: train loss: 0.10795338590059891, test loss: 0.257011210641784\n",
      "epoch 16892: train loss: 0.10795170055225997, test loss: 0.2570121899108765\n",
      "epoch 16893: train loss: 0.10795001533852505, test loss: 0.25701316923525547\n",
      "epoch 16894: train loss: 0.10794833025937461, test loss: 0.2570141486148736\n",
      "epoch 16895: train loss: 0.10794664531478915, test loss: 0.2570151280497766\n",
      "epoch 16896: train loss: 0.10794496050474917, test loss: 0.25701610753994153\n",
      "epoch 16897: train loss: 0.10794327582923514, test loss: 0.2570170870853201\n",
      "epoch 16898: train loss: 0.10794159128822763, test loss: 0.2570180666859313\n",
      "epoch 16899: train loss: 0.10793990688170703, test loss: 0.2570190463417662\n",
      "epoch 16900: train loss: 0.10793822260965395, test loss: 0.25702002605280194\n",
      "epoch 16901: train loss: 0.10793653847204883, test loss: 0.2570210058190181\n",
      "epoch 16902: train loss: 0.10793485446887224, test loss: 0.25702198564040585\n",
      "epoch 16903: train loss: 0.10793317060010466, test loss: 0.25702296551698306\n",
      "epoch 16904: train loss: 0.10793148686572664, test loss: 0.25702394544870133\n",
      "epoch 16905: train loss: 0.1079298032657187, test loss: 0.2570249254355664\n",
      "epoch 16906: train loss: 0.10792811980006135, test loss: 0.25702590547756826\n",
      "epoch 16907: train loss: 0.10792643646873516, test loss: 0.2570268855746999\n",
      "epoch 16908: train loss: 0.10792475327172067, test loss: 0.2570278657269396\n",
      "epoch 16909: train loss: 0.10792307020899837, test loss: 0.25702884593426356\n",
      "epoch 16910: train loss: 0.10792138728054887, test loss: 0.2570298261966926\n",
      "epoch 16911: train loss: 0.10791970448635269, test loss: 0.25703080651417615\n",
      "epoch 16912: train loss: 0.10791802182639039, test loss: 0.2570317868867356\n",
      "epoch 16913: train loss: 0.10791633930064254, test loss: 0.2570327673143453\n",
      "epoch 16914: train loss: 0.10791465690908968, test loss: 0.25703374779701355\n",
      "epoch 16915: train loss: 0.1079129746517124, test loss: 0.2570347283346628\n",
      "epoch 16916: train loss: 0.10791129252849128, test loss: 0.25703570892738026\n",
      "epoch 16917: train loss: 0.10790961053940688, test loss: 0.2570366895750893\n",
      "epoch 16918: train loss: 0.1079079286844398, test loss: 0.2570376702777963\n",
      "epoch 16919: train loss: 0.10790624696357057, test loss: 0.25703865103546386\n",
      "epoch 16920: train loss: 0.10790456537677982, test loss: 0.2570396318481131\n",
      "epoch 16921: train loss: 0.10790288392404815, test loss: 0.25704061271572143\n",
      "epoch 16922: train loss: 0.10790120260535613, test loss: 0.2570415936382795\n",
      "epoch 16923: train loss: 0.10789952142068439, test loss: 0.2570425746157937\n",
      "epoch 16924: train loss: 0.1078978403700135, test loss: 0.25704355564821535\n",
      "epoch 16925: train loss: 0.10789615945332408, test loss: 0.2570445367355345\n",
      "epoch 16926: train loss: 0.10789447867059675, test loss: 0.2570455178777723\n",
      "epoch 16927: train loss: 0.10789279802181215, test loss: 0.25704649907490673\n",
      "epoch 16928: train loss: 0.10789111750695085, test loss: 0.25704748032691455\n",
      "epoch 16929: train loss: 0.10788943712599348, test loss: 0.2570484616337888\n",
      "epoch 16930: train loss: 0.10788775687892072, test loss: 0.25704944299553384\n",
      "epoch 16931: train loss: 0.10788607676571313, test loss: 0.25705042441208675\n",
      "epoch 16932: train loss: 0.10788439678635141, test loss: 0.25705140588352543\n",
      "epoch 16933: train loss: 0.10788271694081619, test loss: 0.2570523874097395\n",
      "epoch 16934: train loss: 0.10788103722908804, test loss: 0.2570533689907941\n",
      "epoch 16935: train loss: 0.1078793576511477, test loss: 0.25705435062662413\n",
      "epoch 16936: train loss: 0.1078776782069758, test loss: 0.25705533231725064\n",
      "epoch 16937: train loss: 0.10787599889655297, test loss: 0.25705631406265056\n",
      "epoch 16938: train loss: 0.1078743197198599, test loss: 0.2570572958628293\n",
      "epoch 16939: train loss: 0.10787264067687723, test loss: 0.2570582777177393\n",
      "epoch 16940: train loss: 0.1078709617675856, test loss: 0.2570592596273831\n",
      "epoch 16941: train loss: 0.10786928299196574, test loss: 0.25706024159179713\n",
      "epoch 16942: train loss: 0.10786760434999834, test loss: 0.2570612236108754\n",
      "epoch 16943: train loss: 0.10786592584166402, test loss: 0.25706220568469296\n",
      "epoch 16944: train loss: 0.10786424746694348, test loss: 0.2570631878132016\n",
      "epoch 16945: train loss: 0.10786256922581743, test loss: 0.2570641699963903\n",
      "epoch 16946: train loss: 0.10786089111826652, test loss: 0.2570651522342539\n",
      "epoch 16947: train loss: 0.10785921314427149, test loss: 0.2570661345267976\n",
      "epoch 16948: train loss: 0.10785753530381305, test loss: 0.25706711687397116\n",
      "epoch 16949: train loss: 0.10785585759687186, test loss: 0.2570680992757665\n",
      "epoch 16950: train loss: 0.10785418002342866, test loss: 0.25706908173220394\n",
      "epoch 16951: train loss: 0.1078525025834641, test loss: 0.2570700642432634\n",
      "epoch 16952: train loss: 0.10785082527695898, test loss: 0.2570710468089194\n",
      "epoch 16953: train loss: 0.10784914810389398, test loss: 0.25707202942913904\n",
      "epoch 16954: train loss: 0.10784747106424984, test loss: 0.25707301210397004\n",
      "epoch 16955: train loss: 0.10784579415800728, test loss: 0.2570739948333608\n",
      "epoch 16956: train loss: 0.10784411738514703, test loss: 0.25707497761731923\n",
      "epoch 16957: train loss: 0.1078424407456498, test loss: 0.2570759604558371\n",
      "epoch 16958: train loss: 0.10784076423949637, test loss: 0.25707694334885023\n",
      "epoch 16959: train loss: 0.10783908786666746, test loss: 0.2570779262964206\n",
      "epoch 16960: train loss: 0.10783741162714384, test loss: 0.25707890929849964\n",
      "epoch 16961: train loss: 0.10783573552090622, test loss: 0.2570798923550489\n",
      "epoch 16962: train loss: 0.1078340595479354, test loss: 0.2570808754660919\n",
      "epoch 16963: train loss: 0.10783238370821213, test loss: 0.257081858631633\n",
      "epoch 16964: train loss: 0.10783070800171717, test loss: 0.25708284185165065\n",
      "epoch 16965: train loss: 0.10782903242843128, test loss: 0.2570838251261082\n",
      "epoch 16966: train loss: 0.10782735698833523, test loss: 0.25708480845499937\n",
      "epoch 16967: train loss: 0.10782568168140981, test loss: 0.25708579183831676\n",
      "epoch 16968: train loss: 0.1078240065076358, test loss: 0.2570867752760651\n",
      "epoch 16969: train loss: 0.10782233146699395, test loss: 0.2570877587682228\n",
      "epoch 16970: train loss: 0.10782065655946507, test loss: 0.2570887423147832\n",
      "epoch 16971: train loss: 0.10781898178503, test loss: 0.25708972591572404\n",
      "epoch 16972: train loss: 0.10781730714366944, test loss: 0.25709070957103697\n",
      "epoch 16973: train loss: 0.10781563263536427, test loss: 0.25709169328071463\n",
      "epoch 16974: train loss: 0.10781395826009522, test loss: 0.25709267704475025\n",
      "epoch 16975: train loss: 0.10781228401784318, test loss: 0.25709366086310703\n",
      "epoch 16976: train loss: 0.10781060990858891, test loss: 0.25709464473580546\n",
      "epoch 16977: train loss: 0.10780893593231322, test loss: 0.25709562866281155\n",
      "epoch 16978: train loss: 0.10780726208899698, test loss: 0.25709661264415656\n",
      "epoch 16979: train loss: 0.10780558837862095, test loss: 0.2570975966797658\n",
      "epoch 16980: train loss: 0.10780391480116598, test loss: 0.25709858076964287\n",
      "epoch 16981: train loss: 0.10780224135661293, test loss: 0.2570995649138243\n",
      "epoch 16982: train loss: 0.10780056804494262, test loss: 0.2571005491122603\n",
      "epoch 16983: train loss: 0.10779889486613584, test loss: 0.2571015333649567\n",
      "epoch 16984: train loss: 0.10779722182017348, test loss: 0.2571025176718761\n",
      "epoch 16985: train loss: 0.1077955489070364, test loss: 0.2571035020330144\n",
      "epoch 16986: train loss: 0.10779387612670542, test loss: 0.2571044864483759\n",
      "epoch 16987: train loss: 0.10779220347916141, test loss: 0.25710547091792485\n",
      "epoch 16988: train loss: 0.10779053096438523, test loss: 0.25710645544168315\n",
      "epoch 16989: train loss: 0.10778885858235773, test loss: 0.257107440019628\n",
      "epoch 16990: train loss: 0.10778718633305978, test loss: 0.2571084246517377\n",
      "epoch 16991: train loss: 0.10778551421647226, test loss: 0.2571094093380056\n",
      "epoch 16992: train loss: 0.10778384223257603, test loss: 0.25711039407839453\n",
      "epoch 16993: train loss: 0.10778217038135197, test loss: 0.25711137887296964\n",
      "epoch 16994: train loss: 0.107780498662781, test loss: 0.25711236372162327\n",
      "epoch 16995: train loss: 0.10777882707684394, test loss: 0.2571133486244034\n",
      "epoch 16996: train loss: 0.10777715562352175, test loss: 0.2571143335813055\n",
      "epoch 16997: train loss: 0.10777548430279527, test loss: 0.2571153185922633\n",
      "epoch 16998: train loss: 0.1077738131146454, test loss: 0.2571163036573117\n",
      "epoch 16999: train loss: 0.1077721420590531, test loss: 0.257117288776431\n",
      "epoch 17000: train loss: 0.10777047113599919, test loss: 0.2571182739496108\n",
      "epoch 17001: train loss: 0.10776880034546464, test loss: 0.2571192591768184\n",
      "epoch 17002: train loss: 0.10776712968743038, test loss: 0.2571202444580878\n",
      "epoch 17003: train loss: 0.10776545916187726, test loss: 0.25712122979336777\n",
      "epoch 17004: train loss: 0.10776378876878624, test loss: 0.2571222151826523\n",
      "epoch 17005: train loss: 0.10776211850813827, test loss: 0.25712320062593336\n",
      "epoch 17006: train loss: 0.10776044837991425, test loss: 0.25712418612321786\n",
      "epoch 17007: train loss: 0.1077587783840951, test loss: 0.2571251716744423\n",
      "epoch 17008: train loss: 0.10775710852066175, test loss: 0.257126157279685\n",
      "epoch 17009: train loss: 0.1077554387895952, test loss: 0.25712714293883715\n",
      "epoch 17010: train loss: 0.10775376919087637, test loss: 0.257128128651964\n",
      "epoch 17011: train loss: 0.1077520997244862, test loss: 0.2571291144190004\n",
      "epoch 17012: train loss: 0.10775043039040563, test loss: 0.25713010023996913\n",
      "epoch 17013: train loss: 0.10774876118861564, test loss: 0.25713108611481816\n",
      "epoch 17014: train loss: 0.10774709211909717, test loss: 0.2571320720435979\n",
      "epoch 17015: train loss: 0.1077454231818312, test loss: 0.2571330580262593\n",
      "epoch 17016: train loss: 0.10774375437679864, test loss: 0.2571340440627656\n",
      "epoch 17017: train loss: 0.1077420857039806, test loss: 0.2571350301531673\n",
      "epoch 17018: train loss: 0.10774041716335793, test loss: 0.2571360162974124\n",
      "epoch 17019: train loss: 0.10773874875491167, test loss: 0.2571370024955107\n",
      "epoch 17020: train loss: 0.1077370804786228, test loss: 0.2571379887474238\n",
      "epoch 17021: train loss: 0.10773541233447226, test loss: 0.25713897505314726\n",
      "epoch 17022: train loss: 0.1077337443224411, test loss: 0.2571399614127148\n",
      "epoch 17023: train loss: 0.10773207644251029, test loss: 0.2571409478260329\n",
      "epoch 17024: train loss: 0.10773040869466083, test loss: 0.2571419342931677\n",
      "epoch 17025: train loss: 0.10772874107887376, test loss: 0.2571429208140534\n",
      "epoch 17026: train loss: 0.10772707359513001, test loss: 0.2571439073887119\n",
      "epoch 17027: train loss: 0.10772540624341069, test loss: 0.257144894017123\n",
      "epoch 17028: train loss: 0.10772373902369672, test loss: 0.25714588069924926\n",
      "epoch 17029: train loss: 0.10772207193596918, test loss: 0.2571468674351548\n",
      "epoch 17030: train loss: 0.10772040498020907, test loss: 0.25714785422473413\n",
      "epoch 17031: train loss: 0.10771873815639743, test loss: 0.2571488410680209\n",
      "epoch 17032: train loss: 0.10771707146451527, test loss: 0.2571498279650234\n",
      "epoch 17033: train loss: 0.10771540490454366, test loss: 0.257150814915692\n",
      "epoch 17034: train loss: 0.10771373847646362, test loss: 0.2571518019200344\n",
      "epoch 17035: train loss: 0.10771207218025619, test loss: 0.25715278897805643\n",
      "epoch 17036: train loss: 0.10771040601590241, test loss: 0.25715377608970785\n",
      "epoch 17037: train loss: 0.10770873998338334, test loss: 0.25715476325498215\n",
      "epoch 17038: train loss: 0.10770707408268006, test loss: 0.25715575047391614\n",
      "epoch 17039: train loss: 0.10770540831377357, test loss: 0.25715673774644476\n",
      "epoch 17040: train loss: 0.107703742676645, test loss: 0.25715772507258877\n",
      "epoch 17041: train loss: 0.10770207717127533, test loss: 0.25715871245232863\n",
      "epoch 17042: train loss: 0.10770041179764572, test loss: 0.2571596998856266\n",
      "epoch 17043: train loss: 0.10769874655573719, test loss: 0.2571606873725052\n",
      "epoch 17044: train loss: 0.10769708144553086, test loss: 0.25716167491297237\n",
      "epoch 17045: train loss: 0.10769541646700777, test loss: 0.2571626625069492\n",
      "epoch 17046: train loss: 0.10769375162014903, test loss: 0.2571636501544858\n",
      "epoch 17047: train loss: 0.1076920869049357, test loss: 0.25716463785554483\n",
      "epoch 17048: train loss: 0.1076904223213489, test loss: 0.2571656256101087\n",
      "epoch 17049: train loss: 0.10768875786936973, test loss: 0.25716661341819536\n",
      "epoch 17050: train loss: 0.10768709354897929, test loss: 0.2571676012797438\n",
      "epoch 17051: train loss: 0.10768542936015865, test loss: 0.2571685891947875\n",
      "epoch 17052: train loss: 0.10768376530288898, test loss: 0.25716957716332073\n",
      "epoch 17053: train loss: 0.10768210137715134, test loss: 0.257170565185292\n",
      "epoch 17054: train loss: 0.10768043758292686, test loss: 0.25717155326069796\n",
      "epoch 17055: train loss: 0.10767877392019667, test loss: 0.25717254138955603\n",
      "epoch 17056: train loss: 0.10767711038894191, test loss: 0.257173529571848\n",
      "epoch 17057: train loss: 0.10767544698914369, test loss: 0.2571745178075362\n",
      "epoch 17058: train loss: 0.10767378372078312, test loss: 0.2571755060966303\n",
      "epoch 17059: train loss: 0.10767212058384137, test loss: 0.25717649443913715\n",
      "epoch 17060: train loss: 0.10767045757829959, test loss: 0.2571774828349767\n",
      "epoch 17061: train loss: 0.10766879470413887, test loss: 0.2571784712842283\n",
      "epoch 17062: train loss: 0.1076671319613404, test loss: 0.25717945978683054\n",
      "epoch 17063: train loss: 0.10766546934988533, test loss: 0.2571804483427723\n",
      "epoch 17064: train loss: 0.10766380686975481, test loss: 0.2571814369520497\n",
      "epoch 17065: train loss: 0.10766214452092998, test loss: 0.2571824256146412\n",
      "epoch 17066: train loss: 0.10766048230339202, test loss: 0.2571834143305678\n",
      "epoch 17067: train loss: 0.1076588202171221, test loss: 0.25718440309976687\n",
      "epoch 17068: train loss: 0.1076571582621014, test loss: 0.2571853919222577\n",
      "epoch 17069: train loss: 0.10765549643831107, test loss: 0.25718638079805084\n",
      "epoch 17070: train loss: 0.1076538347457323, test loss: 0.2571873697270943\n",
      "epoch 17071: train loss: 0.10765217318434628, test loss: 0.25718835870941037\n",
      "epoch 17072: train loss: 0.10765051175413419, test loss: 0.2571893477449639\n",
      "epoch 17073: train loss: 0.10764885045507723, test loss: 0.2571903368337486\n",
      "epoch 17074: train loss: 0.10764718928715655, test loss: 0.2571913259757702\n",
      "epoch 17075: train loss: 0.10764552825035344, test loss: 0.2571923151710109\n",
      "epoch 17076: train loss: 0.107643867344649, test loss: 0.25719330441943206\n",
      "epoch 17077: train loss: 0.10764220657002448, test loss: 0.2571942937210283\n",
      "epoch 17078: train loss: 0.10764054592646109, test loss: 0.2571952830758223\n",
      "epoch 17079: train loss: 0.10763888541394007, test loss: 0.257196272483791\n",
      "epoch 17080: train loss: 0.10763722503244258, test loss: 0.25719726194491477\n",
      "epoch 17081: train loss: 0.10763556478194988, test loss: 0.2571982514591869\n",
      "epoch 17082: train loss: 0.10763390466244319, test loss: 0.25719924102660013\n",
      "epoch 17083: train loss: 0.10763224467390373, test loss: 0.2572002306471197\n",
      "epoch 17084: train loss: 0.10763058481631274, test loss: 0.25720122032075404\n",
      "epoch 17085: train loss: 0.10762892508965144, test loss: 0.25720221004746524\n",
      "epoch 17086: train loss: 0.10762726549390109, test loss: 0.25720319982730655\n",
      "epoch 17087: train loss: 0.10762560602904295, test loss: 0.25720418966019787\n",
      "epoch 17088: train loss: 0.1076239466950582, test loss: 0.2572051795461624\n",
      "epoch 17089: train loss: 0.10762228749192819, test loss: 0.2572061694852067\n",
      "epoch 17090: train loss: 0.1076206284196341, test loss: 0.257207159477283\n",
      "epoch 17091: train loss: 0.10761896947815722, test loss: 0.2572081495223966\n",
      "epoch 17092: train loss: 0.10761731066747877, test loss: 0.25720913962051467\n",
      "epoch 17093: train loss: 0.10761565198758008, test loss: 0.25721012977165697\n",
      "epoch 17094: train loss: 0.1076139934384424, test loss: 0.2572111199758181\n",
      "epoch 17095: train loss: 0.10761233502004698, test loss: 0.2572121102329347\n",
      "epoch 17096: train loss: 0.10761067673237515, test loss: 0.2572131005430576\n",
      "epoch 17097: train loss: 0.10760901857540811, test loss: 0.2572140909061207\n",
      "epoch 17098: train loss: 0.10760736054912723, test loss: 0.257215081322162\n",
      "epoch 17099: train loss: 0.10760570265351375, test loss: 0.2572160717911608\n",
      "epoch 17100: train loss: 0.107604044888549, test loss: 0.25721706231305386\n",
      "epoch 17101: train loss: 0.10760238725421425, test loss: 0.25721805288791877\n",
      "epoch 17102: train loss: 0.10760072975049079, test loss: 0.25721904351564884\n",
      "epoch 17103: train loss: 0.10759907237735997, test loss: 0.2572200341962978\n",
      "epoch 17104: train loss: 0.10759741513480307, test loss: 0.25722102492985505\n",
      "epoch 17105: train loss: 0.10759575802280139, test loss: 0.25722201571625974\n",
      "epoch 17106: train loss: 0.10759410104133629, test loss: 0.2572230065555461\n",
      "epoch 17107: train loss: 0.10759244419038903, test loss: 0.2572239974476956\n",
      "epoch 17108: train loss: 0.10759078746994101, test loss: 0.25722498839270064\n",
      "epoch 17109: train loss: 0.1075891308799735, test loss: 0.25722597939052744\n",
      "epoch 17110: train loss: 0.10758747442046786, test loss: 0.2572269704411664\n",
      "epoch 17111: train loss: 0.10758581809140541, test loss: 0.2572279615446273\n",
      "epoch 17112: train loss: 0.10758416189276748, test loss: 0.257228952700875\n",
      "epoch 17113: train loss: 0.10758250582453543, test loss: 0.2572299439099331\n",
      "epoch 17114: train loss: 0.10758084988669063, test loss: 0.2572309351717778\n",
      "epoch 17115: train loss: 0.10757919407921442, test loss: 0.2572319264863758\n",
      "epoch 17116: train loss: 0.1075775384020881, test loss: 0.2572329178537363\n",
      "epoch 17117: train loss: 0.10757588285529311, test loss: 0.25723390927382117\n",
      "epoch 17118: train loss: 0.10757422743881076, test loss: 0.2572349007466546\n",
      "epoch 17119: train loss: 0.10757257215262243, test loss: 0.2572358922722448\n",
      "epoch 17120: train loss: 0.1075709169967095, test loss: 0.2572368838504983\n",
      "epoch 17121: train loss: 0.1075692619710533, test loss: 0.25723787548146704\n",
      "epoch 17122: train loss: 0.1075676070756353, test loss: 0.2572388671651585\n",
      "epoch 17123: train loss: 0.10756595231043681, test loss: 0.25723985890147943\n",
      "epoch 17124: train loss: 0.10756429767543921, test loss: 0.2572408506905122\n",
      "epoch 17125: train loss: 0.10756264317062392, test loss: 0.2572418425321757\n",
      "epoch 17126: train loss: 0.10756098879597234, test loss: 0.25724283442649354\n",
      "epoch 17127: train loss: 0.10755933455146584, test loss: 0.2572438263734745\n",
      "epoch 17128: train loss: 0.10755768043708581, test loss: 0.2572448183730534\n",
      "epoch 17129: train loss: 0.10755602645281372, test loss: 0.2572458104252536\n",
      "epoch 17130: train loss: 0.1075543725986309, test loss: 0.25724680253003984\n",
      "epoch 17131: train loss: 0.10755271887451881, test loss: 0.25724779468740666\n",
      "epoch 17132: train loss: 0.10755106528045888, test loss: 0.2572487868974035\n",
      "epoch 17133: train loss: 0.10754941181643246, test loss: 0.25724977915993963\n",
      "epoch 17134: train loss: 0.10754775848242104, test loss: 0.2572507714750226\n",
      "epoch 17135: train loss: 0.10754610527840601, test loss: 0.2572517638426594\n",
      "epoch 17136: train loss: 0.10754445220436885, test loss: 0.25725275626286004\n",
      "epoch 17137: train loss: 0.10754279926029092, test loss: 0.25725374873555895\n",
      "epoch 17138: train loss: 0.1075411464461537, test loss: 0.2572547412607799\n",
      "epoch 17139: train loss: 0.10753949376193866, test loss: 0.2572557338384886\n",
      "epoch 17140: train loss: 0.1075378412076272, test loss: 0.2572567264687197\n",
      "epoch 17141: train loss: 0.10753618878320079, test loss: 0.25725771915142553\n",
      "epoch 17142: train loss: 0.1075345364886409, test loss: 0.25725871188658495\n",
      "epoch 17143: train loss: 0.10753288432392895, test loss: 0.2572597046742213\n",
      "epoch 17144: train loss: 0.10753123228904642, test loss: 0.2572606975142991\n",
      "epoch 17145: train loss: 0.10752958038397477, test loss: 0.2572616904068115\n",
      "epoch 17146: train loss: 0.10752792860869549, test loss: 0.2572626833517392\n",
      "epoch 17147: train loss: 0.10752627696319003, test loss: 0.2572636763490893\n",
      "epoch 17148: train loss: 0.10752462544743988, test loss: 0.25726466939887205\n",
      "epoch 17149: train loss: 0.10752297406142652, test loss: 0.25726566250102123\n",
      "epoch 17150: train loss: 0.10752132280513141, test loss: 0.2572666556555607\n",
      "epoch 17151: train loss: 0.10751967167853607, test loss: 0.2572676488624557\n",
      "epoch 17152: train loss: 0.10751802068162199, test loss: 0.25726864212174333\n",
      "epoch 17153: train loss: 0.10751636981437067, test loss: 0.2572696354333304\n",
      "epoch 17154: train loss: 0.10751471907676356, test loss: 0.2572706287972981\n",
      "epoch 17155: train loss: 0.10751306846878218, test loss: 0.2572716222135971\n",
      "epoch 17156: train loss: 0.10751141799040809, test loss: 0.2572726156821919\n",
      "epoch 17157: train loss: 0.10750976764162275, test loss: 0.2572736092031198\n",
      "epoch 17158: train loss: 0.1075081174224077, test loss: 0.25727460277631764\n",
      "epoch 17159: train loss: 0.10750646733274444, test loss: 0.2572755964018083\n",
      "epoch 17160: train loss: 0.10750481737261448, test loss: 0.25727659007955567\n",
      "epoch 17161: train loss: 0.1075031675419994, test loss: 0.2572775838095838\n",
      "epoch 17162: train loss: 0.10750151784088068, test loss: 0.25727857759184247\n",
      "epoch 17163: train loss: 0.10749986826923986, test loss: 0.25727957142637053\n",
      "epoch 17164: train loss: 0.10749821882705851, test loss: 0.25728056531310267\n",
      "epoch 17165: train loss: 0.10749656951431812, test loss: 0.2572815592520913\n",
      "epoch 17166: train loss: 0.10749492033100026, test loss: 0.25728255324324417\n",
      "epoch 17167: train loss: 0.10749327127708648, test loss: 0.25728354728661285\n",
      "epoch 17168: train loss: 0.10749162235255832, test loss: 0.2572845413821762\n",
      "epoch 17169: train loss: 0.10748997355739735, test loss: 0.2572855355299001\n",
      "epoch 17170: train loss: 0.10748832489158515, test loss: 0.25728652972979305\n",
      "epoch 17171: train loss: 0.10748667635510324, test loss: 0.2572875239818196\n",
      "epoch 17172: train loss: 0.10748502794793321, test loss: 0.25728851828600335\n",
      "epoch 17173: train loss: 0.10748337967005661, test loss: 0.2572895126423242\n",
      "epoch 17174: train loss: 0.10748173152145508, test loss: 0.25729050705077605\n",
      "epoch 17175: train loss: 0.1074800835021101, test loss: 0.2572915015113237\n",
      "epoch 17176: train loss: 0.1074784356120033, test loss: 0.2572924960239604\n",
      "epoch 17177: train loss: 0.10747678785111628, test loss: 0.257293490588683\n",
      "epoch 17178: train loss: 0.10747514021943065, test loss: 0.2572944852054983\n",
      "epoch 17179: train loss: 0.10747349271692792, test loss: 0.25729547987435636\n",
      "epoch 17180: train loss: 0.10747184534358975, test loss: 0.25729647459528215\n",
      "epoch 17181: train loss: 0.10747019809939776, test loss: 0.2572974693682682\n",
      "epoch 17182: train loss: 0.10746855098433349, test loss: 0.2572984641932512\n",
      "epoch 17183: train loss: 0.1074669039983786, test loss: 0.2572994590702839\n",
      "epoch 17184: train loss: 0.10746525714151467, test loss: 0.25730045399931717\n",
      "epoch 17185: train loss: 0.10746361041372332, test loss: 0.25730144898034346\n",
      "epoch 17186: train loss: 0.10746196381498621, test loss: 0.25730244401338714\n",
      "epoch 17187: train loss: 0.1074603173452849, test loss: 0.2573034390983996\n",
      "epoch 17188: train loss: 0.10745867100460106, test loss: 0.257304434235375\n",
      "epoch 17189: train loss: 0.10745702479291634, test loss: 0.2573054294242776\n",
      "epoch 17190: train loss: 0.10745537871021231, test loss: 0.25730642466517406\n",
      "epoch 17191: train loss: 0.10745373275647065, test loss: 0.2573074199579728\n",
      "epoch 17192: train loss: 0.10745208693167299, test loss: 0.2573084153027257\n",
      "epoch 17193: train loss: 0.10745044123580101, test loss: 0.2573094106993531\n",
      "epoch 17194: train loss: 0.10744879566883629, test loss: 0.25731040614790934\n",
      "epoch 17195: train loss: 0.10744715023076054, test loss: 0.2573114016483579\n",
      "epoch 17196: train loss: 0.1074455049215554, test loss: 0.25731239720066634\n",
      "epoch 17197: train loss: 0.10744385974120255, test loss: 0.2573133928048558\n",
      "epoch 17198: train loss: 0.10744221468968361, test loss: 0.2573143884609067\n",
      "epoch 17199: train loss: 0.10744056976698026, test loss: 0.25731538416878474\n",
      "epoch 17200: train loss: 0.10743892497307422, test loss: 0.2573163799285432\n",
      "epoch 17201: train loss: 0.10743728030794714, test loss: 0.25731737574010144\n",
      "epoch 17202: train loss: 0.10743563577158068, test loss: 0.25731837160345483\n",
      "epoch 17203: train loss: 0.10743399136395651, test loss: 0.2573193675186725\n",
      "epoch 17204: train loss: 0.10743234708505636, test loss: 0.2573203634856287\n",
      "epoch 17205: train loss: 0.10743070293486187, test loss: 0.25732135950437973\n",
      "epoch 17206: train loss: 0.1074290589133548, test loss: 0.25732235557491645\n",
      "epoch 17207: train loss: 0.10742741502051682, test loss: 0.25732335169720844\n",
      "epoch 17208: train loss: 0.10742577125632961, test loss: 0.2573243478712597\n",
      "epoch 17209: train loss: 0.10742412762077488, test loss: 0.2573253440970392\n",
      "epoch 17210: train loss: 0.10742248411383436, test loss: 0.2573263403745404\n",
      "epoch 17211: train loss: 0.10742084073548976, test loss: 0.25732733670377217\n",
      "epoch 17212: train loss: 0.10741919748572279, test loss: 0.25732833308469927\n",
      "epoch 17213: train loss: 0.10741755436451517, test loss: 0.25732932951731674\n",
      "epoch 17214: train loss: 0.10741591137184861, test loss: 0.25733032600166295\n",
      "epoch 17215: train loss: 0.10741426850770484, test loss: 0.25733132253764424\n",
      "epoch 17216: train loss: 0.10741262577206562, test loss: 0.25733231912528554\n",
      "epoch 17217: train loss: 0.10741098316491268, test loss: 0.2573333157646086\n",
      "epoch 17218: train loss: 0.10740934068622772, test loss: 0.2573343124555522\n",
      "epoch 17219: train loss: 0.10740769833599254, test loss: 0.25733530919812175\n",
      "epoch 17220: train loss: 0.10740605611418882, test loss: 0.25733630599231544\n",
      "epoch 17221: train loss: 0.10740441402079838, test loss: 0.2573373028381266\n",
      "epoch 17222: train loss: 0.10740277205580291, test loss: 0.25733829973556266\n",
      "epoch 17223: train loss: 0.10740113021918422, test loss: 0.2573392966845621\n",
      "epoch 17224: train loss: 0.10739948851092404, test loss: 0.2573402936851179\n",
      "epoch 17225: train loss: 0.10739784693100414, test loss: 0.25734129073725603\n",
      "epoch 17226: train loss: 0.10739620547940629, test loss: 0.25734228784098223\n",
      "epoch 17227: train loss: 0.10739456415611226, test loss: 0.2573432849962065\n",
      "epoch 17228: train loss: 0.10739292296110385, test loss: 0.25734428220298056\n",
      "epoch 17229: train loss: 0.1073912818943628, test loss: 0.2573452794612845\n",
      "epoch 17230: train loss: 0.10738964095587089, test loss: 0.2573462767711125\n",
      "epoch 17231: train loss: 0.10738800014560997, test loss: 0.25734727413243036\n",
      "epoch 17232: train loss: 0.1073863594635618, test loss: 0.257348271545234\n",
      "epoch 17233: train loss: 0.10738471890970815, test loss: 0.25734926900951516\n",
      "epoch 17234: train loss: 0.1073830784840308, test loss: 0.2573502665252861\n",
      "epoch 17235: train loss: 0.10738143818651161, test loss: 0.25735126409249637\n",
      "epoch 17236: train loss: 0.10737979801713238, test loss: 0.25735226171115483\n",
      "epoch 17237: train loss: 0.10737815797587487, test loss: 0.25735325938125664\n",
      "epoch 17238: train loss: 0.10737651806272092, test loss: 0.2573542571027813\n",
      "epoch 17239: train loss: 0.10737487827765238, test loss: 0.25735525487572425\n",
      "epoch 17240: train loss: 0.107373238620651, test loss: 0.25735625270006546\n",
      "epoch 17241: train loss: 0.10737159909169869, test loss: 0.2573572505758296\n",
      "epoch 17242: train loss: 0.10736995969077717, test loss: 0.25735824850293637\n",
      "epoch 17243: train loss: 0.10736832041786841, test loss: 0.25735924648142505\n",
      "epoch 17244: train loss: 0.10736668127295411, test loss: 0.2573602445113051\n",
      "epoch 17245: train loss: 0.10736504225601617, test loss: 0.25736124259251325\n",
      "epoch 17246: train loss: 0.10736340336703645, test loss: 0.2573622407250737\n",
      "epoch 17247: train loss: 0.10736176460599678, test loss: 0.2573632389089512\n",
      "epoch 17248: train loss: 0.10736012597287896, test loss: 0.2573642371441406\n",
      "epoch 17249: train loss: 0.10735848746766495, test loss: 0.257365235430652\n",
      "epoch 17250: train loss: 0.1073568490903365, test loss: 0.2573662337684795\n",
      "epoch 17251: train loss: 0.10735521084087556, test loss: 0.2573672321575594\n",
      "epoch 17252: train loss: 0.10735357271926389, test loss: 0.25736823059791525\n",
      "epoch 17253: train loss: 0.10735193472548346, test loss: 0.257369229089573\n",
      "epoch 17254: train loss: 0.1073502968595161, test loss: 0.25737022763245226\n",
      "epoch 17255: train loss: 0.10734865912134367, test loss: 0.2573712262265921\n",
      "epoch 17256: train loss: 0.10734702151094808, test loss: 0.25737222487195915\n",
      "epoch 17257: train loss: 0.10734538402831119, test loss: 0.2573732235685336\n",
      "epoch 17258: train loss: 0.1073437466734149, test loss: 0.25737422231635315\n",
      "epoch 17259: train loss: 0.10734210944624109, test loss: 0.25737522111535527\n",
      "epoch 17260: train loss: 0.10734047234677167, test loss: 0.25737621996556237\n",
      "epoch 17261: train loss: 0.1073388353749885, test loss: 0.25737721886692644\n",
      "epoch 17262: train loss: 0.10733719853087355, test loss: 0.2573782178194719\n",
      "epoch 17263: train loss: 0.10733556181440868, test loss: 0.25737921682316367\n",
      "epoch 17264: train loss: 0.10733392522557576, test loss: 0.25738021587799786\n",
      "epoch 17265: train loss: 0.10733228876435678, test loss: 0.25738121498401234\n",
      "epoch 17266: train loss: 0.10733065243073359, test loss: 0.25738221414111445\n",
      "epoch 17267: train loss: 0.10732901622468818, test loss: 0.2573832133493274\n",
      "epoch 17268: train loss: 0.10732738014620241, test loss: 0.25738421260864797\n",
      "epoch 17269: train loss: 0.10732574419525821, test loss: 0.25738521191909985\n",
      "epoch 17270: train loss: 0.10732410837183756, test loss: 0.2573862112806028\n",
      "epoch 17271: train loss: 0.10732247267592238, test loss: 0.25738721069318427\n",
      "epoch 17272: train loss: 0.10732083710749456, test loss: 0.2573882101568074\n",
      "epoch 17273: train loss: 0.1073192016665361, test loss: 0.25738920967149664\n",
      "epoch 17274: train loss: 0.1073175663530289, test loss: 0.2573902092372641\n",
      "epoch 17275: train loss: 0.10731593116695493, test loss: 0.2573912088540142\n",
      "epoch 17276: train loss: 0.10731429610829615, test loss: 0.2573922085218008\n",
      "epoch 17277: train loss: 0.10731266117703449, test loss: 0.25739320824060535\n",
      "epoch 17278: train loss: 0.10731102637315194, test loss: 0.25739420801039314\n",
      "epoch 17279: train loss: 0.10730939169663045, test loss: 0.2573952078311602\n",
      "epoch 17280: train loss: 0.107307757147452, test loss: 0.25739620770292754\n",
      "epoch 17281: train loss: 0.10730612272559854, test loss: 0.25739720762563617\n",
      "epoch 17282: train loss: 0.10730448843105204, test loss: 0.25739820759930704\n",
      "epoch 17283: train loss: 0.10730285426379452, test loss: 0.25739920762393703\n",
      "epoch 17284: train loss: 0.1073012202238079, test loss: 0.25740020769950533\n",
      "epoch 17285: train loss: 0.10729958631107422, test loss: 0.2574012078259929\n",
      "epoch 17286: train loss: 0.10729795252557545, test loss: 0.2574022080033508\n",
      "epoch 17287: train loss: 0.10729631886729356, test loss: 0.25740320823166113\n",
      "epoch 17288: train loss: 0.10729468533621059, test loss: 0.25740420851084805\n",
      "epoch 17289: train loss: 0.1072930519323085, test loss: 0.2574052088409042\n",
      "epoch 17290: train loss: 0.1072914186555693, test loss: 0.25740620922185653\n",
      "epoch 17291: train loss: 0.10728978550597505, test loss: 0.2574072096536531\n",
      "epoch 17292: train loss: 0.10728815248350769, test loss: 0.2574082101362915\n",
      "epoch 17293: train loss: 0.10728651958814925, test loss: 0.2574092106697663\n",
      "epoch 17294: train loss: 0.1072848868198818, test loss: 0.2574102112540721\n",
      "epoch 17295: train loss: 0.1072832541786873, test loss: 0.25741121188920346\n",
      "epoch 17296: train loss: 0.1072816216645478, test loss: 0.2574122125751427\n",
      "epoch 17297: train loss: 0.10727998927744534, test loss: 0.2574132133118839\n",
      "epoch 17298: train loss: 0.10727835701736194, test loss: 0.2574142140993933\n",
      "epoch 17299: train loss: 0.10727672488427961, test loss: 0.2574152149376656\n",
      "epoch 17300: train loss: 0.10727509287818045, test loss: 0.2574162158267261\n",
      "epoch 17301: train loss: 0.10727346099904646, test loss: 0.25741721676655355\n",
      "epoch 17302: train loss: 0.10727182924685968, test loss: 0.25741821775708723\n",
      "epoch 17303: train loss: 0.1072701976216022, test loss: 0.2574192187983778\n",
      "epoch 17304: train loss: 0.10726856612325603, test loss: 0.2574202198903651\n",
      "epoch 17305: train loss: 0.10726693475180328, test loss: 0.25742122103307097\n",
      "epoch 17306: train loss: 0.10726530350722598, test loss: 0.2574222222265079\n",
      "epoch 17307: train loss: 0.1072636723895062, test loss: 0.25742322347060964\n",
      "epoch 17308: train loss: 0.107262041398626, test loss: 0.25742422476537463\n",
      "epoch 17309: train loss: 0.10726041053456745, test loss: 0.2574252261108241\n",
      "epoch 17310: train loss: 0.10725877979731266, test loss: 0.25742622750694105\n",
      "epoch 17311: train loss: 0.10725714918684368, test loss: 0.2574272289536906\n",
      "epoch 17312: train loss: 0.1072555187031426, test loss: 0.2574282304510971\n",
      "epoch 17313: train loss: 0.10725388834619151, test loss: 0.25742923199912815\n",
      "epoch 17314: train loss: 0.10725225811597248, test loss: 0.2574302335977478\n",
      "epoch 17315: train loss: 0.10725062801246767, test loss: 0.2574312352469957\n",
      "epoch 17316: train loss: 0.10724899803565907, test loss: 0.25743223694683787\n",
      "epoch 17317: train loss: 0.10724736818552888, test loss: 0.2574332386972404\n",
      "epoch 17318: train loss: 0.10724573846205918, test loss: 0.25743424049822916\n",
      "epoch 17319: train loss: 0.10724410886523204, test loss: 0.2574352423497983\n",
      "epoch 17320: train loss: 0.10724247939502961, test loss: 0.257436244251884\n",
      "epoch 17321: train loss: 0.10724085005143401, test loss: 0.2574372462045421\n",
      "epoch 17322: train loss: 0.10723922083442733, test loss: 0.25743824820772193\n",
      "epoch 17323: train loss: 0.1072375917439917, test loss: 0.2574392502614349\n",
      "epoch 17324: train loss: 0.10723596278010927, test loss: 0.25744025236563206\n",
      "epoch 17325: train loss: 0.10723433394276215, test loss: 0.25744125452033784\n",
      "epoch 17326: train loss: 0.1072327052319325, test loss: 0.257442256725533\n",
      "epoch 17327: train loss: 0.10723107664760242, test loss: 0.257443258981213\n",
      "epoch 17328: train loss: 0.10722944818975406, test loss: 0.2574442612873449\n",
      "epoch 17329: train loss: 0.1072278198583696, test loss: 0.25744526364395254\n",
      "epoch 17330: train loss: 0.10722619165343115, test loss: 0.2574462660510008\n",
      "epoch 17331: train loss: 0.10722456357492087, test loss: 0.25744726850847255\n",
      "epoch 17332: train loss: 0.10722293562282095, test loss: 0.257448271016392\n",
      "epoch 17333: train loss: 0.1072213077971135, test loss: 0.25744927357472497\n",
      "epoch 17334: train loss: 0.1072196800977807, test loss: 0.25745027618343724\n",
      "epoch 17335: train loss: 0.10721805252480474, test loss: 0.2574512788425554\n",
      "epoch 17336: train loss: 0.10721642507816774, test loss: 0.2574522815520577\n",
      "epoch 17337: train loss: 0.10721479775785193, test loss: 0.25745328431194087\n",
      "epoch 17338: train loss: 0.10721317056383946, test loss: 0.2574542871221721\n",
      "epoch 17339: train loss: 0.10721154349611252, test loss: 0.25745528998277367\n",
      "epoch 17340: train loss: 0.10720991655465326, test loss: 0.2574562928936847\n",
      "epoch 17341: train loss: 0.10720828973944392, test loss: 0.2574572958549586\n",
      "epoch 17342: train loss: 0.10720666305046667, test loss: 0.2574582988665474\n",
      "epoch 17343: train loss: 0.1072050364877037, test loss: 0.25745930192841476\n",
      "epoch 17344: train loss: 0.1072034100511372, test loss: 0.2574603050406193\n",
      "epoch 17345: train loss: 0.1072017837407494, test loss: 0.257461308203095\n",
      "epoch 17346: train loss: 0.10720015755652249, test loss: 0.25746231141586634\n",
      "epoch 17347: train loss: 0.10719853149843868, test loss: 0.257463314678887\n",
      "epoch 17348: train loss: 0.10719690556648018, test loss: 0.25746431799218\n",
      "epoch 17349: train loss: 0.1071952797606292, test loss: 0.2574653213557131\n",
      "epoch 17350: train loss: 0.10719365408086798, test loss: 0.2574663247694515\n",
      "epoch 17351: train loss: 0.10719202852717874, test loss: 0.2574673282334487\n",
      "epoch 17352: train loss: 0.10719040309954372, test loss: 0.25746833174764455\n",
      "epoch 17353: train loss: 0.10718877779794511, test loss: 0.25746933531206084\n",
      "epoch 17354: train loss: 0.10718715262236518, test loss: 0.25747033892667964\n",
      "epoch 17355: train loss: 0.10718552757278615, test loss: 0.25747134259146875\n",
      "epoch 17356: train loss: 0.1071839026491903, test loss: 0.2574723463064215\n",
      "epoch 17357: train loss: 0.10718227785155982, test loss: 0.25747335007154926\n",
      "epoch 17358: train loss: 0.10718065317987698, test loss: 0.25747435388680345\n",
      "epoch 17359: train loss: 0.10717902863412405, test loss: 0.25747535775223906\n",
      "epoch 17360: train loss: 0.10717740421428328, test loss: 0.25747636166777793\n",
      "epoch 17361: train loss: 0.10717577992033692, test loss: 0.25747736563344503\n",
      "epoch 17362: train loss: 0.10717415575226723, test loss: 0.25747836964923493\n",
      "epoch 17363: train loss: 0.10717253171005649, test loss: 0.25747937371511725\n",
      "epoch 17364: train loss: 0.10717090779368696, test loss: 0.25748037783109856\n",
      "epoch 17365: train loss: 0.10716928400314092, test loss: 0.25748138199714854\n",
      "epoch 17366: train loss: 0.10716766033840065, test loss: 0.25748238621324554\n",
      "epoch 17367: train loss: 0.10716603679944843, test loss: 0.25748339047943086\n",
      "epoch 17368: train loss: 0.10716441338626655, test loss: 0.2574843947956406\n",
      "epoch 17369: train loss: 0.1071627900988373, test loss: 0.2574853991618994\n",
      "epoch 17370: train loss: 0.10716116693714295, test loss: 0.2574864035781911\n",
      "epoch 17371: train loss: 0.10715954390116583, test loss: 0.2574874080445092\n",
      "epoch 17372: train loss: 0.1071579209908882, test loss: 0.25748841256077504\n",
      "epoch 17373: train loss: 0.10715629820629236, test loss: 0.257489417127105\n",
      "epoch 17374: train loss: 0.10715467554736068, test loss: 0.257490421743405\n",
      "epoch 17375: train loss: 0.10715305301407539, test loss: 0.2574914264096408\n",
      "epoch 17376: train loss: 0.10715143060641888, test loss: 0.25749243112588277\n",
      "epoch 17377: train loss: 0.10714980832437342, test loss: 0.2574934358920388\n",
      "epoch 17378: train loss: 0.10714818616792131, test loss: 0.2574944407081622\n",
      "epoch 17379: train loss: 0.10714656413704493, test loss: 0.2574954455742339\n",
      "epoch 17380: train loss: 0.10714494223172659, test loss: 0.2574964504901934\n",
      "epoch 17381: train loss: 0.10714332045194858, test loss: 0.25749745545609243\n",
      "epoch 17382: train loss: 0.1071416987976933, test loss: 0.2574984604718849\n",
      "epoch 17383: train loss: 0.10714007726894305, test loss: 0.25749946553755104\n",
      "epoch 17384: train loss: 0.10713845586568016, test loss: 0.2575004706531168\n",
      "epoch 17385: train loss: 0.107136834587887, test loss: 0.2575014758185322\n",
      "epoch 17386: train loss: 0.10713521343554593, test loss: 0.25750248103382417\n",
      "epoch 17387: train loss: 0.10713359240863929, test loss: 0.2575034862989889\n",
      "epoch 17388: train loss: 0.1071319715071494, test loss: 0.25750449161396255\n",
      "epoch 17389: train loss: 0.10713035073105867, test loss: 0.25750549697876984\n",
      "epoch 17390: train loss: 0.10712873008034944, test loss: 0.25750650239339307\n",
      "epoch 17391: train loss: 0.10712710955500407, test loss: 0.257507507857829\n",
      "epoch 17392: train loss: 0.10712548915500494, test loss: 0.25750851337202746\n",
      "epoch 17393: train loss: 0.10712386888033444, test loss: 0.2575095189360587\n",
      "epoch 17394: train loss: 0.10712224873097494, test loss: 0.2575105245498603\n",
      "epoch 17395: train loss: 0.10712062870690879, test loss: 0.2575115302134271\n",
      "epoch 17396: train loss: 0.10711900880811841, test loss: 0.2575125359267426\n",
      "epoch 17397: train loss: 0.10711738903458619, test loss: 0.25751354168978496\n",
      "epoch 17398: train loss: 0.10711576938629448, test loss: 0.25751454750258135\n",
      "epoch 17399: train loss: 0.10711414986322573, test loss: 0.2575155533650843\n",
      "epoch 17400: train loss: 0.1071125304653623, test loss: 0.2575165592773186\n",
      "epoch 17401: train loss: 0.10711091119268662, test loss: 0.25751756523924935\n",
      "epoch 17402: train loss: 0.10710929204518106, test loss: 0.2575185712508755\n",
      "epoch 17403: train loss: 0.10710767302282802, test loss: 0.25751957731219105\n",
      "epoch 17404: train loss: 0.10710605412561001, test loss: 0.25752058342320633\n",
      "epoch 17405: train loss: 0.10710443535350933, test loss: 0.25752158958383076\n",
      "epoch 17406: train loss: 0.10710281670650847, test loss: 0.257522595794133\n",
      "epoch 17407: train loss: 0.10710119818458981, test loss: 0.2575236020540642\n",
      "epoch 17408: train loss: 0.10709957978773581, test loss: 0.2575246083636225\n",
      "epoch 17409: train loss: 0.10709796151592889, test loss: 0.257525614722831\n",
      "epoch 17410: train loss: 0.10709634336915147, test loss: 0.2575266211316296\n",
      "epoch 17411: train loss: 0.10709472534738604, test loss: 0.25752762759004144\n",
      "epoch 17412: train loss: 0.10709310745061497, test loss: 0.2575286340980196\n",
      "epoch 17413: train loss: 0.10709148967882072, test loss: 0.2575296406555892\n",
      "epoch 17414: train loss: 0.10708987203198578, test loss: 0.257530647262732\n",
      "epoch 17415: train loss: 0.10708825451009259, test loss: 0.2575316539194149\n",
      "epoch 17416: train loss: 0.10708663711312358, test loss: 0.2575326606256773\n",
      "epoch 17417: train loss: 0.10708501984106121, test loss: 0.2575336673814573\n",
      "epoch 17418: train loss: 0.10708340269388796, test loss: 0.257534674186765\n",
      "epoch 17419: train loss: 0.1070817856715863, test loss: 0.25753568104159735\n",
      "epoch 17420: train loss: 0.10708016877413865, test loss: 0.2575366879459344\n",
      "epoch 17421: train loss: 0.10707855200152756, test loss: 0.25753769489974415\n",
      "epoch 17422: train loss: 0.10707693535373544, test loss: 0.25753870190306816\n",
      "epoch 17423: train loss: 0.10707531883074481, test loss: 0.25753970895587064\n",
      "epoch 17424: train loss: 0.10707370243253816, test loss: 0.25754071605811935\n",
      "epoch 17425: train loss: 0.10707208615909793, test loss: 0.2575417232098412\n",
      "epoch 17426: train loss: 0.10707047001040665, test loss: 0.2575427304110006\n",
      "epoch 17427: train loss: 0.10706885398644679, test loss: 0.25754373766157906\n",
      "epoch 17428: train loss: 0.10706723808720088, test loss: 0.2575447449615896\n",
      "epoch 17429: train loss: 0.10706562231265139, test loss: 0.25754575231102816\n",
      "epoch 17430: train loss: 0.10706400666278082, test loss: 0.25754675970984536\n",
      "epoch 17431: train loss: 0.1070623911375717, test loss: 0.25754776715809824\n",
      "epoch 17432: train loss: 0.10706077573700654, test loss: 0.257548774655692\n",
      "epoch 17433: train loss: 0.10705916046106784, test loss: 0.2575497822026842\n",
      "epoch 17434: train loss: 0.10705754530973813, test loss: 0.2575507897990107\n",
      "epoch 17435: train loss: 0.10705593028299994, test loss: 0.25755179744469847\n",
      "epoch 17436: train loss: 0.10705431538083576, test loss: 0.25755280513974277\n",
      "epoch 17437: train loss: 0.10705270060322819, test loss: 0.2575538128841269\n",
      "epoch 17438: train loss: 0.1070510859501597, test loss: 0.25755482067781593\n",
      "epoch 17439: train loss: 0.10704947142161281, test loss: 0.2575558285207913\n",
      "epoch 17440: train loss: 0.10704785701757011, test loss: 0.2575568364131241\n",
      "epoch 17441: train loss: 0.10704624273801415, test loss: 0.25755784435469165\n",
      "epoch 17442: train loss: 0.10704462858292743, test loss: 0.25755885234558085\n",
      "epoch 17443: train loss: 0.10704301455229254, test loss: 0.25755986038569645\n",
      "epoch 17444: train loss: 0.10704140064609198, test loss: 0.25756086847512605\n",
      "epoch 17445: train loss: 0.10703978686430837, test loss: 0.25756187661374574\n",
      "epoch 17446: train loss: 0.10703817320692424, test loss: 0.2575628848016435\n",
      "epoch 17447: train loss: 0.10703655967392216, test loss: 0.257563893038769\n",
      "epoch 17448: train loss: 0.10703494626528468, test loss: 0.2575649013250878\n",
      "epoch 17449: train loss: 0.1070333329809944, test loss: 0.25756590966062926\n",
      "epoch 17450: train loss: 0.10703171982103388, test loss: 0.25756691804538806\n",
      "epoch 17451: train loss: 0.10703010678538569, test loss: 0.25756792647930105\n",
      "epoch 17452: train loss: 0.10702849387403242, test loss: 0.2575689349623962\n",
      "epoch 17453: train loss: 0.10702688108695668, test loss: 0.25756994349466766\n",
      "epoch 17454: train loss: 0.10702526842414102, test loss: 0.257570952076083\n",
      "epoch 17455: train loss: 0.10702365588556806, test loss: 0.25757196070666916\n",
      "epoch 17456: train loss: 0.10702204347122034, test loss: 0.25757296938636165\n",
      "epoch 17457: train loss: 0.10702043118108052, test loss: 0.2575739781152043\n",
      "epoch 17458: train loss: 0.1070188190151312, test loss: 0.2575749868931611\n",
      "epoch 17459: train loss: 0.10701720697335496, test loss: 0.2575759957202148\n",
      "epoch 17460: train loss: 0.10701559505573442, test loss: 0.25757700459637706\n",
      "epoch 17461: train loss: 0.10701398326225219, test loss: 0.25757801352158494\n",
      "epoch 17462: train loss: 0.1070123715928909, test loss: 0.2575790224959237\n",
      "epoch 17463: train loss: 0.10701076004763312, test loss: 0.25758003151926995\n",
      "epoch 17464: train loss: 0.10700914862646155, test loss: 0.25758104059171205\n",
      "epoch 17465: train loss: 0.10700753732935876, test loss: 0.2575820497131707\n",
      "epoch 17466: train loss: 0.1070059261563074, test loss: 0.257583058883688\n",
      "epoch 17467: train loss: 0.1070043151072901, test loss: 0.25758406810319884\n",
      "epoch 17468: train loss: 0.1070027041822895, test loss: 0.25758507737174713\n",
      "epoch 17469: train loss: 0.10700109338128824, test loss: 0.25758608668929756\n",
      "epoch 17470: train loss: 0.10699948270426898, test loss: 0.2575870960558475\n",
      "epoch 17471: train loss: 0.10699787215121433, test loss: 0.2575881054713659\n",
      "epoch 17472: train loss: 0.10699626172210698, test loss: 0.2575891149358452\n",
      "epoch 17473: train loss: 0.10699465141692956, test loss: 0.25759012444928525\n",
      "epoch 17474: train loss: 0.10699304123566473, test loss: 0.25759113401171185\n",
      "epoch 17475: train loss: 0.10699143117829518, test loss: 0.2575921436230316\n",
      "epoch 17476: train loss: 0.10698982124480354, test loss: 0.25759315328333093\n",
      "epoch 17477: train loss: 0.10698821143517248, test loss: 0.2575941629925182\n",
      "epoch 17478: train loss: 0.1069866017493847, test loss: 0.25759517275061744\n",
      "epoch 17479: train loss: 0.10698499218742287, test loss: 0.2575961825576437\n",
      "epoch 17480: train loss: 0.10698338274926965, test loss: 0.2575971924135302\n",
      "epoch 17481: train loss: 0.10698177343490771, test loss: 0.2575982023183209\n",
      "epoch 17482: train loss: 0.10698016424431976, test loss: 0.25759921227196686\n",
      "epoch 17483: train loss: 0.10697855517748853, test loss: 0.25760022227446466\n",
      "epoch 17484: train loss: 0.10697694623439663, test loss: 0.25760123232584287\n",
      "epoch 17485: train loss: 0.10697533741502678, test loss: 0.2576022424260358\n",
      "epoch 17486: train loss: 0.10697372871936173, test loss: 0.2576032525750706\n",
      "epoch 17487: train loss: 0.10697212014738414, test loss: 0.2576042627729159\n",
      "epoch 17488: train loss: 0.10697051169907672, test loss: 0.25760527301956665\n",
      "epoch 17489: train loss: 0.10696890337442219, test loss: 0.25760628331502144\n",
      "epoch 17490: train loss: 0.10696729517340327, test loss: 0.2576072936592452\n",
      "epoch 17491: train loss: 0.10696568709600264, test loss: 0.25760830405228147\n",
      "epoch 17492: train loss: 0.10696407914220306, test loss: 0.2576093144940653\n",
      "epoch 17493: train loss: 0.10696247131198725, test loss: 0.25761032498460956\n",
      "epoch 17494: train loss: 0.10696086360533791, test loss: 0.25761133552391163\n",
      "epoch 17495: train loss: 0.1069592560222378, test loss: 0.25761234611195216\n",
      "epoch 17496: train loss: 0.10695764856266964, test loss: 0.2576133567486977\n",
      "epoch 17497: train loss: 0.10695604122661617, test loss: 0.2576143674341774\n",
      "epoch 17498: train loss: 0.10695443401406016, test loss: 0.2576153781683575\n",
      "epoch 17499: train loss: 0.10695282692498427, test loss: 0.2576163889512334\n",
      "epoch 17500: train loss: 0.10695121995937136, test loss: 0.25761739978280235\n",
      "epoch 17501: train loss: 0.10694961311720412, test loss: 0.2576184106630613\n",
      "epoch 17502: train loss: 0.10694800639846529, test loss: 0.2576194215919484\n",
      "epoch 17503: train loss: 0.10694639980313767, test loss: 0.25762043256952005\n",
      "epoch 17504: train loss: 0.10694479333120396, test loss: 0.257621443595757\n",
      "epoch 17505: train loss: 0.106943186982647, test loss: 0.25762245467059863\n",
      "epoch 17506: train loss: 0.10694158075744956, test loss: 0.2576234657940832\n",
      "epoch 17507: train loss: 0.10693997465559431, test loss: 0.25762447696615065\n",
      "epoch 17508: train loss: 0.10693836867706413, test loss: 0.2576254881868722\n",
      "epoch 17509: train loss: 0.10693676282184177, test loss: 0.257626499456155\n",
      "epoch 17510: train loss: 0.10693515708990999, test loss: 0.2576275107740101\n",
      "epoch 17511: train loss: 0.10693355148125161, test loss: 0.2576285221404814\n",
      "epoch 17512: train loss: 0.1069319459958494, test loss: 0.25762953355550333\n",
      "epoch 17513: train loss: 0.10693034063368614, test loss: 0.25763054501907434\n",
      "epoch 17514: train loss: 0.10692873539474465, test loss: 0.25763155653119196\n",
      "epoch 17515: train loss: 0.10692713027900773, test loss: 0.2576325680918517\n",
      "epoch 17516: train loss: 0.10692552528645816, test loss: 0.25763357970102263\n",
      "epoch 17517: train loss: 0.10692392041707878, test loss: 0.25763459135869937\n",
      "epoch 17518: train loss: 0.10692231567085236, test loss: 0.25763560306487976\n",
      "epoch 17519: train loss: 0.10692071104776175, test loss: 0.2576366148195926\n",
      "epoch 17520: train loss: 0.10691910654778976, test loss: 0.25763762662274253\n",
      "epoch 17521: train loss: 0.1069175021709192, test loss: 0.2576386384743876\n",
      "epoch 17522: train loss: 0.10691589791713288, test loss: 0.2576396503745111\n",
      "epoch 17523: train loss: 0.10691429378641364, test loss: 0.25764066232307775\n",
      "epoch 17524: train loss: 0.10691268977874432, test loss: 0.2576416743200715\n",
      "epoch 17525: train loss: 0.10691108589410778, test loss: 0.2576426863655186\n",
      "epoch 17526: train loss: 0.1069094821324868, test loss: 0.25764369845937163\n",
      "epoch 17527: train loss: 0.10690787849386423, test loss: 0.25764471060165717\n",
      "epoch 17528: train loss: 0.10690627497822294, test loss: 0.25764572279232706\n",
      "epoch 17529: train loss: 0.10690467158554579, test loss: 0.2576467350314101\n",
      "epoch 17530: train loss: 0.10690306831581559, test loss: 0.25764774731885615\n",
      "epoch 17531: train loss: 0.1069014651690152, test loss: 0.25764875965469336\n",
      "epoch 17532: train loss: 0.10689986214512753, test loss: 0.25764977203888995\n",
      "epoch 17533: train loss: 0.1068982592441354, test loss: 0.25765078447144246\n",
      "epoch 17534: train loss: 0.10689665646602166, test loss: 0.2576517969523159\n",
      "epoch 17535: train loss: 0.10689505381076919, test loss: 0.2576528094815398\n",
      "epoch 17536: train loss: 0.10689345127836093, test loss: 0.2576538220590817\n",
      "epoch 17537: train loss: 0.10689184886877964, test loss: 0.25765483468493705\n",
      "epoch 17538: train loss: 0.10689024658200827, test loss: 0.25765584735907326\n",
      "epoch 17539: train loss: 0.1068886444180297, test loss: 0.2576568600815194\n",
      "epoch 17540: train loss: 0.10688704237682678, test loss: 0.257657872852271\n",
      "epoch 17541: train loss: 0.10688544045838241, test loss: 0.25765888567126555\n",
      "epoch 17542: train loss: 0.1068838386626795, test loss: 0.25765989853854554\n",
      "epoch 17543: train loss: 0.10688223698970095, test loss: 0.25766091145404685\n",
      "epoch 17544: train loss: 0.10688063543942965, test loss: 0.2576619244178143\n",
      "epoch 17545: train loss: 0.10687903401184848, test loss: 0.25766293742978247\n",
      "epoch 17546: train loss: 0.10687743270694039, test loss: 0.25766395048997986\n",
      "epoch 17547: train loss: 0.10687583152468826, test loss: 0.257664963598421\n",
      "epoch 17548: train loss: 0.10687423046507498, test loss: 0.25766597675503844\n",
      "epoch 17549: train loss: 0.1068726295280835, test loss: 0.25766698995984816\n",
      "epoch 17550: train loss: 0.10687102871369676, test loss: 0.2576680032128455\n",
      "epoch 17551: train loss: 0.10686942802189765, test loss: 0.2576690165140446\n",
      "epoch 17552: train loss: 0.10686782745266907, test loss: 0.2576700298633504\n",
      "epoch 17553: train loss: 0.10686622700599402, test loss: 0.25767104326083634\n",
      "epoch 17554: train loss: 0.10686462668185538, test loss: 0.25767205670645515\n",
      "epoch 17555: train loss: 0.10686302648023613, test loss: 0.25767307020020497\n",
      "epoch 17556: train loss: 0.10686142640111913, test loss: 0.257674083742096\n",
      "epoch 17557: train loss: 0.10685982644448742, test loss: 0.25767509733208127\n",
      "epoch 17558: train loss: 0.1068582266103239, test loss: 0.25767611097015747\n",
      "epoch 17559: train loss: 0.1068566268986115, test loss: 0.2576771246563406\n",
      "epoch 17560: train loss: 0.1068550273093332, test loss: 0.2576781383906079\n",
      "epoch 17561: train loss: 0.10685342784247195, test loss: 0.25767915217292997\n",
      "epoch 17562: train loss: 0.10685182849801073, test loss: 0.25768016600333254\n",
      "epoch 17563: train loss: 0.10685022927593249, test loss: 0.2576811798817543\n",
      "epoch 17564: train loss: 0.10684863017622018, test loss: 0.2576821938082535\n",
      "epoch 17565: train loss: 0.10684703119885679, test loss: 0.257683207782764\n",
      "epoch 17566: train loss: 0.10684543234382528, test loss: 0.2576842218052857\n",
      "epoch 17567: train loss: 0.10684383361110862, test loss: 0.257685235875847\n",
      "epoch 17568: train loss: 0.10684223500068984, test loss: 0.25768624999438283\n",
      "epoch 17569: train loss: 0.10684063651255188, test loss: 0.25768726416092336\n",
      "epoch 17570: train loss: 0.10683903814667776, test loss: 0.257688278375433\n",
      "epoch 17571: train loss: 0.10683743990305039, test loss: 0.2576892926379406\n",
      "epoch 17572: train loss: 0.10683584178165287, test loss: 0.25769030694838385\n",
      "epoch 17573: train loss: 0.10683424378246816, test loss: 0.2576913213067897\n",
      "epoch 17574: train loss: 0.10683264590547922, test loss: 0.25769233571312633\n",
      "epoch 17575: train loss: 0.1068310481506691, test loss: 0.25769335016740547\n",
      "epoch 17576: train loss: 0.10682945051802079, test loss: 0.2576943646695802\n",
      "epoch 17577: train loss: 0.10682785300751732, test loss: 0.2576953792197081\n",
      "epoch 17578: train loss: 0.10682625561914168, test loss: 0.25769639381769543\n",
      "epoch 17579: train loss: 0.1068246583528769, test loss: 0.2576974084636013\n",
      "epoch 17580: train loss: 0.106823061208706, test loss: 0.2576984231573762\n",
      "epoch 17581: train loss: 0.106821464186612, test loss: 0.25769943789903477\n",
      "epoch 17582: train loss: 0.10681986728657794, test loss: 0.25770045268854425\n",
      "epoch 17583: train loss: 0.10681827050858683, test loss: 0.25770146752590106\n",
      "epoch 17584: train loss: 0.10681667385262174, test loss: 0.2577024824111035\n",
      "epoch 17585: train loss: 0.10681507731866567, test loss: 0.2577034973441334\n",
      "epoch 17586: train loss: 0.10681348090670166, test loss: 0.25770451232497305\n",
      "epoch 17587: train loss: 0.10681188461671282, test loss: 0.2577055273536522\n",
      "epoch 17588: train loss: 0.10681028844868208, test loss: 0.25770654243012064\n",
      "epoch 17589: train loss: 0.10680869240259265, test loss: 0.2577075575543627\n",
      "epoch 17590: train loss: 0.10680709647842745, test loss: 0.25770857272640685\n",
      "epoch 17591: train loss: 0.10680550067616959, test loss: 0.25770958794620286\n",
      "epoch 17592: train loss: 0.10680390499580213, test loss: 0.25771060321376454\n",
      "epoch 17593: train loss: 0.10680230943730812, test loss: 0.2577116185290909\n",
      "epoch 17594: train loss: 0.10680071400067068, test loss: 0.2577126338921485\n",
      "epoch 17595: train loss: 0.10679911868587283, test loss: 0.25771364930296514\n",
      "epoch 17596: train loss: 0.10679752349289764, test loss: 0.2577146647614612\n",
      "epoch 17597: train loss: 0.10679592842172823, test loss: 0.25771568026768255\n",
      "epoch 17598: train loss: 0.10679433347234762, test loss: 0.257716695821627\n",
      "epoch 17599: train loss: 0.10679273864473898, test loss: 0.25771771142324434\n",
      "epoch 17600: train loss: 0.10679114393888536, test loss: 0.25771872707254917\n",
      "epoch 17601: train loss: 0.10678954935476981, test loss: 0.25771974276950776\n",
      "epoch 17602: train loss: 0.10678795489237547, test loss: 0.25772075851416437\n",
      "epoch 17603: train loss: 0.10678636055168546, test loss: 0.25772177430644105\n",
      "epoch 17604: train loss: 0.10678476633268284, test loss: 0.25772279014636473\n",
      "epoch 17605: train loss: 0.1067831722353507, test loss: 0.2577238060339203\n",
      "epoch 17606: train loss: 0.10678157825967219, test loss: 0.2577248219691185\n",
      "epoch 17607: train loss: 0.10677998440563044, test loss: 0.25772583795189585\n",
      "epoch 17608: train loss: 0.1067783906732085, test loss: 0.2577268539822983\n",
      "epoch 17609: train loss: 0.10677679706238954, test loss: 0.25772787006027614\n",
      "epoch 17610: train loss: 0.10677520357315666, test loss: 0.2577288861858596\n",
      "epoch 17611: train loss: 0.106773610205493, test loss: 0.2577299023589978\n",
      "epoch 17612: train loss: 0.10677201695938168, test loss: 0.25773091857970604\n",
      "epoch 17613: train loss: 0.10677042383480582, test loss: 0.25773193484798285\n",
      "epoch 17614: train loss: 0.10676883083174861, test loss: 0.257732951163764\n",
      "epoch 17615: train loss: 0.1067672379501931, test loss: 0.2577339675271066\n",
      "epoch 17616: train loss: 0.1067656451901225, test loss: 0.2577349839379661\n",
      "epoch 17617: train loss: 0.10676405255151993, test loss: 0.25773600039632333\n",
      "epoch 17618: train loss: 0.10676246003436855, test loss: 0.2577370169022071\n",
      "epoch 17619: train loss: 0.10676086763865152, test loss: 0.25773803345555363\n",
      "epoch 17620: train loss: 0.10675927536435197, test loss: 0.25773905005642406\n",
      "epoch 17621: train loss: 0.10675768321145306, test loss: 0.25774006670472244\n",
      "epoch 17622: train loss: 0.10675609117993798, test loss: 0.257741083400539\n",
      "epoch 17623: train loss: 0.10675449926978989, test loss: 0.25774210014376414\n",
      "epoch 17624: train loss: 0.10675290748099193, test loss: 0.2577431169344437\n",
      "epoch 17625: train loss: 0.10675131581352733, test loss: 0.2577441337725743\n",
      "epoch 17626: train loss: 0.1067497242673792, test loss: 0.2577451506580919\n",
      "epoch 17627: train loss: 0.10674813284253076, test loss: 0.25774616759105734\n",
      "epoch 17628: train loss: 0.10674654153896515, test loss: 0.25774718457140705\n",
      "epoch 17629: train loss: 0.10674495035666563, test loss: 0.25774820159917006\n",
      "epoch 17630: train loss: 0.10674335929561533, test loss: 0.25774921867431233\n",
      "epoch 17631: train loss: 0.10674176835579745, test loss: 0.257750235796832\n",
      "epoch 17632: train loss: 0.1067401775371952, test loss: 0.2577512529666987\n",
      "epoch 17633: train loss: 0.10673858683979176, test loss: 0.25775227018393915\n",
      "epoch 17634: train loss: 0.10673699626357039, test loss: 0.25775328744849013\n",
      "epoch 17635: train loss: 0.10673540580851419, test loss: 0.25775430476039624\n",
      "epoch 17636: train loss: 0.10673381547460649, test loss: 0.25775532211964086\n",
      "epoch 17637: train loss: 0.1067322252618304, test loss: 0.25775633952619115\n",
      "epoch 17638: train loss: 0.1067306351701692, test loss: 0.2577573569800435\n",
      "epoch 17639: train loss: 0.10672904519960608, test loss: 0.25775837448119804\n",
      "epoch 17640: train loss: 0.10672745535012428, test loss: 0.25775939202963727\n",
      "epoch 17641: train loss: 0.106725865621707, test loss: 0.25776040962534286\n",
      "epoch 17642: train loss: 0.10672427601433754, test loss: 0.2577614272683143\n",
      "epoch 17643: train loss: 0.106722686527999, test loss: 0.25776244495856515\n",
      "epoch 17644: train loss: 0.10672109716267475, test loss: 0.2577634626960468\n",
      "epoch 17645: train loss: 0.10671950791834796, test loss: 0.2577644804807566\n",
      "epoch 17646: train loss: 0.10671791879500186, test loss: 0.2577654983127098\n",
      "epoch 17647: train loss: 0.10671632979261975, test loss: 0.25776651619184127\n",
      "epoch 17648: train loss: 0.10671474091118487, test loss: 0.25776753411822884\n",
      "epoch 17649: train loss: 0.10671315215068042, test loss: 0.25776855209180666\n",
      "epoch 17650: train loss: 0.1067115635110897, test loss: 0.2577695701125279\n",
      "epoch 17651: train loss: 0.10670997499239597, test loss: 0.25777058818048265\n",
      "epoch 17652: train loss: 0.10670838659458246, test loss: 0.2577716062955768\n",
      "epoch 17653: train loss: 0.10670679831763247, test loss: 0.2577726244578242\n",
      "epoch 17654: train loss: 0.10670521016152926, test loss: 0.2577736426672244\n",
      "epoch 17655: train loss: 0.10670362212625609, test loss: 0.257774660923759\n",
      "epoch 17656: train loss: 0.10670203421179622, test loss: 0.25777567922742506\n",
      "epoch 17657: train loss: 0.10670044641813298, test loss: 0.2577766975782228\n",
      "epoch 17658: train loss: 0.10669885874524963, test loss: 0.2577777159761196\n",
      "epoch 17659: train loss: 0.10669727119312945, test loss: 0.2577787344211119\n",
      "epoch 17660: train loss: 0.10669568376175573, test loss: 0.25777975291320127\n",
      "epoch 17661: train loss: 0.10669409645111173, test loss: 0.2577807714523519\n",
      "epoch 17662: train loss: 0.10669250926118082, test loss: 0.2577817900385944\n",
      "epoch 17663: train loss: 0.10669092219194623, test loss: 0.25778280867186587\n",
      "epoch 17664: train loss: 0.10668933524339132, test loss: 0.25778382735222544\n",
      "epoch 17665: train loss: 0.10668774841549933, test loss: 0.2577848460796101\n",
      "epoch 17666: train loss: 0.10668616170825362, test loss: 0.25778586485401944\n",
      "epoch 17667: train loss: 0.10668457512163748, test loss: 0.2577868836754501\n",
      "epoch 17668: train loss: 0.10668298865563422, test loss: 0.257787902543854\n",
      "epoch 17669: train loss: 0.10668140231022719, test loss: 0.25778892145932397\n",
      "epoch 17670: train loss: 0.10667981608539968, test loss: 0.25778994042174835\n",
      "epoch 17671: train loss: 0.106678229981135, test loss: 0.25779095943115876\n",
      "epoch 17672: train loss: 0.10667664399741654, test loss: 0.25779197848753616\n",
      "epoch 17673: train loss: 0.10667505813422759, test loss: 0.2577929975908797\n",
      "epoch 17674: train loss: 0.10667347239155149, test loss: 0.25779401674120345\n",
      "epoch 17675: train loss: 0.10667188676937159, test loss: 0.25779503593844294\n",
      "epoch 17676: train loss: 0.1066703012676712, test loss: 0.2577960551826137\n",
      "epoch 17677: train loss: 0.10666871588643372, test loss: 0.2577970744737129\n",
      "epoch 17678: train loss: 0.10666713062564241, test loss: 0.2577980938117098\n",
      "epoch 17679: train loss: 0.10666554548528072, test loss: 0.25779911319660115\n",
      "epoch 17680: train loss: 0.10666396046533194, test loss: 0.2578001326284165\n",
      "epoch 17681: train loss: 0.10666237556577947, test loss: 0.25780115210709353\n",
      "epoch 17682: train loss: 0.10666079078660666, test loss: 0.2578021716326613\n",
      "epoch 17683: train loss: 0.10665920612779684, test loss: 0.25780319120508716\n",
      "epoch 17684: train loss: 0.10665762158933342, test loss: 0.2578042108243697\n",
      "epoch 17685: train loss: 0.10665603717119977, test loss: 0.25780523049047593\n",
      "epoch 17686: train loss: 0.10665445287337921, test loss: 0.2578062502034366\n",
      "epoch 17687: train loss: 0.1066528686958552, test loss: 0.25780726996320247\n",
      "epoch 17688: train loss: 0.10665128463861105, test loss: 0.2578082897698202\n",
      "epoch 17689: train loss: 0.10664970070163018, test loss: 0.2578093096232246\n",
      "epoch 17690: train loss: 0.106648116884896, test loss: 0.25781032952339905\n",
      "epoch 17691: train loss: 0.10664653318839186, test loss: 0.25781134947040685\n",
      "epoch 17692: train loss: 0.10664494961210118, test loss: 0.2578123694641656\n",
      "epoch 17693: train loss: 0.10664336615600732, test loss: 0.25781338950467453\n",
      "epoch 17694: train loss: 0.10664178282009372, test loss: 0.2578144095919493\n",
      "epoch 17695: train loss: 0.10664019960434377, test loss: 0.2578154297259885\n",
      "epoch 17696: train loss: 0.10663861650874086, test loss: 0.2578164499067584\n",
      "epoch 17697: train loss: 0.10663703353326845, test loss: 0.2578174701342577\n",
      "epoch 17698: train loss: 0.10663545067790992, test loss: 0.25781849040845445\n",
      "epoch 17699: train loss: 0.10663386794264867, test loss: 0.2578195107293787\n",
      "epoch 17700: train loss: 0.10663228532746817, test loss: 0.2578205310969991\n",
      "epoch 17701: train loss: 0.1066307028323518, test loss: 0.2578215515112807\n",
      "epoch 17702: train loss: 0.10662912045728301, test loss: 0.257822571972286\n",
      "epoch 17703: train loss: 0.10662753820224521, test loss: 0.2578235924799037\n",
      "epoch 17704: train loss: 0.10662595606722189, test loss: 0.25782461303422893\n",
      "epoch 17705: train loss: 0.10662437405219638, test loss: 0.25782563363520966\n",
      "epoch 17706: train loss: 0.10662279215715223, test loss: 0.2578266542827841\n",
      "epoch 17707: train loss: 0.10662121038207284, test loss: 0.2578276749770299\n",
      "epoch 17708: train loss: 0.10661962872694165, test loss: 0.2578286957178827\n",
      "epoch 17709: train loss: 0.1066180471917421, test loss: 0.25782971650532427\n",
      "epoch 17710: train loss: 0.10661646577645766, test loss: 0.25783073733940287\n",
      "epoch 17711: train loss: 0.1066148844810718, test loss: 0.25783175822005355\n",
      "epoch 17712: train loss: 0.10661330330556795, test loss: 0.25783277914727465\n",
      "epoch 17713: train loss: 0.1066117222499296, test loss: 0.2578338001210961\n",
      "epoch 17714: train loss: 0.10661014131414019, test loss: 0.25783482114145545\n",
      "epoch 17715: train loss: 0.10660856049818325, test loss: 0.25783584220836603\n",
      "epoch 17716: train loss: 0.10660697980204216, test loss: 0.2578368633218424\n",
      "epoch 17717: train loss: 0.10660539922570045, test loss: 0.25783788448182315\n",
      "epoch 17718: train loss: 0.10660381876914161, test loss: 0.2578389056883499\n",
      "epoch 17719: train loss: 0.10660223843234912, test loss: 0.2578399269413788\n",
      "epoch 17720: train loss: 0.10660065821530644, test loss: 0.2578409482409064\n",
      "epoch 17721: train loss: 0.10659907811799706, test loss: 0.2578419695869472\n",
      "epoch 17722: train loss: 0.10659749814040449, test loss: 0.25784299097946944\n",
      "epoch 17723: train loss: 0.10659591828251223, test loss: 0.2578440124184413\n",
      "epoch 17724: train loss: 0.10659433854430374, test loss: 0.25784503390389113\n",
      "epoch 17725: train loss: 0.10659275892576257, test loss: 0.25784605543581995\n",
      "epoch 17726: train loss: 0.10659117942687223, test loss: 0.25784707701416315\n",
      "epoch 17727: train loss: 0.10658960004761617, test loss: 0.2578480986389506\n",
      "epoch 17728: train loss: 0.10658802078797797, test loss: 0.2578491203101829\n",
      "epoch 17729: train loss: 0.1065864416479411, test loss: 0.2578501420278249\n",
      "epoch 17730: train loss: 0.10658486262748909, test loss: 0.2578511637918621\n",
      "epoch 17731: train loss: 0.10658328372660547, test loss: 0.2578521856022931\n",
      "epoch 17732: train loss: 0.10658170494527376, test loss: 0.25785320745913304\n",
      "epoch 17733: train loss: 0.10658012628347749, test loss: 0.2578542293623314\n",
      "epoch 17734: train loss: 0.10657854774120018, test loss: 0.2578552513118903\n",
      "epoch 17735: train loss: 0.10657696931842536, test loss: 0.2578562733078541\n",
      "epoch 17736: train loss: 0.10657539101513662, test loss: 0.2578572953501118\n",
      "epoch 17737: train loss: 0.10657381283131745, test loss: 0.257858317438743\n",
      "epoch 17738: train loss: 0.10657223476695141, test loss: 0.2578593395736976\n",
      "epoch 17739: train loss: 0.10657065682202203, test loss: 0.25786036175494337\n",
      "epoch 17740: train loss: 0.10656907899651291, test loss: 0.25786138398254344\n",
      "epoch 17741: train loss: 0.10656750129040753, test loss: 0.25786240625643153\n",
      "epoch 17742: train loss: 0.10656592370368952, test loss: 0.2578634285765954\n",
      "epoch 17743: train loss: 0.10656434623634238, test loss: 0.25786445094306115\n",
      "epoch 17744: train loss: 0.10656276888834974, test loss: 0.25786547335576704\n",
      "epoch 17745: train loss: 0.1065611916596951, test loss: 0.257866495814775\n",
      "epoch 17746: train loss: 0.10655961455036209, test loss: 0.2578675183199897\n",
      "epoch 17747: train loss: 0.10655803756033425, test loss: 0.2578685408715048\n",
      "epoch 17748: train loss: 0.10655646068959516, test loss: 0.2578695634692243\n",
      "epoch 17749: train loss: 0.10655488393812838, test loss: 0.2578705861131494\n",
      "epoch 17750: train loss: 0.10655330730591757, test loss: 0.2578716088033081\n",
      "epoch 17751: train loss: 0.1065517307929462, test loss: 0.25787263153966933\n",
      "epoch 17752: train loss: 0.10655015439919797, test loss: 0.25787365432223236\n",
      "epoch 17753: train loss: 0.10654857812465642, test loss: 0.25787467715098056\n",
      "epoch 17754: train loss: 0.10654700196930512, test loss: 0.2578757000258818\n",
      "epoch 17755: train loss: 0.10654542593312775, test loss: 0.2578767229469822\n",
      "epoch 17756: train loss: 0.10654385001610783, test loss: 0.2578777459142014\n",
      "epoch 17757: train loss: 0.10654227421822902, test loss: 0.25787876892760314\n",
      "epoch 17758: train loss: 0.1065406985394749, test loss: 0.2578797919871232\n",
      "epoch 17759: train loss: 0.10653912297982909, test loss: 0.25788081509277655\n",
      "epoch 17760: train loss: 0.10653754753927526, test loss: 0.257881838244546\n",
      "epoch 17761: train loss: 0.10653597221779691, test loss: 0.2578828614424321\n",
      "epoch 17762: train loss: 0.10653439701537776, test loss: 0.2578838846863857\n",
      "epoch 17763: train loss: 0.1065328219320014, test loss: 0.2578849079764695\n",
      "epoch 17764: train loss: 0.10653124696765148, test loss: 0.2578859313126199\n",
      "epoch 17765: train loss: 0.10652967212231161, test loss: 0.25788695469483613\n",
      "epoch 17766: train loss: 0.10652809739596542, test loss: 0.2578879781230854\n",
      "epoch 17767: train loss: 0.1065265227885966, test loss: 0.2578890015974312\n",
      "epoch 17768: train loss: 0.1065249483001887, test loss: 0.257890025117808\n",
      "epoch 17769: train loss: 0.10652337393072546, test loss: 0.2578910486841861\n",
      "epoch 17770: train loss: 0.10652179968019046, test loss: 0.25789207229662536\n",
      "epoch 17771: train loss: 0.10652022554856738, test loss: 0.2578930959550639\n",
      "epoch 17772: train loss: 0.10651865153583989, test loss: 0.2578941196594834\n",
      "epoch 17773: train loss: 0.10651707764199159, test loss: 0.2578951434099167\n",
      "epoch 17774: train loss: 0.1065155038670062, test loss: 0.25789616720633163\n",
      "epoch 17775: train loss: 0.10651393021086739, test loss: 0.25789719104872616\n",
      "epoch 17776: train loss: 0.10651235667355877, test loss: 0.25789821493706894\n",
      "epoch 17777: train loss: 0.10651078325506404, test loss: 0.25789923887138955\n",
      "epoch 17778: train loss: 0.10650920995536689, test loss: 0.2579002628516267\n",
      "epoch 17779: train loss: 0.10650763677445099, test loss: 0.25790128687782604\n",
      "epoch 17780: train loss: 0.10650606371229998, test loss: 0.2579023109499382\n",
      "epoch 17781: train loss: 0.1065044907688976, test loss: 0.2579033350679963\n",
      "epoch 17782: train loss: 0.10650291794422753, test loss: 0.25790435923192057\n",
      "epoch 17783: train loss: 0.1065013452382734, test loss: 0.2579053834417719\n",
      "epoch 17784: train loss: 0.10649977265101895, test loss: 0.2579064076974876\n",
      "epoch 17785: train loss: 0.10649820018244789, test loss: 0.25790743199909966\n",
      "epoch 17786: train loss: 0.10649662783254392, test loss: 0.2579084563465587\n",
      "epoch 17787: train loss: 0.10649505560129068, test loss: 0.2579094807398964\n",
      "epoch 17788: train loss: 0.10649348348867195, test loss: 0.2579105051790649\n",
      "epoch 17789: train loss: 0.1064919114946714, test loss: 0.2579115296640946\n",
      "epoch 17790: train loss: 0.10649033961927276, test loss: 0.25791255419493825\n",
      "epoch 17791: train loss: 0.10648876786245974, test loss: 0.2579135787716276\n",
      "epoch 17792: train loss: 0.10648719622421603, test loss: 0.2579146033940977\n",
      "epoch 17793: train loss: 0.1064856247045254, test loss: 0.257915628062412\n",
      "epoch 17794: train loss: 0.10648405330337153, test loss: 0.2579166527764756\n",
      "epoch 17795: train loss: 0.10648248202073818, test loss: 0.2579176775363343\n",
      "epoch 17796: train loss: 0.10648091085660909, test loss: 0.25791870234198766\n",
      "epoch 17797: train loss: 0.10647933981096797, test loss: 0.25791972719338907\n",
      "epoch 17798: train loss: 0.10647776888379855, test loss: 0.25792075209053683\n",
      "epoch 17799: train loss: 0.10647619807508459, test loss: 0.2579217770334469\n",
      "epoch 17800: train loss: 0.10647462738480983, test loss: 0.25792280202208784\n",
      "epoch 17801: train loss: 0.10647305681295802, test loss: 0.2579238270564581\n",
      "epoch 17802: train loss: 0.1064714863595129, test loss: 0.25792485213651095\n",
      "epoch 17803: train loss: 0.10646991602445827, test loss: 0.25792587726230737\n",
      "epoch 17804: train loss: 0.10646834580777781, test loss: 0.25792690243380223\n",
      "epoch 17805: train loss: 0.10646677570945531, test loss: 0.2579279276509765\n",
      "epoch 17806: train loss: 0.10646520572947456, test loss: 0.25792895291383183\n",
      "epoch 17807: train loss: 0.1064636358678193, test loss: 0.2579299782223508\n",
      "epoch 17808: train loss: 0.10646206612447331, test loss: 0.2579310035765185\n",
      "epoch 17809: train loss: 0.1064604964994204, test loss: 0.25793202897634965\n",
      "epoch 17810: train loss: 0.10645892699264425, test loss: 0.2579330544218283\n",
      "epoch 17811: train loss: 0.10645735760412876, test loss: 0.2579340799129209\n",
      "epoch 17812: train loss: 0.10645578833385759, test loss: 0.2579351054496457\n",
      "epoch 17813: train loss: 0.10645421918181462, test loss: 0.25793613103196933\n",
      "epoch 17814: train loss: 0.10645265014798361, test loss: 0.25793715665989025\n",
      "epoch 17815: train loss: 0.10645108123234832, test loss: 0.25793818233344246\n",
      "epoch 17816: train loss: 0.10644951243489259, test loss: 0.25793920805255965\n",
      "epoch 17817: train loss: 0.10644794375560018, test loss: 0.25794023381724407\n",
      "epoch 17818: train loss: 0.10644637519445493, test loss: 0.25794125962747716\n",
      "epoch 17819: train loss: 0.10644480675144063, test loss: 0.25794228548329273\n",
      "epoch 17820: train loss: 0.10644323842654106, test loss: 0.25794331138463994\n",
      "epoch 17821: train loss: 0.1064416702197401, test loss: 0.2579443373315064\n",
      "epoch 17822: train loss: 0.1064401021310215, test loss: 0.25794536332395224\n",
      "epoch 17823: train loss: 0.10643853416036907, test loss: 0.2579463893618521\n",
      "epoch 17824: train loss: 0.10643696630776667, test loss: 0.2579474154452832\n",
      "epoch 17825: train loss: 0.10643539857319811, test loss: 0.25794844157421687\n",
      "epoch 17826: train loss: 0.10643383095664725, test loss: 0.2579494677486656\n",
      "epoch 17827: train loss: 0.10643226345809788, test loss: 0.25795049396856745\n",
      "epoch 17828: train loss: 0.10643069607753382, test loss: 0.2579515202339376\n",
      "epoch 17829: train loss: 0.10642912881493896, test loss: 0.2579525465447752\n",
      "epoch 17830: train loss: 0.1064275616702971, test loss: 0.2579535729010331\n",
      "epoch 17831: train loss: 0.10642599464359209, test loss: 0.25795459930277587\n",
      "epoch 17832: train loss: 0.10642442773480777, test loss: 0.25795562574992154\n",
      "epoch 17833: train loss: 0.10642286094392801, test loss: 0.25795665224250347\n",
      "epoch 17834: train loss: 0.10642129427093665, test loss: 0.2579576787805051\n",
      "epoch 17835: train loss: 0.10641972771581754, test loss: 0.25795870536389454\n",
      "epoch 17836: train loss: 0.10641816127855452, test loss: 0.2579597319927027\n",
      "epoch 17837: train loss: 0.10641659495913153, test loss: 0.257960758666852\n",
      "epoch 17838: train loss: 0.10641502875753234, test loss: 0.25796178538641873\n",
      "epoch 17839: train loss: 0.10641346267374086, test loss: 0.25796281215134287\n",
      "epoch 17840: train loss: 0.106411896707741, test loss: 0.2579638389616053\n",
      "epoch 17841: train loss: 0.10641033085951655, test loss: 0.25796486581720907\n",
      "epoch 17842: train loss: 0.10640876512905147, test loss: 0.25796589271818215\n",
      "epoch 17843: train loss: 0.10640719951632958, test loss: 0.25796691966446345\n",
      "epoch 17844: train loss: 0.10640563402133481, test loss: 0.2579679466560363\n",
      "epoch 17845: train loss: 0.10640406864405101, test loss: 0.2579689736929657\n",
      "epoch 17846: train loss: 0.10640250338446205, test loss: 0.25797000077516863\n",
      "epoch 17847: train loss: 0.10640093824255192, test loss: 0.25797102790266374\n",
      "epoch 17848: train loss: 0.10639937321830444, test loss: 0.25797205507545157\n",
      "epoch 17849: train loss: 0.10639780831170352, test loss: 0.2579730822934977\n",
      "epoch 17850: train loss: 0.10639624352273305, test loss: 0.2579741095568194\n",
      "epoch 17851: train loss: 0.106394678851377, test loss: 0.2579751368654026\n",
      "epoch 17852: train loss: 0.10639311429761918, test loss: 0.257976164219211\n",
      "epoch 17853: train loss: 0.10639154986144359, test loss: 0.2579771916182332\n",
      "epoch 17854: train loss: 0.10638998554283412, test loss: 0.2579782190624991\n",
      "epoch 17855: train loss: 0.10638842134177466, test loss: 0.25797924655200893\n",
      "epoch 17856: train loss: 0.10638685725824917, test loss: 0.2579802740866823\n",
      "epoch 17857: train loss: 0.10638529329224156, test loss: 0.2579813016665511\n",
      "epoch 17858: train loss: 0.10638372944373574, test loss: 0.2579823292916333\n",
      "epoch 17859: train loss: 0.10638216571271567, test loss: 0.25798335696188046\n",
      "epoch 17860: train loss: 0.10638060209916528, test loss: 0.25798438467729207\n",
      "epoch 17861: train loss: 0.10637903860306847, test loss: 0.2579854124378671\n",
      "epoch 17862: train loss: 0.10637747522440924, test loss: 0.25798644024360795\n",
      "epoch 17863: train loss: 0.10637591196317149, test loss: 0.25798746809445045\n",
      "epoch 17864: train loss: 0.10637434881933919, test loss: 0.2579884959904406\n",
      "epoch 17865: train loss: 0.10637278579289626, test loss: 0.25798952393154917\n",
      "epoch 17866: train loss: 0.10637122288382671, test loss: 0.257990551917775\n",
      "epoch 17867: train loss: 0.10636966009211443, test loss: 0.25799157994911776\n",
      "epoch 17868: train loss: 0.10636809741774343, test loss: 0.2579926080255467\n",
      "epoch 17869: train loss: 0.10636653486069765, test loss: 0.2579936361470462\n",
      "epoch 17870: train loss: 0.10636497242096105, test loss: 0.25799466431361506\n",
      "epoch 17871: train loss: 0.10636341009851762, test loss: 0.25799569252525356\n",
      "epoch 17872: train loss: 0.10636184789335132, test loss: 0.25799672078196373\n",
      "epoch 17873: train loss: 0.10636028580544615, test loss: 0.25799774908368056\n",
      "epoch 17874: train loss: 0.10635872383478605, test loss: 0.25799877743046956\n",
      "epoch 17875: train loss: 0.10635716198135502, test loss: 0.2579998058222963\n",
      "epoch 17876: train loss: 0.10635560024513702, test loss: 0.25800083425911446\n",
      "epoch 17877: train loss: 0.10635403862611607, test loss: 0.2580018627409254\n",
      "epoch 17878: train loss: 0.10635247712427613, test loss: 0.2580028912677601\n",
      "epoch 17879: train loss: 0.10635091573960126, test loss: 0.2580039198395863\n",
      "epoch 17880: train loss: 0.10634935447207539, test loss: 0.25800494845639016\n",
      "epoch 17881: train loss: 0.10634779332168255, test loss: 0.2580059771181699\n",
      "epoch 17882: train loss: 0.10634623228840674, test loss: 0.25800700582491054\n",
      "epoch 17883: train loss: 0.10634467137223194, test loss: 0.2580080345765656\n",
      "epoch 17884: train loss: 0.10634311057314222, test loss: 0.25800906337321355\n",
      "epoch 17885: train loss: 0.10634154989112153, test loss: 0.25801009221477583\n",
      "epoch 17886: train loss: 0.10633998932615389, test loss: 0.2580111211012529\n",
      "epoch 17887: train loss: 0.10633842887822337, test loss: 0.2580121500326596\n",
      "epoch 17888: train loss: 0.10633686854731396, test loss: 0.2580131790089657\n",
      "epoch 17889: train loss: 0.10633530833340968, test loss: 0.25801420803017083\n",
      "epoch 17890: train loss: 0.10633374823649458, test loss: 0.25801523709624385\n",
      "epoch 17891: train loss: 0.10633218825655268, test loss: 0.2580162662072\n",
      "epoch 17892: train loss: 0.106330628393568, test loss: 0.2580172953630421\n",
      "epoch 17893: train loss: 0.1063290686475246, test loss: 0.2580183245637514\n",
      "epoch 17894: train loss: 0.1063275090184065, test loss: 0.258019353809282\n",
      "epoch 17895: train loss: 0.10632594950619775, test loss: 0.25802038309965036\n",
      "epoch 17896: train loss: 0.10632439011088245, test loss: 0.2580214124348729\n",
      "epoch 17897: train loss: 0.10632283083244456, test loss: 0.2580224418149162\n",
      "epoch 17898: train loss: 0.10632127167086818, test loss: 0.25802347123976754\n",
      "epoch 17899: train loss: 0.10631971262613739, test loss: 0.2580245007093929\n",
      "epoch 17900: train loss: 0.1063181536982362, test loss: 0.2580255302238589\n",
      "epoch 17901: train loss: 0.1063165948871487, test loss: 0.25802655978306754\n",
      "epoch 17902: train loss: 0.10631503619285898, test loss: 0.25802758938706816\n",
      "epoch 17903: train loss: 0.10631347761535104, test loss: 0.25802861903583163\n",
      "epoch 17904: train loss: 0.10631191915460902, test loss: 0.2580296487293541\n",
      "epoch 17905: train loss: 0.10631036081061696, test loss: 0.2580306784676384\n",
      "epoch 17906: train loss: 0.10630880258335897, test loss: 0.258031708250653\n",
      "epoch 17907: train loss: 0.10630724447281908, test loss: 0.25803273807838173\n",
      "epoch 17908: train loss: 0.1063056864789814, test loss: 0.2580337679508257\n",
      "epoch 17909: train loss: 0.10630412860183006, test loss: 0.25803479786798605\n",
      "epoch 17910: train loss: 0.10630257084134907, test loss: 0.2580358278298614\n",
      "epoch 17911: train loss: 0.1063010131975226, test loss: 0.2580368578364207\n",
      "epoch 17912: train loss: 0.1062994556703347, test loss: 0.2580378878876343\n",
      "epoch 17913: train loss: 0.1062978982597695, test loss: 0.25803891798354783\n",
      "epoch 17914: train loss: 0.10629634096581107, test loss: 0.2580399481241322\n",
      "epoch 17915: train loss: 0.10629478378844352, test loss: 0.25804097830935396\n",
      "epoch 17916: train loss: 0.10629322672765099, test loss: 0.25804200853919995\n",
      "epoch 17917: train loss: 0.10629166978341757, test loss: 0.2580430388137167\n",
      "epoch 17918: train loss: 0.10629011295572738, test loss: 0.2580440691328738\n",
      "epoch 17919: train loss: 0.10628855624456456, test loss: 0.25804509949662296\n",
      "epoch 17920: train loss: 0.1062869996499132, test loss: 0.25804612990496667\n",
      "epoch 17921: train loss: 0.10628544317175742, test loss: 0.25804716035791836\n",
      "epoch 17922: train loss: 0.10628388681008138, test loss: 0.2580481908554492\n",
      "epoch 17923: train loss: 0.1062823305648692, test loss: 0.25804922139757513\n",
      "epoch 17924: train loss: 0.10628077443610501, test loss: 0.2580502519842795\n",
      "epoch 17925: train loss: 0.10627921842377294, test loss: 0.258051282615532\n",
      "epoch 17926: train loss: 0.10627766252785716, test loss: 0.25805231329133393\n",
      "epoch 17927: train loss: 0.10627610674834174, test loss: 0.2580533440116692\n",
      "epoch 17928: train loss: 0.10627455108521094, test loss: 0.25805437477656945\n",
      "epoch 17929: train loss: 0.1062729955384488, test loss: 0.25805540558597334\n",
      "epoch 17930: train loss: 0.10627144010803953, test loss: 0.25805643643987863\n",
      "epoch 17931: train loss: 0.10626988479396732, test loss: 0.2580574673383206\n",
      "epoch 17932: train loss: 0.10626832959621624, test loss: 0.2580584982812194\n",
      "epoch 17933: train loss: 0.1062667745147705, test loss: 0.25805952926863895\n",
      "epoch 17934: train loss: 0.10626521954961429, test loss: 0.25806056030051616\n",
      "epoch 17935: train loss: 0.10626366470073173, test loss: 0.2580615913768828\n",
      "epoch 17936: train loss: 0.10626210996810702, test loss: 0.25806262249770967\n",
      "epoch 17937: train loss: 0.10626055535172432, test loss: 0.2580636536629469\n",
      "epoch 17938: train loss: 0.10625900085156782, test loss: 0.258064684872662\n",
      "epoch 17939: train loss: 0.1062574464676217, test loss: 0.25806571612678897\n",
      "epoch 17940: train loss: 0.10625589219987014, test loss: 0.2580667474253619\n",
      "epoch 17941: train loss: 0.10625433804829731, test loss: 0.2580677787683016\n",
      "epoch 17942: train loss: 0.10625278401288743, test loss: 0.2580688101557051\n",
      "epoch 17943: train loss: 0.10625123009362468, test loss: 0.2580698415874589\n",
      "epoch 17944: train loss: 0.10624967629049326, test loss: 0.25807087306361476\n",
      "epoch 17945: train loss: 0.10624812260347735, test loss: 0.25807190458415546\n",
      "epoch 17946: train loss: 0.10624656903256118, test loss: 0.2580729361490502\n",
      "epoch 17947: train loss: 0.10624501557772893, test loss: 0.25807396775829977\n",
      "epoch 17948: train loss: 0.1062434622389648, test loss: 0.2580749994119045\n",
      "epoch 17949: train loss: 0.10624190901625306, test loss: 0.25807603110983396\n",
      "epoch 17950: train loss: 0.10624035590957787, test loss: 0.2580770628521209\n",
      "epoch 17951: train loss: 0.10623880291892344, test loss: 0.2580780946387176\n",
      "epoch 17952: train loss: 0.10623725004427406, test loss: 0.2580791264696237\n",
      "epoch 17953: train loss: 0.10623569728561388, test loss: 0.258080158344811\n",
      "epoch 17954: train loss: 0.10623414464292714, test loss: 0.2580811902643412\n",
      "epoch 17955: train loss: 0.10623259211619812, test loss: 0.258082222228122\n",
      "epoch 17956: train loss: 0.10623103970541102, test loss: 0.25808325423616724\n",
      "epoch 17957: train loss: 0.10622948741055005, test loss: 0.2580842862884798\n",
      "epoch 17958: train loss: 0.10622793523159947, test loss: 0.25808531838504317\n",
      "epoch 17959: train loss: 0.10622638316854355, test loss: 0.2580863505258919\n",
      "epoch 17960: train loss: 0.1062248312213665, test loss: 0.25808738271094545\n",
      "epoch 17961: train loss: 0.10622327939005256, test loss: 0.25808841494025386\n",
      "epoch 17962: train loss: 0.106221727674586, test loss: 0.2580894472137516\n",
      "epoch 17963: train loss: 0.1062201760749511, test loss: 0.25809047953147374\n",
      "epoch 17964: train loss: 0.10621862459113209, test loss: 0.2580915118934052\n",
      "epoch 17965: train loss: 0.10621707322311319, test loss: 0.2580925442995145\n",
      "epoch 17966: train loss: 0.10621552197087875, test loss: 0.25809357674980077\n",
      "epoch 17967: train loss: 0.10621397083441299, test loss: 0.2580946092442987\n",
      "epoch 17968: train loss: 0.10621241981370017, test loss: 0.258095641782929\n",
      "epoch 17969: train loss: 0.10621086890872455, test loss: 0.2580966743657249\n",
      "epoch 17970: train loss: 0.10620931811947046, test loss: 0.25809770699265333\n",
      "epoch 17971: train loss: 0.10620776744592214, test loss: 0.25809873966371777\n",
      "epoch 17972: train loss: 0.10620621688806388, test loss: 0.25809977237891796\n",
      "epoch 17973: train loss: 0.10620466644587999, test loss: 0.2581008051382382\n",
      "epoch 17974: train loss: 0.1062031161193547, test loss: 0.25810183794168207\n",
      "epoch 17975: train loss: 0.10620156590847238, test loss: 0.2581028707891832\n",
      "epoch 17976: train loss: 0.10620001581321724, test loss: 0.25810390368079067\n",
      "epoch 17977: train loss: 0.10619846583357363, test loss: 0.25810493661650824\n",
      "epoch 17978: train loss: 0.10619691596952585, test loss: 0.25810596959627136\n",
      "epoch 17979: train loss: 0.1061953662210582, test loss: 0.25810700262011305\n",
      "epoch 17980: train loss: 0.10619381658815494, test loss: 0.25810803568800184\n",
      "epoch 17981: train loss: 0.10619226707080046, test loss: 0.2581090687999222\n",
      "epoch 17982: train loss: 0.10619071766897897, test loss: 0.2581101019559094\n",
      "epoch 17983: train loss: 0.10618916838267489, test loss: 0.2581111351558992\n",
      "epoch 17984: train loss: 0.1061876192118725, test loss: 0.2581121683999244\n",
      "epoch 17985: train loss: 0.1061860701565561, test loss: 0.2581132016879376\n",
      "epoch 17986: train loss: 0.10618452121671006, test loss: 0.2581142350199725\n",
      "epoch 17987: train loss: 0.10618297239231862, test loss: 0.2581152683959638\n",
      "epoch 17988: train loss: 0.10618142368336622, test loss: 0.25811630181598233\n",
      "epoch 17989: train loss: 0.10617987508983712, test loss: 0.25811733527992725\n",
      "epoch 17990: train loss: 0.1061783266117157, test loss: 0.2581183687878829\n",
      "epoch 17991: train loss: 0.10617677824898626, test loss: 0.2581194023397531\n",
      "epoch 17992: train loss: 0.10617523000163315, test loss: 0.25812043593557016\n",
      "epoch 17993: train loss: 0.10617368186964075, test loss: 0.25812146957532045\n",
      "epoch 17994: train loss: 0.10617213385299339, test loss: 0.25812250325903763\n",
      "epoch 17995: train loss: 0.10617058595167542, test loss: 0.2581235369866247\n",
      "epoch 17996: train loss: 0.1061690381656712, test loss: 0.2581245707581481\n",
      "epoch 17997: train loss: 0.10616749049496507, test loss: 0.25812560457354594\n",
      "epoch 17998: train loss: 0.1061659429395414, test loss: 0.25812663843286565\n",
      "epoch 17999: train loss: 0.10616439549938458, test loss: 0.258127672336044\n",
      "epoch 18000: train loss: 0.10616284817447895, test loss: 0.2581287062830669\n",
      "epoch 18001: train loss: 0.10616130096480887, test loss: 0.2581297402740005\n",
      "epoch 18002: train loss: 0.10615975387035875, test loss: 0.2581307743087642\n",
      "epoch 18003: train loss: 0.10615820689111295, test loss: 0.2581318083873762\n",
      "epoch 18004: train loss: 0.10615666002705584, test loss: 0.2581328425098202\n",
      "epoch 18005: train loss: 0.10615511327817181, test loss: 0.25813387667608395\n",
      "epoch 18006: train loss: 0.10615356664444522, test loss: 0.2581349108861661\n",
      "epoch 18007: train loss: 0.10615202012586053, test loss: 0.25813594514008625\n",
      "epoch 18008: train loss: 0.10615047372240205, test loss: 0.25813697943776376\n",
      "epoch 18009: train loss: 0.10614892743405424, test loss: 0.2581380137792311\n",
      "epoch 18010: train loss: 0.10614738126080142, test loss: 0.25813904816449174\n",
      "epoch 18011: train loss: 0.10614583520262809, test loss: 0.2581400825935456\n",
      "epoch 18012: train loss: 0.10614428925951858, test loss: 0.2581411170663297\n",
      "epoch 18013: train loss: 0.1061427434314573, test loss: 0.2581421515828943\n",
      "epoch 18014: train loss: 0.10614119771842871, test loss: 0.2581431861431755\n",
      "epoch 18015: train loss: 0.10613965212041716, test loss: 0.2581442207472067\n",
      "epoch 18016: train loss: 0.10613810663740714, test loss: 0.2581452553949568\n",
      "epoch 18017: train loss: 0.10613656126938299, test loss: 0.2581462900864285\n",
      "epoch 18018: train loss: 0.10613501601632917, test loss: 0.2581473248216068\n",
      "epoch 18019: train loss: 0.10613347087823012, test loss: 0.25814835960047505\n",
      "epoch 18020: train loss: 0.10613192585507022, test loss: 0.2581493944230529\n",
      "epoch 18021: train loss: 0.10613038094683395, test loss: 0.25815042928930787\n",
      "epoch 18022: train loss: 0.10612883615350574, test loss: 0.25815146419922574\n",
      "epoch 18023: train loss: 0.10612729147507, test loss: 0.2581524991527761\n",
      "epoch 18024: train loss: 0.10612574691151117, test loss: 0.258153534150008\n",
      "epoch 18025: train loss: 0.10612420246281375, test loss: 0.258154569190891\n",
      "epoch 18026: train loss: 0.10612265812896209, test loss: 0.25815560427539397\n",
      "epoch 18027: train loss: 0.10612111390994071, test loss: 0.2581566394035504\n",
      "epoch 18028: train loss: 0.10611956980573403, test loss: 0.2581576745752983\n",
      "epoch 18029: train loss: 0.10611802581632655, test loss: 0.2581587097906526\n",
      "epoch 18030: train loss: 0.10611648194170269, test loss: 0.2581597450496178\n",
      "epoch 18031: train loss: 0.10611493818184688, test loss: 0.25816078035214524\n",
      "epoch 18032: train loss: 0.10611339453674365, test loss: 0.2581618156982692\n",
      "epoch 18033: train loss: 0.10611185100637745, test loss: 0.25816285108797415\n",
      "epoch 18034: train loss: 0.10611030759073273, test loss: 0.258163886521248\n",
      "epoch 18035: train loss: 0.10610876428979395, test loss: 0.2581649219980401\n",
      "epoch 18036: train loss: 0.10610722110354563, test loss: 0.2581659575183852\n",
      "epoch 18037: train loss: 0.10610567803197223, test loss: 0.25816699308228697\n",
      "epoch 18038: train loss: 0.10610413507505821, test loss: 0.258168028689713\n",
      "epoch 18039: train loss: 0.1061025922327881, test loss: 0.25816906434066433\n",
      "epoch 18040: train loss: 0.10610104950514636, test loss: 0.258170100035094\n",
      "epoch 18041: train loss: 0.10609950689211747, test loss: 0.25817113577303713\n",
      "epoch 18042: train loss: 0.10609796439368595, test loss: 0.2581721715544619\n",
      "epoch 18043: train loss: 0.10609642200983627, test loss: 0.2581732073793871\n",
      "epoch 18044: train loss: 0.10609487974055297, test loss: 0.25817424324778027\n",
      "epoch 18045: train loss: 0.1060933375858205, test loss: 0.25817527915961097\n",
      "epoch 18046: train loss: 0.10609179554562342, test loss: 0.2581763151149143\n",
      "epoch 18047: train loss: 0.1060902536199462, test loss: 0.25817735111367557\n",
      "epoch 18048: train loss: 0.1060887118087734, test loss: 0.25817838715586305\n",
      "epoch 18049: train loss: 0.10608717011208947, test loss: 0.2581794232414445\n",
      "epoch 18050: train loss: 0.10608562852987898, test loss: 0.25818045937047374\n",
      "epoch 18051: train loss: 0.10608408706212641, test loss: 0.2581814955429181\n",
      "epoch 18052: train loss: 0.10608254570881633, test loss: 0.258182531758745\n",
      "epoch 18053: train loss: 0.10608100446993322, test loss: 0.2581835680179917\n",
      "epoch 18054: train loss: 0.10607946334546167, test loss: 0.2581846043205772\n",
      "epoch 18055: train loss: 0.10607792233538614, test loss: 0.25818564066656774\n",
      "epoch 18056: train loss: 0.10607638143969123, test loss: 0.2581866770559018\n",
      "epoch 18057: train loss: 0.10607484065836142, test loss: 0.2581877134885946\n",
      "epoch 18058: train loss: 0.1060732999913813, test loss: 0.258188749964649\n",
      "epoch 18059: train loss: 0.10607175943873541, test loss: 0.25818978648400254\n",
      "epoch 18060: train loss: 0.10607021900040829, test loss: 0.25819082304672203\n",
      "epoch 18061: train loss: 0.10606867867638449, test loss: 0.2581918596527424\n",
      "epoch 18062: train loss: 0.10606713846664859, test loss: 0.2581928963020823\n",
      "epoch 18063: train loss: 0.10606559837118507, test loss: 0.258193932994728\n",
      "epoch 18064: train loss: 0.10606405838997857, test loss: 0.2581949697306654\n",
      "epoch 18065: train loss: 0.10606251852301363, test loss: 0.25819600650986174\n",
      "epoch 18066: train loss: 0.10606097877027479, test loss: 0.25819704333233645\n",
      "epoch 18067: train loss: 0.10605943913174665, test loss: 0.25819808019807505\n",
      "epoch 18068: train loss: 0.10605789960741376, test loss: 0.25819911710707927\n",
      "epoch 18069: train loss: 0.10605636019726074, test loss: 0.2582001540593169\n",
      "epoch 18070: train loss: 0.1060548209012721, test loss: 0.25820119105482403\n",
      "epoch 18071: train loss: 0.10605328171943247, test loss: 0.2582022280935369\n",
      "epoch 18072: train loss: 0.10605174265172643, test loss: 0.25820326517547293\n",
      "epoch 18073: train loss: 0.10605020369813856, test loss: 0.25820430230060326\n",
      "epoch 18074: train loss: 0.10604866485865344, test loss: 0.2582053394689601\n",
      "epoch 18075: train loss: 0.10604712613325568, test loss: 0.25820637668049673\n",
      "epoch 18076: train loss: 0.10604558752192986, test loss: 0.25820741393521623\n",
      "epoch 18077: train loss: 0.10604404902466058, test loss: 0.2582084512331201\n",
      "epoch 18078: train loss: 0.10604251064143246, test loss: 0.2582094885741595\n",
      "epoch 18079: train loss: 0.10604097237223005, test loss: 0.2582105259583718\n",
      "epoch 18080: train loss: 0.10603943421703806, test loss: 0.25821156338575724\n",
      "epoch 18081: train loss: 0.10603789617584101, test loss: 0.25821260085623543\n",
      "epoch 18082: train loss: 0.10603635824862356, test loss: 0.25821363836987477\n",
      "epoch 18083: train loss: 0.10603482043537031, test loss: 0.2582146759266444\n",
      "epoch 18084: train loss: 0.10603328273606588, test loss: 0.2582157135265134\n",
      "epoch 18085: train loss: 0.1060317451506949, test loss: 0.2582167511694837\n",
      "epoch 18086: train loss: 0.106030207679242, test loss: 0.258217788855541\n",
      "epoch 18087: train loss: 0.10602867032169179, test loss: 0.25821882658468714\n",
      "epoch 18088: train loss: 0.10602713307802888, test loss: 0.2582198643569072\n",
      "epoch 18089: train loss: 0.10602559594823799, test loss: 0.2582209021722037\n",
      "epoch 18090: train loss: 0.10602405893230367, test loss: 0.2582219400305466\n",
      "epoch 18091: train loss: 0.10602252203021059, test loss: 0.258222977931953\n",
      "epoch 18092: train loss: 0.10602098524194342, test loss: 0.25822401587639265\n",
      "epoch 18093: train loss: 0.10601944856748678, test loss: 0.25822505386386824\n",
      "epoch 18094: train loss: 0.10601791200682534, test loss: 0.2582260918943484\n",
      "epoch 18095: train loss: 0.10601637555994371, test loss: 0.2582271299678675\n",
      "epoch 18096: train loss: 0.10601483922682657, test loss: 0.2582281680843794\n",
      "epoch 18097: train loss: 0.1060133030074586, test loss: 0.25822920624386897\n",
      "epoch 18098: train loss: 0.10601176690182446, test loss: 0.25823024444637155\n",
      "epoch 18099: train loss: 0.10601023090990874, test loss: 0.2582312826918225\n",
      "epoch 18100: train loss: 0.10600869503169622, test loss: 0.25823232098025833\n",
      "epoch 18101: train loss: 0.10600715926717147, test loss: 0.25823335931168045\n",
      "epoch 18102: train loss: 0.10600562361631923, test loss: 0.25823439768600853\n",
      "epoch 18103: train loss: 0.10600408807912416, test loss: 0.25823543610331146\n",
      "epoch 18104: train loss: 0.10600255265557092, test loss: 0.25823647456352455\n",
      "epoch 18105: train loss: 0.10600101734564422, test loss: 0.2582375130666673\n",
      "epoch 18106: train loss: 0.10599948214932871, test loss: 0.25823855161273995\n",
      "epoch 18107: train loss: 0.10599794706660912, test loss: 0.2582395902017142\n",
      "epoch 18108: train loss: 0.10599641209747011, test loss: 0.2582406288335751\n",
      "epoch 18109: train loss: 0.10599487724189638, test loss: 0.258241667508323\n",
      "epoch 18110: train loss: 0.10599334249987262, test loss: 0.25824270622596257\n",
      "epoch 18111: train loss: 0.10599180787138356, test loss: 0.25824374498647773\n",
      "epoch 18112: train loss: 0.10599027335641388, test loss: 0.2582447837898394\n",
      "epoch 18113: train loss: 0.10598873895494827, test loss: 0.25824582263604745\n",
      "epoch 18114: train loss: 0.10598720466697148, test loss: 0.2582468615250907\n",
      "epoch 18115: train loss: 0.10598567049246821, test loss: 0.2582479004570019\n",
      "epoch 18116: train loss: 0.10598413643142311, test loss: 0.25824893943171867\n",
      "epoch 18117: train loss: 0.105982602483821, test loss: 0.2582499784492589\n",
      "epoch 18118: train loss: 0.10598106864964656, test loss: 0.25825101750962565\n",
      "epoch 18119: train loss: 0.10597953492888447, test loss: 0.25825205661275374\n",
      "epoch 18120: train loss: 0.10597800132151951, test loss: 0.25825309575871436\n",
      "epoch 18121: train loss: 0.10597646782753639, test loss: 0.2582541349474237\n",
      "epoch 18122: train loss: 0.10597493444691984, test loss: 0.2582551741789198\n",
      "epoch 18123: train loss: 0.1059734011796546, test loss: 0.2582562134531706\n",
      "epoch 18124: train loss: 0.10597186802572542, test loss: 0.25825725277019507\n",
      "epoch 18125: train loss: 0.10597033498511703, test loss: 0.25825829212993057\n",
      "epoch 18126: train loss: 0.10596880205781414, test loss: 0.258259331532445\n",
      "epoch 18127: train loss: 0.10596726924380157, test loss: 0.2582603709776567\n",
      "epoch 18128: train loss: 0.105965736543064, test loss: 0.2582614104656018\n",
      "epoch 18129: train loss: 0.10596420395558624, test loss: 0.2582624499962515\n",
      "epoch 18130: train loss: 0.105962671481353, test loss: 0.2582634895696221\n",
      "epoch 18131: train loss: 0.10596113912034905, test loss: 0.2582645291856523\n",
      "epoch 18132: train loss: 0.10595960687255915, test loss: 0.25826556884440705\n",
      "epoch 18133: train loss: 0.1059580747379681, test loss: 0.2582666085457778\n",
      "epoch 18134: train loss: 0.10595654271656063, test loss: 0.2582676482898614\n",
      "epoch 18135: train loss: 0.10595501080832154, test loss: 0.258268688076597\n",
      "epoch 18136: train loss: 0.10595347901323557, test loss: 0.2582697279059706\n",
      "epoch 18137: train loss: 0.1059519473312875, test loss: 0.25827076777798325\n",
      "epoch 18138: train loss: 0.10595041576246216, test loss: 0.2582718076926376\n",
      "epoch 18139: train loss: 0.10594888430674426, test loss: 0.2582728476498874\n",
      "epoch 18140: train loss: 0.10594735296411863, test loss: 0.2582738876498004\n",
      "epoch 18141: train loss: 0.10594582173457003, test loss: 0.2582749276922801\n",
      "epoch 18142: train loss: 0.10594429061808329, test loss: 0.2582759677773462\n",
      "epoch 18143: train loss: 0.10594275961464318, test loss: 0.2582770079050338\n",
      "epoch 18144: train loss: 0.10594122872423448, test loss: 0.25827804807527827\n",
      "epoch 18145: train loss: 0.10593969794684202, test loss: 0.2582790882880829\n",
      "epoch 18146: train loss: 0.10593816728245059, test loss: 0.2582801285434674\n",
      "epoch 18147: train loss: 0.10593663673104503, test loss: 0.2582811688413825\n",
      "epoch 18148: train loss: 0.10593510629261008, test loss: 0.25828220918183287\n",
      "epoch 18149: train loss: 0.1059335759671306, test loss: 0.2582832495648534\n",
      "epoch 18150: train loss: 0.1059320457545914, test loss: 0.25828428999038094\n",
      "epoch 18151: train loss: 0.10593051565497728, test loss: 0.25828533045841684\n",
      "epoch 18152: train loss: 0.10592898566827307, test loss: 0.25828637096894824\n",
      "epoch 18153: train loss: 0.10592745579446358, test loss: 0.25828741152199247\n",
      "epoch 18154: train loss: 0.1059259260335337, test loss: 0.2582884521175387\n",
      "epoch 18155: train loss: 0.10592439638546818, test loss: 0.2582894927555537\n",
      "epoch 18156: train loss: 0.10592286685025186, test loss: 0.2582905334360408\n",
      "epoch 18157: train loss: 0.10592133742786963, test loss: 0.25829157415900317\n",
      "epoch 18158: train loss: 0.10591980811830627, test loss: 0.2582926149244096\n",
      "epoch 18159: train loss: 0.10591827892154665, test loss: 0.258293655732262\n",
      "epoch 18160: train loss: 0.1059167498375756, test loss: 0.25829469658253046\n",
      "epoch 18161: train loss: 0.10591522086637802, test loss: 0.2582957374752361\n",
      "epoch 18162: train loss: 0.1059136920079387, test loss: 0.2582967784103619\n",
      "epoch 18163: train loss: 0.10591216326224245, test loss: 0.2582978193879123\n",
      "epoch 18164: train loss: 0.10591063462927425, test loss: 0.25829886040785677\n",
      "epoch 18165: train loss: 0.10590910610901887, test loss: 0.2582999014701967\n",
      "epoch 18166: train loss: 0.1059075777014612, test loss: 0.25830094257490344\n",
      "epoch 18167: train loss: 0.10590604940658607, test loss: 0.258301983721993\n",
      "epoch 18168: train loss: 0.1059045212243784, test loss: 0.25830302491145607\n",
      "epoch 18169: train loss: 0.10590299315482303, test loss: 0.2583040661432588\n",
      "epoch 18170: train loss: 0.10590146519790483, test loss: 0.25830510741743856\n",
      "epoch 18171: train loss: 0.10589993735360868, test loss: 0.25830614873391405\n",
      "epoch 18172: train loss: 0.1058984096219195, test loss: 0.25830719009275543\n",
      "epoch 18173: train loss: 0.10589688200282207, test loss: 0.25830823149393134\n",
      "epoch 18174: train loss: 0.10589535449630139, test loss: 0.2583092729373794\n",
      "epoch 18175: train loss: 0.10589382710234226, test loss: 0.2583103144231501\n",
      "epoch 18176: train loss: 0.10589229982092965, test loss: 0.25831135595121507\n",
      "epoch 18177: train loss: 0.10589077265204835, test loss: 0.2583123975215758\n",
      "epoch 18178: train loss: 0.10588924559568337, test loss: 0.25831343913421706\n",
      "epoch 18179: train loss: 0.10588771865181953, test loss: 0.2583144807891128\n",
      "epoch 18180: train loss: 0.10588619182044176, test loss: 0.25831552248627804\n",
      "epoch 18181: train loss: 0.10588466510153496, test loss: 0.2583165642256847\n",
      "epoch 18182: train loss: 0.10588313849508403, test loss: 0.2583176060073333\n",
      "epoch 18183: train loss: 0.10588161200107389, test loss: 0.2583186478312141\n",
      "epoch 18184: train loss: 0.10588008561948946, test loss: 0.25831968969732505\n",
      "epoch 18185: train loss: 0.10587855935031568, test loss: 0.258320731605673\n",
      "epoch 18186: train loss: 0.10587703319353742, test loss: 0.25832177355620856\n",
      "epoch 18187: train loss: 0.1058755071491396, test loss: 0.2583228155489346\n",
      "epoch 18188: train loss: 0.10587398121710719, test loss: 0.2583238575838554\n",
      "epoch 18189: train loss: 0.10587245539742508, test loss: 0.25832489966098926\n",
      "epoch 18190: train loss: 0.10587092969007823, test loss: 0.25832594178024076\n",
      "epoch 18191: train loss: 0.10586940409505155, test loss: 0.25832698394171044\n",
      "epoch 18192: train loss: 0.10586787861232999, test loss: 0.25832802614528555\n",
      "epoch 18193: train loss: 0.1058663532418985, test loss: 0.25832906839105263\n",
      "epoch 18194: train loss: 0.10586482798374197, test loss: 0.25833011067893097\n",
      "epoch 18195: train loss: 0.10586330283784542, test loss: 0.2583311530089569\n",
      "epoch 18196: train loss: 0.10586177780419376, test loss: 0.25833219538106605\n",
      "epoch 18197: train loss: 0.10586025288277191, test loss: 0.25833323779533\n",
      "epoch 18198: train loss: 0.10585872807356492, test loss: 0.2583342802516501\n",
      "epoch 18199: train loss: 0.10585720337655764, test loss: 0.25833532275009574\n",
      "epoch 18200: train loss: 0.10585567879173509, test loss: 0.2583363652906369\n",
      "epoch 18201: train loss: 0.10585415431908221, test loss: 0.25833740787322657\n",
      "epoch 18202: train loss: 0.10585262995858397, test loss: 0.2583384504978849\n",
      "epoch 18203: train loss: 0.10585110571022537, test loss: 0.2583394931646151\n",
      "epoch 18204: train loss: 0.10584958157399132, test loss: 0.25834053587338457\n",
      "epoch 18205: train loss: 0.10584805754986686, test loss: 0.2583415786242159\n",
      "epoch 18206: train loss: 0.10584653363783693, test loss: 0.258342621417076\n",
      "epoch 18207: train loss: 0.10584500983788651, test loss: 0.25834366425193617\n",
      "epoch 18208: train loss: 0.10584348615000057, test loss: 0.2583447071288332\n",
      "epoch 18209: train loss: 0.10584196257416412, test loss: 0.2583457500477197\n",
      "epoch 18210: train loss: 0.10584043911036214, test loss: 0.2583467930086143\n",
      "epoch 18211: train loss: 0.10583891575857968, test loss: 0.2583478360114885\n",
      "epoch 18212: train loss: 0.1058373925188016, test loss: 0.25834887905637693\n",
      "epoch 18213: train loss: 0.10583586939101304, test loss: 0.2583499221432158\n",
      "epoch 18214: train loss: 0.10583434637519894, test loss: 0.25835096527199314\n",
      "epoch 18215: train loss: 0.10583282347134425, test loss: 0.25835200844274503\n",
      "epoch 18216: train loss: 0.10583130067943404, test loss: 0.25835305165543987\n",
      "epoch 18217: train loss: 0.10582977799945333, test loss: 0.25835409491010003\n",
      "epoch 18218: train loss: 0.10582825543138709, test loss: 0.25835513820664163\n",
      "epoch 18219: train loss: 0.10582673297522037, test loss: 0.2583561815451361\n",
      "epoch 18220: train loss: 0.10582521063093814, test loss: 0.25835722492552093\n",
      "epoch 18221: train loss: 0.10582368839852549, test loss: 0.25835826834779785\n",
      "epoch 18222: train loss: 0.10582216627796737, test loss: 0.25835931181200367\n",
      "epoch 18223: train loss: 0.10582064426924885, test loss: 0.25836035531804286\n",
      "epoch 18224: train loss: 0.10581912237235497, test loss: 0.25836139886601656\n",
      "epoch 18225: train loss: 0.1058176005872707, test loss: 0.25836244245582796\n",
      "epoch 18226: train loss: 0.10581607891398115, test loss: 0.25836348608751464\n",
      "epoch 18227: train loss: 0.10581455735247132, test loss: 0.25836452976101315\n",
      "epoch 18228: train loss: 0.10581303590272625, test loss: 0.25836557347637445\n",
      "epoch 18229: train loss: 0.10581151456473098, test loss: 0.2583666172335554\n",
      "epoch 18230: train loss: 0.10580999333847058, test loss: 0.25836766103258824\n",
      "epoch 18231: train loss: 0.10580847222393006, test loss: 0.25836870487341285\n",
      "epoch 18232: train loss: 0.1058069512210945, test loss: 0.2583697487560481\n",
      "epoch 18233: train loss: 0.10580543032994896, test loss: 0.2583707926804971\n",
      "epoch 18234: train loss: 0.1058039095504785, test loss: 0.2583718366467291\n",
      "epoch 18235: train loss: 0.10580238888266814, test loss: 0.2583728806547309\n",
      "epoch 18236: train loss: 0.10580086832650298, test loss: 0.2583739247045061\n",
      "epoch 18237: train loss: 0.10579934788196811, test loss: 0.2583749687960578\n",
      "epoch 18238: train loss: 0.10579782754904855, test loss: 0.25837601292933887\n",
      "epoch 18239: train loss: 0.10579630732772938, test loss: 0.2583770571043851\n",
      "epoch 18240: train loss: 0.10579478721799568, test loss: 0.258378101321153\n",
      "epoch 18241: train loss: 0.10579326721983254, test loss: 0.25837914557967456\n",
      "epoch 18242: train loss: 0.10579174733322504, test loss: 0.25838018987988975\n",
      "epoch 18243: train loss: 0.10579022755815824, test loss: 0.2583812342218166\n",
      "epoch 18244: train loss: 0.10578870789461725, test loss: 0.25838227860545987\n",
      "epoch 18245: train loss: 0.10578718834258716, test loss: 0.2583833230307718\n",
      "epoch 18246: train loss: 0.10578566890205306, test loss: 0.2583843674978236\n",
      "epoch 18247: train loss: 0.10578414957300004, test loss: 0.25838541200648285\n",
      "epoch 18248: train loss: 0.10578263035541319, test loss: 0.25838645655687204\n",
      "epoch 18249: train loss: 0.10578111124927762, test loss: 0.25838750114889225\n",
      "epoch 18250: train loss: 0.10577959225457842, test loss: 0.25838854578254744\n",
      "epoch 18251: train loss: 0.10577807337130071, test loss: 0.258389590457858\n",
      "epoch 18252: train loss: 0.10577655459942963, test loss: 0.25839063517477795\n",
      "epoch 18253: train loss: 0.1057750359389502, test loss: 0.25839167993334256\n",
      "epoch 18254: train loss: 0.10577351738984765, test loss: 0.25839272473352226\n",
      "epoch 18255: train loss: 0.10577199895210702, test loss: 0.2583937695753204\n",
      "epoch 18256: train loss: 0.10577048062571343, test loss: 0.2583948144587243\n",
      "epoch 18257: train loss: 0.10576896241065206, test loss: 0.2583958593837037\n",
      "epoch 18258: train loss: 0.10576744430690802, test loss: 0.25839690435026264\n",
      "epoch 18259: train loss: 0.10576592631446637, test loss: 0.2583979493583852\n",
      "epoch 18260: train loss: 0.10576440843331232, test loss: 0.25839899440806\n",
      "epoch 18261: train loss: 0.10576289066343098, test loss: 0.2584000394993229\n",
      "epoch 18262: train loss: 0.1057613730048075, test loss: 0.2584010846321113\n",
      "epoch 18263: train loss: 0.10575985545742699, test loss: 0.2584021298064468\n",
      "epoch 18264: train loss: 0.10575833802127459, test loss: 0.2584031750223291\n",
      "epoch 18265: train loss: 0.10575682069633549, test loss: 0.25840422027969745\n",
      "epoch 18266: train loss: 0.10575530348259482, test loss: 0.2584052655786228\n",
      "epoch 18267: train loss: 0.1057537863800377, test loss: 0.2584063109189887\n",
      "epoch 18268: train loss: 0.10575226938864937, test loss: 0.2584073563008838\n",
      "epoch 18269: train loss: 0.10575075250841488, test loss: 0.25840840172427904\n",
      "epoch 18270: train loss: 0.10574923573931948, test loss: 0.2584094471891431\n",
      "epoch 18271: train loss: 0.10574771908134827, test loss: 0.2584104926954632\n",
      "epoch 18272: train loss: 0.10574620253448642, test loss: 0.2584115382432254\n",
      "epoch 18273: train loss: 0.10574468609871915, test loss: 0.2584125838324684\n",
      "epoch 18274: train loss: 0.10574316977403159, test loss: 0.25841362946316077\n",
      "epoch 18275: train loss: 0.10574165356040893, test loss: 0.25841467513527355\n",
      "epoch 18276: train loss: 0.10574013745783635, test loss: 0.2584157208488422\n",
      "epoch 18277: train loss: 0.10573862146629899, test loss: 0.2584167666037879\n",
      "epoch 18278: train loss: 0.10573710558578212, test loss: 0.2584178124001636\n",
      "epoch 18279: train loss: 0.10573558981627083, test loss: 0.25841885823790595\n",
      "epoch 18280: train loss: 0.10573407415775037, test loss: 0.2584199041170864\n",
      "epoch 18281: train loss: 0.10573255861020589, test loss: 0.2584209500376232\n",
      "epoch 18282: train loss: 0.10573104317362265, test loss: 0.25842199599953963\n",
      "epoch 18283: train loss: 0.10572952784798577, test loss: 0.258423042002818\n",
      "epoch 18284: train loss: 0.1057280126332805, test loss: 0.2584240880474495\n",
      "epoch 18285: train loss: 0.10572649752949202, test loss: 0.25842513413345153\n",
      "epoch 18286: train loss: 0.10572498253660555, test loss: 0.25842618026077896\n",
      "epoch 18287: train loss: 0.1057234676546063, test loss: 0.2584272264294512\n",
      "epoch 18288: train loss: 0.10572195288347946, test loss: 0.25842827263942314\n",
      "epoch 18289: train loss: 0.10572043822321028, test loss: 0.2584293188907303\n",
      "epoch 18290: train loss: 0.10571892367378392, test loss: 0.2584303651832925\n",
      "epoch 18291: train loss: 0.10571740923518569, test loss: 0.25843141151721566\n",
      "epoch 18292: train loss: 0.10571589490740071, test loss: 0.2584324578924015\n",
      "epoch 18293: train loss: 0.10571438069041428, test loss: 0.2584335043088698\n",
      "epoch 18294: train loss: 0.10571286658421158, test loss: 0.25843455076659255\n",
      "epoch 18295: train loss: 0.10571135258877788, test loss: 0.25843559726560533\n",
      "epoch 18296: train loss: 0.10570983870409842, test loss: 0.25843664380586134\n",
      "epoch 18297: train loss: 0.10570832493015839, test loss: 0.2584376903873492\n",
      "epoch 18298: train loss: 0.10570681126694305, test loss: 0.2584387370101053\n",
      "epoch 18299: train loss: 0.10570529771443767, test loss: 0.2584397836740664\n",
      "epoch 18300: train loss: 0.10570378427262743, test loss: 0.25844083037926885\n",
      "epoch 18301: train loss: 0.10570227094149766, test loss: 0.25844187712563366\n",
      "epoch 18302: train loss: 0.10570075772103354, test loss: 0.25844292391324886\n",
      "epoch 18303: train loss: 0.10569924461122039, test loss: 0.25844397074204783\n",
      "epoch 18304: train loss: 0.10569773161204342, test loss: 0.2584450176120383\n",
      "epoch 18305: train loss: 0.1056962187234879, test loss: 0.2584460645231883\n",
      "epoch 18306: train loss: 0.10569470594553909, test loss: 0.2584471114755193\n",
      "epoch 18307: train loss: 0.10569319327818226, test loss: 0.2584481584689836\n",
      "epoch 18308: train loss: 0.10569168072140268, test loss: 0.2584492055036197\n",
      "epoch 18309: train loss: 0.10569016827518564, test loss: 0.25845025257943155\n",
      "epoch 18310: train loss: 0.10568865593951635, test loss: 0.25845129969633684\n",
      "epoch 18311: train loss: 0.10568714371438016, test loss: 0.2584523468543747\n",
      "epoch 18312: train loss: 0.10568563159976228, test loss: 0.2584533940535497\n",
      "epoch 18313: train loss: 0.10568411959564805, test loss: 0.2584544412938129\n",
      "epoch 18314: train loss: 0.10568260770202274, test loss: 0.25845548857515327\n",
      "epoch 18315: train loss: 0.1056810959188716, test loss: 0.258456535897642\n",
      "epoch 18316: train loss: 0.10567958424618001, test loss: 0.2584575832611804\n",
      "epoch 18317: train loss: 0.10567807268393319, test loss: 0.25845863066578895\n",
      "epoch 18318: train loss: 0.10567656123211641, test loss: 0.2584596781114746\n",
      "epoch 18319: train loss: 0.10567504989071505, test loss: 0.25846072559818667\n",
      "epoch 18320: train loss: 0.10567353865971434, test loss: 0.258461773126001\n",
      "epoch 18321: train loss: 0.10567202753909964, test loss: 0.25846282069481563\n",
      "epoch 18322: train loss: 0.1056705165288562, test loss: 0.2584638683046882\n",
      "epoch 18323: train loss: 0.1056690056289694, test loss: 0.25846491595558807\n",
      "epoch 18324: train loss: 0.1056674948394245, test loss: 0.25846596364746827\n",
      "epoch 18325: train loss: 0.10566598416020685, test loss: 0.2584670113804002\n",
      "epoch 18326: train loss: 0.10566447359130171, test loss: 0.2584680591542884\n",
      "epoch 18327: train loss: 0.10566296313269447, test loss: 0.2584691069691847\n",
      "epoch 18328: train loss: 0.10566145278437039, test loss: 0.2584701548250435\n",
      "epoch 18329: train loss: 0.10565994254631488, test loss: 0.25847120272190216\n",
      "epoch 18330: train loss: 0.1056584324185132, test loss: 0.2584722506596993\n",
      "epoch 18331: train loss: 0.10565692240095069, test loss: 0.2584732986384861\n",
      "epoch 18332: train loss: 0.10565541249361272, test loss: 0.258474346658202\n",
      "epoch 18333: train loss: 0.10565390269648461, test loss: 0.2584753947188319\n",
      "epoch 18334: train loss: 0.10565239300955169, test loss: 0.25847644282044946\n",
      "epoch 18335: train loss: 0.1056508834327993, test loss: 0.25847749096292183\n",
      "epoch 18336: train loss: 0.10564937396621282, test loss: 0.258478539146339\n",
      "epoch 18337: train loss: 0.10564786460977757, test loss: 0.25847958737066973\n",
      "epoch 18338: train loss: 0.1056463553634789, test loss: 0.25848063563590234\n",
      "epoch 18339: train loss: 0.10564484622730216, test loss: 0.2584816839420063\n",
      "epoch 18340: train loss: 0.10564333720123274, test loss: 0.2584827322889871\n",
      "epoch 18341: train loss: 0.10564182828525599, test loss: 0.2584837806768121\n",
      "epoch 18342: train loss: 0.10564031947935724, test loss: 0.2584848291055558\n",
      "epoch 18343: train loss: 0.10563881078352191, test loss: 0.2584858775751377\n",
      "epoch 18344: train loss: 0.10563730219773532, test loss: 0.2584869260855435\n",
      "epoch 18345: train loss: 0.10563579372198287, test loss: 0.2584879746367946\n",
      "epoch 18346: train loss: 0.10563428535624993, test loss: 0.25848902322886047\n",
      "epoch 18347: train loss: 0.10563277710052187, test loss: 0.2584900718617642\n",
      "epoch 18348: train loss: 0.10563126895478407, test loss: 0.25849112053545836\n",
      "epoch 18349: train loss: 0.1056297609190219, test loss: 0.25849216924999757\n",
      "epoch 18350: train loss: 0.10562825299322079, test loss: 0.2584932180053008\n",
      "epoch 18351: train loss: 0.10562674517736607, test loss: 0.2584942668013906\n",
      "epoch 18352: train loss: 0.10562523747144319, test loss: 0.25849531563825373\n",
      "epoch 18353: train loss: 0.10562372987543749, test loss: 0.2584963645159099\n",
      "epoch 18354: train loss: 0.10562222238933437, test loss: 0.25849741343428023\n",
      "epoch 18355: train loss: 0.1056207150131193, test loss: 0.2584984623934358\n",
      "epoch 18356: train loss: 0.10561920774677759, test loss: 0.2584995113933305\n",
      "epoch 18357: train loss: 0.10561770059029472, test loss: 0.25850056043395075\n",
      "epoch 18358: train loss: 0.10561619354365603, test loss: 0.25850160951531875\n",
      "epoch 18359: train loss: 0.10561468660684699, test loss: 0.2585026586373874\n",
      "epoch 18360: train loss: 0.105613179779853, test loss: 0.25850370780017884\n",
      "epoch 18361: train loss: 0.10561167306265944, test loss: 0.25850475700367903\n",
      "epoch 18362: train loss: 0.10561016645525174, test loss: 0.25850580624784164\n",
      "epoch 18363: train loss: 0.10560865995761538, test loss: 0.25850685553268815\n",
      "epoch 18364: train loss: 0.10560715356973571, test loss: 0.2585079048582564\n",
      "epoch 18365: train loss: 0.10560564729159819, test loss: 0.2585089542244331\n",
      "epoch 18366: train loss: 0.10560414112318824, test loss: 0.25851000363130605\n",
      "epoch 18367: train loss: 0.10560263506449133, test loss: 0.25851105307882916\n",
      "epoch 18368: train loss: 0.10560112911549283, test loss: 0.2585121025669553\n",
      "epoch 18369: train loss: 0.10559962327617821, test loss: 0.25851315209574094\n",
      "epoch 18370: train loss: 0.10559811754653292, test loss: 0.2585142016651714\n",
      "epoch 18371: train loss: 0.1055966119265424, test loss: 0.2585152512752025\n",
      "epoch 18372: train loss: 0.10559510641619213, test loss: 0.2585163009258543\n",
      "epoch 18373: train loss: 0.1055936010154675, test loss: 0.2585173506170793\n",
      "epoch 18374: train loss: 0.10559209572435396, test loss: 0.25851840034891815\n",
      "epoch 18375: train loss: 0.10559059054283702, test loss: 0.2585194501213041\n",
      "epoch 18376: train loss: 0.10558908547090208, test loss: 0.2585204999342964\n",
      "epoch 18377: train loss: 0.10558758050853463, test loss: 0.2585215497878625\n",
      "epoch 18378: train loss: 0.10558607565572016, test loss: 0.2585225996819727\n",
      "epoch 18379: train loss: 0.1055845709124441, test loss: 0.25852364961661634\n",
      "epoch 18380: train loss: 0.10558306627869193, test loss: 0.2585246995918486\n",
      "epoch 18381: train loss: 0.1055815617544491, test loss: 0.25852574960756997\n",
      "epoch 18382: train loss: 0.10558005733970109, test loss: 0.2585267996638197\n",
      "epoch 18383: train loss: 0.10557855303443342, test loss: 0.25852784976058724\n",
      "epoch 18384: train loss: 0.10557704883863153, test loss: 0.2585288998978738\n",
      "epoch 18385: train loss: 0.10557554475228091, test loss: 0.2585299500756516\n",
      "epoch 18386: train loss: 0.10557404077536703, test loss: 0.25853100029394244\n",
      "epoch 18387: train loss: 0.10557253690787541, test loss: 0.2585320505526986\n",
      "epoch 18388: train loss: 0.10557103314979152, test loss: 0.25853310085190884\n",
      "epoch 18389: train loss: 0.10556952950110085, test loss: 0.25853415119157647\n",
      "epoch 18390: train loss: 0.10556802596178892, test loss: 0.25853520157174037\n",
      "epoch 18391: train loss: 0.10556652253184119, test loss: 0.25853625199233643\n",
      "epoch 18392: train loss: 0.10556501921124323, test loss: 0.2585373024533878\n",
      "epoch 18393: train loss: 0.10556351599998048, test loss: 0.2585383529548646\n",
      "epoch 18394: train loss: 0.10556201289803846, test loss: 0.2585394034967175\n",
      "epoch 18395: train loss: 0.10556050990540271, test loss: 0.2585404540790395\n",
      "epoch 18396: train loss: 0.10555900702205868, test loss: 0.25854150470174875\n",
      "epoch 18397: train loss: 0.10555750424799194, test loss: 0.2585425553648669\n",
      "epoch 18398: train loss: 0.10555600158318802, test loss: 0.258543606068381\n",
      "epoch 18399: train loss: 0.10555449902763239, test loss: 0.25854465681226185\n",
      "epoch 18400: train loss: 0.1055529965813106, test loss: 0.2585457075964976\n",
      "epoch 18401: train loss: 0.10555149424420819, test loss: 0.258546758421126\n",
      "epoch 18402: train loss: 0.10554999201631066, test loss: 0.2585478092860843\n",
      "epoch 18403: train loss: 0.10554848989760358, test loss: 0.25854886019141027\n",
      "epoch 18404: train loss: 0.10554698788807246, test loss: 0.25854991113707476\n",
      "epoch 18405: train loss: 0.10554548598770282, test loss: 0.25855096212304823\n",
      "epoch 18406: train loss: 0.10554398419648021, test loss: 0.2585520131494042\n",
      "epoch 18407: train loss: 0.1055424825143902, test loss: 0.2585530642160102\n",
      "epoch 18408: train loss: 0.10554098094141835, test loss: 0.25855411532293937\n",
      "epoch 18409: train loss: 0.10553947947755013, test loss: 0.25855516647019633\n",
      "epoch 18410: train loss: 0.10553797812277114, test loss: 0.258556217657717\n",
      "epoch 18411: train loss: 0.10553647687706696, test loss: 0.2585572688855235\n",
      "epoch 18412: train loss: 0.10553497574042313, test loss: 0.25855832015358593\n",
      "epoch 18413: train loss: 0.10553347471282518, test loss: 0.25855937146192637\n",
      "epoch 18414: train loss: 0.10553197379425867, test loss: 0.2585604228104993\n",
      "epoch 18415: train loss: 0.10553047298470922, test loss: 0.2585614741993426\n",
      "epoch 18416: train loss: 0.10552897228416235, test loss: 0.2585625256284253\n",
      "epoch 18417: train loss: 0.10552747169260364, test loss: 0.2585635770977549\n",
      "epoch 18418: train loss: 0.10552597121001868, test loss: 0.2585646286072838\n",
      "epoch 18419: train loss: 0.10552447083639305, test loss: 0.25856568015703274\n",
      "epoch 18420: train loss: 0.10552297057171228, test loss: 0.25856673174697414\n",
      "epoch 18421: train loss: 0.105521470415962, test loss: 0.258567783377128\n",
      "epoch 18422: train loss: 0.10551997036912777, test loss: 0.25856883504746664\n",
      "epoch 18423: train loss: 0.1055184704311952, test loss: 0.2585698867579931\n",
      "epoch 18424: train loss: 0.10551697060214986, test loss: 0.2585709385086969\n",
      "epoch 18425: train loss: 0.10551547088197732, test loss: 0.25857199029952915\n",
      "epoch 18426: train loss: 0.10551397127066324, test loss: 0.2585730421305657\n",
      "epoch 18427: train loss: 0.10551247176819317, test loss: 0.2585740940017074\n",
      "epoch 18428: train loss: 0.10551097237455272, test loss: 0.2585751459130101\n",
      "epoch 18429: train loss: 0.10550947308972748, test loss: 0.258576197864445\n",
      "epoch 18430: train loss: 0.10550797391370312, test loss: 0.258577249855999\n",
      "epoch 18431: train loss: 0.10550647484646515, test loss: 0.2585783018876598\n",
      "epoch 18432: train loss: 0.10550497588799923, test loss: 0.25857935395943427\n",
      "epoch 18433: train loss: 0.10550347703829097, test loss: 0.2585804060712909\n",
      "epoch 18434: train loss: 0.10550197829732604, test loss: 0.2585814582232519\n",
      "epoch 18435: train loss: 0.10550047966508996, test loss: 0.2585825104152883\n",
      "epoch 18436: train loss: 0.10549898114156842, test loss: 0.25858356264740445\n",
      "epoch 18437: train loss: 0.10549748272674705, test loss: 0.25858461491957274\n",
      "epoch 18438: train loss: 0.10549598442061142, test loss: 0.2585856672317969\n",
      "epoch 18439: train loss: 0.10549448622314722, test loss: 0.25858671958408047\n",
      "epoch 18440: train loss: 0.10549298813434006, test loss: 0.2585877719763786\n",
      "epoch 18441: train loss: 0.10549149015417558, test loss: 0.2585888244087304\n",
      "epoch 18442: train loss: 0.10548999228263942, test loss: 0.2585898768810895\n",
      "epoch 18443: train loss: 0.10548849451971719, test loss: 0.25859092939349354\n",
      "epoch 18444: train loss: 0.10548699686539459, test loss: 0.2585919819458811\n",
      "epoch 18445: train loss: 0.10548549931965724, test loss: 0.25859303453825583\n",
      "epoch 18446: train loss: 0.10548400188249078, test loss: 0.2585940871706228\n",
      "epoch 18447: train loss: 0.10548250455388086, test loss: 0.25859513984298715\n",
      "epoch 18448: train loss: 0.10548100733381315, test loss: 0.25859619255530064\n",
      "epoch 18449: train loss: 0.10547951022227332, test loss: 0.2585972453076069\n",
      "epoch 18450: train loss: 0.10547801321924698, test loss: 0.25859829809987195\n",
      "epoch 18451: train loss: 0.10547651632471985, test loss: 0.25859935093205266\n",
      "epoch 18452: train loss: 0.10547501953867759, test loss: 0.25860040380420357\n",
      "epoch 18453: train loss: 0.10547352286110583, test loss: 0.25860145671626195\n",
      "epoch 18454: train loss: 0.10547202629199028, test loss: 0.258602509668266\n",
      "epoch 18455: train loss: 0.10547052983131656, test loss: 0.258603562660171\n",
      "epoch 18456: train loss: 0.1054690334790704, test loss: 0.25860461569196347\n",
      "epoch 18457: train loss: 0.10546753723523748, test loss: 0.2586056687636826\n",
      "epoch 18458: train loss: 0.10546604109980347, test loss: 0.258606721875284\n",
      "epoch 18459: train loss: 0.10546454507275403, test loss: 0.2586077750267703\n",
      "epoch 18460: train loss: 0.10546304915407488, test loss: 0.25860882821811404\n",
      "epoch 18461: train loss: 0.10546155334375168, test loss: 0.25860988144933506\n",
      "epoch 18462: train loss: 0.10546005764177017, test loss: 0.2586109347203898\n",
      "epoch 18463: train loss: 0.10545856204811599, test loss: 0.2586119880313149\n",
      "epoch 18464: train loss: 0.10545706656277488, test loss: 0.25861304138208346\n",
      "epoch 18465: train loss: 0.10545557118573255, test loss: 0.25861409477268404\n",
      "epoch 18466: train loss: 0.10545407591697467, test loss: 0.25861514820310116\n",
      "epoch 18467: train loss: 0.10545258075648693, test loss: 0.2586162016733428\n",
      "epoch 18468: train loss: 0.10545108570425507, test loss: 0.2586172551833781\n",
      "epoch 18469: train loss: 0.10544959076026483, test loss: 0.2586183087332303\n",
      "epoch 18470: train loss: 0.1054480959245019, test loss: 0.25861936232285265\n",
      "epoch 18471: train loss: 0.10544660119695194, test loss: 0.25862041595224977\n",
      "epoch 18472: train loss: 0.10544510657760077, test loss: 0.25862146962142807\n",
      "epoch 18473: train loss: 0.10544361206643404, test loss: 0.25862252333040914\n",
      "epoch 18474: train loss: 0.1054421176634375, test loss: 0.25862357707907724\n",
      "epoch 18475: train loss: 0.10544062336859689, test loss: 0.25862463086757786\n",
      "epoch 18476: train loss: 0.10543912918189793, test loss: 0.2586256846957741\n",
      "epoch 18477: train loss: 0.10543763510332634, test loss: 0.258626738563692\n",
      "epoch 18478: train loss: 0.10543614113286788, test loss: 0.2586277924713529\n",
      "epoch 18479: train loss: 0.10543464727050829, test loss: 0.25862884641870937\n",
      "epoch 18480: train loss: 0.10543315351623328, test loss: 0.25862990040578476\n",
      "epoch 18481: train loss: 0.10543165987002862, test loss: 0.25863095443256834\n",
      "epoch 18482: train loss: 0.10543016633188004, test loss: 0.258632008499046\n",
      "epoch 18483: train loss: 0.10542867290177331, test loss: 0.25863306260518926\n",
      "epoch 18484: train loss: 0.10542717957969419, test loss: 0.25863411675102127\n",
      "epoch 18485: train loss: 0.10542568636562841, test loss: 0.25863517093651084\n",
      "epoch 18486: train loss: 0.1054241932595617, test loss: 0.25863622516170204\n",
      "epoch 18487: train loss: 0.1054227002614799, test loss: 0.2586372794264738\n",
      "epoch 18488: train loss: 0.10542120737136873, test loss: 0.25863833373092193\n",
      "epoch 18489: train loss: 0.10541971458921395, test loss: 0.25863938807501496\n",
      "epoch 18490: train loss: 0.10541822191500132, test loss: 0.25864044245872564\n",
      "epoch 18491: train loss: 0.10541672934871665, test loss: 0.2586414968820399\n",
      "epoch 18492: train loss: 0.10541523689034568, test loss: 0.2586425513449825\n",
      "epoch 18493: train loss: 0.10541374453987419, test loss: 0.2586436058475236\n",
      "epoch 18494: train loss: 0.105412252297288, test loss: 0.2586446603896491\n",
      "epoch 18495: train loss: 0.10541076016257282, test loss: 0.25864571497135047\n",
      "epoch 18496: train loss: 0.1054092681357145, test loss: 0.25864676959263044\n",
      "epoch 18497: train loss: 0.10540777621669878, test loss: 0.2586478242534952\n",
      "epoch 18498: train loss: 0.10540628440551149, test loss: 0.25864887895391603\n",
      "epoch 18499: train loss: 0.1054047927021384, test loss: 0.25864993369388173\n",
      "epoch 18500: train loss: 0.10540330110656532, test loss: 0.25865098847337964\n",
      "epoch 18501: train loss: 0.10540180961877801, test loss: 0.2586520432924489\n",
      "epoch 18502: train loss: 0.10540031823876231, test loss: 0.25865309815100973\n",
      "epoch 18503: train loss: 0.10539882696650403, test loss: 0.25865415304908324\n",
      "epoch 18504: train loss: 0.10539733580198897, test loss: 0.25865520798671243\n",
      "epoch 18505: train loss: 0.10539584474520289, test loss: 0.25865626296383043\n",
      "epoch 18506: train loss: 0.10539435379613167, test loss: 0.2586573179803925\n",
      "epoch 18507: train loss: 0.10539286295476107, test loss: 0.25865837303650685\n",
      "epoch 18508: train loss: 0.10539137222107696, test loss: 0.25865942813207704\n",
      "epoch 18509: train loss: 0.10538988159506509, test loss: 0.25866048326710667\n",
      "epoch 18510: train loss: 0.10538839107671137, test loss: 0.25866153844160145\n",
      "epoch 18511: train loss: 0.10538690066600155, test loss: 0.25866259365553446\n",
      "epoch 18512: train loss: 0.1053854103629215, test loss: 0.2586636489089256\n",
      "epoch 18513: train loss: 0.10538392016745704, test loss: 0.25866470420176435\n",
      "epoch 18514: train loss: 0.10538243007959398, test loss: 0.2586657595340221\n",
      "epoch 18515: train loss: 0.10538094009931818, test loss: 0.2586668149057031\n",
      "epoch 18516: train loss: 0.10537945022661548, test loss: 0.25866787031677946\n",
      "epoch 18517: train loss: 0.10537796046147171, test loss: 0.25866892576729034\n",
      "epoch 18518: train loss: 0.10537647080387273, test loss: 0.2586699812571729\n",
      "epoch 18519: train loss: 0.10537498125380436, test loss: 0.2586710367864497\n",
      "epoch 18520: train loss: 0.10537349181125247, test loss: 0.2586720923551262\n",
      "epoch 18521: train loss: 0.1053720024762029, test loss: 0.2586731479631397\n",
      "epoch 18522: train loss: 0.10537051324864151, test loss: 0.2586742036105456\n",
      "epoch 18523: train loss: 0.10536902412855419, test loss: 0.25867525929729984\n",
      "epoch 18524: train loss: 0.10536753511592675, test loss: 0.25867631502340677\n",
      "epoch 18525: train loss: 0.10536604621074504, test loss: 0.25867737078881897\n",
      "epoch 18526: train loss: 0.10536455741299498, test loss: 0.25867842659359597\n",
      "epoch 18527: train loss: 0.1053630687226624, test loss: 0.25867948243767425\n",
      "epoch 18528: train loss: 0.10536158013973321, test loss: 0.2586805383210931\n",
      "epoch 18529: train loss: 0.10536009166419323, test loss: 0.25868159424377096\n",
      "epoch 18530: train loss: 0.10535860329602836, test loss: 0.2586826502058003\n",
      "epoch 18531: train loss: 0.10535711503522449, test loss: 0.2586837062070845\n",
      "epoch 18532: train loss: 0.10535562688176751, test loss: 0.2586847622476618\n",
      "epoch 18533: train loss: 0.10535413883564326, test loss: 0.25868581832752235\n",
      "epoch 18534: train loss: 0.10535265089683767, test loss: 0.25868687444665317\n",
      "epoch 18535: train loss: 0.1053511630653366, test loss: 0.2586879306050251\n",
      "epoch 18536: train loss: 0.10534967534112594, test loss: 0.2586889868026451\n",
      "epoch 18537: train loss: 0.1053481877241916, test loss: 0.2586900430395004\n",
      "epoch 18538: train loss: 0.10534670021451947, test loss: 0.25869109931561485\n",
      "epoch 18539: train loss: 0.1053452128120955, test loss: 0.2586921556309235\n",
      "epoch 18540: train loss: 0.1053437255169055, test loss: 0.25869321198545114\n",
      "epoch 18541: train loss: 0.10534223832893544, test loss: 0.2586942683792191\n",
      "epoch 18542: train loss: 0.10534075124817122, test loss: 0.2586953248121658\n",
      "epoch 18543: train loss: 0.1053392642745987, test loss: 0.2586963812842958\n",
      "epoch 18544: train loss: 0.10533777740820387, test loss: 0.25869743779559606\n",
      "epoch 18545: train loss: 0.10533629064897258, test loss: 0.25869849434614534\n",
      "epoch 18546: train loss: 0.1053348039968908, test loss: 0.2586995509357903\n",
      "epoch 18547: train loss: 0.10533331745194442, test loss: 0.2587006075646239\n",
      "epoch 18548: train loss: 0.10533183101411936, test loss: 0.25870166423258256\n",
      "epoch 18549: train loss: 0.10533034468340158, test loss: 0.2587027209397242\n",
      "epoch 18550: train loss: 0.10532885845977696, test loss: 0.2587037776859513\n",
      "epoch 18551: train loss: 0.10532737234323147, test loss: 0.25870483447137393\n",
      "epoch 18552: train loss: 0.10532588633375103, test loss: 0.2587058912958745\n",
      "epoch 18553: train loss: 0.10532440043132159, test loss: 0.2587069481594785\n",
      "epoch 18554: train loss: 0.10532291463592905, test loss: 0.25870800506218844\n",
      "epoch 18555: train loss: 0.10532142894755941, test loss: 0.2587090620039975\n",
      "epoch 18556: train loss: 0.10531994336619857, test loss: 0.2587101189848901\n",
      "epoch 18557: train loss: 0.10531845789183251, test loss: 0.25871117600487487\n",
      "epoch 18558: train loss: 0.10531697252444715, test loss: 0.25871223306392066\n",
      "epoch 18559: train loss: 0.10531548726402847, test loss: 0.2587132901620166\n",
      "epoch 18560: train loss: 0.10531400211056237, test loss: 0.2587143472991869\n",
      "epoch 18561: train loss: 0.10531251706403488, test loss: 0.2587154044754025\n",
      "epoch 18562: train loss: 0.10531103212443194, test loss: 0.2587164616906693\n",
      "epoch 18563: train loss: 0.10530954729173947, test loss: 0.2587175189449395\n",
      "epoch 18564: train loss: 0.10530806256594351, test loss: 0.2587185762382378\n",
      "epoch 18565: train loss: 0.10530657794702997, test loss: 0.2587196335705358\n",
      "epoch 18566: train loss: 0.10530509343498483, test loss: 0.2587206909418546\n",
      "epoch 18567: train loss: 0.10530360902979409, test loss: 0.25872174835215084\n",
      "epoch 18568: train loss: 0.10530212473144371, test loss: 0.25872280580146234\n",
      "epoch 18569: train loss: 0.10530064053991964, test loss: 0.2587238632897627\n",
      "epoch 18570: train loss: 0.10529915645520792, test loss: 0.25872492081700527\n",
      "epoch 18571: train loss: 0.1052976724772945, test loss: 0.25872597838324685\n",
      "epoch 18572: train loss: 0.10529618860616537, test loss: 0.258727035988426\n",
      "epoch 18573: train loss: 0.10529470484180652, test loss: 0.25872809363258126\n",
      "epoch 18574: train loss: 0.10529322118420394, test loss: 0.25872915131563196\n",
      "epoch 18575: train loss: 0.10529173763334362, test loss: 0.25873020903763666\n",
      "epoch 18576: train loss: 0.10529025418921162, test loss: 0.25873126679854974\n",
      "epoch 18577: train loss: 0.10528877085179385, test loss: 0.2587323245984114\n",
      "epoch 18578: train loss: 0.10528728762107634, test loss: 0.25873338243715926\n",
      "epoch 18579: train loss: 0.10528580449704511, test loss: 0.2587344403148146\n",
      "epoch 18580: train loss: 0.1052843214796862, test loss: 0.25873549823133185\n",
      "epoch 18581: train loss: 0.10528283856898554, test loss: 0.2587365561867885\n",
      "epoch 18582: train loss: 0.1052813557649292, test loss: 0.2587376141810836\n",
      "epoch 18583: train loss: 0.1052798730675032, test loss: 0.2587386722142783\n",
      "epoch 18584: train loss: 0.10527839047669353, test loss: 0.25873973028628794\n",
      "epoch 18585: train loss: 0.10527690799248622, test loss: 0.2587407883971737\n",
      "epoch 18586: train loss: 0.10527542561486732, test loss: 0.2587418465469058\n",
      "epoch 18587: train loss: 0.10527394334382283, test loss: 0.2587429047354911\n",
      "epoch 18588: train loss: 0.10527246117933878, test loss: 0.2587439629628825\n",
      "epoch 18589: train loss: 0.10527097912140121, test loss: 0.25874502122906834\n",
      "epoch 18590: train loss: 0.10526949716999613, test loss: 0.25874607953412626\n",
      "epoch 18591: train loss: 0.10526801532510963, test loss: 0.25874713787795467\n",
      "epoch 18592: train loss: 0.10526653358672768, test loss: 0.25874819626058004\n",
      "epoch 18593: train loss: 0.1052650519548364, test loss: 0.2587492546819905\n",
      "epoch 18594: train loss: 0.10526357042942176, test loss: 0.25875031314219166\n",
      "epoch 18595: train loss: 0.10526208901046986, test loss: 0.25875137164113676\n",
      "epoch 18596: train loss: 0.10526060769796673, test loss: 0.2587524301788498\n",
      "epoch 18597: train loss: 0.10525912649189845, test loss: 0.25875348875533777\n",
      "epoch 18598: train loss: 0.10525764539225102, test loss: 0.2587545473705547\n",
      "epoch 18599: train loss: 0.10525616439901053, test loss: 0.25875560602453956\n",
      "epoch 18600: train loss: 0.10525468351216306, test loss: 0.2587566647172314\n",
      "epoch 18601: train loss: 0.10525320273169464, test loss: 0.2587577234486869\n",
      "epoch 18602: train loss: 0.10525172205759135, test loss: 0.2587587822187895\n",
      "epoch 18603: train loss: 0.10525024148983927, test loss: 0.25875984102763644\n",
      "epoch 18604: train loss: 0.10524876102842445, test loss: 0.2587608998751947\n",
      "epoch 18605: train loss: 0.105247280673333, test loss: 0.25876195876143754\n",
      "epoch 18606: train loss: 0.10524580042455092, test loss: 0.2587630176863535\n",
      "epoch 18607: train loss: 0.10524432028206436, test loss: 0.2587640766499314\n",
      "epoch 18608: train loss: 0.10524284024585938, test loss: 0.25876513565221226\n",
      "epoch 18609: train loss: 0.10524136031592207, test loss: 0.2587661946931148\n",
      "epoch 18610: train loss: 0.10523988049223851, test loss: 0.2587672537726998\n",
      "epoch 18611: train loss: 0.1052384007747948, test loss: 0.25876831289090035\n",
      "epoch 18612: train loss: 0.10523692116357701, test loss: 0.2587693720477601\n",
      "epoch 18613: train loss: 0.10523544165857125, test loss: 0.2587704312432307\n",
      "epoch 18614: train loss: 0.10523396225976361, test loss: 0.2587714904773381\n",
      "epoch 18615: train loss: 0.10523248296714019, test loss: 0.25877254975005337\n",
      "epoch 18616: train loss: 0.10523100378068712, test loss: 0.25877360906132924\n",
      "epoch 18617: train loss: 0.10522952470039046, test loss: 0.25877466841124247\n",
      "epoch 18618: train loss: 0.10522804572623634, test loss: 0.25877572779972996\n",
      "epoch 18619: train loss: 0.10522656685821088, test loss: 0.25877678722679676\n",
      "epoch 18620: train loss: 0.1052250880963002, test loss: 0.2587778466924346\n",
      "epoch 18621: train loss: 0.10522360944049038, test loss: 0.2587789061966451\n",
      "epoch 18622: train loss: 0.10522213089076755, test loss: 0.2587799657394054\n",
      "epoch 18623: train loss: 0.10522065244711787, test loss: 0.25878102532071684\n",
      "epoch 18624: train loss: 0.1052191741095274, test loss: 0.25878208494057087\n",
      "epoch 18625: train loss: 0.1052176958779823, test loss: 0.25878314459892005\n",
      "epoch 18626: train loss: 0.10521621775246871, test loss: 0.2587842042958249\n",
      "epoch 18627: train loss: 0.10521473973297274, test loss: 0.2587852640312559\n",
      "epoch 18628: train loss: 0.10521326181948051, test loss: 0.25878632380514927\n",
      "epoch 18629: train loss: 0.10521178401197823, test loss: 0.25878738361758197\n",
      "epoch 18630: train loss: 0.10521030631045196, test loss: 0.2587884434684902\n",
      "epoch 18631: train loss: 0.10520882871488788, test loss: 0.2587895033578794\n",
      "epoch 18632: train loss: 0.1052073512252721, test loss: 0.25879056328574024\n",
      "epoch 18633: train loss: 0.10520587384159079, test loss: 0.2587916232520788\n",
      "epoch 18634: train loss: 0.10520439656383011, test loss: 0.2587926832568832\n",
      "epoch 18635: train loss: 0.1052029193919762, test loss: 0.2587937433001077\n",
      "epoch 18636: train loss: 0.10520144232601521, test loss: 0.258794803381793\n",
      "epoch 18637: train loss: 0.10519996536593329, test loss: 0.2587958635019298\n",
      "epoch 18638: train loss: 0.10519848851171665, test loss: 0.2587969236604711\n",
      "epoch 18639: train loss: 0.10519701176335138, test loss: 0.25879798385742353\n",
      "epoch 18640: train loss: 0.10519553512082372, test loss: 0.25879904409282783\n",
      "epoch 18641: train loss: 0.10519405858411975, test loss: 0.258800104366604\n",
      "epoch 18642: train loss: 0.10519258215322569, test loss: 0.25880116467877573\n",
      "epoch 18643: train loss: 0.10519110582812775, test loss: 0.25880222502933203\n",
      "epoch 18644: train loss: 0.10518962960881204, test loss: 0.2588032854182803\n",
      "epoch 18645: train loss: 0.10518815349526477, test loss: 0.25880434584558965\n",
      "epoch 18646: train loss: 0.10518667748747211, test loss: 0.25880540631128524\n",
      "epoch 18647: train loss: 0.10518520158542025, test loss: 0.25880646681528763\n",
      "epoch 18648: train loss: 0.10518372578909536, test loss: 0.2588075273577069\n",
      "epoch 18649: train loss: 0.10518225009848364, test loss: 0.2588085879383918\n",
      "epoch 18650: train loss: 0.10518077451357132, test loss: 0.25880964855743666\n",
      "epoch 18651: train loss: 0.10517929903434453, test loss: 0.25881070921481525\n",
      "epoch 18652: train loss: 0.10517782366078951, test loss: 0.2588117699105127\n",
      "epoch 18653: train loss: 0.1051763483928924, test loss: 0.2588128306445029\n",
      "epoch 18654: train loss: 0.10517487323063947, test loss: 0.25881389141682865\n",
      "epoch 18655: train loss: 0.10517339817401691, test loss: 0.2588149522274054\n",
      "epoch 18656: train loss: 0.10517192322301089, test loss: 0.2588160130762959\n",
      "epoch 18657: train loss: 0.10517044837760764, test loss: 0.2588170739634153\n",
      "epoch 18658: train loss: 0.10516897363779337, test loss: 0.2588181348888437\n",
      "epoch 18659: train loss: 0.10516749900355431, test loss: 0.25881919585251656\n",
      "epoch 18660: train loss: 0.10516602447487665, test loss: 0.2588202568544402\n",
      "epoch 18661: train loss: 0.10516455005174663, test loss: 0.25882131789462276\n",
      "epoch 18662: train loss: 0.10516307573415049, test loss: 0.2588223789730342\n",
      "epoch 18663: train loss: 0.10516160152207439, test loss: 0.25882344008969904\n",
      "epoch 18664: train loss: 0.1051601274155046, test loss: 0.2588245012445347\n",
      "epoch 18665: train loss: 0.10515865341442733, test loss: 0.2588255624376039\n",
      "epoch 18666: train loss: 0.10515717951882887, test loss: 0.2588266236688754\n",
      "epoch 18667: train loss: 0.10515570572869541, test loss: 0.25882768493835734\n",
      "epoch 18668: train loss: 0.1051542320440132, test loss: 0.2588287462460032\n",
      "epoch 18669: train loss: 0.10515275846476842, test loss: 0.25882980759183705\n",
      "epoch 18670: train loss: 0.10515128499094739, test loss: 0.2588308689758671\n",
      "epoch 18671: train loss: 0.10514981162253634, test loss: 0.258831930398045\n",
      "epoch 18672: train loss: 0.10514833835952146, test loss: 0.2588329918583787\n",
      "epoch 18673: train loss: 0.1051468652018891, test loss: 0.25883405335685755\n",
      "epoch 18674: train loss: 0.10514539214962544, test loss: 0.2588351148934713\n",
      "epoch 18675: train loss: 0.10514391920271676, test loss: 0.2588361764682255\n",
      "epoch 18676: train loss: 0.1051424463611493, test loss: 0.2588372380811107\n",
      "epoch 18677: train loss: 0.10514097362490933, test loss: 0.25883829973213196\n",
      "epoch 18678: train loss: 0.10513950099398311, test loss: 0.25883936142124453\n",
      "epoch 18679: train loss: 0.10513802846835694, test loss: 0.2588404231484363\n",
      "epoch 18680: train loss: 0.10513655604801704, test loss: 0.2588414849137521\n",
      "epoch 18681: train loss: 0.1051350837329497, test loss: 0.2588425467171419\n",
      "epoch 18682: train loss: 0.1051336115231412, test loss: 0.25884360855859895\n",
      "epoch 18683: train loss: 0.10513213941857781, test loss: 0.25884467043816267\n",
      "epoch 18684: train loss: 0.1051306674192458, test loss: 0.25884573235577063\n",
      "epoch 18685: train loss: 0.10512919552513146, test loss: 0.25884679431141205\n",
      "epoch 18686: train loss: 0.10512772373622109, test loss: 0.2588478563051111\n",
      "epoch 18687: train loss: 0.10512625205250097, test loss: 0.25884891833687435\n",
      "epoch 18688: train loss: 0.10512478047395733, test loss: 0.25884998040662\n",
      "epoch 18689: train loss: 0.10512330900057654, test loss: 0.2588510425144109\n",
      "epoch 18690: train loss: 0.10512183763234485, test loss: 0.25885210466021563\n",
      "epoch 18691: train loss: 0.10512036636924858, test loss: 0.25885316684402404\n",
      "epoch 18692: train loss: 0.105118895211274, test loss: 0.2588542290658242\n",
      "epoch 18693: train loss: 0.10511742415840747, test loss: 0.2588552913256622\n",
      "epoch 18694: train loss: 0.10511595321063524, test loss: 0.25885635362343656\n",
      "epoch 18695: train loss: 0.10511448236794362, test loss: 0.25885741595920736\n",
      "epoch 18696: train loss: 0.10511301163031893, test loss: 0.25885847833292813\n",
      "epoch 18697: train loss: 0.1051115409977475, test loss: 0.25885954074462453\n",
      "epoch 18698: train loss: 0.10511007047021562, test loss: 0.258860603194266\n",
      "epoch 18699: train loss: 0.1051086000477096, test loss: 0.258861665681863\n",
      "epoch 18700: train loss: 0.10510712973021578, test loss: 0.2588627282073504\n",
      "epoch 18701: train loss: 0.10510565951772048, test loss: 0.2588637907708044\n",
      "epoch 18702: train loss: 0.10510418941021, test loss: 0.2588648533721625\n",
      "epoch 18703: train loss: 0.10510271940767073, test loss: 0.2588659160114506\n",
      "epoch 18704: train loss: 0.10510124951008891, test loss: 0.25886697868865716\n",
      "epoch 18705: train loss: 0.10509977971745094, test loss: 0.25886804140373515\n",
      "epoch 18706: train loss: 0.10509831002974314, test loss: 0.2588691041566917\n",
      "epoch 18707: train loss: 0.10509684044695185, test loss: 0.258870166947534\n",
      "epoch 18708: train loss: 0.10509537096906339, test loss: 0.25887122977625354\n",
      "epoch 18709: train loss: 0.10509390159606409, test loss: 0.2588722926428375\n",
      "epoch 18710: train loss: 0.10509243232794034, test loss: 0.2588733555472767\n",
      "epoch 18711: train loss: 0.10509096316467849, test loss: 0.25887441848961346\n",
      "epoch 18712: train loss: 0.10508949410626485, test loss: 0.25887548146974615\n",
      "epoch 18713: train loss: 0.10508802515268577, test loss: 0.25887654448770275\n",
      "epoch 18714: train loss: 0.10508655630392764, test loss: 0.2588776075435064\n",
      "epoch 18715: train loss: 0.10508508755997681, test loss: 0.2588786706371107\n",
      "epoch 18716: train loss: 0.1050836189208196, test loss: 0.2588797337685248\n",
      "epoch 18717: train loss: 0.10508215038644245, test loss: 0.2588807969377727\n",
      "epoch 18718: train loss: 0.10508068195683165, test loss: 0.25888186014477205\n",
      "epoch 18719: train loss: 0.10507921363197362, test loss: 0.25888292338956537\n",
      "epoch 18720: train loss: 0.10507774541185472, test loss: 0.2588839866721607\n",
      "epoch 18721: train loss: 0.10507627729646128, test loss: 0.25888504999251283\n",
      "epoch 18722: train loss: 0.10507480928577972, test loss: 0.2588861133506451\n",
      "epoch 18723: train loss: 0.1050733413797964, test loss: 0.258887176746531\n",
      "epoch 18724: train loss: 0.10507187357849773, test loss: 0.25888824018015855\n",
      "epoch 18725: train loss: 0.10507040588187003, test loss: 0.2588893036515165\n",
      "epoch 18726: train loss: 0.10506893828989977, test loss: 0.25889036716061364\n",
      "epoch 18727: train loss: 0.10506747080257331, test loss: 0.25889143070740345\n",
      "epoch 18728: train loss: 0.10506600341987696, test loss: 0.25889249429194594\n",
      "epoch 18729: train loss: 0.10506453614179721, test loss: 0.2588935579142139\n",
      "epoch 18730: train loss: 0.10506306896832043, test loss: 0.2588946215741255\n",
      "epoch 18731: train loss: 0.10506160189943302, test loss: 0.2588956852717948\n",
      "epoch 18732: train loss: 0.10506013493512135, test loss: 0.25889674900710324\n",
      "epoch 18733: train loss: 0.10505866807537187, test loss: 0.2588978127801124\n",
      "epoch 18734: train loss: 0.10505720132017096, test loss: 0.2588988765907597\n",
      "epoch 18735: train loss: 0.10505573466950503, test loss: 0.2588999404391039\n",
      "epoch 18736: train loss: 0.1050542681233605, test loss: 0.2589010043250813\n",
      "epoch 18737: train loss: 0.10505280168172376, test loss: 0.25890206824868256\n",
      "epoch 18738: train loss: 0.10505133534458125, test loss: 0.25890313220996797\n",
      "epoch 18739: train loss: 0.10504986911191938, test loss: 0.25890419620887356\n",
      "epoch 18740: train loss: 0.1050484029837246, test loss: 0.258905260245389\n",
      "epoch 18741: train loss: 0.1050469369599833, test loss: 0.2589063243195044\n",
      "epoch 18742: train loss: 0.10504547104068189, test loss: 0.258907388431262\n",
      "epoch 18743: train loss: 0.10504400522580683, test loss: 0.2589084525805792\n",
      "epoch 18744: train loss: 0.10504253951534455, test loss: 0.2589095167675197\n",
      "epoch 18745: train loss: 0.10504107390928148, test loss: 0.25891058099201564\n",
      "epoch 18746: train loss: 0.10503960840760407, test loss: 0.25891164525409666\n",
      "epoch 18747: train loss: 0.10503814301029869, test loss: 0.2589127095537494\n",
      "epoch 18748: train loss: 0.10503667771735188, test loss: 0.25891377389098236\n",
      "epoch 18749: train loss: 0.10503521252875003, test loss: 0.2589148382657303\n",
      "epoch 18750: train loss: 0.1050337474444796, test loss: 0.2589159026780569\n",
      "epoch 18751: train loss: 0.10503228246452703, test loss: 0.258916967127913\n",
      "epoch 18752: train loss: 0.10503081758887879, test loss: 0.2589180316152541\n",
      "epoch 18753: train loss: 0.1050293528175213, test loss: 0.25891909614019737\n",
      "epoch 18754: train loss: 0.10502788815044106, test loss: 0.2589201607026029\n",
      "epoch 18755: train loss: 0.10502642358762448, test loss: 0.25892122530251555\n",
      "epoch 18756: train loss: 0.10502495912905807, test loss: 0.258922289939889\n",
      "epoch 18757: train loss: 0.10502349477472826, test loss: 0.2589233546148045\n",
      "epoch 18758: train loss: 0.10502203052462153, test loss: 0.25892441932719457\n",
      "epoch 18759: train loss: 0.10502056637872435, test loss: 0.2589254840770524\n",
      "epoch 18760: train loss: 0.1050191023370232, test loss: 0.25892654886436495\n",
      "epoch 18761: train loss: 0.10501763839950452, test loss: 0.25892761368917727\n",
      "epoch 18762: train loss: 0.10501617456615485, test loss: 0.2589286785514242\n",
      "epoch 18763: train loss: 0.10501471083696061, test loss: 0.25892974345109665\n",
      "epoch 18764: train loss: 0.1050132472119083, test loss: 0.2589308083882005\n",
      "epoch 18765: train loss: 0.10501178369098442, test loss: 0.25893187336274465\n",
      "epoch 18766: train loss: 0.10501032027417542, test loss: 0.25893293837470144\n",
      "epoch 18767: train loss: 0.10500885696146785, test loss: 0.2589340034240946\n",
      "epoch 18768: train loss: 0.10500739375284814, test loss: 0.25893506851084375\n",
      "epoch 18769: train loss: 0.1050059306483028, test loss: 0.2589361336350086\n",
      "epoch 18770: train loss: 0.10500446764781836, test loss: 0.25893719879659843\n",
      "epoch 18771: train loss: 0.10500300475138129, test loss: 0.2589382639955492\n",
      "epoch 18772: train loss: 0.1050015419589781, test loss: 0.2589393292318501\n",
      "epoch 18773: train loss: 0.10500007927059532, test loss: 0.2589403945055618\n",
      "epoch 18774: train loss: 0.10499861668621939, test loss: 0.2589414598165674\n",
      "epoch 18775: train loss: 0.10499715420583686, test loss: 0.2589425251649827\n",
      "epoch 18776: train loss: 0.10499569182943425, test loss: 0.25894359055068883\n",
      "epoch 18777: train loss: 0.1049942295569981, test loss: 0.25894465597378474\n",
      "epoch 18778: train loss: 0.10499276738851485, test loss: 0.25894572143416705\n",
      "epoch 18779: train loss: 0.10499130532397107, test loss: 0.2589467869318663\n",
      "epoch 18780: train loss: 0.10498984336335329, test loss: 0.2589478524669219\n",
      "epoch 18781: train loss: 0.10498838150664801, test loss: 0.25894891803925335\n",
      "epoch 18782: train loss: 0.10498691975384178, test loss: 0.25894998364885075\n",
      "epoch 18783: train loss: 0.1049854581049211, test loss: 0.2589510492957954\n",
      "epoch 18784: train loss: 0.10498399655987253, test loss: 0.25895211497996556\n",
      "epoch 18785: train loss: 0.10498253511868258, test loss: 0.25895318070142487\n",
      "epoch 18786: train loss: 0.10498107378133784, test loss: 0.25895424646018317\n",
      "epoch 18787: train loss: 0.10497961254782474, test loss: 0.2589553122561538\n",
      "epoch 18788: train loss: 0.10497815141812997, test loss: 0.2589563780894031\n",
      "epoch 18789: train loss: 0.10497669039223995, test loss: 0.2589574439598624\n",
      "epoch 18790: train loss: 0.10497522947014129, test loss: 0.25895850986756186\n",
      "epoch 18791: train loss: 0.10497376865182051, test loss: 0.25895957581252543\n",
      "epoch 18792: train loss: 0.10497230793726418, test loss: 0.2589606417946887\n",
      "epoch 18793: train loss: 0.10497084732645884, test loss: 0.25896170781406075\n",
      "epoch 18794: train loss: 0.10496938681939108, test loss: 0.25896277387062894\n",
      "epoch 18795: train loss: 0.10496792641604742, test loss: 0.25896383996436756\n",
      "epoch 18796: train loss: 0.10496646611641447, test loss: 0.25896490609535716\n",
      "epoch 18797: train loss: 0.10496500592047874, test loss: 0.2589659722634942\n",
      "epoch 18798: train loss: 0.10496354582822683, test loss: 0.25896703846880753\n",
      "epoch 18799: train loss: 0.10496208583964528, test loss: 0.25896810471124865\n",
      "epoch 18800: train loss: 0.10496062595472068, test loss: 0.2589691709908827\n",
      "epoch 18801: train loss: 0.10495916617343966, test loss: 0.2589702373076795\n",
      "epoch 18802: train loss: 0.10495770649578869, test loss: 0.25897130366161064\n",
      "epoch 18803: train loss: 0.10495624692175444, test loss: 0.25897237005263046\n",
      "epoch 18804: train loss: 0.10495478745132346, test loss: 0.25897343648083776\n",
      "epoch 18805: train loss: 0.10495332808448228, test loss: 0.25897450294611346\n",
      "epoch 18806: train loss: 0.10495186882121757, test loss: 0.2589755694485195\n",
      "epoch 18807: train loss: 0.1049504096615159, test loss: 0.25897663598802895\n",
      "epoch 18808: train loss: 0.10494895060536386, test loss: 0.25897770256464847\n",
      "epoch 18809: train loss: 0.10494749165274801, test loss: 0.25897876917833235\n",
      "epoch 18810: train loss: 0.10494603280365497, test loss: 0.2589798358291083\n",
      "epoch 18811: train loss: 0.10494457405807134, test loss: 0.25898090251692707\n",
      "epoch 18812: train loss: 0.10494311541598372, test loss: 0.2589819692418534\n",
      "epoch 18813: train loss: 0.10494165687737875, test loss: 0.25898303600382133\n",
      "epoch 18814: train loss: 0.10494019844224298, test loss: 0.2589841028028411\n",
      "epoch 18815: train loss: 0.10493874011056306, test loss: 0.25898516963890045\n",
      "epoch 18816: train loss: 0.10493728188232555, test loss: 0.25898623651198954\n",
      "epoch 18817: train loss: 0.10493582375751714, test loss: 0.25898730342210097\n",
      "epoch 18818: train loss: 0.10493436573612439, test loss: 0.2589883703692395\n",
      "epoch 18819: train loss: 0.10493290781813394, test loss: 0.2589894373533608\n",
      "epoch 18820: train loss: 0.1049314500035324, test loss: 0.2589905043745268\n",
      "epoch 18821: train loss: 0.10492999229230643, test loss: 0.2589915714326731\n",
      "epoch 18822: train loss: 0.10492853468444262, test loss: 0.25899263852779036\n",
      "epoch 18823: train loss: 0.10492707717992758, test loss: 0.25899370565992236\n",
      "epoch 18824: train loss: 0.104925619778748, test loss: 0.2589947728289877\n",
      "epoch 18825: train loss: 0.10492416248089048, test loss: 0.25899584003504733\n",
      "epoch 18826: train loss: 0.10492270528634168, test loss: 0.2589969072780554\n",
      "epoch 18827: train loss: 0.1049212481950882, test loss: 0.25899797455802154\n",
      "epoch 18828: train loss: 0.10491979120711671, test loss: 0.2589990418749161\n",
      "epoch 18829: train loss: 0.10491833432241386, test loss: 0.2590001092287105\n",
      "epoch 18830: train loss: 0.10491687754096626, test loss: 0.25900117661947075\n",
      "epoch 18831: train loss: 0.10491542086276061, test loss: 0.25900224404716404\n",
      "epoch 18832: train loss: 0.10491396428778352, test loss: 0.25900331151173056\n",
      "epoch 18833: train loss: 0.10491250781602166, test loss: 0.2590043790132285\n",
      "epoch 18834: train loss: 0.1049110514474617, test loss: 0.25900544655161556\n",
      "epoch 18835: train loss: 0.10490959518209025, test loss: 0.2590065141268986\n",
      "epoch 18836: train loss: 0.10490813901989406, test loss: 0.2590075817390306\n",
      "epoch 18837: train loss: 0.1049066829608597, test loss: 0.259008649388076\n",
      "epoch 18838: train loss: 0.1049052270049739, test loss: 0.2590097170739499\n",
      "epoch 18839: train loss: 0.10490377115222332, test loss: 0.25901078479668077\n",
      "epoch 18840: train loss: 0.10490231540259458, test loss: 0.2590118525562764\n",
      "epoch 18841: train loss: 0.10490085975607442, test loss: 0.25901292035274587\n",
      "epoch 18842: train loss: 0.10489940421264951, test loss: 0.25901398818600346\n",
      "epoch 18843: train loss: 0.10489794877230647, test loss: 0.2590150560561165\n",
      "epoch 18844: train loss: 0.10489649343503203, test loss: 0.2590161239630169\n",
      "epoch 18845: train loss: 0.10489503820081286, test loss: 0.2590171919067698\n",
      "epoch 18846: train loss: 0.10489358306963564, test loss: 0.2590182598872916\n",
      "epoch 18847: train loss: 0.10489212804148709, test loss: 0.25901932790460913\n",
      "epoch 18848: train loss: 0.10489067311635386, test loss: 0.259020395958749\n",
      "epoch 18849: train loss: 0.10488921829422267, test loss: 0.2590214640496471\n",
      "epoch 18850: train loss: 0.10488776357508023, test loss: 0.2590225321773114\n",
      "epoch 18851: train loss: 0.1048863089589132, test loss: 0.25902360034176847\n",
      "epoch 18852: train loss: 0.1048848544457083, test loss: 0.25902466854293543\n",
      "epoch 18853: train loss: 0.10488340003545224, test loss: 0.25902573678087626\n",
      "epoch 18854: train loss: 0.1048819457281317, test loss: 0.2590268050555994\n",
      "epoch 18855: train loss: 0.10488049152373342, test loss: 0.25902787336702054\n",
      "epoch 18856: train loss: 0.10487903742224411, test loss: 0.2590289417151672\n",
      "epoch 18857: train loss: 0.10487758342365046, test loss: 0.25903001010004917\n",
      "epoch 18858: train loss: 0.1048761295279392, test loss: 0.2590310785216187\n",
      "epoch 18859: train loss: 0.10487467573509704, test loss: 0.2590321469799026\n",
      "epoch 18860: train loss: 0.1048732220451107, test loss: 0.2590332154749095\n",
      "epoch 18861: train loss: 0.10487176845796693, test loss: 0.2590342840065569\n",
      "epoch 18862: train loss: 0.10487031497365244, test loss: 0.25903535257494503\n",
      "epoch 18863: train loss: 0.10486886159215394, test loss: 0.2590364211799719\n",
      "epoch 18864: train loss: 0.10486740831345816, test loss: 0.2590374898216656\n",
      "epoch 18865: train loss: 0.10486595513755191, test loss: 0.2590385585000144\n",
      "epoch 18866: train loss: 0.10486450206442179, test loss: 0.259039627215048\n",
      "epoch 18867: train loss: 0.10486304909405465, test loss: 0.2590406959666989\n",
      "epoch 18868: train loss: 0.10486159622643719, test loss: 0.25904176475495866\n",
      "epoch 18869: train loss: 0.10486014346155613, test loss: 0.25904283357990865\n",
      "epoch 18870: train loss: 0.10485869079939829, test loss: 0.2590439024414318\n",
      "epoch 18871: train loss: 0.1048572382399503, test loss: 0.2590449713395883\n",
      "epoch 18872: train loss: 0.10485578578319903, test loss: 0.25904604027435085\n",
      "epoch 18873: train loss: 0.10485433342913116, test loss: 0.25904710924569285\n",
      "epoch 18874: train loss: 0.10485288117773348, test loss: 0.25904817825365806\n",
      "epoch 18875: train loss: 0.10485142902899273, test loss: 0.25904924729818257\n",
      "epoch 18876: train loss: 0.10484997698289567, test loss: 0.2590503163792933\n",
      "epoch 18877: train loss: 0.10484852503942906, test loss: 0.2590513854969821\n",
      "epoch 18878: train loss: 0.10484707319857968, test loss: 0.259052454651199\n",
      "epoch 18879: train loss: 0.1048456214603343, test loss: 0.25905352384199354\n",
      "epoch 18880: train loss: 0.10484416982467967, test loss: 0.25905459306931666\n",
      "epoch 18881: train loss: 0.10484271829160256, test loss: 0.25905566233319605\n",
      "epoch 18882: train loss: 0.10484126686108977, test loss: 0.25905673163358467\n",
      "epoch 18883: train loss: 0.10483981553312809, test loss: 0.25905780097053066\n",
      "epoch 18884: train loss: 0.10483836430770424, test loss: 0.2590588703439667\n",
      "epoch 18885: train loss: 0.10483691318480502, test loss: 0.259059939753902\n",
      "epoch 18886: train loss: 0.10483546216441725, test loss: 0.2590610092003285\n",
      "epoch 18887: train loss: 0.10483401124652769, test loss: 0.2590620786832513\n",
      "epoch 18888: train loss: 0.10483256043112316, test loss: 0.2590631482026841\n",
      "epoch 18889: train loss: 0.10483110971819042, test loss: 0.25906421775857585\n",
      "epoch 18890: train loss: 0.10482965910771626, test loss: 0.2590652873509576\n",
      "epoch 18891: train loss: 0.10482820859968747, test loss: 0.2590663569798\n",
      "epoch 18892: train loss: 0.10482675819409092, test loss: 0.2590674266450909\n",
      "epoch 18893: train loss: 0.10482530789091334, test loss: 0.2590684963468254\n",
      "epoch 18894: train loss: 0.10482385769014155, test loss: 0.25906956608495435\n",
      "epoch 18895: train loss: 0.10482240759176235, test loss: 0.25907063585957796\n",
      "epoch 18896: train loss: 0.10482095759576257, test loss: 0.2590717056705967\n",
      "epoch 18897: train loss: 0.10481950770212901, test loss: 0.25907277551803565\n",
      "epoch 18898: train loss: 0.10481805791084849, test loss: 0.2590738454018874\n",
      "epoch 18899: train loss: 0.10481660822190783, test loss: 0.25907491532212396\n",
      "epoch 18900: train loss: 0.10481515863529381, test loss: 0.259075985278789\n",
      "epoch 18901: train loss: 0.10481370915099332, test loss: 0.2590770552718007\n",
      "epoch 18902: train loss: 0.10481225976899314, test loss: 0.25907812530120533\n",
      "epoch 18903: train loss: 0.10481081048928008, test loss: 0.25907919536700996\n",
      "epoch 18904: train loss: 0.10480936131184099, test loss: 0.2590802654691516\n",
      "epoch 18905: train loss: 0.10480791223666273, test loss: 0.25908133560763863\n",
      "epoch 18906: train loss: 0.10480646326373208, test loss: 0.2590824057824806\n",
      "epoch 18907: train loss: 0.10480501439303591, test loss: 0.2590834759936845\n",
      "epoch 18908: train loss: 0.10480356562456106, test loss: 0.2590845462411867\n",
      "epoch 18909: train loss: 0.10480211695829432, test loss: 0.2590856165250522\n",
      "epoch 18910: train loss: 0.1048006683942226, test loss: 0.2590866868452156\n",
      "epoch 18911: train loss: 0.10479921993233272, test loss: 0.2590877572016864\n",
      "epoch 18912: train loss: 0.10479777157261153, test loss: 0.2590888275944908\n",
      "epoch 18913: train loss: 0.10479632331504586, test loss: 0.2590898980235455\n",
      "epoch 18914: train loss: 0.10479487515962259, test loss: 0.2590909684889174\n",
      "epoch 18915: train loss: 0.10479342710632857, test loss: 0.25909203899057537\n",
      "epoch 18916: train loss: 0.10479197915515062, test loss: 0.25909310952847514\n",
      "epoch 18917: train loss: 0.10479053130607566, test loss: 0.25909418010268015\n",
      "epoch 18918: train loss: 0.10478908355909052, test loss: 0.2590952507131265\n",
      "epoch 18919: train loss: 0.10478763591418208, test loss: 0.25909632135980326\n",
      "epoch 18920: train loss: 0.10478618837133716, test loss: 0.2590973920427401\n",
      "epoch 18921: train loss: 0.10478474093054273, test loss: 0.2590984627618891\n",
      "epoch 18922: train loss: 0.10478329359178556, test loss: 0.2590995335172964\n",
      "epoch 18923: train loss: 0.10478184635505255, test loss: 0.25910060430891635\n",
      "epoch 18924: train loss: 0.10478039922033061, test loss: 0.25910167513677507\n",
      "epoch 18925: train loss: 0.10477895218760659, test loss: 0.259102746000809\n",
      "epoch 18926: train loss: 0.10477750525686741, test loss: 0.25910381690104656\n",
      "epoch 18927: train loss: 0.1047760584280999, test loss: 0.2591048878374769\n",
      "epoch 18928: train loss: 0.10477461170129096, test loss: 0.25910595881007226\n",
      "epoch 18929: train loss: 0.1047731650764275, test loss: 0.25910702981886\n",
      "epoch 18930: train loss: 0.10477171855349644, test loss: 0.259108100863833\n",
      "epoch 18931: train loss: 0.10477027213248462, test loss: 0.2591091719449052\n",
      "epoch 18932: train loss: 0.10476882581337893, test loss: 0.25911024306218017\n",
      "epoch 18933: train loss: 0.10476737959616633, test loss: 0.25911131421559097\n",
      "epoch 18934: train loss: 0.10476593348083366, test loss: 0.25911238540514964\n",
      "epoch 18935: train loss: 0.10476448746736786, test loss: 0.25911345663084395\n",
      "epoch 18936: train loss: 0.10476304155575582, test loss: 0.25911452789264755\n",
      "epoch 18937: train loss: 0.10476159574598444, test loss: 0.25911559919057026\n",
      "epoch 18938: train loss: 0.10476015003804068, test loss: 0.25911667052458354\n",
      "epoch 18939: train loss: 0.10475870443191138, test loss: 0.2591177418946953\n",
      "epoch 18940: train loss: 0.10475725892758352, test loss: 0.2591188133009166\n",
      "epoch 18941: train loss: 0.10475581352504396, test loss: 0.25911988474325676\n",
      "epoch 18942: train loss: 0.10475436822427968, test loss: 0.2591209562216126\n",
      "epoch 18943: train loss: 0.10475292302527756, test loss: 0.25912202773606874\n",
      "epoch 18944: train loss: 0.10475147792802454, test loss: 0.2591230992865774\n",
      "epoch 18945: train loss: 0.10475003293250756, test loss: 0.2591241708731678\n",
      "epoch 18946: train loss: 0.10474858803871352, test loss: 0.2591252424957744\n",
      "epoch 18947: train loss: 0.10474714324662941, test loss: 0.25912631415446136\n",
      "epoch 18948: train loss: 0.1047456985562421, test loss: 0.2591273858491473\n",
      "epoch 18949: train loss: 0.10474425396753854, test loss: 0.259128457579858\n",
      "epoch 18950: train loss: 0.1047428094805057, test loss: 0.2591295293466034\n",
      "epoch 18951: train loss: 0.1047413650951305, test loss: 0.2591306011493577\n",
      "epoch 18952: train loss: 0.10473992081139989, test loss: 0.25913167298810846\n",
      "epoch 18953: train loss: 0.10473847662930083, test loss: 0.2591327448628298\n",
      "epoch 18954: train loss: 0.10473703254882025, test loss: 0.259133816773568\n",
      "epoch 18955: train loss: 0.10473558856994511, test loss: 0.259134888720275\n",
      "epoch 18956: train loss: 0.10473414469266235, test loss: 0.2591359607029819\n",
      "epoch 18957: train loss: 0.10473270091695895, test loss: 0.2591370327216211\n",
      "epoch 18958: train loss: 0.10473125724282188, test loss: 0.2591381047762603\n",
      "epoch 18959: train loss: 0.10472981367023804, test loss: 0.2591391768668314\n",
      "epoch 18960: train loss: 0.10472837019919445, test loss: 0.2591402489933271\n",
      "epoch 18961: train loss: 0.10472692682967807, test loss: 0.25914132115577465\n",
      "epoch 18962: train loss: 0.10472548356167585, test loss: 0.2591423933541299\n",
      "epoch 18963: train loss: 0.1047240403951748, test loss: 0.25914346558841794\n",
      "epoch 18964: train loss: 0.10472259733016184, test loss: 0.25914453785864894\n",
      "epoch 18965: train loss: 0.10472115436662398, test loss: 0.25914561016474147\n",
      "epoch 18966: train loss: 0.10471971150454816, test loss: 0.2591466825067586\n",
      "epoch 18967: train loss: 0.10471826874392143, test loss: 0.2591477548846719\n",
      "epoch 18968: train loss: 0.10471682608473071, test loss: 0.25914882729847505\n",
      "epoch 18969: train loss: 0.10471538352696301, test loss: 0.25914989974811964\n",
      "epoch 18970: train loss: 0.10471394107060529, test loss: 0.25915097223363504\n",
      "epoch 18971: train loss: 0.10471249871564461, test loss: 0.2591520447550658\n",
      "epoch 18972: train loss: 0.10471105646206791, test loss: 0.25915311731229346\n",
      "epoch 18973: train loss: 0.10470961430986218, test loss: 0.2591541899053824\n",
      "epoch 18974: train loss: 0.10470817225901444, test loss: 0.2591552625343422\n",
      "epoch 18975: train loss: 0.10470673030951168, test loss: 0.2591563351990901\n",
      "epoch 18976: train loss: 0.1047052884613409, test loss: 0.25915740789968844\n",
      "epoch 18977: train loss: 0.10470384671448912, test loss: 0.25915848063609526\n",
      "epoch 18978: train loss: 0.10470240506894334, test loss: 0.25915955340831665\n",
      "epoch 18979: train loss: 0.10470096352469054, test loss: 0.2591606262163461\n",
      "epoch 18980: train loss: 0.10469952208171777, test loss: 0.2591616990601361\n",
      "epoch 18981: train loss: 0.10469808074001204, test loss: 0.25916277193975257\n",
      "epoch 18982: train loss: 0.10469663949956035, test loss: 0.25916384485511\n",
      "epoch 18983: train loss: 0.10469519836034971, test loss: 0.259164917806258\n",
      "epoch 18984: train loss: 0.10469375732236717, test loss: 0.2591659907932039\n",
      "epoch 18985: train loss: 0.10469231638559975, test loss: 0.2591670638158657\n",
      "epoch 18986: train loss: 0.10469087555003448, test loss: 0.2591681368742894\n",
      "epoch 18987: train loss: 0.10468943481565834, test loss: 0.2591692099684482\n",
      "epoch 18988: train loss: 0.10468799418245844, test loss: 0.25917028309835155\n",
      "epoch 18989: train loss: 0.10468655365042175, test loss: 0.2591713562639705\n",
      "epoch 18990: train loss: 0.10468511321953532, test loss: 0.25917242946533525\n",
      "epoch 18991: train loss: 0.1046836728897862, test loss: 0.25917350270237965\n",
      "epoch 18992: train loss: 0.10468223266116145, test loss: 0.2591745759751877\n",
      "epoch 18993: train loss: 0.10468079253364804, test loss: 0.25917564928363945\n",
      "epoch 18994: train loss: 0.10467935250723309, test loss: 0.2591767226277997\n",
      "epoch 18995: train loss: 0.10467791258190363, test loss: 0.2591777960076236\n",
      "epoch 18996: train loss: 0.10467647275764669, test loss: 0.2591788694231379\n",
      "epoch 18997: train loss: 0.10467503303444936, test loss: 0.2591799428743524\n",
      "epoch 18998: train loss: 0.10467359341229862, test loss: 0.25918101636118496\n",
      "epoch 18999: train loss: 0.10467215389118162, test loss: 0.25918208988369906\n",
      "epoch 19000: train loss: 0.10467071447108536, test loss: 0.2591831634418325\n",
      "epoch 19001: train loss: 0.10466927515199691, test loss: 0.259184237035611\n",
      "epoch 19002: train loss: 0.10466783593390336, test loss: 0.2591853106650269\n",
      "epoch 19003: train loss: 0.10466639681679173, test loss: 0.2591863843300713\n",
      "epoch 19004: train loss: 0.10466495780064912, test loss: 0.25918745803073734\n",
      "epoch 19005: train loss: 0.1046635188854626, test loss: 0.2591885317669933\n",
      "epoch 19006: train loss: 0.10466208007121929, test loss: 0.25918960553886955\n",
      "epoch 19007: train loss: 0.10466064135790618, test loss: 0.2591906793463199\n",
      "epoch 19008: train loss: 0.10465920274551038, test loss: 0.2591917531893914\n",
      "epoch 19009: train loss: 0.10465776423401899, test loss: 0.2591928270680379\n",
      "epoch 19010: train loss: 0.10465632582341908, test loss: 0.2591939009822145\n",
      "epoch 19011: train loss: 0.10465488751369774, test loss: 0.2591949749319843\n",
      "epoch 19012: train loss: 0.10465344930484206, test loss: 0.2591960489173209\n",
      "epoch 19013: train loss: 0.10465201119683912, test loss: 0.2591971229381796\n",
      "epoch 19014: train loss: 0.10465057318967602, test loss: 0.2591981969946242\n",
      "epoch 19015: train loss: 0.10464913528333984, test loss: 0.2591992710865911\n",
      "epoch 19016: train loss: 0.10464769747781773, test loss: 0.25920034521405133\n",
      "epoch 19017: train loss: 0.10464625977309672, test loss: 0.2592014193770542\n",
      "epoch 19018: train loss: 0.10464482216916395, test loss: 0.25920249357560676\n",
      "epoch 19019: train loss: 0.1046433846660065, test loss: 0.2592035678096095\n",
      "epoch 19020: train loss: 0.10464194726361153, test loss: 0.2592046420791284\n",
      "epoch 19021: train loss: 0.1046405099619661, test loss: 0.25920571638413237\n",
      "epoch 19022: train loss: 0.10463907276105734, test loss: 0.2592067907246161\n",
      "epoch 19023: train loss: 0.10463763566087236, test loss: 0.25920786510060806\n",
      "epoch 19024: train loss: 0.10463619866139826, test loss: 0.25920893951204077\n",
      "epoch 19025: train loss: 0.1046347617626222, test loss: 0.25921001395894466\n",
      "epoch 19026: train loss: 0.10463332496453127, test loss: 0.25921108844131174\n",
      "epoch 19027: train loss: 0.1046318882671126, test loss: 0.2592121629591506\n",
      "epoch 19028: train loss: 0.10463045167035333, test loss: 0.25921323751237674\n",
      "epoch 19029: train loss: 0.10462901517424054, test loss: 0.2592143121010584\n",
      "epoch 19030: train loss: 0.10462757877876143, test loss: 0.25921538672514927\n",
      "epoch 19031: train loss: 0.10462614248390309, test loss: 0.25921646138467674\n",
      "epoch 19032: train loss: 0.10462470628965267, test loss: 0.259217536079614\n",
      "epoch 19033: train loss: 0.1046232701959973, test loss: 0.25921861080995084\n",
      "epoch 19034: train loss: 0.10462183420292412, test loss: 0.2592196855756806\n",
      "epoch 19035: train loss: 0.10462039831042028, test loss: 0.25922076037681185\n",
      "epoch 19036: train loss: 0.10461896251847289, test loss: 0.2592218352132997\n",
      "epoch 19037: train loss: 0.10461752682706915, test loss: 0.25922291008517234\n",
      "epoch 19038: train loss: 0.10461609123619621, test loss: 0.25922398499241955\n",
      "epoch 19039: train loss: 0.10461465574584118, test loss: 0.2592250599350361\n",
      "epoch 19040: train loss: 0.10461322035599122, test loss: 0.25922613491299024\n",
      "epoch 19041: train loss: 0.10461178506663352, test loss: 0.2592272099262769\n",
      "epoch 19042: train loss: 0.1046103498777552, test loss: 0.2592282849748858\n",
      "epoch 19043: train loss: 0.10460891478934344, test loss: 0.25922936005884517\n",
      "epoch 19044: train loss: 0.10460747980138542, test loss: 0.25923043517814764\n",
      "epoch 19045: train loss: 0.10460604491386827, test loss: 0.2592315103327275\n",
      "epoch 19046: train loss: 0.1046046101267792, test loss: 0.25923258552267003\n",
      "epoch 19047: train loss: 0.10460317544010532, test loss: 0.2592336607478531\n",
      "epoch 19048: train loss: 0.10460174085383389, test loss: 0.25923473600836394\n",
      "epoch 19049: train loss: 0.104600306367952, test loss: 0.25923581130413487\n",
      "epoch 19050: train loss: 0.1045988719824469, test loss: 0.2592368866351982\n",
      "epoch 19051: train loss: 0.10459743769730573, test loss: 0.2592379620015426\n",
      "epoch 19052: train loss: 0.10459600351251565, test loss: 0.2592390374031418\n",
      "epoch 19053: train loss: 0.10459456942806387, test loss: 0.25924011283998594\n",
      "epoch 19054: train loss: 0.10459313544393761, test loss: 0.25924118831210596\n",
      "epoch 19055: train loss: 0.10459170156012404, test loss: 0.25924226381945353\n",
      "epoch 19056: train loss: 0.1045902677766103, test loss: 0.25924333936204097\n",
      "epoch 19057: train loss: 0.10458883409338365, test loss: 0.2592444149398206\n",
      "epoch 19058: train loss: 0.10458740051043126, test loss: 0.259245490552859\n",
      "epoch 19059: train loss: 0.10458596702774034, test loss: 0.25924656620111064\n",
      "epoch 19060: train loss: 0.10458453364529807, test loss: 0.2592476418845287\n",
      "epoch 19061: train loss: 0.10458310036309168, test loss: 0.2592487176031985\n",
      "epoch 19062: train loss: 0.10458166718110835, test loss: 0.2592497933570377\n",
      "epoch 19063: train loss: 0.10458023409933531, test loss: 0.25925086914603684\n",
      "epoch 19064: train loss: 0.10457880111775975, test loss: 0.25925194497024245\n",
      "epoch 19065: train loss: 0.10457736823636891, test loss: 0.25925302082961127\n",
      "epoch 19066: train loss: 0.10457593545515, test loss: 0.259254096724131\n",
      "epoch 19067: train loss: 0.10457450277409017, test loss: 0.259255172653796\n",
      "epoch 19068: train loss: 0.10457307019317674, test loss: 0.25925624861863594\n",
      "epoch 19069: train loss: 0.1045716377123969, test loss: 0.2592573246185818\n",
      "epoch 19070: train loss: 0.10457020533173786, test loss: 0.2592584006536673\n",
      "epoch 19071: train loss: 0.10456877305118685, test loss: 0.2592594767239182\n",
      "epoch 19072: train loss: 0.1045673408707311, test loss: 0.2592605528292709\n",
      "epoch 19073: train loss: 0.10456590879035783, test loss: 0.25926162896971816\n",
      "epoch 19074: train loss: 0.10456447681005429, test loss: 0.2592627051452306\n",
      "epoch 19075: train loss: 0.10456304492980772, test loss: 0.25926378135589323\n",
      "epoch 19076: train loss: 0.10456161314960535, test loss: 0.25926485760164364\n",
      "epoch 19077: train loss: 0.10456018146943442, test loss: 0.2592659338824525\n",
      "epoch 19078: train loss: 0.1045587498892822, test loss: 0.25926701019833215\n",
      "epoch 19079: train loss: 0.1045573184091359, test loss: 0.25926808654927147\n",
      "epoch 19080: train loss: 0.10455588702898275, test loss: 0.2592691629353016\n",
      "epoch 19081: train loss: 0.10455445574881007, test loss: 0.25927023935639454\n",
      "epoch 19082: train loss: 0.10455302456860506, test loss: 0.25927131581250545\n",
      "epoch 19083: train loss: 0.10455159348835501, test loss: 0.2592723923036609\n",
      "epoch 19084: train loss: 0.10455016250804712, test loss: 0.2592734688298544\n",
      "epoch 19085: train loss: 0.10454873162766871, test loss: 0.25927454539103845\n",
      "epoch 19086: train loss: 0.10454730084720704, test loss: 0.259275621987245\n",
      "epoch 19087: train loss: 0.1045458701666493, test loss: 0.2592766986184818\n",
      "epoch 19088: train loss: 0.10454443958598286, test loss: 0.25927777528470386\n",
      "epoch 19089: train loss: 0.10454300910519493, test loss: 0.25927885198593864\n",
      "epoch 19090: train loss: 0.10454157872427278, test loss: 0.2592799287221629\n",
      "epoch 19091: train loss: 0.1045401484432037, test loss: 0.2592810054933261\n",
      "epoch 19092: train loss: 0.10453871826197494, test loss: 0.25928208229949773\n",
      "epoch 19093: train loss: 0.10453728818057381, test loss: 0.2592831591406315\n",
      "epoch 19094: train loss: 0.10453585819898759, test loss: 0.2592842360167182\n",
      "epoch 19095: train loss: 0.10453442831720355, test loss: 0.259285312927769\n",
      "epoch 19096: train loss: 0.10453299853520896, test loss: 0.25928638987373787\n",
      "epoch 19097: train loss: 0.10453156885299114, test loss: 0.25928746685465404\n",
      "epoch 19098: train loss: 0.10453013927053739, test loss: 0.25928854387048966\n",
      "epoch 19099: train loss: 0.10452870978783495, test loss: 0.2592896209212747\n",
      "epoch 19100: train loss: 0.10452728040487112, test loss: 0.2592906980069429\n",
      "epoch 19101: train loss: 0.10452585112163326, test loss: 0.2592917751275066\n",
      "epoch 19102: train loss: 0.10452442193810862, test loss: 0.25929285228301363\n",
      "epoch 19103: train loss: 0.10452299285428453, test loss: 0.2592939294733611\n",
      "epoch 19104: train loss: 0.10452156387014824, test loss: 0.25929500669865474\n",
      "epoch 19105: train loss: 0.1045201349856871, test loss: 0.25929608395878956\n",
      "epoch 19106: train loss: 0.10451870620088842, test loss: 0.25929716125379576\n",
      "epoch 19107: train loss: 0.1045172775157395, test loss: 0.2592982385836672\n",
      "epoch 19108: train loss: 0.10451584893022765, test loss: 0.25929931594841277\n",
      "epoch 19109: train loss: 0.10451442044434017, test loss: 0.2593003933479859\n",
      "epoch 19110: train loss: 0.10451299205806444, test loss: 0.259301470782418\n",
      "epoch 19111: train loss: 0.1045115637713877, test loss: 0.2593025482516596\n",
      "epoch 19112: train loss: 0.10451013558429734, test loss: 0.2593036257557436\n",
      "epoch 19113: train loss: 0.10450870749678065, test loss: 0.2593047032946408\n",
      "epoch 19114: train loss: 0.10450727950882495, test loss: 0.2593057808683441\n",
      "epoch 19115: train loss: 0.10450585162041758, test loss: 0.2593068584768826\n",
      "epoch 19116: train loss: 0.10450442383154587, test loss: 0.25930793612022895\n",
      "epoch 19117: train loss: 0.1045029961421972, test loss: 0.2593090137983377\n",
      "epoch 19118: train loss: 0.10450156855235881, test loss: 0.25931009151121814\n",
      "epoch 19119: train loss: 0.10450014106201813, test loss: 0.25931116925888303\n",
      "epoch 19120: train loss: 0.10449871367116245, test loss: 0.25931224704134104\n",
      "epoch 19121: train loss: 0.10449728637977912, test loss: 0.25931332485852754\n",
      "epoch 19122: train loss: 0.10449585918785552, test loss: 0.2593144027104732\n",
      "epoch 19123: train loss: 0.10449443209537895, test loss: 0.25931548059718845\n",
      "epoch 19124: train loss: 0.1044930051023368, test loss: 0.2593165585186265\n",
      "epoch 19125: train loss: 0.1044915782087164, test loss: 0.2593176364748178\n",
      "epoch 19126: train loss: 0.10449015141450509, test loss: 0.25931871446573385\n",
      "epoch 19127: train loss: 0.10448872471969026, test loss: 0.2593197924913682\n",
      "epoch 19128: train loss: 0.10448729812425926, test loss: 0.25932087055171094\n",
      "epoch 19129: train loss: 0.10448587162819944, test loss: 0.25932194864675506\n",
      "epoch 19130: train loss: 0.10448444523149818, test loss: 0.25932302677649227\n",
      "epoch 19131: train loss: 0.10448301893414282, test loss: 0.2593241049409152\n",
      "epoch 19132: train loss: 0.10448159273612077, test loss: 0.2593251831400325\n",
      "epoch 19133: train loss: 0.10448016663741937, test loss: 0.25932626137379944\n",
      "epoch 19134: train loss: 0.104478740638026, test loss: 0.25932733964228377\n",
      "epoch 19135: train loss: 0.10447731473792804, test loss: 0.25932841794538086\n",
      "epoch 19136: train loss: 0.10447588893711285, test loss: 0.2593294962831418\n",
      "epoch 19137: train loss: 0.10447446323556782, test loss: 0.25933057465555753\n",
      "epoch 19138: train loss: 0.10447303763328039, test loss: 0.25933165306263906\n",
      "epoch 19139: train loss: 0.10447161213023787, test loss: 0.2593327315043211\n",
      "epoch 19140: train loss: 0.10447018672642766, test loss: 0.25933380998067174\n",
      "epoch 19141: train loss: 0.10446876142183717, test loss: 0.2593348884915875\n",
      "epoch 19142: train loss: 0.10446733621645377, test loss: 0.25933596703713574\n",
      "epoch 19143: train loss: 0.10446591111026486, test loss: 0.25933704561729076\n",
      "epoch 19144: train loss: 0.10446448610325784, test loss: 0.25933812423202324\n",
      "epoch 19145: train loss: 0.10446306119542015, test loss: 0.2593392028813667\n",
      "epoch 19146: train loss: 0.10446163638673912, test loss: 0.2593402815652904\n",
      "epoch 19147: train loss: 0.10446021167720222, test loss: 0.25934136028380733\n",
      "epoch 19148: train loss: 0.10445878706679677, test loss: 0.25934243903683196\n",
      "epoch 19149: train loss: 0.10445736255551026, test loss: 0.2593435178244718\n",
      "epoch 19150: train loss: 0.10445593814333007, test loss: 0.25934459664666015\n",
      "epoch 19151: train loss: 0.1044545138302436, test loss: 0.2593456755033904\n",
      "epoch 19152: train loss: 0.10445308961623828, test loss: 0.2593467543946545\n",
      "epoch 19153: train loss: 0.10445166550130153, test loss: 0.2593478333204429\n",
      "epoch 19154: train loss: 0.10445024148542076, test loss: 0.2593489122807504\n",
      "epoch 19155: train loss: 0.1044488175685834, test loss: 0.25934999127560276\n",
      "epoch 19156: train loss: 0.10444739375077684, test loss: 0.25935107030493837\n",
      "epoch 19157: train loss: 0.10444597003198858, test loss: 0.2593521493687858\n",
      "epoch 19158: train loss: 0.10444454641220599, test loss: 0.25935322846713704\n",
      "epoch 19159: train loss: 0.10444312289141651, test loss: 0.2593543075999848\n",
      "epoch 19160: train loss: 0.10444169946960756, test loss: 0.2593553867672814\n",
      "epoch 19161: train loss: 0.1044402761467666, test loss: 0.25935646596907896\n",
      "epoch 19162: train loss: 0.1044388529228811, test loss: 0.2593575452053277\n",
      "epoch 19163: train loss: 0.10443742979793842, test loss: 0.2593586244760606\n",
      "epoch 19164: train loss: 0.10443600677192606, test loss: 0.2593597037812302\n",
      "epoch 19165: train loss: 0.10443458384483144, test loss: 0.25936078312086774\n",
      "epoch 19166: train loss: 0.10443316101664202, test loss: 0.2593618624949444\n",
      "epoch 19167: train loss: 0.10443173828734523, test loss: 0.2593629419034146\n",
      "epoch 19168: train loss: 0.10443031565692854, test loss: 0.259364021346348\n",
      "epoch 19169: train loss: 0.10442889312537941, test loss: 0.25936510082367875\n",
      "epoch 19170: train loss: 0.10442747069268528, test loss: 0.2593661803354363\n",
      "epoch 19171: train loss: 0.10442604835883362, test loss: 0.25936725988155684\n",
      "epoch 19172: train loss: 0.10442462612381187, test loss: 0.2593683394621265\n",
      "epoch 19173: train loss: 0.10442320398760749, test loss: 0.25936941907706024\n",
      "epoch 19174: train loss: 0.10442178195020797, test loss: 0.2593704987263535\n",
      "epoch 19175: train loss: 0.1044203600116008, test loss: 0.2593715784100509\n",
      "epoch 19176: train loss: 0.10441893817177342, test loss: 0.25937265812809196\n",
      "epoch 19177: train loss: 0.10441751643071327, test loss: 0.25937373788052326\n",
      "epoch 19178: train loss: 0.1044160947884079, test loss: 0.2593748176672995\n",
      "epoch 19179: train loss: 0.10441467324484471, test loss: 0.2593758974884133\n",
      "epoch 19180: train loss: 0.1044132518000112, test loss: 0.2593769773438738\n",
      "epoch 19181: train loss: 0.10441183045389486, test loss: 0.2593780572336562\n",
      "epoch 19182: train loss: 0.1044104092064832, test loss: 0.25937913715780975\n",
      "epoch 19183: train loss: 0.10440898805776368, test loss: 0.25938021711620984\n",
      "epoch 19184: train loss: 0.10440756700772376, test loss: 0.2593812971089661\n",
      "epoch 19185: train loss: 0.104406146056351, test loss: 0.2593823771360112\n",
      "epoch 19186: train loss: 0.10440472520363282, test loss: 0.259383457197357\n",
      "epoch 19187: train loss: 0.10440330444955677, test loss: 0.2593845372929581\n",
      "epoch 19188: train loss: 0.10440188379411032, test loss: 0.25938561742290234\n",
      "epoch 19189: train loss: 0.10440046323728097, test loss: 0.25938669758708327\n",
      "epoch 19190: train loss: 0.10439904277905622, test loss: 0.25938777778553634\n",
      "epoch 19191: train loss: 0.10439762241942357, test loss: 0.25938885801826933\n",
      "epoch 19192: train loss: 0.10439620215837055, test loss: 0.259389938285236\n",
      "epoch 19193: train loss: 0.10439478199588464, test loss: 0.2593910185864504\n",
      "epoch 19194: train loss: 0.10439336193195338, test loss: 0.2593920989219008\n",
      "epoch 19195: train loss: 0.10439194196656425, test loss: 0.2593931792915839\n",
      "epoch 19196: train loss: 0.1043905220997048, test loss: 0.2593942596955078\n",
      "epoch 19197: train loss: 0.10438910233136252, test loss: 0.2593953401336262\n",
      "epoch 19198: train loss: 0.10438768266152493, test loss: 0.25939642060601015\n",
      "epoch 19199: train loss: 0.10438626309017959, test loss: 0.2593975011125354\n",
      "epoch 19200: train loss: 0.10438484361731398, test loss: 0.2593985816532715\n",
      "epoch 19201: train loss: 0.10438342424291562, test loss: 0.25939966222821\n",
      "epoch 19202: train loss: 0.1043820049669721, test loss: 0.2594007428373061\n",
      "epoch 19203: train loss: 0.10438058578947089, test loss: 0.2594018234806068\n",
      "epoch 19204: train loss: 0.10437916671039955, test loss: 0.25940290415806944\n",
      "epoch 19205: train loss: 0.10437774772974562, test loss: 0.2594039848696842\n",
      "epoch 19206: train loss: 0.10437632884749665, test loss: 0.2594050656154816\n",
      "epoch 19207: train loss: 0.10437491006364011, test loss: 0.25940614639537857\n",
      "epoch 19208: train loss: 0.10437349137816362, test loss: 0.25940722720946263\n",
      "epoch 19209: train loss: 0.10437207279105469, test loss: 0.25940830805764786\n",
      "epoch 19210: train loss: 0.10437065430230089, test loss: 0.25940938893996757\n",
      "epoch 19211: train loss: 0.10436923591188974, test loss: 0.2594104698564108\n",
      "epoch 19212: train loss: 0.10436781761980882, test loss: 0.2594115508069737\n",
      "epoch 19213: train loss: 0.10436639942604564, test loss: 0.2594126317916254\n",
      "epoch 19214: train loss: 0.10436498133058782, test loss: 0.2594137128103992\n",
      "epoch 19215: train loss: 0.10436356333342285, test loss: 0.2594147938632497\n",
      "epoch 19216: train loss: 0.10436214543453834, test loss: 0.2594158749501666\n",
      "epoch 19217: train loss: 0.10436072763392186, test loss: 0.2594169560711811\n",
      "epoch 19218: train loss: 0.10435930993156091, test loss: 0.2594180372262685\n",
      "epoch 19219: train loss: 0.10435789232744312, test loss: 0.2594191184153811\n",
      "epoch 19220: train loss: 0.10435647482155602, test loss: 0.25942019963858765\n",
      "epoch 19221: train loss: 0.10435505741388723, test loss: 0.2594212808958055\n",
      "epoch 19222: train loss: 0.10435364010442431, test loss: 0.25942236218710113\n",
      "epoch 19223: train loss: 0.10435222289315478, test loss: 0.2594234435123913\n",
      "epoch 19224: train loss: 0.10435080578006627, test loss: 0.2594245248717468\n",
      "epoch 19225: train loss: 0.10434938876514636, test loss: 0.25942560626510003\n",
      "epoch 19226: train loss: 0.10434797184838261, test loss: 0.25942668769248334\n",
      "epoch 19227: train loss: 0.10434655502976263, test loss: 0.2594277691538882\n",
      "epoch 19228: train loss: 0.10434513830927403, test loss: 0.2594288506492303\n",
      "epoch 19229: train loss: 0.10434372168690433, test loss: 0.2594299321786172\n",
      "epoch 19230: train loss: 0.10434230516264115, test loss: 0.2594310137419851\n",
      "epoch 19231: train loss: 0.10434088873647213, test loss: 0.25943209533928563\n",
      "epoch 19232: train loss: 0.10433947240838477, test loss: 0.25943317697062745\n",
      "epoch 19233: train loss: 0.10433805617836679, test loss: 0.2594342586358504\n",
      "epoch 19234: train loss: 0.10433664004640571, test loss: 0.2594353403350994\n",
      "epoch 19235: train loss: 0.10433522401248915, test loss: 0.25943642206825074\n",
      "epoch 19236: train loss: 0.10433380807660474, test loss: 0.2594375038353758\n",
      "epoch 19237: train loss: 0.10433239223874004, test loss: 0.2594385856364079\n",
      "epoch 19238: train loss: 0.10433097649888272, test loss: 0.259439667471378\n",
      "epoch 19239: train loss: 0.10432956085702033, test loss: 0.2594407493402805\n",
      "epoch 19240: train loss: 0.10432814531314051, test loss: 0.25944183124312375\n",
      "epoch 19241: train loss: 0.1043267298672309, test loss: 0.2594429131798259\n",
      "epoch 19242: train loss: 0.10432531451927911, test loss: 0.2594439951504357\n",
      "epoch 19243: train loss: 0.10432389926927275, test loss: 0.2594450771549464\n",
      "epoch 19244: train loss: 0.10432248411719942, test loss: 0.2594461591933304\n",
      "epoch 19245: train loss: 0.10432106906304682, test loss: 0.25944724126564017\n",
      "epoch 19246: train loss: 0.10431965410680251, test loss: 0.25944832337178814\n",
      "epoch 19247: train loss: 0.10431823924845413, test loss: 0.2594494055117876\n",
      "epoch 19248: train loss: 0.10431682448798933, test loss: 0.25945048768563156\n",
      "epoch 19249: train loss: 0.10431540982539574, test loss: 0.2594515698933677\n",
      "epoch 19250: train loss: 0.104313995260661, test loss: 0.2594526521349155\n",
      "epoch 19251: train loss: 0.10431258079377273, test loss: 0.2594537344103411\n",
      "epoch 19252: train loss: 0.1043111664247186, test loss: 0.259454816719562\n",
      "epoch 19253: train loss: 0.10430975215348624, test loss: 0.2594558990626078\n",
      "epoch 19254: train loss: 0.1043083379800633, test loss: 0.25945698143949175\n",
      "epoch 19255: train loss: 0.10430692390443742, test loss: 0.25945806385016573\n",
      "epoch 19256: train loss: 0.10430550992659626, test loss: 0.2594591462946263\n",
      "epoch 19257: train loss: 0.10430409604652745, test loss: 0.2594602287728999\n",
      "epoch 19258: train loss: 0.1043026822642187, test loss: 0.25946131128498173\n",
      "epoch 19259: train loss: 0.1043012685796576, test loss: 0.2594623938308043\n",
      "epoch 19260: train loss: 0.10429985499283184, test loss: 0.25946347641044165\n",
      "epoch 19261: train loss: 0.10429844150372912, test loss: 0.2594645590237849\n",
      "epoch 19262: train loss: 0.10429702811233703, test loss: 0.25946564167096586\n",
      "epoch 19263: train loss: 0.1042956148186433, test loss: 0.25946672435181917\n",
      "epoch 19264: train loss: 0.10429420162263554, test loss: 0.25946780706645545\n",
      "epoch 19265: train loss: 0.10429278852430147, test loss: 0.2594688898148471\n",
      "epoch 19266: train loss: 0.10429137552362874, test loss: 0.259469972596929\n",
      "epoch 19267: train loss: 0.10428996262060504, test loss: 0.25947105541279075\n",
      "epoch 19268: train loss: 0.10428854981521805, test loss: 0.25947213826230847\n",
      "epoch 19269: train loss: 0.10428713710745541, test loss: 0.2594732211455921\n",
      "epoch 19270: train loss: 0.10428572449730485, test loss: 0.2594743040625554\n",
      "epoch 19271: train loss: 0.10428431198475402, test loss: 0.2594753870132284\n",
      "epoch 19272: train loss: 0.10428289956979063, test loss: 0.2594764699975485\n",
      "epoch 19273: train loss: 0.10428148725240236, test loss: 0.2594775530155849\n",
      "epoch 19274: train loss: 0.10428007503257689, test loss: 0.2594786360672908\n",
      "epoch 19275: train loss: 0.10427866291030191, test loss: 0.25947971915265894\n",
      "epoch 19276: train loss: 0.10427725088556518, test loss: 0.259480802271722\n",
      "epoch 19277: train loss: 0.10427583895835428, test loss: 0.25948188542439315\n",
      "epoch 19278: train loss: 0.10427442712865702, test loss: 0.2594829686107058\n",
      "epoch 19279: train loss: 0.10427301539646104, test loss: 0.25948405183068984\n",
      "epoch 19280: train loss: 0.10427160376175407, test loss: 0.2594851350843188\n",
      "epoch 19281: train loss: 0.1042701922245238, test loss: 0.25948621837154723\n",
      "epoch 19282: train loss: 0.10426878078475793, test loss: 0.2594873016924064\n",
      "epoch 19283: train loss: 0.10426736944244422, test loss: 0.2594883850468698\n",
      "epoch 19284: train loss: 0.10426595819757033, test loss: 0.2594894684349686\n",
      "epoch 19285: train loss: 0.104264547050124, test loss: 0.25949055185663694\n",
      "epoch 19286: train loss: 0.10426313600009296, test loss: 0.2594916353118688\n",
      "epoch 19287: train loss: 0.1042617250474649, test loss: 0.2594927188007524\n",
      "epoch 19288: train loss: 0.10426031419222756, test loss: 0.2594938023231649\n",
      "epoch 19289: train loss: 0.10425890343436865, test loss: 0.2594948858791757\n",
      "epoch 19290: train loss: 0.10425749277387591, test loss: 0.25949596946872083\n",
      "epoch 19291: train loss: 0.10425608221073707, test loss: 0.25949705309186993\n",
      "epoch 19292: train loss: 0.10425467174493985, test loss: 0.25949813674851874\n",
      "epoch 19293: train loss: 0.104253261376472, test loss: 0.2594992204386988\n",
      "epoch 19294: train loss: 0.10425185110532123, test loss: 0.2595003041624609\n",
      "epoch 19295: train loss: 0.1042504409314753, test loss: 0.25950138791971966\n",
      "epoch 19296: train loss: 0.10424903085492193, test loss: 0.25950247171054674\n",
      "epoch 19297: train loss: 0.1042476208756489, test loss: 0.25950355553487614\n",
      "epoch 19298: train loss: 0.10424621099364391, test loss: 0.25950463939266155\n",
      "epoch 19299: train loss: 0.10424480120889473, test loss: 0.25950572328399235\n",
      "epoch 19300: train loss: 0.10424339152138909, test loss: 0.2595068072087862\n",
      "epoch 19301: train loss: 0.10424198193111475, test loss: 0.2595078911670916\n",
      "epoch 19302: train loss: 0.10424057243805947, test loss: 0.259508975158881\n",
      "epoch 19303: train loss: 0.10423916304221102, test loss: 0.2595100591841314\n",
      "epoch 19304: train loss: 0.10423775374355713, test loss: 0.25951114324287294\n",
      "epoch 19305: train loss: 0.10423634454208557, test loss: 0.25951222733503815\n",
      "epoch 19306: train loss: 0.10423493543778412, test loss: 0.25951331146069967\n",
      "epoch 19307: train loss: 0.10423352643064049, test loss: 0.2595143956197725\n",
      "epoch 19308: train loss: 0.10423211752064251, test loss: 0.25951547981228695\n",
      "epoch 19309: train loss: 0.10423070870777792, test loss: 0.2595165640382379\n",
      "epoch 19310: train loss: 0.10422929999203447, test loss: 0.25951764829757806\n",
      "epoch 19311: train loss: 0.10422789137339995, test loss: 0.2595187325903779\n",
      "epoch 19312: train loss: 0.10422648285186219, test loss: 0.25951981691659065\n",
      "epoch 19313: train loss: 0.10422507442740887, test loss: 0.25952090127619337\n",
      "epoch 19314: train loss: 0.10422366610002783, test loss: 0.25952198566917456\n",
      "epoch 19315: train loss: 0.10422225786970682, test loss: 0.25952307009556885\n",
      "epoch 19316: train loss: 0.10422084973643367, test loss: 0.2595241545553282\n",
      "epoch 19317: train loss: 0.10421944170019612, test loss: 0.2595252390484861\n",
      "epoch 19318: train loss: 0.104218033760982, test loss: 0.2595263235750144\n",
      "epoch 19319: train loss: 0.10421662591877905, test loss: 0.25952740813490777\n",
      "epoch 19320: train loss: 0.10421521817357511, test loss: 0.2595284927281383\n",
      "epoch 19321: train loss: 0.10421381052535797, test loss: 0.2595295773547003\n",
      "epoch 19322: train loss: 0.10421240297411538, test loss: 0.25953066201464436\n",
      "epoch 19323: train loss: 0.10421099551983516, test loss: 0.2595317467079029\n",
      "epoch 19324: train loss: 0.10420958816250518, test loss: 0.2595328314344918\n",
      "epoch 19325: train loss: 0.10420818090211316, test loss: 0.2595339161944018\n",
      "epoch 19326: train loss: 0.10420677373864695, test loss: 0.2595350009876071\n",
      "epoch 19327: train loss: 0.10420536667209429, test loss: 0.2595360858141404\n",
      "epoch 19328: train loss: 0.1042039597024431, test loss: 0.2595371706739931\n",
      "epoch 19329: train loss: 0.10420255282968112, test loss: 0.25953825556709964\n",
      "epoch 19330: train loss: 0.10420114605379618, test loss: 0.25953934049351235\n",
      "epoch 19331: train loss: 0.10419973937477607, test loss: 0.2595404254532059\n",
      "epoch 19332: train loss: 0.10419833279260869, test loss: 0.25954151044615087\n",
      "epoch 19333: train loss: 0.10419692630728178, test loss: 0.2595425954723817\n",
      "epoch 19334: train loss: 0.10419551991878322, test loss: 0.25954368053187005\n",
      "epoch 19335: train loss: 0.10419411362710078, test loss: 0.2595447656245715\n",
      "epoch 19336: train loss: 0.10419270743222231, test loss: 0.25954585075055625\n",
      "epoch 19337: train loss: 0.10419130133413568, test loss: 0.2595469359097777\n",
      "epoch 19338: train loss: 0.10418989533282864, test loss: 0.2595480211022302\n",
      "epoch 19339: train loss: 0.10418848942828911, test loss: 0.2595491063279066\n",
      "epoch 19340: train loss: 0.10418708362050488, test loss: 0.2595501915867597\n",
      "epoch 19341: train loss: 0.1041856779094638, test loss: 0.2595512768788821\n",
      "epoch 19342: train loss: 0.10418427229515373, test loss: 0.25955236220418515\n",
      "epoch 19343: train loss: 0.1041828667775625, test loss: 0.25955344756264664\n",
      "epoch 19344: train loss: 0.10418146135667794, test loss: 0.25955453295435377\n",
      "epoch 19345: train loss: 0.10418005603248788, test loss: 0.2595556183792437\n",
      "epoch 19346: train loss: 0.10417865080498023, test loss: 0.25955670383725016\n",
      "epoch 19347: train loss: 0.10417724567414281, test loss: 0.2595577893284814\n",
      "epoch 19348: train loss: 0.10417584063996346, test loss: 0.25955887485285506\n",
      "epoch 19349: train loss: 0.10417443570243008, test loss: 0.2595599604103628\n",
      "epoch 19350: train loss: 0.10417303086153047, test loss: 0.259561046001057\n",
      "epoch 19351: train loss: 0.10417162611725253, test loss: 0.25956213162485187\n",
      "epoch 19352: train loss: 0.10417022146958413, test loss: 0.2595632172818186\n",
      "epoch 19353: train loss: 0.10416881691851312, test loss: 0.25956430297187216\n",
      "epoch 19354: train loss: 0.10416741246402736, test loss: 0.25956538869508594\n",
      "epoch 19355: train loss: 0.10416600810611473, test loss: 0.2595664744514306\n",
      "epoch 19356: train loss: 0.10416460384476309, test loss: 0.2595675602408211\n",
      "epoch 19357: train loss: 0.10416319967996035, test loss: 0.2595686460633517\n",
      "epoch 19358: train loss: 0.10416179561169434, test loss: 0.2595697319189333\n",
      "epoch 19359: train loss: 0.10416039163995297, test loss: 0.2595708178076594\n",
      "epoch 19360: train loss: 0.10415898776472413, test loss: 0.25957190372944217\n",
      "epoch 19361: train loss: 0.10415758398599564, test loss: 0.2595729896842796\n",
      "epoch 19362: train loss: 0.10415618030375544, test loss: 0.2595740756722179\n",
      "epoch 19363: train loss: 0.10415477671799143, test loss: 0.2595751616931755\n",
      "epoch 19364: train loss: 0.10415337322869146, test loss: 0.2595762477472035\n",
      "epoch 19365: train loss: 0.10415196983584343, test loss: 0.2595773338342559\n",
      "epoch 19366: train loss: 0.10415056653943523, test loss: 0.25957841995438463\n",
      "epoch 19367: train loss: 0.10414916333945481, test loss: 0.2595795061075031\n",
      "epoch 19368: train loss: 0.10414776023589, test loss: 0.2595805922936856\n",
      "epoch 19369: train loss: 0.10414635722872871, test loss: 0.25958167851284414\n",
      "epoch 19370: train loss: 0.10414495431795888, test loss: 0.25958276476505304\n",
      "epoch 19371: train loss: 0.10414355150356838, test loss: 0.2595838510502454\n",
      "epoch 19372: train loss: 0.10414214878554513, test loss: 0.2595849373684514\n",
      "epoch 19373: train loss: 0.10414074616387707, test loss: 0.2595860237196097\n",
      "epoch 19374: train loss: 0.10413934363855205, test loss: 0.25958711010374913\n",
      "epoch 19375: train loss: 0.10413794120955801, test loss: 0.2595881965209246\n",
      "epoch 19376: train loss: 0.10413653887688287, test loss: 0.2595892829710088\n",
      "epoch 19377: train loss: 0.10413513664051456, test loss: 0.25959036945407477\n",
      "epoch 19378: train loss: 0.10413373450044099, test loss: 0.25959145597011796\n",
      "epoch 19379: train loss: 0.10413233245665009, test loss: 0.25959254251908875\n",
      "epoch 19380: train loss: 0.10413093050912973, test loss: 0.2595936291010399\n",
      "epoch 19381: train loss: 0.10412952865786788, test loss: 0.25959471571588844\n",
      "epoch 19382: train loss: 0.10412812690285252, test loss: 0.25959580236366464\n",
      "epoch 19383: train loss: 0.10412672524407149, test loss: 0.2595968890443638\n",
      "epoch 19384: train loss: 0.10412532368151277, test loss: 0.25959797575797644\n",
      "epoch 19385: train loss: 0.10412392221516431, test loss: 0.25959906250449866\n",
      "epoch 19386: train loss: 0.104122520845014, test loss: 0.2596001492839222\n",
      "epoch 19387: train loss: 0.10412111957104982, test loss: 0.2596012360962405\n",
      "epoch 19388: train loss: 0.10411971839325972, test loss: 0.25960232294144603\n",
      "epoch 19389: train loss: 0.10411831731163158, test loss: 0.2596034098195538\n",
      "epoch 19390: train loss: 0.10411691632615337, test loss: 0.25960449673051583\n",
      "epoch 19391: train loss: 0.10411551543681309, test loss: 0.25960558367434705\n",
      "epoch 19392: train loss: 0.10411411464359865, test loss: 0.25960667065103943\n",
      "epoch 19393: train loss: 0.10411271394649801, test loss: 0.25960775766056654\n",
      "epoch 19394: train loss: 0.10411131334549914, test loss: 0.25960884470298073\n",
      "epoch 19395: train loss: 0.10410991284058994, test loss: 0.2596099317781784\n",
      "epoch 19396: train loss: 0.10410851243175843, test loss: 0.25961101888626953\n",
      "epoch 19397: train loss: 0.10410711211899257, test loss: 0.25961210602714896\n",
      "epoch 19398: train loss: 0.10410571190228027, test loss: 0.25961319320082915\n",
      "epoch 19399: train loss: 0.10410431178160953, test loss: 0.2596142804073462\n",
      "epoch 19400: train loss: 0.10410291175696834, test loss: 0.25961536764666954\n",
      "epoch 19401: train loss: 0.10410151182834464, test loss: 0.2596164549187954\n",
      "epoch 19402: train loss: 0.1041001119957264, test loss: 0.2596175422237165\n",
      "epoch 19403: train loss: 0.10409871225910161, test loss: 0.259618629561424\n",
      "epoch 19404: train loss: 0.10409731261845824, test loss: 0.25961971693191477\n",
      "epoch 19405: train loss: 0.10409591307378427, test loss: 0.25962080433516116\n",
      "epoch 19406: train loss: 0.10409451362506766, test loss: 0.25962189177115585\n",
      "epoch 19407: train loss: 0.10409311427229646, test loss: 0.2596229792399118\n",
      "epoch 19408: train loss: 0.10409171501545857, test loss: 0.25962406674146404\n",
      "epoch 19409: train loss: 0.10409031585454201, test loss: 0.25962515427570393\n",
      "epoch 19410: train loss: 0.10408891678953477, test loss: 0.25962624184270716\n",
      "epoch 19411: train loss: 0.10408751782042486, test loss: 0.259627329442465\n",
      "epoch 19412: train loss: 0.10408611894720028, test loss: 0.25962841707489215\n",
      "epoch 19413: train loss: 0.10408472016984899, test loss: 0.2596295047401009\n",
      "epoch 19414: train loss: 0.10408332148835896, test loss: 0.25963059243796716\n",
      "epoch 19415: train loss: 0.1040819229027183, test loss: 0.2596316801685625\n",
      "epoch 19416: train loss: 0.1040805244129149, test loss: 0.2596327679318613\n",
      "epoch 19417: train loss: 0.10407912601893683, test loss: 0.2596338557278149\n",
      "epoch 19418: train loss: 0.10407772772077209, test loss: 0.2596349435564989\n",
      "epoch 19419: train loss: 0.10407632951840863, test loss: 0.2596360314178267\n",
      "epoch 19420: train loss: 0.10407493141183455, test loss: 0.2596371193118297\n",
      "epoch 19421: train loss: 0.10407353340103782, test loss: 0.2596382072384661\n",
      "epoch 19422: train loss: 0.10407213548600643, test loss: 0.2596392951978035\n",
      "epoch 19423: train loss: 0.10407073766672845, test loss: 0.2596403831897584\n",
      "epoch 19424: train loss: 0.10406933994319183, test loss: 0.2596414712143652\n",
      "epoch 19425: train loss: 0.10406794231538466, test loss: 0.259642559271616\n",
      "epoch 19426: train loss: 0.10406654478329495, test loss: 0.25964364736152273\n",
      "epoch 19427: train loss: 0.1040651473469107, test loss: 0.25964473548400263\n",
      "epoch 19428: train loss: 0.10406375000621995, test loss: 0.25964582363910776\n",
      "epoch 19429: train loss: 0.10406235276121074, test loss: 0.25964691182681043\n",
      "epoch 19430: train loss: 0.10406095561187108, test loss: 0.2596480000471438\n",
      "epoch 19431: train loss: 0.10405955855818905, test loss: 0.25964908830006517\n",
      "epoch 19432: train loss: 0.10405816160015263, test loss: 0.2596501765855626\n",
      "epoch 19433: train loss: 0.10405676473774988, test loss: 0.25965126490367485\n",
      "epoch 19434: train loss: 0.10405536797096886, test loss: 0.25965235325433184\n",
      "epoch 19435: train loss: 0.10405397129979761, test loss: 0.2596534416375695\n",
      "epoch 19436: train loss: 0.10405257472422416, test loss: 0.25965453005331984\n",
      "epoch 19437: train loss: 0.10405117824423656, test loss: 0.25965561850165714\n",
      "epoch 19438: train loss: 0.10404978185982287, test loss: 0.25965670698255494\n",
      "epoch 19439: train loss: 0.10404838557097111, test loss: 0.25965779549600565\n",
      "epoch 19440: train loss: 0.10404698937766937, test loss: 0.2596588840419645\n",
      "epoch 19441: train loss: 0.1040455932799057, test loss: 0.2596599726204648\n",
      "epoch 19442: train loss: 0.10404419727766814, test loss: 0.2596610612314597\n",
      "epoch 19443: train loss: 0.10404280137094479, test loss: 0.25966214987498387\n",
      "epoch 19444: train loss: 0.10404140555972369, test loss: 0.2596632385510288\n",
      "epoch 19445: train loss: 0.10404000984399288, test loss: 0.2596643272595697\n",
      "epoch 19446: train loss: 0.10403861422374044, test loss: 0.2596654160006001\n",
      "epoch 19447: train loss: 0.10403721869895445, test loss: 0.2596665047740939\n",
      "epoch 19448: train loss: 0.10403582326962299, test loss: 0.25966759358010344\n",
      "epoch 19449: train loss: 0.10403442793573411, test loss: 0.25966868241854457\n",
      "epoch 19450: train loss: 0.1040330326972759, test loss: 0.259669771289507\n",
      "epoch 19451: train loss: 0.10403163755423644, test loss: 0.25967086019288893\n",
      "epoch 19452: train loss: 0.10403024250660381, test loss: 0.259671949128742\n",
      "epoch 19453: train loss: 0.10402884755436606, test loss: 0.25967303809699904\n",
      "epoch 19454: train loss: 0.10402745269751133, test loss: 0.25967412709777404\n",
      "epoch 19455: train loss: 0.10402605793602766, test loss: 0.25967521613092326\n",
      "epoch 19456: train loss: 0.10402466326990316, test loss: 0.25967630519649637\n",
      "epoch 19457: train loss: 0.1040232686991259, test loss: 0.25967739429449055\n",
      "epoch 19458: train loss: 0.10402187422368399, test loss: 0.25967848342491784\n",
      "epoch 19459: train loss: 0.10402047984356555, test loss: 0.25967957258777064\n",
      "epoch 19460: train loss: 0.10401908555875862, test loss: 0.2596806617830049\n",
      "epoch 19461: train loss: 0.10401769136925135, test loss: 0.2596817510106139\n",
      "epoch 19462: train loss: 0.10401629727503177, test loss: 0.2596828402705911\n",
      "epoch 19463: train loss: 0.10401490327608806, test loss: 0.2596839295629699\n",
      "epoch 19464: train loss: 0.10401350937240832, test loss: 0.2596850188877434\n",
      "epoch 19465: train loss: 0.1040121155639806, test loss: 0.25968610824482796\n",
      "epoch 19466: train loss: 0.10401072185079308, test loss: 0.25968719763433373\n",
      "epoch 19467: train loss: 0.10400932823283382, test loss: 0.25968828705615715\n",
      "epoch 19468: train loss: 0.10400793471009095, test loss: 0.25968937651033097\n",
      "epoch 19469: train loss: 0.10400654128255259, test loss: 0.25969046599683054\n",
      "epoch 19470: train loss: 0.10400514795020684, test loss: 0.2596915555156858\n",
      "epoch 19471: train loss: 0.10400375471304184, test loss: 0.25969264506683365\n",
      "epoch 19472: train loss: 0.10400236157104568, test loss: 0.2596937346503475\n",
      "epoch 19473: train loss: 0.10400096852420657, test loss: 0.25969482426614143\n",
      "epoch 19474: train loss: 0.10399957557251255, test loss: 0.2596959139142476\n",
      "epoch 19475: train loss: 0.10399818271595176, test loss: 0.2596970035946209\n",
      "epoch 19476: train loss: 0.10399678995451236, test loss: 0.25969809330735427\n",
      "epoch 19477: train loss: 0.10399539728818247, test loss: 0.25969918305232303\n",
      "epoch 19478: train loss: 0.10399400471695025, test loss: 0.2597002728295813\n",
      "epoch 19479: train loss: 0.10399261224080378, test loss: 0.25970136263908106\n",
      "epoch 19480: train loss: 0.10399121985973123, test loss: 0.2597024524808757\n",
      "epoch 19481: train loss: 0.10398982757372076, test loss: 0.25970354235491994\n",
      "epoch 19482: train loss: 0.1039884353827605, test loss: 0.25970463226123036\n",
      "epoch 19483: train loss: 0.1039870432868386, test loss: 0.259705722199776\n",
      "epoch 19484: train loss: 0.10398565128594317, test loss: 0.2597068121705534\n",
      "epoch 19485: train loss: 0.10398425938006241, test loss: 0.25970790217355716\n",
      "epoch 19486: train loss: 0.10398286756918448, test loss: 0.259708992208779\n",
      "epoch 19487: train loss: 0.10398147585329748, test loss: 0.2597100822762539\n",
      "epoch 19488: train loss: 0.1039800842323896, test loss: 0.2597111723759154\n",
      "epoch 19489: train loss: 0.10397869270644901, test loss: 0.2597122625077786\n",
      "epoch 19490: train loss: 0.10397730127546381, test loss: 0.25971335267183604\n",
      "epoch 19491: train loss: 0.10397590993942224, test loss: 0.2597144428681213\n",
      "epoch 19492: train loss: 0.10397451869831242, test loss: 0.25971553309654793\n",
      "epoch 19493: train loss: 0.10397312755212255, test loss: 0.2597166233571731\n",
      "epoch 19494: train loss: 0.10397173650084075, test loss: 0.25971771364994733\n",
      "epoch 19495: train loss: 0.10397034554445522, test loss: 0.2597188039749057\n",
      "epoch 19496: train loss: 0.10396895468295415, test loss: 0.2597198943320408\n",
      "epoch 19497: train loss: 0.1039675639163257, test loss: 0.2597209847213087\n",
      "epoch 19498: train loss: 0.10396617324455804, test loss: 0.2597220751427028\n",
      "epoch 19499: train loss: 0.10396478266763935, test loss: 0.2597231655962565\n",
      "epoch 19500: train loss: 0.1039633921855578, test loss: 0.2597242560818843\n",
      "epoch 19501: train loss: 0.10396200179830163, test loss: 0.2597253465996989\n",
      "epoch 19502: train loss: 0.10396061150585897, test loss: 0.2597264371496352\n",
      "epoch 19503: train loss: 0.10395922130821802, test loss: 0.2597275277316472\n",
      "epoch 19504: train loss: 0.10395783120536697, test loss: 0.25972861834578864\n",
      "epoch 19505: train loss: 0.10395644119729404, test loss: 0.2597297089920122\n",
      "epoch 19506: train loss: 0.10395505128398738, test loss: 0.25973079967033463\n",
      "epoch 19507: train loss: 0.10395366146543522, test loss: 0.2597318903807078\n",
      "epoch 19508: train loss: 0.10395227174162575, test loss: 0.2597329811232056\n",
      "epoch 19509: train loss: 0.10395088211254716, test loss: 0.2597340718977448\n",
      "epoch 19510: train loss: 0.10394949257818765, test loss: 0.259735162704336\n",
      "epoch 19511: train loss: 0.10394810313853547, test loss: 0.2597362535430337\n",
      "epoch 19512: train loss: 0.10394671379357877, test loss: 0.2597373444137734\n",
      "epoch 19513: train loss: 0.1039453245433058, test loss: 0.2597384353165284\n",
      "epoch 19514: train loss: 0.10394393538770474, test loss: 0.2597395262513327\n",
      "epoch 19515: train loss: 0.10394254632676382, test loss: 0.25974061721814024\n",
      "epoch 19516: train loss: 0.10394115736047127, test loss: 0.25974170821698506\n",
      "epoch 19517: train loss: 0.1039397684888153, test loss: 0.2597427992478826\n",
      "epoch 19518: train loss: 0.10393837971178409, test loss: 0.2597438903107841\n",
      "epoch 19519: train loss: 0.1039369910293659, test loss: 0.2597449814056649\n",
      "epoch 19520: train loss: 0.10393560244154894, test loss: 0.2597460725325198\n",
      "epoch 19521: train loss: 0.10393421394832147, test loss: 0.25974716369140144\n",
      "epoch 19522: train loss: 0.10393282554967166, test loss: 0.25974825488226594\n",
      "epoch 19523: train loss: 0.10393143724558783, test loss: 0.2597493461051052\n",
      "epoch 19524: train loss: 0.1039300490360581, test loss: 0.25975043735991477\n",
      "epoch 19525: train loss: 0.1039286609210708, test loss: 0.2597515286466872\n",
      "epoch 19526: train loss: 0.10392727290061408, test loss: 0.25975261996541876\n",
      "epoch 19527: train loss: 0.10392588497467624, test loss: 0.2597537113161021\n",
      "epoch 19528: train loss: 0.10392449714324553, test loss: 0.2597548026987299\n",
      "epoch 19529: train loss: 0.10392310940631017, test loss: 0.25975589411330036\n",
      "epoch 19530: train loss: 0.10392172176385837, test loss: 0.25975698555978277\n",
      "epoch 19531: train loss: 0.10392033421587846, test loss: 0.25975807703821535\n",
      "epoch 19532: train loss: 0.10391894676235859, test loss: 0.2597591685485295\n",
      "epoch 19533: train loss: 0.10391755940328709, test loss: 0.2597602600907598\n",
      "epoch 19534: train loss: 0.10391617213865217, test loss: 0.25976135166492137\n",
      "epoch 19535: train loss: 0.10391478496844211, test loss: 0.25976244327096804\n",
      "epoch 19536: train loss: 0.10391339789264517, test loss: 0.2597635349089128\n",
      "epoch 19537: train loss: 0.10391201091124959, test loss: 0.25976462657873095\n",
      "epoch 19538: train loss: 0.10391062402424364, test loss: 0.25976571828041667\n",
      "epoch 19539: train loss: 0.10390923723161558, test loss: 0.2597668100140042\n",
      "epoch 19540: train loss: 0.10390785053335369, test loss: 0.2597679017794469\n",
      "epoch 19541: train loss: 0.1039064639294462, test loss: 0.25976899357674044\n",
      "epoch 19542: train loss: 0.10390507741988145, test loss: 0.2597700854058766\n",
      "epoch 19543: train loss: 0.10390369100464766, test loss: 0.25977117726689053\n",
      "epoch 19544: train loss: 0.10390230468373311, test loss: 0.25977226915971696\n",
      "epoch 19545: train loss: 0.1039009184571261, test loss: 0.2597733610843694\n",
      "epoch 19546: train loss: 0.10389953232481489, test loss: 0.25977445304084334\n",
      "epoch 19547: train loss: 0.10389814628678773, test loss: 0.2597755450291323\n",
      "epoch 19548: train loss: 0.10389676034303295, test loss: 0.25977663704926995\n",
      "epoch 19549: train loss: 0.10389537449353883, test loss: 0.2597777291011916\n",
      "epoch 19550: train loss: 0.10389398873829364, test loss: 0.2597788211848895\n",
      "epoch 19551: train loss: 0.10389260307728566, test loss: 0.2597799133004394\n",
      "epoch 19552: train loss: 0.10389121751050322, test loss: 0.25978100544771443\n",
      "epoch 19553: train loss: 0.10388983203793459, test loss: 0.25978209762677046\n",
      "epoch 19554: train loss: 0.10388844665956806, test loss: 0.25978318983762105\n",
      "epoch 19555: train loss: 0.10388706137539196, test loss: 0.25978428208022025\n",
      "epoch 19556: train loss: 0.10388567618539453, test loss: 0.2597853743546018\n",
      "epoch 19557: train loss: 0.10388429108956411, test loss: 0.2597864666607212\n",
      "epoch 19558: train loss: 0.103882906087889, test loss: 0.2597875589986107\n",
      "epoch 19559: train loss: 0.10388152118035751, test loss: 0.2597886513682057\n",
      "epoch 19560: train loss: 0.10388013636695792, test loss: 0.25978974376956293\n",
      "epoch 19561: train loss: 0.1038787516476786, test loss: 0.2597908362026315\n",
      "epoch 19562: train loss: 0.1038773670225078, test loss: 0.2597919286673696\n",
      "epoch 19563: train loss: 0.10387598249143386, test loss: 0.25979302116389014\n",
      "epoch 19564: train loss: 0.10387459805444511, test loss: 0.2597941136920895\n",
      "epoch 19565: train loss: 0.10387321371152985, test loss: 0.25979520625199826\n",
      "epoch 19566: train loss: 0.1038718294626764, test loss: 0.2597962988435932\n",
      "epoch 19567: train loss: 0.10387044530787307, test loss: 0.25979739146686653\n",
      "epoch 19568: train loss: 0.10386906124710822, test loss: 0.25979848412183626\n",
      "epoch 19569: train loss: 0.10386767728037016, test loss: 0.2597995768084543\n",
      "epoch 19570: train loss: 0.1038662934076472, test loss: 0.2598006695267538\n",
      "epoch 19571: train loss: 0.1038649096289277, test loss: 0.25980176227673046\n",
      "epoch 19572: train loss: 0.1038635259442, test loss: 0.25980285505832046\n",
      "epoch 19573: train loss: 0.1038621423534524, test loss: 0.25980394787157485\n",
      "epoch 19574: train loss: 0.10386075885667324, test loss: 0.25980504071644916\n",
      "epoch 19575: train loss: 0.10385937545385086, test loss: 0.2598061335930191\n",
      "epoch 19576: train loss: 0.10385799214497364, test loss: 0.25980722650115795\n",
      "epoch 19577: train loss: 0.10385660893002989, test loss: 0.2598083194409394\n",
      "epoch 19578: train loss: 0.10385522580900794, test loss: 0.25980941241233846\n",
      "epoch 19579: train loss: 0.10385384278189619, test loss: 0.25981050541533046\n",
      "epoch 19580: train loss: 0.10385245984868295, test loss: 0.2598115984499475\n",
      "epoch 19581: train loss: 0.10385107700935656, test loss: 0.25981269151610537\n",
      "epoch 19582: train loss: 0.1038496942639054, test loss: 0.2598137846139194\n",
      "epoch 19583: train loss: 0.10384831161231783, test loss: 0.25981487774328316\n",
      "epoch 19584: train loss: 0.10384692905458222, test loss: 0.2598159709042307\n",
      "epoch 19585: train loss: 0.10384554659068687, test loss: 0.2598170640966938\n",
      "epoch 19586: train loss: 0.1038441642206202, test loss: 0.2598181573207532\n",
      "epoch 19587: train loss: 0.10384278194437055, test loss: 0.2598192505764186\n",
      "epoch 19588: train loss: 0.1038413997619263, test loss: 0.2598203438635646\n",
      "epoch 19589: train loss: 0.1038400176732758, test loss: 0.25982143718226786\n",
      "epoch 19590: train loss: 0.10383863567840741, test loss: 0.2598225305325214\n",
      "epoch 19591: train loss: 0.10383725377730954, test loss: 0.2598236239142995\n",
      "epoch 19592: train loss: 0.10383587196997056, test loss: 0.2598247173276167\n",
      "epoch 19593: train loss: 0.1038344902563788, test loss: 0.2598258107723882\n",
      "epoch 19594: train loss: 0.10383310863652269, test loss: 0.2598269042487079\n",
      "epoch 19595: train loss: 0.10383172711039061, test loss: 0.2598279977565501\n",
      "epoch 19596: train loss: 0.10383034567797089, test loss: 0.25982909129585013\n",
      "epoch 19597: train loss: 0.10382896433925197, test loss: 0.2598301848666616\n",
      "epoch 19598: train loss: 0.10382758309422219, test loss: 0.25983127846893855\n",
      "epoch 19599: train loss: 0.10382620194286998, test loss: 0.25983237210271526\n",
      "epoch 19600: train loss: 0.10382482088518373, test loss: 0.2598334657679485\n",
      "epoch 19601: train loss: 0.10382343992115178, test loss: 0.25983455946465167\n",
      "epoch 19602: train loss: 0.10382205905076262, test loss: 0.2598356531927969\n",
      "epoch 19603: train loss: 0.10382067827400457, test loss: 0.2598367469523815\n",
      "epoch 19604: train loss: 0.10381929759086603, test loss: 0.2598378407434606\n",
      "epoch 19605: train loss: 0.10381791700133541, test loss: 0.2598389345659263\n",
      "epoch 19606: train loss: 0.10381653650540117, test loss: 0.25984002841985393\n",
      "epoch 19607: train loss: 0.10381515610305164, test loss: 0.25984112230517814\n",
      "epoch 19608: train loss: 0.10381377579427527, test loss: 0.25984221622193443\n",
      "epoch 19609: train loss: 0.10381239557906045, test loss: 0.25984331017011614\n",
      "epoch 19610: train loss: 0.10381101545739561, test loss: 0.259844404149657\n",
      "epoch 19611: train loss: 0.10380963542926917, test loss: 0.25984549816063324\n",
      "epoch 19612: train loss: 0.1038082554946695, test loss: 0.25984659220297934\n",
      "epoch 19613: train loss: 0.10380687565358505, test loss: 0.25984768627672894\n",
      "epoch 19614: train loss: 0.10380549590600424, test loss: 0.2598487803818376\n",
      "epoch 19615: train loss: 0.10380411625191549, test loss: 0.25984987451833824\n",
      "epoch 19616: train loss: 0.10380273669130723, test loss: 0.25985096868618707\n",
      "epoch 19617: train loss: 0.10380135722416788, test loss: 0.259852062885417\n",
      "epoch 19618: train loss: 0.10379997785048585, test loss: 0.2598531571159648\n",
      "epoch 19619: train loss: 0.10379859857024959, test loss: 0.25985425137788354\n",
      "epoch 19620: train loss: 0.10379721938344755, test loss: 0.259855345671089\n",
      "epoch 19621: train loss: 0.10379584029006814, test loss: 0.2598564399956945\n",
      "epoch 19622: train loss: 0.10379446129009978, test loss: 0.25985753435159553\n",
      "epoch 19623: train loss: 0.10379308238353094, test loss: 0.2598586287388453\n",
      "epoch 19624: train loss: 0.10379170357035004, test loss: 0.25985972315736094\n",
      "epoch 19625: train loss: 0.10379032485054555, test loss: 0.2598608176072145\n",
      "epoch 19626: train loss: 0.1037889462241059, test loss: 0.2598619120883624\n",
      "epoch 19627: train loss: 0.10378756769101953, test loss: 0.2598630066007977\n",
      "epoch 19628: train loss: 0.10378618925127489, test loss: 0.25986410114451725\n",
      "epoch 19629: train loss: 0.10378481090486041, test loss: 0.2598651957195331\n",
      "epoch 19630: train loss: 0.10378343265176458, test loss: 0.2598662903258015\n",
      "epoch 19631: train loss: 0.10378205449197585, test loss: 0.2598673849633373\n",
      "epoch 19632: train loss: 0.10378067642548267, test loss: 0.25986847963213244\n",
      "epoch 19633: train loss: 0.10377929845227349, test loss: 0.2598695743322068\n",
      "epoch 19634: train loss: 0.10377792057233677, test loss: 0.2598706690635103\n",
      "epoch 19635: train loss: 0.103776542785661, test loss: 0.25987176382606064\n",
      "epoch 19636: train loss: 0.10377516509223461, test loss: 0.25987285861985043\n",
      "epoch 19637: train loss: 0.10377378749204608, test loss: 0.25987395344485487\n",
      "epoch 19638: train loss: 0.10377240998508387, test loss: 0.259875048301109\n",
      "epoch 19639: train loss: 0.10377103257133652, test loss: 0.2598761431885475\n",
      "epoch 19640: train loss: 0.10376965525079239, test loss: 0.2598772381072051\n",
      "epoch 19641: train loss: 0.10376827802344003, test loss: 0.25987833305705677\n",
      "epoch 19642: train loss: 0.10376690088926792, test loss: 0.2598794280380965\n",
      "epoch 19643: train loss: 0.10376552384826448, test loss: 0.25988052305035964\n",
      "epoch 19644: train loss: 0.10376414690041827, test loss: 0.25988161809375976\n",
      "epoch 19645: train loss: 0.10376277004571771, test loss: 0.2598827131683736\n",
      "epoch 19646: train loss: 0.10376139328415132, test loss: 0.2598838082741544\n",
      "epoch 19647: train loss: 0.10376001661570756, test loss: 0.2598849034110977\n",
      "epoch 19648: train loss: 0.10375864004037495, test loss: 0.2598859985791969\n",
      "epoch 19649: train loss: 0.10375726355814198, test loss: 0.25988709377844815\n",
      "epoch 19650: train loss: 0.10375588716899711, test loss: 0.25988818900880617\n",
      "epoch 19651: train loss: 0.10375451087292889, test loss: 0.259889284270345\n",
      "epoch 19652: train loss: 0.10375313466992576, test loss: 0.25989037956297956\n",
      "epoch 19653: train loss: 0.10375175855997626, test loss: 0.259891474886786\n",
      "epoch 19654: train loss: 0.10375038254306888, test loss: 0.25989257024167683\n",
      "epoch 19655: train loss: 0.10374900661919212, test loss: 0.2598936656276878\n",
      "epoch 19656: train loss: 0.10374763078833446, test loss: 0.2598947610447728\n",
      "epoch 19657: train loss: 0.10374625505048449, test loss: 0.2598958564930096\n",
      "epoch 19658: train loss: 0.10374487940563064, test loss: 0.25989695197226864\n",
      "epoch 19659: train loss: 0.10374350385376144, test loss: 0.2598980474826684\n",
      "epoch 19660: train loss: 0.10374212839486542, test loss: 0.25989914302412365\n",
      "epoch 19661: train loss: 0.10374075302893111, test loss: 0.25990023859666644\n",
      "epoch 19662: train loss: 0.10373937775594698, test loss: 0.25990133420031486\n",
      "epoch 19663: train loss: 0.1037380025759016, test loss: 0.2599024298349799\n",
      "epoch 19664: train loss: 0.10373662748878346, test loss: 0.25990352550067847\n",
      "epoch 19665: train loss: 0.10373525249458108, test loss: 0.25990462119744673\n",
      "epoch 19666: train loss: 0.10373387759328305, test loss: 0.2599057169252374\n",
      "epoch 19667: train loss: 0.1037325027848778, test loss: 0.25990681268408616\n",
      "epoch 19668: train loss: 0.10373112806935395, test loss: 0.25990790847396883\n",
      "epoch 19669: train loss: 0.10372975344669996, test loss: 0.2599090042948381\n",
      "epoch 19670: train loss: 0.1037283789169044, test loss: 0.25991010014675014\n",
      "epoch 19671: train loss: 0.10372700447995581, test loss: 0.2599111960296603\n",
      "epoch 19672: train loss: 0.10372563013584273, test loss: 0.25991229194358395\n",
      "epoch 19673: train loss: 0.10372425588455371, test loss: 0.25991338788847235\n",
      "epoch 19674: train loss: 0.10372288172607724, test loss: 0.2599144838643437\n",
      "epoch 19675: train loss: 0.10372150766040193, test loss: 0.2599155798712329\n",
      "epoch 19676: train loss: 0.10372013368751629, test loss: 0.25991667590911455\n",
      "epoch 19677: train loss: 0.10371875980740887, test loss: 0.2599177719779019\n",
      "epoch 19678: train loss: 0.10371738602006823, test loss: 0.25991886807771164\n",
      "epoch 19679: train loss: 0.10371601232548294, test loss: 0.25991996420845703\n",
      "epoch 19680: train loss: 0.1037146387236415, test loss: 0.259921060370114\n",
      "epoch 19681: train loss: 0.10371326521453252, test loss: 0.2599221565627592\n",
      "epoch 19682: train loss: 0.10371189179814456, test loss: 0.2599232527863657\n",
      "epoch 19683: train loss: 0.10371051847446616, test loss: 0.25992434904084755\n",
      "epoch 19684: train loss: 0.10370914524348585, test loss: 0.25992544532628065\n",
      "epoch 19685: train loss: 0.10370777210519228, test loss: 0.25992654164262047\n",
      "epoch 19686: train loss: 0.10370639905957392, test loss: 0.25992763798988255\n",
      "epoch 19687: train loss: 0.10370502610661941, test loss: 0.2599287343680392\n",
      "epoch 19688: train loss: 0.1037036532463173, test loss: 0.25992983077708753\n",
      "epoch 19689: train loss: 0.10370228047865616, test loss: 0.25993092721704164\n",
      "epoch 19690: train loss: 0.10370090780362456, test loss: 0.2599320236878773\n",
      "epoch 19691: train loss: 0.1036995352212111, test loss: 0.2599331201895891\n",
      "epoch 19692: train loss: 0.10369816273140434, test loss: 0.25993421672217104\n",
      "epoch 19693: train loss: 0.10369679033419285, test loss: 0.2599353132856621\n",
      "epoch 19694: train loss: 0.10369541802956522, test loss: 0.25993640987997046\n",
      "epoch 19695: train loss: 0.10369404581751006, test loss: 0.2599375065051369\n",
      "epoch 19696: train loss: 0.10369267369801595, test loss: 0.259938603161153\n",
      "epoch 19697: train loss: 0.10369130167107145, test loss: 0.2599396998480167\n",
      "epoch 19698: train loss: 0.10368992973666515, test loss: 0.25994079656567975\n",
      "epoch 19699: train loss: 0.10368855789478572, test loss: 0.2599418933142214\n",
      "epoch 19700: train loss: 0.10368718614542168, test loss: 0.25994299009355193\n",
      "epoch 19701: train loss: 0.10368581448856162, test loss: 0.2599440869037102\n",
      "epoch 19702: train loss: 0.10368444292419418, test loss: 0.2599451837446891\n",
      "epoch 19703: train loss: 0.10368307145230796, test loss: 0.2599462806164442\n",
      "epoch 19704: train loss: 0.10368170007289156, test loss: 0.25994737751900937\n",
      "epoch 19705: train loss: 0.10368032878593357, test loss: 0.25994847445234215\n",
      "epoch 19706: train loss: 0.10367895759142258, test loss: 0.25994957141647534\n",
      "epoch 19707: train loss: 0.10367758648934723, test loss: 0.2599506684114067\n",
      "epoch 19708: train loss: 0.10367621547969616, test loss: 0.25995176543708887\n",
      "epoch 19709: train loss: 0.10367484456245794, test loss: 0.25995286249351635\n",
      "epoch 19710: train loss: 0.10367347373762117, test loss: 0.25995395958072653\n",
      "epoch 19711: train loss: 0.10367210300517452, test loss: 0.2599550566986737\n",
      "epoch 19712: train loss: 0.10367073236510656, test loss: 0.25995615384735143\n",
      "epoch 19713: train loss: 0.10366936181740595, test loss: 0.25995725102679806\n",
      "epoch 19714: train loss: 0.10366799136206127, test loss: 0.2599583482369655\n",
      "epoch 19715: train loss: 0.10366662099906122, test loss: 0.25995944547785027\n",
      "epoch 19716: train loss: 0.10366525072839435, test loss: 0.2599605427494477\n",
      "epoch 19717: train loss: 0.10366388055004933, test loss: 0.2599616400517931\n",
      "epoch 19718: train loss: 0.10366251046401477, test loss: 0.2599627373847996\n",
      "epoch 19719: train loss: 0.10366114047027931, test loss: 0.25996383474854434\n",
      "epoch 19720: train loss: 0.1036597705688316, test loss: 0.2599649321429824\n",
      "epoch 19721: train loss: 0.1036584007596603, test loss: 0.25996602956806825\n",
      "epoch 19722: train loss: 0.103657031042754, test loss: 0.2599671270238767\n",
      "epoch 19723: train loss: 0.10365566141810134, test loss: 0.25996822451032264\n",
      "epoch 19724: train loss: 0.10365429188569103, test loss: 0.25996932202744316\n",
      "epoch 19725: train loss: 0.10365292244551164, test loss: 0.25997041957525124\n",
      "epoch 19726: train loss: 0.10365155309755185, test loss: 0.2599715171536839\n",
      "epoch 19727: train loss: 0.10365018384180033, test loss: 0.25997261476281563\n",
      "epoch 19728: train loss: 0.10364881467824567, test loss: 0.2599737124025404\n",
      "epoch 19729: train loss: 0.1036474456068766, test loss: 0.2599748100729144\n",
      "epoch 19730: train loss: 0.10364607662768172, test loss: 0.25997590777393287\n",
      "epoch 19731: train loss: 0.10364470774064972, test loss: 0.25997700550555186\n",
      "epoch 19732: train loss: 0.10364333894576923, test loss: 0.25997810326780507\n",
      "epoch 19733: train loss: 0.10364197024302894, test loss: 0.25997920106064787\n",
      "epoch 19734: train loss: 0.10364060163241753, test loss: 0.2599802988841161\n",
      "epoch 19735: train loss: 0.10363923311392362, test loss: 0.2599813967381652\n",
      "epoch 19736: train loss: 0.10363786468753589, test loss: 0.25998249462278766\n",
      "epoch 19737: train loss: 0.103636496353243, test loss: 0.25998359253802256\n",
      "epoch 19738: train loss: 0.10363512811103368, test loss: 0.2599846904838249\n",
      "epoch 19739: train loss: 0.10363375996089655, test loss: 0.25998578846024734\n",
      "epoch 19740: train loss: 0.10363239190282028, test loss: 0.25998688646718576\n",
      "epoch 19741: train loss: 0.10363102393679359, test loss: 0.2599879845046774\n",
      "epoch 19742: train loss: 0.10362965606280511, test loss: 0.2599890825727751\n",
      "epoch 19743: train loss: 0.10362828828084357, test loss: 0.25999018067137525\n",
      "epoch 19744: train loss: 0.10362692059089763, test loss: 0.2599912788005117\n",
      "epoch 19745: train loss: 0.10362555299295596, test loss: 0.25999237696018257\n",
      "epoch 19746: train loss: 0.1036241854870073, test loss: 0.25999347515038024\n",
      "epoch 19747: train loss: 0.10362281807304029, test loss: 0.259994573371144\n",
      "epoch 19748: train loss: 0.10362145075104362, test loss: 0.2599956716223842\n",
      "epoch 19749: train loss: 0.10362008352100605, test loss: 0.25999676990413784\n",
      "epoch 19750: train loss: 0.10361871638291617, test loss: 0.2599978682164025\n",
      "epoch 19751: train loss: 0.10361734933676278, test loss: 0.25999896655912935\n",
      "epoch 19752: train loss: 0.1036159823825345, test loss: 0.2600000649323984\n",
      "epoch 19753: train loss: 0.10361461552022008, test loss: 0.2600011633361005\n",
      "epoch 19754: train loss: 0.10361324874980823, test loss: 0.2600022617702929\n",
      "epoch 19755: train loss: 0.10361188207128763, test loss: 0.26000336023501347\n",
      "epoch 19756: train loss: 0.10361051548464698, test loss: 0.26000445873013334\n",
      "epoch 19757: train loss: 0.103609148989875, test loss: 0.2600055572557302\n",
      "epoch 19758: train loss: 0.10360778258696042, test loss: 0.26000665581175825\n",
      "epoch 19759: train loss: 0.10360641627589193, test loss: 0.2600077543982534\n",
      "epoch 19760: train loss: 0.10360505005665828, test loss: 0.2600088530151919\n",
      "epoch 19761: train loss: 0.10360368392924818, test loss: 0.2600099516625462\n",
      "epoch 19762: train loss: 0.10360231789365029, test loss: 0.2600110503403561\n",
      "epoch 19763: train loss: 0.10360095194985339, test loss: 0.26001214904853276\n",
      "epoch 19764: train loss: 0.1035995860978462, test loss: 0.26001324778719487\n",
      "epoch 19765: train loss: 0.10359822033761741, test loss: 0.2600143465562153\n",
      "epoch 19766: train loss: 0.10359685466915579, test loss: 0.26001544535562876\n",
      "epoch 19767: train loss: 0.10359548909245006, test loss: 0.26001654418547404\n",
      "epoch 19768: train loss: 0.10359412360748894, test loss: 0.260017643045664\n",
      "epoch 19769: train loss: 0.10359275821426117, test loss: 0.26001874193627456\n",
      "epoch 19770: train loss: 0.10359139291275549, test loss: 0.2600198408572205\n",
      "epoch 19771: train loss: 0.10359002770296061, test loss: 0.26002093980858026\n",
      "epoch 19772: train loss: 0.10358866258486531, test loss: 0.26002203879026636\n",
      "epoch 19773: train loss: 0.1035872975584583, test loss: 0.26002313780235486\n",
      "epoch 19774: train loss: 0.10358593262372835, test loss: 0.2600242368447393\n",
      "epoch 19775: train loss: 0.10358456778066419, test loss: 0.2600253359175186\n",
      "epoch 19776: train loss: 0.10358320302925456, test loss: 0.2600264350205655\n",
      "epoch 19777: train loss: 0.10358183836948824, test loss: 0.26002753415399676\n",
      "epoch 19778: train loss: 0.10358047380135396, test loss: 0.26002863331772724\n",
      "epoch 19779: train loss: 0.10357910932484046, test loss: 0.2600297325118131\n",
      "epoch 19780: train loss: 0.10357774493993652, test loss: 0.26003083173618996\n",
      "epoch 19781: train loss: 0.10357638064663088, test loss: 0.2600319309908925\n",
      "epoch 19782: train loss: 0.10357501644491234, test loss: 0.26003303027587565\n",
      "epoch 19783: train loss: 0.10357365233476959, test loss: 0.26003412959113475\n",
      "epoch 19784: train loss: 0.10357228831619146, test loss: 0.2600352289367084\n",
      "epoch 19785: train loss: 0.10357092438916668, test loss: 0.26003632831254836\n",
      "epoch 19786: train loss: 0.10356956055368402, test loss: 0.26003742771867094\n",
      "epoch 19787: train loss: 0.10356819680973224, test loss: 0.2600385271550539\n",
      "epoch 19788: train loss: 0.10356683315730017, test loss: 0.26003962662168906\n",
      "epoch 19789: train loss: 0.10356546959637652, test loss: 0.2600407261185943\n",
      "epoch 19790: train loss: 0.10356410612695008, test loss: 0.260041825645745\n",
      "epoch 19791: train loss: 0.10356274274900965, test loss: 0.2600429252031376\n",
      "epoch 19792: train loss: 0.10356137946254396, test loss: 0.2600440247907862\n",
      "epoch 19793: train loss: 0.10356001626754185, test loss: 0.2600451244086649\n",
      "epoch 19794: train loss: 0.10355865316399208, test loss: 0.26004622405673067\n",
      "epoch 19795: train loss: 0.10355729015188343, test loss: 0.2600473237350405\n",
      "epoch 19796: train loss: 0.1035559272312047, test loss: 0.2600484234435688\n",
      "epoch 19797: train loss: 0.10355456440194466, test loss: 0.2600495231822702\n",
      "epoch 19798: train loss: 0.10355320166409211, test loss: 0.26005062295120107\n",
      "epoch 19799: train loss: 0.10355183901763584, test loss: 0.260051722750317\n",
      "epoch 19800: train loss: 0.10355047646256466, test loss: 0.2600528225795923\n",
      "epoch 19801: train loss: 0.10354911399886733, test loss: 0.2600539224391061\n",
      "epoch 19802: train loss: 0.10354775162653272, test loss: 0.26005502232874955\n",
      "epoch 19803: train loss: 0.10354638934554955, test loss: 0.26005612224856\n",
      "epoch 19804: train loss: 0.10354502715590667, test loss: 0.2600572221985536\n",
      "epoch 19805: train loss: 0.10354366505759288, test loss: 0.26005832217868524\n",
      "epoch 19806: train loss: 0.10354230305059697, test loss: 0.26005942218897005\n",
      "epoch 19807: train loss: 0.10354094113490779, test loss: 0.26006052222936493\n",
      "epoch 19808: train loss: 0.1035395793105141, test loss: 0.2600616222999449\n",
      "epoch 19809: train loss: 0.10353821757740472, test loss: 0.26006272240062567\n",
      "epoch 19810: train loss: 0.10353685593556852, test loss: 0.26006382253146426\n",
      "epoch 19811: train loss: 0.10353549438499426, test loss: 0.2600649226924135\n",
      "epoch 19812: train loss: 0.10353413292567076, test loss: 0.2600660228834497\n",
      "epoch 19813: train loss: 0.10353277155758686, test loss: 0.26006712310456787\n",
      "epoch 19814: train loss: 0.10353141028073139, test loss: 0.2600682233558066\n",
      "epoch 19815: train loss: 0.10353004909509318, test loss: 0.26006932363716023\n",
      "epoch 19816: train loss: 0.103528688000661, test loss: 0.2600704239485815\n",
      "epoch 19817: train loss: 0.10352732699742376, test loss: 0.2600715242901118\n",
      "epoch 19818: train loss: 0.10352596608537024, test loss: 0.2600726246616596\n",
      "epoch 19819: train loss: 0.10352460526448928, test loss: 0.26007372506330656\n",
      "epoch 19820: train loss: 0.10352324453476971, test loss: 0.2600748254950047\n",
      "epoch 19821: train loss: 0.10352188389620036, test loss: 0.26007592595679424\n",
      "epoch 19822: train loss: 0.1035205233487701, test loss: 0.26007702644858405\n",
      "epoch 19823: train loss: 0.10351916289246776, test loss: 0.26007812697045596\n",
      "epoch 19824: train loss: 0.10351780252728217, test loss: 0.2600792275223219\n",
      "epoch 19825: train loss: 0.10351644225320218, test loss: 0.26008032810425924\n",
      "epoch 19826: train loss: 0.10351508207021663, test loss: 0.2600814287161813\n",
      "epoch 19827: train loss: 0.10351372197831438, test loss: 0.260082529358127\n",
      "epoch 19828: train loss: 0.10351236197748427, test loss: 0.260083630030092\n",
      "epoch 19829: train loss: 0.10351100206771516, test loss: 0.26008473073206917\n",
      "epoch 19830: train loss: 0.10350964224899592, test loss: 0.2600858314640573\n",
      "epoch 19831: train loss: 0.10350828252131537, test loss: 0.26008693222598883\n",
      "epoch 19832: train loss: 0.10350692288466239, test loss: 0.2600880330179624\n",
      "epoch 19833: train loss: 0.10350556333902583, test loss: 0.2600891338398919\n",
      "epoch 19834: train loss: 0.10350420388439456, test loss: 0.26009023469179526\n",
      "epoch 19835: train loss: 0.10350284452075748, test loss: 0.2600913355736447\n",
      "epoch 19836: train loss: 0.10350148524810336, test loss: 0.26009243648547853\n",
      "epoch 19837: train loss: 0.10350012606642116, test loss: 0.2600935374272722\n",
      "epoch 19838: train loss: 0.10349876697569968, test loss: 0.26009463839898006\n",
      "epoch 19839: train loss: 0.10349740797592787, test loss: 0.2600957394006604\n",
      "epoch 19840: train loss: 0.10349604906709456, test loss: 0.2600968404322666\n",
      "epoch 19841: train loss: 0.10349469024918861, test loss: 0.26009794149381693\n",
      "epoch 19842: train loss: 0.10349333152219893, test loss: 0.2600990425852639\n",
      "epoch 19843: train loss: 0.10349197288611438, test loss: 0.2601001437066654\n",
      "epoch 19844: train loss: 0.10349061434092383, test loss: 0.26010124485793734\n",
      "epoch 19845: train loss: 0.1034892558866162, test loss: 0.26010234603913385\n",
      "epoch 19846: train loss: 0.10348789752318034, test loss: 0.2601034472502113\n",
      "epoch 19847: train loss: 0.10348653925060518, test loss: 0.2601045484912087\n",
      "epoch 19848: train loss: 0.10348518106887955, test loss: 0.2601056497620766\n",
      "epoch 19849: train loss: 0.1034838229779924, test loss: 0.26010675106281445\n",
      "epoch 19850: train loss: 0.10348246497793259, test loss: 0.26010785239345857\n",
      "epoch 19851: train loss: 0.10348110706868903, test loss: 0.2601089537539205\n",
      "epoch 19852: train loss: 0.1034797492502506, test loss: 0.2601100551442791\n",
      "epoch 19853: train loss: 0.10347839152260623, test loss: 0.2601111565644905\n",
      "epoch 19854: train loss: 0.10347703388574477, test loss: 0.2601122580145507\n",
      "epoch 19855: train loss: 0.10347567633965517, test loss: 0.2601133594944534\n",
      "epoch 19856: train loss: 0.1034743188843263, test loss: 0.26011446100419616\n",
      "epoch 19857: train loss: 0.10347296151974712, test loss: 0.2601155625437343\n",
      "epoch 19858: train loss: 0.10347160424590647, test loss: 0.26011666411314394\n",
      "epoch 19859: train loss: 0.10347024706279334, test loss: 0.26011776571234096\n",
      "epoch 19860: train loss: 0.10346888997039656, test loss: 0.26011886734136164\n",
      "epoch 19861: train loss: 0.10346753296870512, test loss: 0.26011996900020057\n",
      "epoch 19862: train loss: 0.10346617605770786, test loss: 0.2601210706887931\n",
      "epoch 19863: train loss: 0.10346481923739377, test loss: 0.26012217240721813\n",
      "epoch 19864: train loss: 0.10346346250775174, test loss: 0.2601232741554096\n",
      "epoch 19865: train loss: 0.10346210586877069, test loss: 0.26012437593340315\n",
      "epoch 19866: train loss: 0.10346074932043953, test loss: 0.26012547774115513\n",
      "epoch 19867: train loss: 0.10345939286274723, test loss: 0.26012657957865937\n",
      "epoch 19868: train loss: 0.10345803649568266, test loss: 0.2601276814459357\n",
      "epoch 19869: train loss: 0.10345668021923479, test loss: 0.2601287833429778\n",
      "epoch 19870: train loss: 0.10345532403339255, test loss: 0.2601298852697406\n",
      "epoch 19871: train loss: 0.1034539679381449, test loss: 0.26013098722630473\n",
      "epoch 19872: train loss: 0.10345261193348071, test loss: 0.26013208921255926\n",
      "epoch 19873: train loss: 0.10345125601938897, test loss: 0.26013319122858536\n",
      "epoch 19874: train loss: 0.1034499001958586, test loss: 0.26013429327427506\n",
      "epoch 19875: train loss: 0.10344854446287854, test loss: 0.26013539534974944\n",
      "epoch 19876: train loss: 0.10344718882043775, test loss: 0.26013649745487955\n",
      "epoch 19877: train loss: 0.10344583326852516, test loss: 0.2601375995897437\n",
      "epoch 19878: train loss: 0.10344447780712972, test loss: 0.26013870175429615\n",
      "epoch 19879: train loss: 0.1034431224362404, test loss: 0.26013980394853364\n",
      "epoch 19880: train loss: 0.10344176715584613, test loss: 0.2601409061724963\n",
      "epoch 19881: train loss: 0.10344041196593587, test loss: 0.260142008426092\n",
      "epoch 19882: train loss: 0.10343905686649854, test loss: 0.2601431107094036\n",
      "epoch 19883: train loss: 0.1034377018575232, test loss: 0.2601442130223643\n",
      "epoch 19884: train loss: 0.10343634693899868, test loss: 0.2601453153649882\n",
      "epoch 19885: train loss: 0.10343499211091403, test loss: 0.26014641773727576\n",
      "epoch 19886: train loss: 0.1034336373732582, test loss: 0.26014752013919923\n",
      "epoch 19887: train loss: 0.10343228272602012, test loss: 0.26014862257077565\n",
      "epoch 19888: train loss: 0.1034309281691888, test loss: 0.26014972503200334\n",
      "epoch 19889: train loss: 0.10342957370275314, test loss: 0.2601508275228132\n",
      "epoch 19890: train loss: 0.10342821932670222, test loss: 0.26015193004328535\n",
      "epoch 19891: train loss: 0.10342686504102491, test loss: 0.2601530325933972\n",
      "epoch 19892: train loss: 0.10342551084571024, test loss: 0.26015413517310065\n",
      "epoch 19893: train loss: 0.10342415674074719, test loss: 0.2601552377824135\n",
      "epoch 19894: train loss: 0.10342280272612474, test loss: 0.2601563404213326\n",
      "epoch 19895: train loss: 0.10342144880183182, test loss: 0.26015744308981076\n",
      "epoch 19896: train loss: 0.10342009496785746, test loss: 0.26015854578793024\n",
      "epoch 19897: train loss: 0.10341874122419062, test loss: 0.2601596485156003\n",
      "epoch 19898: train loss: 0.10341738757082033, test loss: 0.26016075127286054\n",
      "epoch 19899: train loss: 0.10341603400773551, test loss: 0.2601618540596876\n",
      "epoch 19900: train loss: 0.1034146805349252, test loss: 0.26016295687605373\n",
      "epoch 19901: train loss: 0.1034133271523784, test loss: 0.26016405972202106\n",
      "epoch 19902: train loss: 0.10341197386008405, test loss: 0.2601651625975213\n",
      "epoch 19903: train loss: 0.10341062065803118, test loss: 0.2601662655025506\n",
      "epoch 19904: train loss: 0.10340926754620881, test loss: 0.26016736843714583\n",
      "epoch 19905: train loss: 0.10340791452460593, test loss: 0.2601684714012218\n",
      "epoch 19906: train loss: 0.10340656159321152, test loss: 0.26016957439490035\n",
      "epoch 19907: train loss: 0.10340520875201459, test loss: 0.26017067741805\n",
      "epoch 19908: train loss: 0.10340385600100416, test loss: 0.26017178047073075\n",
      "epoch 19909: train loss: 0.10340250334016922, test loss: 0.2601728835528976\n",
      "epoch 19910: train loss: 0.10340115076949878, test loss: 0.2601739866646083\n",
      "epoch 19911: train loss: 0.10339979828898187, test loss: 0.26017508980577647\n",
      "epoch 19912: train loss: 0.1033984458986075, test loss: 0.2601761929764607\n",
      "epoch 19913: train loss: 0.10339709359836466, test loss: 0.2601772961766152\n",
      "epoch 19914: train loss: 0.1033957413882424, test loss: 0.26017839940623616\n",
      "epoch 19915: train loss: 0.10339438926822973, test loss: 0.26017950266532064\n",
      "epoch 19916: train loss: 0.10339303723831567, test loss: 0.2601806059539273\n",
      "epoch 19917: train loss: 0.10339168529848923, test loss: 0.2601817092719477\n",
      "epoch 19918: train loss: 0.10339033344873944, test loss: 0.26018281261941906\n",
      "epoch 19919: train loss: 0.10338898168905533, test loss: 0.2601839159963375\n",
      "epoch 19920: train loss: 0.10338763001942597, test loss: 0.26018501940274447\n",
      "epoch 19921: train loss: 0.10338627843984032, test loss: 0.2601861228385067\n",
      "epoch 19922: train loss: 0.10338492695028745, test loss: 0.26018722630376895\n",
      "epoch 19923: train loss: 0.10338357555075639, test loss: 0.2601883297984213\n",
      "epoch 19924: train loss: 0.10338222424123618, test loss: 0.26018943332246297\n",
      "epoch 19925: train loss: 0.10338087302171584, test loss: 0.2601905368759718\n",
      "epoch 19926: train loss: 0.10337952189218443, test loss: 0.2601916404588597\n",
      "epoch 19927: train loss: 0.10337817085263101, test loss: 0.26019274407114545\n",
      "epoch 19928: train loss: 0.10337681990304459, test loss: 0.2601938477128255\n",
      "epoch 19929: train loss: 0.10337546904341424, test loss: 0.2601949513838736\n",
      "epoch 19930: train loss: 0.103374118273729, test loss: 0.26019605508431015\n",
      "epoch 19931: train loss: 0.1033727675939779, test loss: 0.2601971588141284\n",
      "epoch 19932: train loss: 0.10337141700414999, test loss: 0.2601982625733039\n",
      "epoch 19933: train loss: 0.10337006650423439, test loss: 0.26019936636185464\n",
      "epoch 19934: train loss: 0.10336871609422009, test loss: 0.26020047017973624\n",
      "epoch 19935: train loss: 0.10336736577409616, test loss: 0.2602015740269852\n",
      "epoch 19936: train loss: 0.10336601554385166, test loss: 0.26020267790359997\n",
      "epoch 19937: train loss: 0.10336466540347569, test loss: 0.26020378180949033\n",
      "epoch 19938: train loss: 0.10336331535295726, test loss: 0.2602048857447801\n",
      "epoch 19939: train loss: 0.10336196539228545, test loss: 0.2602059897093397\n",
      "epoch 19940: train loss: 0.10336061552144934, test loss: 0.2602070937032493\n",
      "epoch 19941: train loss: 0.103359265740438, test loss: 0.2602081977264637\n",
      "epoch 19942: train loss: 0.10335791604924051, test loss: 0.26020930177895757\n",
      "epoch 19943: train loss: 0.10335656644784588, test loss: 0.2602104058607695\n",
      "epoch 19944: train loss: 0.10335521693624328, test loss: 0.26021150997191705\n",
      "epoch 19945: train loss: 0.10335386751442172, test loss: 0.26021261411231217\n",
      "epoch 19946: train loss: 0.10335251818237029, test loss: 0.2602137182819738\n",
      "epoch 19947: train loss: 0.1033511689400781, test loss: 0.2602148224809386\n",
      "epoch 19948: train loss: 0.10334981978753419, test loss: 0.26021592670911964\n",
      "epoch 19949: train loss: 0.1033484707247277, test loss: 0.26021703096663984\n",
      "epoch 19950: train loss: 0.10334712175164767, test loss: 0.2602181352533693\n",
      "epoch 19951: train loss: 0.10334577286828318, test loss: 0.2602192395693884\n",
      "epoch 19952: train loss: 0.10334442407462337, test loss: 0.2602203439146089\n",
      "epoch 19953: train loss: 0.10334307537065729, test loss: 0.2602214482891128\n",
      "epoch 19954: train loss: 0.10334172675637404, test loss: 0.2602225526928111\n",
      "epoch 19955: train loss: 0.10334037823176274, test loss: 0.2602236571257426\n",
      "epoch 19956: train loss: 0.10333902979681245, test loss: 0.2602247615879046\n",
      "epoch 19957: train loss: 0.10333768145151233, test loss: 0.26022586607929205\n",
      "epoch 19958: train loss: 0.10333633319585141, test loss: 0.2602269705998614\n",
      "epoch 19959: train loss: 0.10333498502981882, test loss: 0.26022807514962787\n",
      "epoch 19960: train loss: 0.10333363695340371, test loss: 0.26022917972863213\n",
      "epoch 19961: train loss: 0.10333228896659512, test loss: 0.26023028433678647\n",
      "epoch 19962: train loss: 0.1033309410693822, test loss: 0.260231388974127\n",
      "epoch 19963: train loss: 0.10332959326175403, test loss: 0.2602324936406743\n",
      "epoch 19964: train loss: 0.10332824554369978, test loss: 0.26023359833638027\n",
      "epoch 19965: train loss: 0.10332689791520851, test loss: 0.26023470306126417\n",
      "epoch 19966: train loss: 0.10332555037626935, test loss: 0.26023580781528094\n",
      "epoch 19967: train loss: 0.10332420292687143, test loss: 0.26023691259846643\n",
      "epoch 19968: train loss: 0.10332285556700388, test loss: 0.2602380174108203\n",
      "epoch 19969: train loss: 0.10332150829665579, test loss: 0.26023912225229445\n",
      "epoch 19970: train loss: 0.10332016111581631, test loss: 0.26024022712286565\n",
      "epoch 19971: train loss: 0.10331881402447454, test loss: 0.26024133202261535\n",
      "epoch 19972: train loss: 0.10331746702261967, test loss: 0.2602424369514765\n",
      "epoch 19973: train loss: 0.10331612011024076, test loss: 0.26024354190948656\n",
      "epoch 19974: train loss: 0.10331477328732695, test loss: 0.2602446468965377\n",
      "epoch 19975: train loss: 0.10331342655386741, test loss: 0.2602457519127529\n",
      "epoch 19976: train loss: 0.10331207990985128, test loss: 0.26024685695806515\n",
      "epoch 19977: train loss: 0.10331073335526765, test loss: 0.26024796203243006\n",
      "epoch 19978: train loss: 0.10330938689010569, test loss: 0.26024906713590656\n",
      "epoch 19979: train loss: 0.10330804051435453, test loss: 0.26025017226844793\n",
      "epoch 19980: train loss: 0.10330669422800337, test loss: 0.26025127743009563\n",
      "epoch 19981: train loss: 0.10330534803104126, test loss: 0.26025238262080214\n",
      "epoch 19982: train loss: 0.10330400192345741, test loss: 0.26025348784056423\n",
      "epoch 19983: train loss: 0.10330265590524096, test loss: 0.26025459308937854\n",
      "epoch 19984: train loss: 0.10330130997638105, test loss: 0.2602556983672423\n",
      "epoch 19985: train loss: 0.10329996413686685, test loss: 0.26025680367413184\n",
      "epoch 19986: train loss: 0.10329861838668748, test loss: 0.2602579090100847\n",
      "epoch 19987: train loss: 0.10329727272583213, test loss: 0.2602590143750757\n",
      "epoch 19988: train loss: 0.10329592715428994, test loss: 0.2602601197690604\n",
      "epoch 19989: train loss: 0.10329458167205009, test loss: 0.26026122519209915\n",
      "epoch 19990: train loss: 0.10329323627910174, test loss: 0.26026233064414406\n",
      "epoch 19991: train loss: 0.10329189097543404, test loss: 0.2602634361251961\n",
      "epoch 19992: train loss: 0.10329054576103616, test loss: 0.2602645416352476\n",
      "epoch 19993: train loss: 0.10328920063589725, test loss: 0.26026564717429823\n",
      "epoch 19994: train loss: 0.10328785560000654, test loss: 0.2602667527423431\n",
      "epoch 19995: train loss: 0.10328651065335313, test loss: 0.2602678583393351\n",
      "epoch 19996: train loss: 0.10328516579592625, test loss: 0.26026896396536026\n",
      "epoch 19997: train loss: 0.10328382102771502, test loss: 0.2602700696203265\n",
      "epoch 19998: train loss: 0.10328247634870866, test loss: 0.2602711753042755\n",
      "epoch 19999: train loss: 0.10328113175889636, test loss: 0.26027228101715905\n",
      "epoch 20000: train loss: 0.10327978725826727, test loss: 0.260273386758996\n",
      "Confusion Matrix:\n",
      "[[89  5]\n",
      " [10 96]]\n",
      "Accuracy: 0.925\n",
      "Classification Report (scikit-learn):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.95      0.92        94\n",
      "         1.0       0.95      0.91      0.93       106\n",
      "\n",
      "    accuracy                           0.93       200\n",
      "   macro avg       0.92      0.93      0.92       200\n",
      "weighted avg       0.93      0.93      0.93       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha = 0.0005 # 学习率，试着自己调整一下\n",
    "n_steps = 20000 # epoch迭代次数，试着自己调整一下\n",
    "\n",
    "w, ll_train, ll_test = fit_w(X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha)\n",
    "\n",
    "X_tilde_test_rbf = get_x_tilde(RBF(l, X_test, X_train))\n",
    "y_pred_prob = predict(X_tilde_test_rbf, w)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy:\", np.mean(y_test == y_pred))\n",
    "print(\"Classification Report (scikit-learn):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "plot_loss(-ll_train)\n",
    "plot_loss(-ll_test)\n",
    "plot_predictive_distribution(X, y, w, lambda x : RBF(l, x, X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (scikit-learn):\n",
      "[[89  5]\n",
      " [15 91]]\n",
      "Accuracy (scikit-learn): 0.9\n",
      "Classification Report (scikit-learn):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.95      0.90        94\n",
      "         1.0       0.95      0.86      0.90       106\n",
      "\n",
      "    accuracy                           0.90       200\n",
      "   macro avg       0.90      0.90      0.90       200\n",
      "weighted avg       0.90      0.90      0.90       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 调用逻辑斯谛回归的轮子，看看效果如何\n",
    "model = LogisticRegression() \n",
    "model.fit(X_tilde_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred_sklearn = model.predict(X_tilde_test)\n",
    "\n",
    "# 输出评价指标\n",
    "print(\"Confusion Matrix (scikit-learn):\")\n",
    "print(confusion_matrix(y_test, y_pred_sklearn))\n",
    "print(\"Accuracy (scikit-learn):\", accuracy_score(y_test, y_pred_sklearn))\n",
    "print(\"Classification Report (scikit-learn):\")\n",
    "print(classification_report(y_test, y_pred_sklearn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (scikit-learn):\n",
      "[[83 11]\n",
      " [15 91]]\n",
      "Accuracy (scikit-learn): 0.87\n",
      "Classification Report (scikit-learn):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.88      0.86        94\n",
      "         1.0       0.89      0.86      0.88       106\n",
      "\n",
      "    accuracy                           0.87       200\n",
      "   macro avg       0.87      0.87      0.87       200\n",
      "weighted avg       0.87      0.87      0.87       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 调用强大的分类器SVM试试\n",
    "model = SVC(kernel='rbf', C=50, gamma='scale') # TODO: 研究这里参数的含义，试着调整调整参数\n",
    "model.fit(X_tilde_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred_sklearn = model.predict(X_tilde_test)\n",
    "\n",
    "# 输出评价指标\n",
    "print(\"Confusion Matrix (scikit-learn):\")\n",
    "print(confusion_matrix(y_test, y_pred_sklearn))\n",
    "print(\"Accuracy (scikit-learn):\", accuracy_score(y_test, y_pred_sklearn))\n",
    "print(\"Classification Report (scikit-learn):\")\n",
    "print(classification_report(y_test, y_pred_sklearn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
